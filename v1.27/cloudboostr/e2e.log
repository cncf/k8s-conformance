  I0110 15:26:39.387173      23 e2e.go:117] Starting e2e run "59d9f44f-6074-423b-8ef4-6590885bb6d0" on Ginkgo node 1
  Jan 10 15:26:39.451: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1704900399 - will randomize all specs

Will run 378 of 7207 specs
------------------------------
[ReportBeforeSuite] 
test/e2e/e2e_test.go:148
[ReportBeforeSuite] PASSED [0.000 seconds]
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
  Jan 10 15:26:39.843: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  Jan 10 15:26:39.846: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
  Jan 10 15:26:40.002: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
  Jan 10 15:26:40.055: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-flannel' (0 seconds elapsed)
  Jan 10 15:26:40.055: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-arm' (0 seconds elapsed)
  Jan 10 15:26:40.055: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-arm64' (0 seconds elapsed)
  Jan 10 15:26:40.055: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-ppc64le' (0 seconds elapsed)
  Jan 10 15:26:40.055: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-s390x' (0 seconds elapsed)
  Jan 10 15:26:40.055: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
  Jan 10 15:26:40.055: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'nodelocaldns' (0 seconds elapsed)
  Jan 10 15:26:40.055: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'vsphere-cloud-controller-manager' (0 seconds elapsed)
  Jan 10 15:26:40.055: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'vsphere-csi-node' (0 seconds elapsed)
  Jan 10 15:26:40.055: INFO: e2e test version: v1.27.1
  Jan 10 15:26:40.058: INFO: kube-apiserver version: v1.27.5
  Jan 10 15:26:40.058: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  Jan 10 15:26:40.068: INFO: Cluster IP family: ipv4
[SynchronizedBeforeSuite] PASSED [0.225 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:250
  STEP: Creating a kubernetes client @ 01/10/24 15:26:40.573
  Jan 10 15:26:40.573: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename downward-api @ 01/10/24 15:26:40.575
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:26:40.612
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:26:40.618
  STEP: Creating a pod to test downward API volume plugin @ 01/10/24 15:26:40.623
  STEP: Saw pod success @ 01/10/24 15:26:44.689
  Jan 10 15:26:44.700: INFO: Trying to get logs from node env1-test-worker-1 pod downwardapi-volume-27348f48-604f-4ac1-95f2-ed005aac2323 container client-container: <nil>
  STEP: delete the pod @ 01/10/24 15:26:44.744
  Jan 10 15:26:44.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8560" for this suite. @ 01/10/24 15:26:44.808
• [4.257 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]
test/e2e/apps/deployment.go:185
  STEP: Creating a kubernetes client @ 01/10/24 15:26:44.834
  Jan 10 15:26:44.834: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename deployment @ 01/10/24 15:26:44.836
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:26:44.889
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:26:44.898
  STEP: creating a Deployment @ 01/10/24 15:26:44.93
  STEP: waiting for Deployment to be created @ 01/10/24 15:26:44.946
  STEP: waiting for all Replicas to be Ready @ 01/10/24 15:26:44.95
  Jan 10 15:26:44.953: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jan 10 15:26:44.953: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jan 10 15:26:45.009: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jan 10 15:26:45.009: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jan 10 15:26:45.054: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jan 10 15:26:45.054: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jan 10 15:26:45.176: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jan 10 15:26:45.176: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jan 10 15:26:46.296: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Jan 10 15:26:46.297: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Jan 10 15:26:46.433: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 2 and labels map[test-deployment-static:true]
  STEP: patching the Deployment @ 01/10/24 15:26:46.433
  W0110 15:26:46.450539      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Jan 10 15:26:46.455: INFO: observed event type ADDED
  STEP: waiting for Replicas to scale @ 01/10/24 15:26:46.455
  Jan 10 15:26:46.460: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 0
  Jan 10 15:26:46.460: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 0
  Jan 10 15:26:46.460: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 0
  Jan 10 15:26:46.460: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 0
  Jan 10 15:26:46.461: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 0
  Jan 10 15:26:46.462: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 0
  Jan 10 15:26:46.462: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 0
  Jan 10 15:26:46.462: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 0
  Jan 10 15:26:46.463: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 1
  Jan 10 15:26:46.463: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 1
  Jan 10 15:26:46.463: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 2
  Jan 10 15:26:46.463: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 2
  Jan 10 15:26:46.464: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 2
  Jan 10 15:26:46.464: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 2
  Jan 10 15:26:46.498: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 2
  Jan 10 15:26:46.498: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 2
  Jan 10 15:26:46.549: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 2
  Jan 10 15:26:46.549: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 2
  Jan 10 15:26:46.613: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 1
  Jan 10 15:26:46.613: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 1
  Jan 10 15:26:48.345: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 2
  Jan 10 15:26:48.345: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 2
  Jan 10 15:26:48.405: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 1
  STEP: listing Deployments @ 01/10/24 15:26:48.405
  Jan 10 15:26:48.419: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
  STEP: updating the Deployment @ 01/10/24 15:26:48.421
  Jan 10 15:26:48.447: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 1
  STEP: fetching the DeploymentStatus @ 01/10/24 15:26:48.447
  Jan 10 15:26:48.467: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jan 10 15:26:48.485: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jan 10 15:26:48.548: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jan 10 15:26:48.637: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jan 10 15:26:48.675: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jan 10 15:26:49.556: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Jan 10 15:26:50.394: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  Jan 10 15:26:50.534: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Jan 10 15:26:50.598: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Jan 10 15:26:51.527: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  STEP: patching the DeploymentStatus @ 01/10/24 15:26:51.61
  STEP: fetching the DeploymentStatus @ 01/10/24 15:26:51.629
  Jan 10 15:26:51.639: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 1
  Jan 10 15:26:51.640: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 1
  Jan 10 15:26:51.641: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 1
  Jan 10 15:26:51.641: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 1
  Jan 10 15:26:51.641: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 1
  Jan 10 15:26:51.641: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 2
  Jan 10 15:26:51.642: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 3
  Jan 10 15:26:51.642: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 2
  Jan 10 15:26:51.642: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 2
  Jan 10 15:26:51.642: INFO: observed Deployment test-deployment in namespace deployment-2407 with ReadyReplicas 3
  STEP: deleting the Deployment @ 01/10/24 15:26:51.643
  Jan 10 15:26:51.701: INFO: observed event type MODIFIED
  Jan 10 15:26:51.703: INFO: observed event type MODIFIED
  Jan 10 15:26:51.703: INFO: observed event type MODIFIED
  Jan 10 15:26:51.707: INFO: observed event type MODIFIED
  Jan 10 15:26:51.707: INFO: observed event type MODIFIED
  Jan 10 15:26:51.708: INFO: observed event type MODIFIED
  Jan 10 15:26:51.708: INFO: observed event type MODIFIED
  Jan 10 15:26:51.710: INFO: observed event type MODIFIED
  Jan 10 15:26:51.711: INFO: observed event type MODIFIED
  Jan 10 15:26:51.711: INFO: observed event type MODIFIED
  Jan 10 15:26:51.711: INFO: observed event type MODIFIED
  Jan 10 15:26:51.725: INFO: Log out all the ReplicaSets if there is no deployment created
  Jan 10 15:26:51.734: INFO: ReplicaSet "test-deployment-5b5dcbcd95":
  &ReplicaSet{ObjectMeta:{test-deployment-5b5dcbcd95  deployment-2407  a7c04b16-ccef-465e-8a5e-c0f8834126b7 186760628 4 2024-01-10 15:26:46 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 1af01fb9-11fc-4e96-9e59-1a797fbf06f4 0xc001bd4ae7 0xc001bd4ae8}] [] [{kube-controller-manager Update apps/v1 2024-01-10 15:26:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1af01fb9-11fc-4e96-9e59-1a797fbf06f4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-10 15:26:51 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 5b5dcbcd95,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001bd4b70 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  Jan 10 15:26:51.746: INFO: pod: "test-deployment-5b5dcbcd95-2hst8":
  &Pod{ObjectMeta:{test-deployment-5b5dcbcd95-2hst8 test-deployment-5b5dcbcd95- deployment-2407  de50b11b-b878-46b4-974b-0ab6ea4c6bfb 186760624 0 2024-01-10 15:26:48 +0000 UTC 2024-01-10 15:26:52 +0000 UTC 0xc00199ec30 map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-5b5dcbcd95 a7c04b16-ccef-465e-8a5e-c0f8834126b7 0xc00199ec67 0xc00199ec68}] [] [{kube-controller-manager Update v1 2024-01-10 15:26:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a7c04b16-ccef-465e-8a5e-c0f8834126b7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-10 15:26:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.67.207\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6jvqw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6jvqw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 15:26:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 15:26:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 15:26:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 15:26:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.200,PodIP:10.233.67.207,StartTime:2024-01-10 15:26:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-10 15:26:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:sha256:e6f1816883972d4be47bd48879a08919b96afcd344132622e4d444987919323c,ContainerID:containerd://805845cb9e74599112df862c1eb6c717cf50d7977425e43bb187cdd23def6e69,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.67.207,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Jan 10 15:26:51.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-2407" for this suite. @ 01/10/24 15:26:51.76
• [6.954 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:46
  STEP: Creating a kubernetes client @ 01/10/24 15:26:51.788
  Jan 10 15:26:51.788: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename projected @ 01/10/24 15:26:51.79
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:26:51.828
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:26:51.834
  STEP: Creating projection with secret that has name projected-secret-test-d53bc93e-4326-41e7-abb8-dedf79bc5488 @ 01/10/24 15:26:51.841
  STEP: Creating a pod to test consume secrets @ 01/10/24 15:26:51.854
  STEP: Saw pod success @ 01/10/24 15:26:55.905
  Jan 10 15:26:55.913: INFO: Trying to get logs from node env1-test-worker-1 pod pod-projected-secrets-298afe98-4d26-40ba-9e59-f614853d8832 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 01/10/24 15:26:55.936
  Jan 10 15:26:55.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4216" for this suite. @ 01/10/24 15:26:55.997
• [4.233 seconds]
------------------------------
[sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]
test/e2e/network/endpointslice.go:355
  STEP: Creating a kubernetes client @ 01/10/24 15:26:56.023
  Jan 10 15:26:56.023: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename endpointslice @ 01/10/24 15:26:56.025
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:26:56.083
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:26:56.089
  STEP: getting /apis @ 01/10/24 15:26:56.097
  STEP: getting /apis/discovery.k8s.io @ 01/10/24 15:26:56.111
  STEP: getting /apis/discovery.k8s.iov1 @ 01/10/24 15:26:56.114
  STEP: creating @ 01/10/24 15:26:56.118
  STEP: getting @ 01/10/24 15:26:56.162
  STEP: listing @ 01/10/24 15:26:56.171
  STEP: watching @ 01/10/24 15:26:56.181
  Jan 10 15:26:56.181: INFO: starting watch
  STEP: cluster-wide listing @ 01/10/24 15:26:56.184
  STEP: cluster-wide watching @ 01/10/24 15:26:56.216
  Jan 10 15:26:56.216: INFO: starting watch
  STEP: patching @ 01/10/24 15:26:56.219
  STEP: updating @ 01/10/24 15:26:56.233
  Jan 10 15:26:56.260: INFO: waiting for watch events with expected annotations
  Jan 10 15:26:56.260: INFO: saw patched and updated annotations
  STEP: deleting @ 01/10/24 15:26:56.26
  STEP: deleting a collection @ 01/10/24 15:26:56.301
  Jan 10 15:26:56.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-8094" for this suite. @ 01/10/24 15:26:56.385
• [0.381 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:214
  STEP: Creating a kubernetes client @ 01/10/24 15:26:56.407
  Jan 10 15:26:56.407: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename container-probe @ 01/10/24 15:26:56.41
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:26:56.447
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:26:56.456
  STEP: Creating pod test-webserver-702eae50-d048-4a7c-9118-21ce416c064d in namespace container-probe-9994 @ 01/10/24 15:26:56.463
  Jan 10 15:26:58.535: INFO: Started pod test-webserver-702eae50-d048-4a7c-9118-21ce416c064d in namespace container-probe-9994
  STEP: checking the pod's current state and verifying that restartCount is present @ 01/10/24 15:26:58.535
  Jan 10 15:26:58.549: INFO: Initial restart count of pod test-webserver-702eae50-d048-4a7c-9118-21ce416c064d is 0
  Jan 10 15:30:59.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 01/10/24 15:30:59.942
  STEP: Destroying namespace "container-probe-9994" for this suite. @ 01/10/24 15:30:59.985
• [243.600 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:168
  STEP: Creating a kubernetes client @ 01/10/24 15:31:00.009
  Jan 10 15:31:00.009: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename container-probe @ 01/10/24 15:31:00.01
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:31:00.047
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:31:00.056
  STEP: Creating pod liveness-e0ce3c76-1a22-4373-945e-2963ffc94d0a in namespace container-probe-7859 @ 01/10/24 15:31:00.065
  Jan 10 15:31:02.121: INFO: Started pod liveness-e0ce3c76-1a22-4373-945e-2963ffc94d0a in namespace container-probe-7859
  STEP: checking the pod's current state and verifying that restartCount is present @ 01/10/24 15:31:02.121
  Jan 10 15:31:02.130: INFO: Initial restart count of pod liveness-e0ce3c76-1a22-4373-945e-2963ffc94d0a is 0
  Jan 10 15:31:22.242: INFO: Restart count of pod container-probe-7859/liveness-e0ce3c76-1a22-4373-945e-2963ffc94d0a is now 1 (20.112688881s elapsed)
  Jan 10 15:31:22.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 01/10/24 15:31:22.256
  STEP: Destroying namespace "container-probe-7859" for this suite. @ 01/10/24 15:31:22.289
• [22.300 seconds]
------------------------------
SS
------------------------------
[sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:175
  STEP: Creating a kubernetes client @ 01/10/24 15:31:22.31
  Jan 10 15:31:22.310: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename configmap @ 01/10/24 15:31:22.312
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:31:22.353
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:31:22.36
  STEP: Creating configMap with name configmap-test-upd-96e8027a-3d18-447c-9187-c2239efd9ba6 @ 01/10/24 15:31:22.38
  STEP: Creating the pod @ 01/10/24 15:31:22.392
  STEP: Waiting for pod with text data @ 01/10/24 15:31:26.47
  STEP: Waiting for pod with binary data @ 01/10/24 15:31:26.545
  Jan 10 15:31:26.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-4072" for this suite. @ 01/10/24 15:31:26.579
• [4.295 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:216
  STEP: Creating a kubernetes client @ 01/10/24 15:31:26.609
  Jan 10 15:31:26.609: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename container-runtime @ 01/10/24 15:31:26.612
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:31:26.658
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:31:26.668
  STEP: create the container @ 01/10/24 15:31:26.676
  W0110 15:31:26.705029      23 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Failed @ 01/10/24 15:31:26.705
  STEP: get the container status @ 01/10/24 15:31:30.766
  STEP: the container should be terminated @ 01/10/24 15:31:30.776
  STEP: the termination message should be set @ 01/10/24 15:31:30.776
  Jan 10 15:31:30.776: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 01/10/24 15:31:30.776
  Jan 10 15:31:30.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-2083" for this suite. @ 01/10/24 15:31:30.837
• [4.254 seconds]
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]
test/e2e/scheduling/predicates.go:444
  STEP: Creating a kubernetes client @ 01/10/24 15:31:30.863
  Jan 10 15:31:30.864: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename sched-pred @ 01/10/24 15:31:30.865
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:31:30.908
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:31:30.918
  Jan 10 15:31:30.930: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jan 10 15:31:30.966: INFO: Waiting for terminating namespaces to be deleted...
  Jan 10 15:31:30.975: INFO: 
  Logging pods the apiserver thinks is on node env1-test-worker-0 before test
  Jan 10 15:31:31.009: INFO: filebeat-filebeat-q5bzw from filebeat started at 2024-01-10 01:14:23 +0000 UTC (1 container statuses recorded)
  Jan 10 15:31:31.009: INFO: 	Container filebeat ready: false, restart count 0
  Jan 10 15:31:31.009: INFO: kube-flannel-r8g5h from kube-system started at 2024-01-09 16:24:44 +0000 UTC (1 container statuses recorded)
  Jan 10 15:31:31.009: INFO: 	Container kube-flannel ready: true, restart count 0
  Jan 10 15:31:31.009: INFO: kube-proxy-445hs from kube-system started at 2024-01-10 09:13:13 +0000 UTC (1 container statuses recorded)
  Jan 10 15:31:31.009: INFO: 	Container kube-proxy ready: true, restart count 0
  Jan 10 15:31:31.009: INFO: metrics-server-6b7574f5b-jmbtm from kube-system started at 2024-01-09 16:32:13 +0000 UTC (1 container statuses recorded)
  Jan 10 15:31:31.009: INFO: 	Container metrics-server ready: true, restart count 0
  Jan 10 15:31:31.009: INFO: nginx-proxy-env1-test-worker-0 from kube-system started at 2024-01-09 16:31:48 +0000 UTC (1 container statuses recorded)
  Jan 10 15:31:31.009: INFO: 	Container nginx-proxy ready: true, restart count 0
  Jan 10 15:31:31.009: INFO: nodelocaldns-vkvkp from kube-system started at 2024-01-09 15:52:20 +0000 UTC (1 container statuses recorded)
  Jan 10 15:31:31.009: INFO: 	Container node-cache ready: true, restart count 0
  Jan 10 15:31:31.009: INFO: vsphere-csi-node-qkfth from kube-system started at 2024-01-10 09:21:38 +0000 UTC (3 container statuses recorded)
  Jan 10 15:31:31.009: INFO: 	Container liveness-probe ready: true, restart count 0
  Jan 10 15:31:31.009: INFO: 	Container node-driver-registrar ready: true, restart count 0
  Jan 10 15:31:31.009: INFO: 	Container vsphere-csi-node ready: true, restart count 0
  Jan 10 15:31:31.009: INFO: prometheus-kube-prometheus-operator-5f847644d6-gkll8 from prometheus started at 2024-01-10 03:09:01 +0000 UTC (1 container statuses recorded)
  Jan 10 15:31:31.009: INFO: 	Container kube-prometheus-stack ready: true, restart count 0
  Jan 10 15:31:31.009: INFO: prometheus-kube-state-metrics-f8b6b59f-g2nrl from prometheus started at 2024-01-10 03:09:01 +0000 UTC (1 container statuses recorded)
  Jan 10 15:31:31.009: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Jan 10 15:31:31.009: INFO: prometheus-prometheus-kube-prometheus-prometheus-0 from prometheus started at 2024-01-10 03:09:07 +0000 UTC (3 container statuses recorded)
  Jan 10 15:31:31.010: INFO: 	Container config-reloader ready: true, restart count 0
  Jan 10 15:31:31.010: INFO: 	Container prometheus ready: false, restart count 149
  Jan 10 15:31:31.010: INFO: 	Container thanos-sidecar ready: true, restart count 0
  Jan 10 15:31:31.010: INFO: prometheus-prometheus-node-exporter-4nknm from prometheus started at 2024-01-10 03:09:01 +0000 UTC (1 container statuses recorded)
  Jan 10 15:31:31.010: INFO: 	Container node-exporter ready: true, restart count 0
  Jan 10 15:31:31.010: INFO: thanos-query-6f697d54b8-sp4fg from prometheus started at 2024-01-10 13:06:38 +0000 UTC (1 container statuses recorded)
  Jan 10 15:31:31.010: INFO: 	Container query ready: true, restart count 0
  Jan 10 15:31:31.010: INFO: sonobuoy-systemd-logs-daemon-set-187fce9e0ddc499b-9m7s6 from sonobuoy started at 2024-01-10 15:26:38 +0000 UTC (2 container statuses recorded)
  Jan 10 15:31:31.010: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jan 10 15:31:31.010: INFO: 	Container systemd-logs ready: true, restart count 0
  Jan 10 15:31:31.010: INFO: traefik-ingress-g4tjs from traefik-ingress started at 2024-01-10 14:39:42 +0000 UTC (1 container statuses recorded)
  Jan 10 15:31:31.010: INFO: 	Container traefik-ingress ready: true, restart count 0
  Jan 10 15:31:31.010: INFO: velero-794b84894f-nwdwf from velero started at 2024-01-10 13:26:26 +0000 UTC (1 container statuses recorded)
  Jan 10 15:31:31.010: INFO: 	Container velero ready: true, restart count 0
  Jan 10 15:31:31.010: INFO: 
  Logging pods the apiserver thinks is on node env1-test-worker-1 before test
  Jan 10 15:31:31.038: INFO: pod-configmaps-c5283538-ea2a-4c64-aea5-df86d3f46a20 from configmap-4072 started at 2024-01-10 15:31:22 +0000 UTC (2 container statuses recorded)
  Jan 10 15:31:31.038: INFO: 	Container agnhost-container ready: true, restart count 0
  Jan 10 15:31:31.038: INFO: 	Container configmap-volume-binary-test ready: false, restart count 0
  Jan 10 15:31:31.038: INFO: filebeat-filebeat-qtfxn from filebeat started at 2024-01-10 13:26:52 +0000 UTC (1 container statuses recorded)
  Jan 10 15:31:31.038: INFO: 	Container filebeat ready: false, restart count 0
  Jan 10 15:31:31.038: INFO: kube-flannel-jxf5s from kube-system started at 2024-01-09 16:24:09 +0000 UTC (1 container statuses recorded)
  Jan 10 15:31:31.038: INFO: 	Container kube-flannel ready: true, restart count 0
  Jan 10 15:31:31.038: INFO: kube-proxy-78tcd from kube-system started at 2024-01-10 09:13:13 +0000 UTC (1 container statuses recorded)
  Jan 10 15:31:31.038: INFO: 	Container kube-proxy ready: true, restart count 0
  Jan 10 15:31:31.038: INFO: nginx-proxy-env1-test-worker-1 from kube-system started at 2024-01-09 16:38:31 +0000 UTC (1 container statuses recorded)
  Jan 10 15:31:31.039: INFO: 	Container nginx-proxy ready: true, restart count 0
  Jan 10 15:31:31.039: INFO: nodelocaldns-7qx4w from kube-system started at 2024-01-09 15:52:12 +0000 UTC (1 container statuses recorded)
  Jan 10 15:31:31.039: INFO: 	Container node-cache ready: true, restart count 0
  Jan 10 15:31:31.039: INFO: vsphere-csi-node-lr59t from kube-system started at 2024-01-10 09:21:38 +0000 UTC (3 container statuses recorded)
  Jan 10 15:31:31.039: INFO: 	Container liveness-probe ready: true, restart count 0
  Jan 10 15:31:31.039: INFO: 	Container node-driver-registrar ready: true, restart count 0
  Jan 10 15:31:31.039: INFO: 	Container vsphere-csi-node ready: true, restart count 0
  Jan 10 15:31:31.039: INFO: prometheus-prometheus-node-exporter-2wr55 from prometheus started at 2024-01-10 13:26:52 +0000 UTC (1 container statuses recorded)
  Jan 10 15:31:31.039: INFO: 	Container node-exporter ready: true, restart count 0
  Jan 10 15:31:31.039: INFO: sonobuoy from sonobuoy started at 2024-01-10 15:26:36 +0000 UTC (1 container statuses recorded)
  Jan 10 15:31:31.039: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jan 10 15:31:31.039: INFO: sonobuoy-e2e-job-b46f6697883e4f52 from sonobuoy started at 2024-01-10 15:26:37 +0000 UTC (2 container statuses recorded)
  Jan 10 15:31:31.039: INFO: 	Container e2e ready: true, restart count 0
  Jan 10 15:31:31.039: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jan 10 15:31:31.039: INFO: sonobuoy-systemd-logs-daemon-set-187fce9e0ddc499b-nntgv from sonobuoy started at 2024-01-10 15:26:38 +0000 UTC (2 container statuses recorded)
  Jan 10 15:31:31.039: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jan 10 15:31:31.039: INFO: 	Container systemd-logs ready: true, restart count 0
  Jan 10 15:31:31.039: INFO: traefik-ingress-mzrnh from traefik-ingress started at 2024-01-10 13:26:52 +0000 UTC (1 container statuses recorded)
  Jan 10 15:31:31.039: INFO: 	Container traefik-ingress ready: true, restart count 0
  STEP: Trying to schedule Pod with nonempty NodeSelector. @ 01/10/24 15:31:31.04
  STEP: Considering event: 
  Type = [Warning], Name = [restricted-pod.17a9062547ea95d3], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 node(s) didn't match Pod's node affinity/selector, 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/5 nodes are available: 5 Preemption is not helpful for scheduling..] @ 01/10/24 15:31:31.148
  Jan 10 15:31:32.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-1613" for this suite. @ 01/10/24 15:31:32.155
• [1.315 seconds]
------------------------------
[sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:91
  STEP: Creating a kubernetes client @ 01/10/24 15:31:32.179
  Jan 10 15:31:32.179: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename downward-api @ 01/10/24 15:31:32.181
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:31:32.25
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:31:32.262
  STEP: Creating a pod to test downward api env vars @ 01/10/24 15:31:32.268
  STEP: Saw pod success @ 01/10/24 15:31:36.329
  Jan 10 15:31:36.348: INFO: Trying to get logs from node env1-test-worker-1 pod downward-api-68a4df13-7cfc-4952-ba0f-4b3923211fd2 container dapi-container: <nil>
  STEP: delete the pod @ 01/10/24 15:31:36.382
  Jan 10 15:31:36.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-752" for this suite. @ 01/10/24 15:31:36.467
• [4.324 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]
test/e2e/apimachinery/resource_quota.go:946
  STEP: Creating a kubernetes client @ 01/10/24 15:31:36.505
  Jan 10 15:31:36.505: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename resourcequota @ 01/10/24 15:31:36.507
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:31:36.568
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:31:36.575
  STEP: Creating a ResourceQuota @ 01/10/24 15:31:36.581
  STEP: Getting a ResourceQuota @ 01/10/24 15:31:36.598
  STEP: Listing all ResourceQuotas with LabelSelector @ 01/10/24 15:31:36.608
  STEP: Patching the ResourceQuota @ 01/10/24 15:31:36.621
  STEP: Deleting a Collection of ResourceQuotas @ 01/10/24 15:31:36.644
  STEP: Verifying the deleted ResourceQuota @ 01/10/24 15:31:36.684
  Jan 10 15:31:36.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2239" for this suite. @ 01/10/24 15:31:36.715
• [0.240 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:208
  STEP: Creating a kubernetes client @ 01/10/24 15:31:36.749
  Jan 10 15:31:36.749: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename downward-api @ 01/10/24 15:31:36.75
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:31:36.824
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:31:36.831
  STEP: Creating a pod to test downward API volume plugin @ 01/10/24 15:31:36.839
  STEP: Saw pod success @ 01/10/24 15:31:40.911
  Jan 10 15:31:40.920: INFO: Trying to get logs from node env1-test-worker-1 pod downwardapi-volume-11bc60f5-a8b6-4ab7-b375-2b9426785146 container client-container: <nil>
  STEP: delete the pod @ 01/10/24 15:31:40.94
  Jan 10 15:31:40.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8342" for this suite. @ 01/10/24 15:31:41.003
• [4.284 seconds]
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]
test/e2e/kubectl/kubectl.go:1701
  STEP: Creating a kubernetes client @ 01/10/24 15:31:41.034
  Jan 10 15:31:41.034: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename kubectl @ 01/10/24 15:31:41.036
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:31:41.096
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:31:41.104
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 01/10/24 15:31:41.114
  Jan 10 15:31:41.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-3863 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
  Jan 10 15:31:41.384: INFO: stderr: ""
  Jan 10 15:31:41.384: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 01/10/24 15:31:41.384
  Jan 10 15:31:41.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-3863 delete pods e2e-test-httpd-pod'
  Jan 10 15:31:43.729: INFO: stderr: ""
  Jan 10 15:31:43.729: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Jan 10 15:31:43.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3863" for this suite. @ 01/10/24 15:31:43.746
• [2.726 seconds]
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]
test/e2e/scheduling/preemption.go:624
  STEP: Creating a kubernetes client @ 01/10/24 15:31:43.761
  Jan 10 15:31:43.761: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename sched-preemption @ 01/10/24 15:31:43.763
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:31:43.813
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:31:43.818
  Jan 10 15:31:43.852: INFO: Waiting up to 1m0s for all nodes to be ready
  Jan 10 15:32:43.976: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 01/10/24 15:32:43.992
  Jan 10 15:32:43.992: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename sched-preemption-path @ 01/10/24 15:32:43.995
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:32:44.053
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:32:44.06
  STEP: Finding an available node @ 01/10/24 15:32:44.068
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 01/10/24 15:32:44.069
  STEP: Explicitly delete pod here to free the resource it takes. @ 01/10/24 15:32:46.125
  Jan 10 15:32:46.158: INFO: found a healthy node: env1-test-worker-1
  Jan 10 15:32:52.414: INFO: pods created so far: [1 1 1]
  Jan 10 15:32:52.414: INFO: length of pods created so far: 3
  Jan 10 15:32:54.461: INFO: pods created so far: [2 2 1]
  Jan 10 15:33:01.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jan 10 15:33:01.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-3050" for this suite. @ 01/10/24 15:33:01.759
  STEP: Destroying namespace "sched-preemption-5737" for this suite. @ 01/10/24 15:33:01.793
• [78.049 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]
test/e2e/apimachinery/resource_quota.go:328
  STEP: Creating a kubernetes client @ 01/10/24 15:33:01.81
  Jan 10 15:33:01.810: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename resourcequota @ 01/10/24 15:33:01.812
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:33:01.857
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:33:01.862
  STEP: Counting existing ResourceQuota @ 01/10/24 15:33:18.876
  STEP: Creating a ResourceQuota @ 01/10/24 15:33:23.887
  STEP: Ensuring resource quota status is calculated @ 01/10/24 15:33:23.897
  STEP: Creating a ConfigMap @ 01/10/24 15:33:25.909
  STEP: Ensuring resource quota status captures configMap creation @ 01/10/24 15:33:25.939
  STEP: Deleting a ConfigMap @ 01/10/24 15:33:27.948
  STEP: Ensuring resource quota status released usage @ 01/10/24 15:33:27.966
  Jan 10 15:33:29.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-675" for this suite. @ 01/10/24 15:33:29.993
• [28.199 seconds]
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]
test/e2e/apps/statefulset.go:327
  STEP: Creating a kubernetes client @ 01/10/24 15:33:30.01
  Jan 10 15:33:30.010: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename statefulset @ 01/10/24 15:33:30.012
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:33:30.056
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:33:30.061
  STEP: Creating service test in namespace statefulset-3475 @ 01/10/24 15:33:30.068
  STEP: Creating a new StatefulSet @ 01/10/24 15:33:30.085
  Jan 10 15:33:30.126: INFO: Found 0 stateful pods, waiting for 3
  Jan 10 15:33:40.138: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Jan 10 15:33:40.138: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Jan 10 15:33:40.138: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 01/10/24 15:33:40.168
  Jan 10 15:33:40.207: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 01/10/24 15:33:40.208
  STEP: Not applying an update when the partition is greater than the number of replicas @ 01/10/24 15:33:50.258
  STEP: Performing a canary update @ 01/10/24 15:33:50.258
  Jan 10 15:33:50.301: INFO: Updating stateful set ss2
  Jan 10 15:33:50.333: INFO: Waiting for Pod statefulset-3475/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  STEP: Restoring Pods to the correct revision when they are deleted @ 01/10/24 15:34:00.353
  Jan 10 15:34:00.434: INFO: Found 1 stateful pods, waiting for 3
  Jan 10 15:34:10.447: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Jan 10 15:34:10.448: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Jan 10 15:34:10.448: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Performing a phased rolling update @ 01/10/24 15:34:10.467
  Jan 10 15:34:10.505: INFO: Updating stateful set ss2
  Jan 10 15:34:10.531: INFO: Waiting for Pod statefulset-3475/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  Jan 10 15:34:20.592: INFO: Updating stateful set ss2
  Jan 10 15:34:20.628: INFO: Waiting for StatefulSet statefulset-3475/ss2 to complete update
  Jan 10 15:34:20.628: INFO: Waiting for Pod statefulset-3475/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  Jan 10 15:34:30.647: INFO: Deleting all statefulset in ns statefulset-3475
  Jan 10 15:34:30.654: INFO: Scaling statefulset ss2 to 0
  Jan 10 15:34:40.707: INFO: Waiting for statefulset status.replicas updated to 0
  Jan 10 15:34:40.714: INFO: Deleting statefulset ss2
  Jan 10 15:34:40.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-3475" for this suite. @ 01/10/24 15:34:40.761
• [70.765 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply an update to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:370
  STEP: Creating a kubernetes client @ 01/10/24 15:34:40.787
  Jan 10 15:34:40.787: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename namespaces @ 01/10/24 15:34:40.789
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:34:40.833
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:34:40.841
  STEP: Updating Namespace "namespaces-4060" @ 01/10/24 15:34:40.848
  Jan 10 15:34:40.867: INFO: Namespace "namespaces-4060" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"59d9f44f-6074-423b-8ef4-6590885bb6d0", "kubernetes.io/metadata.name":"namespaces-4060", "namespaces-4060":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
  Jan 10 15:34:40.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-4060" for this suite. @ 01/10/24 15:34:40.888
• [0.123 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:61
  STEP: Creating a kubernetes client @ 01/10/24 15:34:40.911
  Jan 10 15:34:40.911: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename containers @ 01/10/24 15:34:40.914
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:34:40.963
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:34:40.97
  STEP: Creating a pod to test override arguments @ 01/10/24 15:34:40.976
  STEP: Saw pod success @ 01/10/24 15:34:45.033
  Jan 10 15:34:45.040: INFO: Trying to get logs from node env1-test-worker-1 pod client-containers-3ce7ef93-6ef6-4b07-a03a-1125a1169595 container agnhost-container: <nil>
  STEP: delete the pod @ 01/10/24 15:34:45.081
  Jan 10 15:34:45.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-537" for this suite. @ 01/10/24 15:34:45.142
• [4.243 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment should validate Deployment Status endpoints [Conformance]
test/e2e/apps/deployment.go:485
  STEP: Creating a kubernetes client @ 01/10/24 15:34:45.158
  Jan 10 15:34:45.158: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename deployment @ 01/10/24 15:34:45.159
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:34:45.212
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:34:45.22
  STEP: creating a Deployment @ 01/10/24 15:34:45.236
  Jan 10 15:34:45.236: INFO: Creating simple deployment test-deployment-rs595
  Jan 10 15:34:45.279: INFO: deployment "test-deployment-rs595" doesn't have the required revision set
  STEP: Getting /status @ 01/10/24 15:34:47.311
  Jan 10 15:34:47.324: INFO: Deployment test-deployment-rs595 has Conditions: [{Available True 2024-01-10 15:34:46 +0000 UTC 2024-01-10 15:34:46 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2024-01-10 15:34:46 +0000 UTC 2024-01-10 15:34:45 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-rs595-5994cf9475" has successfully progressed.}]
  STEP: updating Deployment Status @ 01/10/24 15:34:47.324
  Jan 10 15:34:47.347: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.January, 10, 15, 34, 46, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 10, 15, 34, 46, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 10, 15, 34, 46, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 10, 15, 34, 45, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-rs595-5994cf9475\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Deployment status to be updated @ 01/10/24 15:34:47.348
  Jan 10 15:34:47.353: INFO: Observed &Deployment event: ADDED
  Jan 10 15:34:47.353: INFO: Observed Deployment test-deployment-rs595 in namespace deployment-7386 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-10 15:34:45 +0000 UTC 2024-01-10 15:34:45 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-rs595-5994cf9475"}
  Jan 10 15:34:47.353: INFO: Observed &Deployment event: MODIFIED
  Jan 10 15:34:47.353: INFO: Observed Deployment test-deployment-rs595 in namespace deployment-7386 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-10 15:34:45 +0000 UTC 2024-01-10 15:34:45 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-rs595-5994cf9475"}
  Jan 10 15:34:47.353: INFO: Observed Deployment test-deployment-rs595 in namespace deployment-7386 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2024-01-10 15:34:45 +0000 UTC 2024-01-10 15:34:45 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jan 10 15:34:47.354: INFO: Observed &Deployment event: MODIFIED
  Jan 10 15:34:47.354: INFO: Observed Deployment test-deployment-rs595 in namespace deployment-7386 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2024-01-10 15:34:45 +0000 UTC 2024-01-10 15:34:45 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jan 10 15:34:47.354: INFO: Observed Deployment test-deployment-rs595 in namespace deployment-7386 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-10 15:34:45 +0000 UTC 2024-01-10 15:34:45 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-rs595-5994cf9475" is progressing.}
  Jan 10 15:34:47.354: INFO: Observed &Deployment event: MODIFIED
  Jan 10 15:34:47.354: INFO: Observed Deployment test-deployment-rs595 in namespace deployment-7386 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2024-01-10 15:34:46 +0000 UTC 2024-01-10 15:34:46 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jan 10 15:34:47.354: INFO: Observed Deployment test-deployment-rs595 in namespace deployment-7386 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-10 15:34:46 +0000 UTC 2024-01-10 15:34:45 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-rs595-5994cf9475" has successfully progressed.}
  Jan 10 15:34:47.355: INFO: Observed &Deployment event: MODIFIED
  Jan 10 15:34:47.355: INFO: Observed Deployment test-deployment-rs595 in namespace deployment-7386 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2024-01-10 15:34:46 +0000 UTC 2024-01-10 15:34:46 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jan 10 15:34:47.355: INFO: Observed Deployment test-deployment-rs595 in namespace deployment-7386 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-10 15:34:46 +0000 UTC 2024-01-10 15:34:45 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-rs595-5994cf9475" has successfully progressed.}
  Jan 10 15:34:47.355: INFO: Found Deployment test-deployment-rs595 in namespace deployment-7386 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jan 10 15:34:47.355: INFO: Deployment test-deployment-rs595 has an updated status
  STEP: patching the Statefulset Status @ 01/10/24 15:34:47.355
  Jan 10 15:34:47.355: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Jan 10 15:34:47.372: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Deployment status to be patched @ 01/10/24 15:34:47.372
  Jan 10 15:34:47.377: INFO: Observed &Deployment event: ADDED
  Jan 10 15:34:47.377: INFO: Observed deployment test-deployment-rs595 in namespace deployment-7386 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-10 15:34:45 +0000 UTC 2024-01-10 15:34:45 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-rs595-5994cf9475"}
  Jan 10 15:34:47.378: INFO: Observed &Deployment event: MODIFIED
  Jan 10 15:34:47.379: INFO: Observed deployment test-deployment-rs595 in namespace deployment-7386 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-10 15:34:45 +0000 UTC 2024-01-10 15:34:45 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-rs595-5994cf9475"}
  Jan 10 15:34:47.380: INFO: Observed deployment test-deployment-rs595 in namespace deployment-7386 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2024-01-10 15:34:45 +0000 UTC 2024-01-10 15:34:45 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jan 10 15:34:47.380: INFO: Observed &Deployment event: MODIFIED
  Jan 10 15:34:47.380: INFO: Observed deployment test-deployment-rs595 in namespace deployment-7386 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2024-01-10 15:34:45 +0000 UTC 2024-01-10 15:34:45 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jan 10 15:34:47.380: INFO: Observed deployment test-deployment-rs595 in namespace deployment-7386 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-10 15:34:45 +0000 UTC 2024-01-10 15:34:45 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-rs595-5994cf9475" is progressing.}
  Jan 10 15:34:47.381: INFO: Observed &Deployment event: MODIFIED
  Jan 10 15:34:47.381: INFO: Observed deployment test-deployment-rs595 in namespace deployment-7386 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2024-01-10 15:34:46 +0000 UTC 2024-01-10 15:34:46 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jan 10 15:34:47.381: INFO: Observed deployment test-deployment-rs595 in namespace deployment-7386 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-10 15:34:46 +0000 UTC 2024-01-10 15:34:45 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-rs595-5994cf9475" has successfully progressed.}
  Jan 10 15:34:47.381: INFO: Observed &Deployment event: MODIFIED
  Jan 10 15:34:47.381: INFO: Observed deployment test-deployment-rs595 in namespace deployment-7386 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2024-01-10 15:34:46 +0000 UTC 2024-01-10 15:34:46 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jan 10 15:34:47.381: INFO: Observed deployment test-deployment-rs595 in namespace deployment-7386 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2024-01-10 15:34:46 +0000 UTC 2024-01-10 15:34:45 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-rs595-5994cf9475" has successfully progressed.}
  Jan 10 15:34:47.381: INFO: Observed deployment test-deployment-rs595 in namespace deployment-7386 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jan 10 15:34:47.382: INFO: Observed &Deployment event: MODIFIED
  Jan 10 15:34:47.382: INFO: Found deployment test-deployment-rs595 in namespace deployment-7386 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
  Jan 10 15:34:47.382: INFO: Deployment test-deployment-rs595 has a patched status
  Jan 10 15:34:47.390: INFO: Deployment "test-deployment-rs595":
  &Deployment{ObjectMeta:{test-deployment-rs595  deployment-7386  aadb4028-9762-4d79-afd8-1a44323f6e68 186763359 1 2024-01-10 15:34:45 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2024-01-10 15:34:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2024-01-10 15:34:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2024-01-10 15:34:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004f91228 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-rs595-5994cf9475",LastUpdateTime:2024-01-10 15:34:47 +0000 UTC,LastTransitionTime:2024-01-10 15:34:47 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jan 10 15:34:47.399: INFO: New ReplicaSet "test-deployment-rs595-5994cf9475" of Deployment "test-deployment-rs595":
  &ReplicaSet{ObjectMeta:{test-deployment-rs595-5994cf9475  deployment-7386  01731b84-0f5f-4e8d-9dfa-b490d3f369ee 186763315 1 2024-01-10 15:34:45 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-rs595 aadb4028-9762-4d79-afd8-1a44323f6e68 0xc004f91617 0xc004f91618}] [] [{kube-controller-manager Update apps/v1 2024-01-10 15:34:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aadb4028-9762-4d79-afd8-1a44323f6e68\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-10 15:34:46 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 5994cf9475,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004f916c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jan 10 15:34:47.410: INFO: Pod "test-deployment-rs595-5994cf9475-sdxjd" is available:
  &Pod{ObjectMeta:{test-deployment-rs595-5994cf9475-sdxjd test-deployment-rs595-5994cf9475- deployment-7386  83476d4e-f689-4922-a1ae-0876c0d9223f 186763314 0 2024-01-10 15:34:45 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [{apps/v1 ReplicaSet test-deployment-rs595-5994cf9475 01731b84-0f5f-4e8d-9dfa-b490d3f369ee 0xc004f91e77 0xc004f91e78}] [] [{kube-controller-manager Update v1 2024-01-10 15:34:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"01731b84-0f5f-4e8d-9dfa-b490d3f369ee\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-10 15:34:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.41\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cwvql,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cwvql,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 15:34:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 15:34:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 15:34:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 15:34:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.201,PodIP:10.233.68.41,StartTime:2024-01-10 15:34:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-10 15:34:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://54a436d0a1f218b94bb2ff50008281963c8fd788b538f5d705ef7ae36d30157b,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.41,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 10 15:34:47.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-7386" for this suite. @ 01/10/24 15:34:47.428
• [2.298 seconds]
------------------------------
S
------------------------------
[sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]
test/e2e/apps/replica_set.go:131
  STEP: Creating a kubernetes client @ 01/10/24 15:34:47.456
  Jan 10 15:34:47.456: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename replicaset @ 01/10/24 15:34:47.458
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:34:47.497
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:34:47.503
  STEP: Given a Pod with a 'name' label pod-adoption-release is created @ 01/10/24 15:34:47.509
  STEP: When a replicaset with a matching selector is created @ 01/10/24 15:34:49.562
  STEP: Then the orphan pod is adopted @ 01/10/24 15:34:49.575
  STEP: When the matched label of one of its pods change @ 01/10/24 15:34:50.597
  Jan 10 15:34:50.604: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 01/10/24 15:34:50.631
  Jan 10 15:34:51.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-3569" for this suite. @ 01/10/24 15:34:51.664
• [4.224 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2224
  STEP: Creating a kubernetes client @ 01/10/24 15:34:51.683
  Jan 10 15:34:51.683: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename services @ 01/10/24 15:34:51.686
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:34:51.754
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:34:51.761
  STEP: creating service in namespace services-9054 @ 01/10/24 15:34:51.768
  STEP: creating service affinity-nodeport-transition in namespace services-9054 @ 01/10/24 15:34:51.768
  STEP: creating replication controller affinity-nodeport-transition in namespace services-9054 @ 01/10/24 15:34:51.813
  I0110 15:34:51.829494      23 runners.go:194] Created replication controller with name: affinity-nodeport-transition, namespace: services-9054, replica count: 3
  I0110 15:34:54.880990      23 runners.go:194] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jan 10 15:34:54.906: INFO: Creating new exec pod
  Jan 10 15:34:57.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=services-9054 exec execpod-affinityqqghh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
  Jan 10 15:34:58.314: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
  Jan 10 15:34:58.314: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 10 15:34:58.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=services-9054 exec execpod-affinityqqghh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.39.250 80'
  Jan 10 15:34:58.675: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.39.250 80\nConnection to 10.233.39.250 80 port [tcp/http] succeeded!\n"
  Jan 10 15:34:58.675: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 10 15:34:58.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=services-9054 exec execpod-affinityqqghh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.61.1.200 30958'
  Jan 10 15:34:59.040: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.61.1.200 30958\nConnection to 10.61.1.200 30958 port [tcp/*] succeeded!\n"
  Jan 10 15:34:59.040: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 10 15:34:59.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=services-9054 exec execpod-affinityqqghh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.61.1.201 30958'
  Jan 10 15:34:59.362: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.61.1.201 30958\nConnection to 10.61.1.201 30958 port [tcp/*] succeeded!\n"
  Jan 10 15:34:59.362: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 10 15:34:59.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=services-9054 exec execpod-affinityqqghh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.61.1.200:30958/ ; done'
  Jan 10 15:34:59.929: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30958/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30958/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30958/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30958/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30958/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30958/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30958/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30958/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30958/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30958/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30958/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30958/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30958/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30958/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30958/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30958/\n"
  Jan 10 15:34:59.929: INFO: stdout: "\naffinity-nodeport-transition-ld987\naffinity-nodeport-transition-tff86\naffinity-nodeport-transition-x9fgr\naffinity-nodeport-transition-ld987\naffinity-nodeport-transition-tff86\naffinity-nodeport-transition-x9fgr\naffinity-nodeport-transition-ld987\naffinity-nodeport-transition-tff86\naffinity-nodeport-transition-x9fgr\naffinity-nodeport-transition-ld987\naffinity-nodeport-transition-tff86\naffinity-nodeport-transition-x9fgr\naffinity-nodeport-transition-ld987\naffinity-nodeport-transition-tff86\naffinity-nodeport-transition-x9fgr\naffinity-nodeport-transition-ld987"
  Jan 10 15:34:59.929: INFO: Received response from host: affinity-nodeport-transition-ld987
  Jan 10 15:34:59.929: INFO: Received response from host: affinity-nodeport-transition-tff86
  Jan 10 15:34:59.929: INFO: Received response from host: affinity-nodeport-transition-x9fgr
  Jan 10 15:34:59.929: INFO: Received response from host: affinity-nodeport-transition-ld987
  Jan 10 15:34:59.929: INFO: Received response from host: affinity-nodeport-transition-tff86
  Jan 10 15:34:59.929: INFO: Received response from host: affinity-nodeport-transition-x9fgr
  Jan 10 15:34:59.929: INFO: Received response from host: affinity-nodeport-transition-ld987
  Jan 10 15:34:59.929: INFO: Received response from host: affinity-nodeport-transition-tff86
  Jan 10 15:34:59.929: INFO: Received response from host: affinity-nodeport-transition-x9fgr
  Jan 10 15:34:59.929: INFO: Received response from host: affinity-nodeport-transition-ld987
  Jan 10 15:34:59.929: INFO: Received response from host: affinity-nodeport-transition-tff86
  Jan 10 15:34:59.929: INFO: Received response from host: affinity-nodeport-transition-x9fgr
  Jan 10 15:34:59.929: INFO: Received response from host: affinity-nodeport-transition-ld987
  Jan 10 15:34:59.929: INFO: Received response from host: affinity-nodeport-transition-tff86
  Jan 10 15:34:59.929: INFO: Received response from host: affinity-nodeport-transition-x9fgr
  Jan 10 15:34:59.929: INFO: Received response from host: affinity-nodeport-transition-ld987
  Jan 10 15:34:59.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=services-9054 exec execpod-affinityqqghh -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.61.1.200:30958/ ; done'
  Jan 10 15:35:00.454: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30958/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30958/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30958/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30958/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30958/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30958/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30958/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30958/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30958/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30958/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30958/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30958/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30958/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30958/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30958/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:30958/\n"
  Jan 10 15:35:00.454: INFO: stdout: "\naffinity-nodeport-transition-x9fgr\naffinity-nodeport-transition-x9fgr\naffinity-nodeport-transition-x9fgr\naffinity-nodeport-transition-x9fgr\naffinity-nodeport-transition-x9fgr\naffinity-nodeport-transition-x9fgr\naffinity-nodeport-transition-x9fgr\naffinity-nodeport-transition-x9fgr\naffinity-nodeport-transition-x9fgr\naffinity-nodeport-transition-x9fgr\naffinity-nodeport-transition-x9fgr\naffinity-nodeport-transition-x9fgr\naffinity-nodeport-transition-x9fgr\naffinity-nodeport-transition-x9fgr\naffinity-nodeport-transition-x9fgr\naffinity-nodeport-transition-x9fgr"
  Jan 10 15:35:00.454: INFO: Received response from host: affinity-nodeport-transition-x9fgr
  Jan 10 15:35:00.454: INFO: Received response from host: affinity-nodeport-transition-x9fgr
  Jan 10 15:35:00.454: INFO: Received response from host: affinity-nodeport-transition-x9fgr
  Jan 10 15:35:00.454: INFO: Received response from host: affinity-nodeport-transition-x9fgr
  Jan 10 15:35:00.454: INFO: Received response from host: affinity-nodeport-transition-x9fgr
  Jan 10 15:35:00.454: INFO: Received response from host: affinity-nodeport-transition-x9fgr
  Jan 10 15:35:00.454: INFO: Received response from host: affinity-nodeport-transition-x9fgr
  Jan 10 15:35:00.454: INFO: Received response from host: affinity-nodeport-transition-x9fgr
  Jan 10 15:35:00.454: INFO: Received response from host: affinity-nodeport-transition-x9fgr
  Jan 10 15:35:00.454: INFO: Received response from host: affinity-nodeport-transition-x9fgr
  Jan 10 15:35:00.454: INFO: Received response from host: affinity-nodeport-transition-x9fgr
  Jan 10 15:35:00.454: INFO: Received response from host: affinity-nodeport-transition-x9fgr
  Jan 10 15:35:00.455: INFO: Received response from host: affinity-nodeport-transition-x9fgr
  Jan 10 15:35:00.455: INFO: Received response from host: affinity-nodeport-transition-x9fgr
  Jan 10 15:35:00.455: INFO: Received response from host: affinity-nodeport-transition-x9fgr
  Jan 10 15:35:00.455: INFO: Received response from host: affinity-nodeport-transition-x9fgr
  Jan 10 15:35:00.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jan 10 15:35:00.483: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-9054, will wait for the garbage collector to delete the pods @ 01/10/24 15:35:00.514
  Jan 10 15:35:00.585: INFO: Deleting ReplicationController affinity-nodeport-transition took: 13.365344ms
  Jan 10 15:35:00.685: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.388719ms
  STEP: Destroying namespace "services-9054" for this suite. @ 01/10/24 15:35:03.147
• [11.484 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:148
  STEP: Creating a kubernetes client @ 01/10/24 15:35:03.169
  Jan 10 15:35:03.169: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename kubelet-test @ 01/10/24 15:35:03.173
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:35:03.217
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:35:03.225
  STEP: Waiting for pod completion @ 01/10/24 15:35:03.251
  Jan 10 15:35:07.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-9701" for this suite. @ 01/10/24 15:35:07.318
• [4.165 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]
test/e2e/apps/disruption.go:141
  STEP: Creating a kubernetes client @ 01/10/24 15:35:07.335
  Jan 10 15:35:07.335: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename disruption @ 01/10/24 15:35:07.338
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:35:07.374
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:35:07.381
  STEP: Waiting for the pdb to be processed @ 01/10/24 15:35:07.398
  STEP: Waiting for all pods to be running @ 01/10/24 15:35:09.466
  Jan 10 15:35:09.529: INFO: running pods: 0 < 3
  Jan 10 15:35:11.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2551" for this suite. @ 01/10/24 15:35:11.568
• [4.257 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:47
  STEP: Creating a kubernetes client @ 01/10/24 15:35:11.596
  Jan 10 15:35:11.596: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename projected @ 01/10/24 15:35:11.598
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:35:11.633
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:35:11.643
  STEP: Creating configMap with name projected-configmap-test-volume-ec12a9e3-56a2-48cc-9089-e87168a1c095 @ 01/10/24 15:35:11.65
  STEP: Creating a pod to test consume configMaps @ 01/10/24 15:35:11.663
  STEP: Saw pod success @ 01/10/24 15:35:15.72
  Jan 10 15:35:15.734: INFO: Trying to get logs from node env1-test-worker-1 pod pod-projected-configmaps-08f91bd2-886c-416b-81e9-12766cdf9ba0 container agnhost-container: <nil>
  STEP: delete the pod @ 01/10/24 15:35:15.756
  Jan 10 15:35:15.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3738" for this suite. @ 01/10/24 15:35:15.838
• [4.262 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
test/e2e/apps/statefulset.go:591
  STEP: Creating a kubernetes client @ 01/10/24 15:35:15.863
  Jan 10 15:35:15.863: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename statefulset @ 01/10/24 15:35:15.865
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:35:15.915
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:35:15.923
  STEP: Creating service test in namespace statefulset-1475 @ 01/10/24 15:35:15.93
  STEP: Initializing watcher for selector baz=blah,foo=bar @ 01/10/24 15:35:15.947
  STEP: Creating stateful set ss in namespace statefulset-1475 @ 01/10/24 15:35:15.963
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1475 @ 01/10/24 15:35:15.986
  Jan 10 15:35:15.999: INFO: Found 0 stateful pods, waiting for 1
  Jan 10 15:35:26.012: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod @ 01/10/24 15:35:26.012
  Jan 10 15:35:26.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=statefulset-1475 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jan 10 15:35:26.352: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jan 10 15:35:26.352: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jan 10 15:35:26.352: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jan 10 15:35:26.361: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  Jan 10 15:35:36.373: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jan 10 15:35:36.373: INFO: Waiting for statefulset status.replicas updated to 0
  Jan 10 15:35:36.422: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999257s
  Jan 10 15:35:37.432: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.98644104s
  Jan 10 15:35:38.441: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.97639233s
  Jan 10 15:35:39.452: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.96634393s
  Jan 10 15:35:40.465: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.956305792s
  Jan 10 15:35:41.473: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.943312906s
  Jan 10 15:35:42.486: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.935327135s
  Jan 10 15:35:43.496: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.922334559s
  Jan 10 15:35:44.503: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.91217009s
  Jan 10 15:35:45.518: INFO: Verifying statefulset ss doesn't scale past 1 for another 903.953365ms
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1475 @ 01/10/24 15:35:46.518
  Jan 10 15:35:46.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=statefulset-1475 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jan 10 15:35:46.861: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jan 10 15:35:46.861: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jan 10 15:35:46.861: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jan 10 15:35:46.872: INFO: Found 1 stateful pods, waiting for 3
  Jan 10 15:35:56.884: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Jan 10 15:35:56.884: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Jan 10 15:35:56.884: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Verifying that stateful set ss was scaled up in order @ 01/10/24 15:35:56.884
  STEP: Scale down will halt with unhealthy stateful pod @ 01/10/24 15:35:56.884
  Jan 10 15:35:56.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=statefulset-1475 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jan 10 15:35:57.246: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jan 10 15:35:57.246: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jan 10 15:35:57.246: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jan 10 15:35:57.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=statefulset-1475 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jan 10 15:35:57.625: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jan 10 15:35:57.625: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jan 10 15:35:57.625: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jan 10 15:35:57.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=statefulset-1475 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jan 10 15:35:57.976: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jan 10 15:35:57.976: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jan 10 15:35:57.976: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jan 10 15:35:57.976: INFO: Waiting for statefulset status.replicas updated to 0
  Jan 10 15:35:57.985: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
  Jan 10 15:36:08.006: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jan 10 15:36:08.006: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Jan 10 15:36:08.006: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Jan 10 15:36:08.051: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999207s
  Jan 10 15:36:09.061: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.980531014s
  Jan 10 15:36:10.073: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.970161128s
  Jan 10 15:36:11.086: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.958480836s
  Jan 10 15:36:12.097: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.94599563s
  Jan 10 15:36:13.107: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.934619667s
  Jan 10 15:36:14.116: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.924744058s
  Jan 10 15:36:15.131: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.915207533s
  Jan 10 15:36:16.142: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.89997928s
  Jan 10 15:36:17.155: INFO: Verifying statefulset ss doesn't scale past 3 for another 889.052256ms
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1475 @ 01/10/24 15:36:18.156
  Jan 10 15:36:18.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=statefulset-1475 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jan 10 15:36:18.487: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jan 10 15:36:18.487: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jan 10 15:36:18.487: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jan 10 15:36:18.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=statefulset-1475 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jan 10 15:36:18.838: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jan 10 15:36:18.838: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jan 10 15:36:18.838: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jan 10 15:36:18.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=statefulset-1475 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jan 10 15:36:19.138: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jan 10 15:36:19.138: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jan 10 15:36:19.138: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jan 10 15:36:19.138: INFO: Scaling statefulset ss to 0
  STEP: Verifying that stateful set ss was scaled down in reverse order @ 01/10/24 15:36:29.176
  Jan 10 15:36:29.177: INFO: Deleting all statefulset in ns statefulset-1475
  Jan 10 15:36:29.186: INFO: Scaling statefulset ss to 0
  Jan 10 15:36:29.214: INFO: Waiting for statefulset status.replicas updated to 0
  Jan 10 15:36:29.223: INFO: Deleting statefulset ss
  Jan 10 15:36:29.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-1475" for this suite. @ 01/10/24 15:36:29.275
• [73.434 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:151
  STEP: Creating a kubernetes client @ 01/10/24 15:36:29.298
  Jan 10 15:36:29.298: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename container-probe @ 01/10/24 15:36:29.3
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:36:29.359
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:36:29.367
  STEP: Creating pod busybox-ef9d7fe6-0dc8-4fd0-b3eb-75af79c9271f in namespace container-probe-2470 @ 01/10/24 15:36:29.373
  Jan 10 15:36:31.409: INFO: Started pod busybox-ef9d7fe6-0dc8-4fd0-b3eb-75af79c9271f in namespace container-probe-2470
  STEP: checking the pod's current state and verifying that restartCount is present @ 01/10/24 15:36:31.409
  Jan 10 15:36:31.418: INFO: Initial restart count of pod busybox-ef9d7fe6-0dc8-4fd0-b3eb-75af79c9271f is 0
  Jan 10 15:40:32.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 01/10/24 15:40:32.803
  STEP: Destroying namespace "container-probe-2470" for this suite. @ 01/10/24 15:40:32.834
• [243.552 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:124
  STEP: Creating a kubernetes client @ 01/10/24 15:40:32.861
  Jan 10 15:40:32.861: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename projected @ 01/10/24 15:40:32.863
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:40:32.894
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:40:32.902
  STEP: Creating projection with configMap that has name projected-configmap-test-upd-e22ab235-50f0-4c13-be5b-28fa37d044e8 @ 01/10/24 15:40:32.919
  STEP: Creating the pod @ 01/10/24 15:40:32.93
  STEP: Updating configmap projected-configmap-test-upd-e22ab235-50f0-4c13-be5b-28fa37d044e8 @ 01/10/24 15:40:35.039
  STEP: waiting to observe update in volume @ 01/10/24 15:40:35.05
  Jan 10 15:40:37.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1375" for this suite. @ 01/10/24 15:40:37.103
• [4.256 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]
test/e2e/apimachinery/resource_quota.go:101
  STEP: Creating a kubernetes client @ 01/10/24 15:40:37.117
  Jan 10 15:40:37.117: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename resourcequota @ 01/10/24 15:40:37.119
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:40:37.164
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:40:37.17
  STEP: Counting existing ResourceQuota @ 01/10/24 15:40:37.176
  STEP: Creating a ResourceQuota @ 01/10/24 15:40:42.187
  STEP: Ensuring resource quota status is calculated @ 01/10/24 15:40:42.204
  STEP: Creating a Service @ 01/10/24 15:40:44.212
  STEP: Creating a NodePort Service @ 01/10/24 15:40:44.293
  STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota @ 01/10/24 15:40:44.446
  STEP: Ensuring resource quota status captures service creation @ 01/10/24 15:40:44.526
  STEP: Deleting Services @ 01/10/24 15:40:46.536
  STEP: Ensuring resource quota status released usage @ 01/10/24 15:40:46.631
  Jan 10 15:40:48.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7470" for this suite. @ 01/10/24 15:40:48.659
• [11.562 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-auth] SubjectReview should support SubjectReview API operations [Conformance]
test/e2e/auth/subjectreviews.go:50
  STEP: Creating a kubernetes client @ 01/10/24 15:40:48.682
  Jan 10 15:40:48.682: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename subjectreview @ 01/10/24 15:40:48.684
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:40:48.73
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:40:48.737
  STEP: Creating a Serviceaccount "e2e" in namespace "subjectreview-2966" @ 01/10/24 15:40:48.743
  Jan 10 15:40:48.757: INFO: saUsername: "system:serviceaccount:subjectreview-2966:e2e"
  Jan 10 15:40:48.757: INFO: saGroups: []string{"system:authenticated", "system:serviceaccounts", "system:serviceaccounts:subjectreview-2966"}
  Jan 10 15:40:48.757: INFO: saUID: "91e0efa9-9c22-45f5-a5f0-7f6c3fee989d"
  STEP: Creating clientset to impersonate "system:serviceaccount:subjectreview-2966:e2e" @ 01/10/24 15:40:48.757
  STEP: Creating SubjectAccessReview for "system:serviceaccount:subjectreview-2966:e2e" @ 01/10/24 15:40:48.758
  Jan 10 15:40:48.763: INFO: sarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  STEP: Verifying as "system:serviceaccount:subjectreview-2966:e2e" api 'list' configmaps in "subjectreview-2966" namespace @ 01/10/24 15:40:48.763
  Jan 10 15:40:48.767: INFO: SubjectAccessReview has been verified
  STEP: Creating a LocalSubjectAccessReview for "system:serviceaccount:subjectreview-2966:e2e" @ 01/10/24 15:40:48.767
  Jan 10 15:40:48.772: INFO: lsarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  Jan 10 15:40:48.772: INFO: LocalSubjectAccessReview has been verified
  Jan 10 15:40:48.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subjectreview-2966" for this suite. @ 01/10/24 15:40:48.788
• [0.131 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:571
  STEP: Creating a kubernetes client @ 01/10/24 15:40:48.816
  Jan 10 15:40:48.816: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename webhook @ 01/10/24 15:40:48.818
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:40:48.892
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:40:48.899
  STEP: Setting up server cert @ 01/10/24 15:40:48.985
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 01/10/24 15:40:50.73
  STEP: Deploying the webhook pod @ 01/10/24 15:40:50.749
  STEP: Wait for the deployment to be ready @ 01/10/24 15:40:50.78
  Jan 10 15:40:50.805: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 01/10/24 15:40:52.834
  STEP: Verifying the service has paired with the endpoint @ 01/10/24 15:40:52.864
  Jan 10 15:40:53.864: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 01/10/24 15:40:54.1
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 01/10/24 15:40:54.213
  STEP: Deleting the collection of validation webhooks @ 01/10/24 15:40:54.289
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 01/10/24 15:40:54.727
  Jan 10 15:40:54.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-7461" for this suite. @ 01/10/24 15:40:55.067
  STEP: Destroying namespace "webhook-markers-2873" for this suite. @ 01/10/24 15:40:55.091
• [6.314 seconds]
------------------------------
[sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:46
  STEP: Creating a kubernetes client @ 01/10/24 15:40:55.13
  Jan 10 15:40:55.130: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename secrets @ 01/10/24 15:40:55.132
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:40:55.176
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:40:55.183
  STEP: Creating secret with name secret-test-cdd27720-49d4-4d86-9060-db3e7e3bead3 @ 01/10/24 15:40:55.191
  STEP: Creating a pod to test consume secrets @ 01/10/24 15:40:55.203
  STEP: Saw pod success @ 01/10/24 15:40:59.261
  Jan 10 15:40:59.272: INFO: Trying to get logs from node env1-test-worker-1 pod pod-secrets-d811300b-7f2c-4023-9a6d-69dcd069b10c container secret-env-test: <nil>
  STEP: delete the pod @ 01/10/24 15:40:59.299
  Jan 10 15:40:59.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6687" for this suite. @ 01/10/24 15:40:59.364
• [4.255 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:113
  STEP: Creating a kubernetes client @ 01/10/24 15:40:59.388
  Jan 10 15:40:59.388: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename deployment @ 01/10/24 15:40:59.391
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:40:59.44
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:40:59.447
  Jan 10 15:40:59.455: INFO: Creating deployment "test-recreate-deployment"
  Jan 10 15:40:59.480: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
  Jan 10 15:40:59.538: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
  Jan 10 15:41:01.566: INFO: Waiting deployment "test-recreate-deployment" to complete
  Jan 10 15:41:01.584: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
  Jan 10 15:41:01.627: INFO: Updating deployment test-recreate-deployment
  Jan 10 15:41:01.628: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
  Jan 10 15:41:01.960: INFO: Deployment "test-recreate-deployment":
  &Deployment{ObjectMeta:{test-recreate-deployment  deployment-1703  05b95207-1d74-4120-8018-14e4675b9872 186765614 2 2024-01-10 15:40:59 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2024-01-10 15:41:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-10 15:41:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004b607c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2024-01-10 15:41:01 +0000 UTC,LastTransitionTime:2024-01-10 15:41:01 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-54757ffd6c" is progressing.,LastUpdateTime:2024-01-10 15:41:01 +0000 UTC,LastTransitionTime:2024-01-10 15:40:59 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

  Jan 10 15:41:01.971: INFO: New ReplicaSet "test-recreate-deployment-54757ffd6c" of Deployment "test-recreate-deployment":
  &ReplicaSet{ObjectMeta:{test-recreate-deployment-54757ffd6c  deployment-1703  82e4f458-281b-448b-80be-8baf04b71300 186765610 1 2024-01-10 15:41:01 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 05b95207-1d74-4120-8018-14e4675b9872 0xc004242e47 0xc004242e48}] [] [{kube-controller-manager Update apps/v1 2024-01-10 15:41:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"05b95207-1d74-4120-8018-14e4675b9872\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-10 15:41:01 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 54757ffd6c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004242ee8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jan 10 15:41:01.971: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
  Jan 10 15:41:01.971: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6c99bf8bf6  deployment-1703  02d7107c-4d7b-4125-8cbf-96a1dea5e93d 186765599 2 2024-01-10 15:40:59 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 05b95207-1d74-4120-8018-14e4675b9872 0xc004242f57 0xc004242f58}] [] [{kube-controller-manager Update apps/v1 2024-01-10 15:41:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"05b95207-1d74-4120-8018-14e4675b9872\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-10 15:41:01 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6c99bf8bf6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004243018 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jan 10 15:41:01.984: INFO: Pod "test-recreate-deployment-54757ffd6c-fzqwm" is not available:
  &Pod{ObjectMeta:{test-recreate-deployment-54757ffd6c-fzqwm test-recreate-deployment-54757ffd6c- deployment-1703  cb4b8649-2e85-4537-81da-59a3144dd517 186765611 0 2024-01-10 15:41:01 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [{apps/v1 ReplicaSet test-recreate-deployment-54757ffd6c 82e4f458-281b-448b-80be-8baf04b71300 0xc0027bbca7 0xc0027bbca8}] [] [{kube-controller-manager Update v1 2024-01-10 15:41:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"82e4f458-281b-448b-80be-8baf04b71300\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-10 15:41:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qs6px,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qs6px,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 15:41:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 15:41:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 15:41:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 15:41:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.201,PodIP:,StartTime:2024-01-10 15:41:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 10 15:41:01.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-1703" for this suite. @ 01/10/24 15:41:02
• [2.623 seconds]
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:217
  STEP: Creating a kubernetes client @ 01/10/24 15:41:02.012
  Jan 10 15:41:02.012: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename emptydir @ 01/10/24 15:41:02.014
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:41:02.045
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:41:02.052
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 01/10/24 15:41:02.061
  STEP: Saw pod success @ 01/10/24 15:41:06.117
  Jan 10 15:41:06.127: INFO: Trying to get logs from node env1-test-worker-1 pod pod-b4fed50a-da7d-4bdc-b38c-d11e85bb0625 container test-container: <nil>
  STEP: delete the pod @ 01/10/24 15:41:06.145
  Jan 10 15:41:06.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1655" for this suite. @ 01/10/24 15:41:06.208
• [4.213 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should delete a collection of services [Conformance]
test/e2e/network/service.go:3548
  STEP: Creating a kubernetes client @ 01/10/24 15:41:06.231
  Jan 10 15:41:06.231: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename services @ 01/10/24 15:41:06.234
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:41:06.278
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:41:06.284
  STEP: creating a collection of services @ 01/10/24 15:41:06.29
  Jan 10 15:41:06.290: INFO: Creating e2e-svc-a-vwrlv
  Jan 10 15:41:06.312: INFO: Creating e2e-svc-b-qfr74
  Jan 10 15:41:06.346: INFO: Creating e2e-svc-c-x8hkk
  STEP: deleting service collection @ 01/10/24 15:41:06.39
  Jan 10 15:41:06.513: INFO: Collection of services has been deleted
  Jan 10 15:41:06.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-6917" for this suite. @ 01/10/24 15:41:06.529
• [0.324 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
test/e2e/apps/daemon_set.go:374
  STEP: Creating a kubernetes client @ 01/10/24 15:41:06.559
  Jan 10 15:41:06.559: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename daemonsets @ 01/10/24 15:41:06.562
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:41:06.619
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:41:06.625
  Jan 10 15:41:06.695: INFO: Creating simple daemon set daemon-set
  STEP: Check that daemon pods launch on every node of the cluster. @ 01/10/24 15:41:06.716
  Jan 10 15:41:06.786: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 15:41:06.786: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 15:41:06.786: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 15:41:06.804: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 10 15:41:06.804: INFO: Node env1-test-worker-0 is running 0 daemon pod, expected 1
  Jan 10 15:41:07.830: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 15:41:07.830: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 15:41:07.830: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 15:41:07.839: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 10 15:41:07.839: INFO: Node env1-test-worker-0 is running 0 daemon pod, expected 1
  Jan 10 15:41:08.820: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 15:41:08.820: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 15:41:08.820: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 15:41:08.831: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jan 10 15:41:08.831: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Update daemon pods image. @ 01/10/24 15:41:08.878
  STEP: Check that daemon pods images are updated. @ 01/10/24 15:41:08.929
  Jan 10 15:41:08.936: INFO: Wrong image for pod: daemon-set-9rhbf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jan 10 15:41:08.936: INFO: Wrong image for pod: daemon-set-q4fvb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jan 10 15:41:08.948: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 15:41:08.948: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 15:41:08.948: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 15:41:09.958: INFO: Wrong image for pod: daemon-set-q4fvb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jan 10 15:41:09.972: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 15:41:09.972: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 15:41:09.972: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 15:41:10.960: INFO: Wrong image for pod: daemon-set-q4fvb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jan 10 15:41:10.974: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 15:41:10.974: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 15:41:10.974: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 15:41:12.011: INFO: Pod daemon-set-l58r2 is not available
  Jan 10 15:41:12.011: INFO: Wrong image for pod: daemon-set-q4fvb. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jan 10 15:41:12.246: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 15:41:12.246: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 15:41:12.246: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 15:41:12.973: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 15:41:12.973: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 15:41:12.974: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 15:41:13.958: INFO: Pod daemon-set-fr8wz is not available
  Jan 10 15:41:13.976: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 15:41:13.976: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 15:41:13.976: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  STEP: Check that daemon pods are still running on every node of the cluster. @ 01/10/24 15:41:13.976
  Jan 10 15:41:13.989: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 15:41:13.989: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 15:41:13.989: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 15:41:14.002: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jan 10 15:41:14.002: INFO: Node env1-test-worker-1 is running 0 daemon pod, expected 1
  Jan 10 15:41:15.017: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 15:41:15.017: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 15:41:15.018: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 15:41:15.028: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jan 10 15:41:15.028: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 01/10/24 15:41:15.075
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1959, will wait for the garbage collector to delete the pods @ 01/10/24 15:41:15.075
  Jan 10 15:41:15.156: INFO: Deleting DaemonSet.extensions daemon-set took: 18.730508ms
  Jan 10 15:41:15.256: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.532927ms
  Jan 10 15:41:17.966: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 10 15:41:17.966: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jan 10 15:41:17.978: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"186765838"},"items":null}

  Jan 10 15:41:17.989: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"186765839"},"items":null}

  Jan 10 15:41:18.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-1959" for this suite. @ 01/10/24 15:41:18.044
• [11.501 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services should complete a service status lifecycle [Conformance]
test/e2e/network/service.go:3322
  STEP: Creating a kubernetes client @ 01/10/24 15:41:18.062
  Jan 10 15:41:18.062: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename services @ 01/10/24 15:41:18.064
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:41:18.106
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:41:18.112
  STEP: creating a Service @ 01/10/24 15:41:18.126
  STEP: watching for the Service to be added @ 01/10/24 15:41:18.15
  Jan 10 15:41:18.153: INFO: Found Service test-service-hvqfq in namespace services-2238 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
  Jan 10 15:41:18.153: INFO: Service test-service-hvqfq created
  STEP: Getting /status @ 01/10/24 15:41:18.153
  Jan 10 15:41:18.164: INFO: Service test-service-hvqfq has LoadBalancer: {[]}
  STEP: patching the ServiceStatus @ 01/10/24 15:41:18.164
  STEP: watching for the Service to be patched @ 01/10/24 15:41:18.178
  Jan 10 15:41:18.182: INFO: observed Service test-service-hvqfq in namespace services-2238 with annotations: map[] & LoadBalancer: {[]}
  Jan 10 15:41:18.182: INFO: Found Service test-service-hvqfq in namespace services-2238 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
  Jan 10 15:41:18.182: INFO: Service test-service-hvqfq has service status patched
  STEP: updating the ServiceStatus @ 01/10/24 15:41:18.182
  Jan 10 15:41:18.211: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Service to be updated @ 01/10/24 15:41:18.211
  Jan 10 15:41:18.215: INFO: Observed Service test-service-hvqfq in namespace services-2238 with annotations: map[] & Conditions: {[]}
  Jan 10 15:41:18.215: INFO: Observed event: &Service{ObjectMeta:{test-service-hvqfq  services-2238  334ee79d-8ff3-4be0-acc8-42cfb7bb1de5 186765846 0 2024-01-10 15:41:18 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2024-01-10 15:41:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2024-01-10 15:41:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.233.10.198,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.233.10.198],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
  Jan 10 15:41:18.215: INFO: Found Service test-service-hvqfq in namespace services-2238 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jan 10 15:41:18.215: INFO: Service test-service-hvqfq has service status updated
  STEP: patching the service @ 01/10/24 15:41:18.215
  STEP: watching for the Service to be patched @ 01/10/24 15:41:18.233
  Jan 10 15:41:18.237: INFO: observed Service test-service-hvqfq in namespace services-2238 with labels: map[test-service-static:true]
  Jan 10 15:41:18.237: INFO: observed Service test-service-hvqfq in namespace services-2238 with labels: map[test-service-static:true]
  Jan 10 15:41:18.238: INFO: observed Service test-service-hvqfq in namespace services-2238 with labels: map[test-service-static:true]
  Jan 10 15:41:18.238: INFO: Found Service test-service-hvqfq in namespace services-2238 with labels: map[test-service:patched test-service-static:true]
  Jan 10 15:41:18.238: INFO: Service test-service-hvqfq patched
  STEP: deleting the service @ 01/10/24 15:41:18.238
  STEP: watching for the Service to be deleted @ 01/10/24 15:41:18.277
  Jan 10 15:41:18.282: INFO: Observed event: ADDED
  Jan 10 15:41:18.282: INFO: Observed event: MODIFIED
  Jan 10 15:41:18.282: INFO: Observed event: MODIFIED
  Jan 10 15:41:18.282: INFO: Observed event: MODIFIED
  Jan 10 15:41:18.282: INFO: Found Service test-service-hvqfq in namespace services-2238 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
  Jan 10 15:41:18.282: INFO: Service test-service-hvqfq deleted
  Jan 10 15:41:18.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-2238" for this suite. @ 01/10/24 15:41:18.296
• [0.251 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] PodTemplates should replace a pod template [Conformance]
test/e2e/common/node/podtemplates.go:176
  STEP: Creating a kubernetes client @ 01/10/24 15:41:18.314
  Jan 10 15:41:18.314: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename podtemplate @ 01/10/24 15:41:18.318
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:41:18.367
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:41:18.374
  STEP: Create a pod template @ 01/10/24 15:41:18.381
  STEP: Replace a pod template @ 01/10/24 15:41:18.399
  Jan 10 15:41:18.422: INFO: Found updated podtemplate annotation: "true"

  Jan 10 15:41:18.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-393" for this suite. @ 01/10/24 15:41:18.433
• [0.156 seconds]
------------------------------
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]
test/e2e/apps/statefulset.go:981
  STEP: Creating a kubernetes client @ 01/10/24 15:41:18.47
  Jan 10 15:41:18.471: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename statefulset @ 01/10/24 15:41:18.472
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:41:18.522
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:41:18.53
  STEP: Creating service test in namespace statefulset-35 @ 01/10/24 15:41:18.538
  STEP: Creating statefulset ss in namespace statefulset-35 @ 01/10/24 15:41:18.572
  Jan 10 15:41:18.604: INFO: Found 0 stateful pods, waiting for 1
  Jan 10 15:41:28.613: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Patch Statefulset to include a label @ 01/10/24 15:41:28.632
  STEP: Getting /status @ 01/10/24 15:41:28.668
  Jan 10 15:41:28.678: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
  STEP: updating the StatefulSet Status @ 01/10/24 15:41:28.678
  Jan 10 15:41:28.705: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the statefulset status to be updated @ 01/10/24 15:41:28.705
  Jan 10 15:41:28.710: INFO: Observed &StatefulSet event: ADDED
  Jan 10 15:41:28.710: INFO: Found Statefulset ss in namespace statefulset-35 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jan 10 15:41:28.710: INFO: Statefulset ss has an updated status
  STEP: patching the Statefulset Status @ 01/10/24 15:41:28.71
  Jan 10 15:41:28.710: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Jan 10 15:41:28.732: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Statefulset status to be patched @ 01/10/24 15:41:28.732
  Jan 10 15:41:28.737: INFO: Observed &StatefulSet event: ADDED
  Jan 10 15:41:28.737: INFO: Deleting all statefulset in ns statefulset-35
  Jan 10 15:41:28.744: INFO: Scaling statefulset ss to 0
  Jan 10 15:41:38.832: INFO: Waiting for statefulset status.replicas updated to 0
  Jan 10 15:41:38.841: INFO: Deleting statefulset ss
  Jan 10 15:41:38.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-35" for this suite. @ 01/10/24 15:41:38.966
• [20.522 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods should get a host IP [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:205
  STEP: Creating a kubernetes client @ 01/10/24 15:41:38.994
  Jan 10 15:41:38.994: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename pods @ 01/10/24 15:41:38.996
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:41:39.063
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:41:39.069
  STEP: creating pod @ 01/10/24 15:41:39.075
  Jan 10 15:41:41.135: INFO: Pod pod-hostip-b4e0c30f-fd1c-4fb0-8559-820a5228d828 has hostIP: 10.61.1.201
  Jan 10 15:41:41.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9711" for this suite. @ 01/10/24 15:41:41.15
• [2.174 seconds]
------------------------------
SSSS
------------------------------
[sig-network] DNS should provide DNS for services  [Conformance]
test/e2e/network/dns.go:137
  STEP: Creating a kubernetes client @ 01/10/24 15:41:41.171
  Jan 10 15:41:41.171: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename dns @ 01/10/24 15:41:41.175
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:41:41.262
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:41:41.27
  STEP: Creating a test headless service @ 01/10/24 15:41:41.277
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4222.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4222.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4222.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4222.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4222.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4222.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4222.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4222.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4222.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4222.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4222.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4222.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 94.63.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.63.94_udp@PTR;check="$$(dig +tcp +noall +answer +search 94.63.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.63.94_tcp@PTR;sleep 1; done
   @ 01/10/24 15:41:41.331
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4222.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4222.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4222.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4222.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4222.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4222.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4222.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4222.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4222.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4222.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4222.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4222.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 94.63.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.63.94_udp@PTR;check="$$(dig +tcp +noall +answer +search 94.63.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.63.94_tcp@PTR;sleep 1; done
   @ 01/10/24 15:41:41.331
  STEP: creating a pod to probe DNS @ 01/10/24 15:41:41.331
  STEP: submitting the pod to kubernetes @ 01/10/24 15:41:41.331
  STEP: retrieving the pod @ 01/10/24 15:41:43.399
  STEP: looking for the results for each expected name from probers @ 01/10/24 15:41:43.407
  Jan 10 15:41:43.417: INFO: Unable to read wheezy_udp@dns-test-service.dns-4222.svc.cluster.local from pod dns-4222/dns-test-fd990c3b-8b6e-4e31-9045-b587b169a62b: the server could not find the requested resource (get pods dns-test-fd990c3b-8b6e-4e31-9045-b587b169a62b)
  Jan 10 15:41:43.425: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4222.svc.cluster.local from pod dns-4222/dns-test-fd990c3b-8b6e-4e31-9045-b587b169a62b: the server could not find the requested resource (get pods dns-test-fd990c3b-8b6e-4e31-9045-b587b169a62b)
  Jan 10 15:41:43.433: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4222.svc.cluster.local from pod dns-4222/dns-test-fd990c3b-8b6e-4e31-9045-b587b169a62b: the server could not find the requested resource (get pods dns-test-fd990c3b-8b6e-4e31-9045-b587b169a62b)
  Jan 10 15:41:43.446: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4222.svc.cluster.local from pod dns-4222/dns-test-fd990c3b-8b6e-4e31-9045-b587b169a62b: the server could not find the requested resource (get pods dns-test-fd990c3b-8b6e-4e31-9045-b587b169a62b)
  Jan 10 15:41:43.497: INFO: Unable to read jessie_udp@dns-test-service.dns-4222.svc.cluster.local from pod dns-4222/dns-test-fd990c3b-8b6e-4e31-9045-b587b169a62b: the server could not find the requested resource (get pods dns-test-fd990c3b-8b6e-4e31-9045-b587b169a62b)
  Jan 10 15:41:43.511: INFO: Unable to read jessie_tcp@dns-test-service.dns-4222.svc.cluster.local from pod dns-4222/dns-test-fd990c3b-8b6e-4e31-9045-b587b169a62b: the server could not find the requested resource (get pods dns-test-fd990c3b-8b6e-4e31-9045-b587b169a62b)
  Jan 10 15:41:43.521: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4222.svc.cluster.local from pod dns-4222/dns-test-fd990c3b-8b6e-4e31-9045-b587b169a62b: the server could not find the requested resource (get pods dns-test-fd990c3b-8b6e-4e31-9045-b587b169a62b)
  Jan 10 15:41:43.530: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4222.svc.cluster.local from pod dns-4222/dns-test-fd990c3b-8b6e-4e31-9045-b587b169a62b: the server could not find the requested resource (get pods dns-test-fd990c3b-8b6e-4e31-9045-b587b169a62b)
  Jan 10 15:41:43.576: INFO: Lookups using dns-4222/dns-test-fd990c3b-8b6e-4e31-9045-b587b169a62b failed for: [wheezy_udp@dns-test-service.dns-4222.svc.cluster.local wheezy_tcp@dns-test-service.dns-4222.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4222.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4222.svc.cluster.local jessie_udp@dns-test-service.dns-4222.svc.cluster.local jessie_tcp@dns-test-service.dns-4222.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4222.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4222.svc.cluster.local]

  Jan 10 15:41:48.767: INFO: DNS probes using dns-4222/dns-test-fd990c3b-8b6e-4e31-9045-b587b169a62b succeeded

  Jan 10 15:41:48.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 01/10/24 15:41:48.784
  STEP: deleting the test service @ 01/10/24 15:41:48.845
  STEP: deleting the test headless service @ 01/10/24 15:41:49.004
  STEP: Destroying namespace "dns-4222" for this suite. @ 01/10/24 15:41:49.045
• [7.895 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:124
  STEP: Creating a kubernetes client @ 01/10/24 15:41:49.067
  Jan 10 15:41:49.067: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename pod-network-test @ 01/10/24 15:41:49.068
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:41:49.121
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:41:49.13
  STEP: Performing setup for networking test in namespace pod-network-test-6861 @ 01/10/24 15:41:49.138
  STEP: creating a selector @ 01/10/24 15:41:49.14
  STEP: Creating the service pods in kubernetes @ 01/10/24 15:41:49.141
  Jan 10 15:41:49.141: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 01/10/24 15:42:11.385
  Jan 10 15:42:13.463: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
  Jan 10 15:42:13.463: INFO: Going to poll 10.233.67.215 on port 8081 at least 0 times, with a maximum of 34 tries before failing
  Jan 10 15:42:13.474: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.67.215 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6861 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 10 15:42:13.474: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  Jan 10 15:42:13.475: INFO: ExecWithOptions: Clientset creation
  Jan 10 15:42:13.476: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-6861/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.67.215+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jan 10 15:42:14.675: INFO: Found all 1 expected endpoints: [netserver-0]
  Jan 10 15:42:14.675: INFO: Going to poll 10.233.68.66 on port 8081 at least 0 times, with a maximum of 34 tries before failing
  Jan 10 15:42:14.687: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.68.66 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6861 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 10 15:42:14.687: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  Jan 10 15:42:14.689: INFO: ExecWithOptions: Clientset creation
  Jan 10 15:42:14.689: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-6861/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.68.66+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jan 10 15:42:15.830: INFO: Found all 1 expected endpoints: [netserver-1]
  Jan 10 15:42:15.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-6861" for this suite. @ 01/10/24 15:42:15.849
• [26.798 seconds]
------------------------------
SS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:135
  STEP: Creating a kubernetes client @ 01/10/24 15:42:15.866
  Jan 10 15:42:15.866: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 01/10/24 15:42:15.868
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:42:15.927
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:42:15.933
  STEP: create the container to handle the HTTPGet hook request. @ 01/10/24 15:42:15.956
  STEP: create the pod with lifecycle hook @ 01/10/24 15:42:18.022
  STEP: check poststart hook @ 01/10/24 15:42:20.085
  STEP: delete the pod with lifecycle hook @ 01/10/24 15:42:20.104
  Jan 10 15:42:22.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-1504" for this suite. @ 01/10/24 15:42:22.162
• [6.318 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:135
  STEP: Creating a kubernetes client @ 01/10/24 15:42:22.199
  Jan 10 15:42:22.199: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename kubelet-test @ 01/10/24 15:42:22.2
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:42:22.255
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:42:22.261
  Jan 10 15:42:22.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-6365" for this suite. @ 01/10/24 15:42:22.347
• [0.171 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount projected service account token [Conformance]
test/e2e/auth/service_accounts.go:275
  STEP: Creating a kubernetes client @ 01/10/24 15:42:22.372
  Jan 10 15:42:22.372: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename svcaccounts @ 01/10/24 15:42:22.375
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:42:22.425
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:42:22.432
  STEP: Creating a pod to test service account token:  @ 01/10/24 15:42:22.44
  STEP: Saw pod success @ 01/10/24 15:42:26.514
  Jan 10 15:42:26.521: INFO: Trying to get logs from node env1-test-worker-1 pod test-pod-faebc9a6-0a70-4601-afec-68150912015e container agnhost-container: <nil>
  STEP: delete the pod @ 01/10/24 15:42:26.54
  Jan 10 15:42:26.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-1998" for this suite. @ 01/10/24 15:42:26.61
• [4.259 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:375
  STEP: Creating a kubernetes client @ 01/10/24 15:42:26.641
  Jan 10 15:42:26.641: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename projected @ 01/10/24 15:42:26.644
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:42:26.69
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:42:26.701
  STEP: Creating configMap with name projected-configmap-test-volume-9206b166-8030-4868-93bd-b724a0820f2b @ 01/10/24 15:42:26.708
  STEP: Creating a pod to test consume configMaps @ 01/10/24 15:42:26.718
  STEP: Saw pod success @ 01/10/24 15:42:30.796
  Jan 10 15:42:30.804: INFO: Trying to get logs from node env1-test-worker-1 pod pod-projected-configmaps-87932e19-164b-4be4-94d9-cb925519a457 container projected-configmap-volume-test: <nil>
  STEP: delete the pod @ 01/10/24 15:42:30.822
  Jan 10 15:42:30.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3082" for this suite. @ 01/10/24 15:42:30.875
• [4.258 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]
test/e2e/scheduling/predicates.go:467
  STEP: Creating a kubernetes client @ 01/10/24 15:42:30.902
  Jan 10 15:42:30.902: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename sched-pred @ 01/10/24 15:42:30.904
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:42:30.946
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:42:30.953
  Jan 10 15:42:30.959: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jan 10 15:42:30.992: INFO: Waiting for terminating namespaces to be deleted...
  Jan 10 15:42:31.001: INFO: 
  Logging pods the apiserver thinks is on node env1-test-worker-0 before test
  Jan 10 15:42:31.034: INFO: filebeat-filebeat-q5bzw from filebeat started at 2024-01-10 01:14:23 +0000 UTC (1 container statuses recorded)
  Jan 10 15:42:31.034: INFO: 	Container filebeat ready: false, restart count 0
  Jan 10 15:42:31.034: INFO: kube-flannel-r8g5h from kube-system started at 2024-01-09 16:24:44 +0000 UTC (1 container statuses recorded)
  Jan 10 15:42:31.034: INFO: 	Container kube-flannel ready: true, restart count 0
  Jan 10 15:42:31.034: INFO: kube-proxy-445hs from kube-system started at 2024-01-10 09:13:13 +0000 UTC (1 container statuses recorded)
  Jan 10 15:42:31.034: INFO: 	Container kube-proxy ready: true, restart count 0
  Jan 10 15:42:31.034: INFO: metrics-server-6b7574f5b-jmbtm from kube-system started at 2024-01-09 16:32:13 +0000 UTC (1 container statuses recorded)
  Jan 10 15:42:31.034: INFO: 	Container metrics-server ready: true, restart count 0
  Jan 10 15:42:31.034: INFO: nginx-proxy-env1-test-worker-0 from kube-system started at 2024-01-09 16:31:48 +0000 UTC (1 container statuses recorded)
  Jan 10 15:42:31.034: INFO: 	Container nginx-proxy ready: true, restart count 0
  Jan 10 15:42:31.034: INFO: nodelocaldns-vkvkp from kube-system started at 2024-01-09 15:52:20 +0000 UTC (1 container statuses recorded)
  Jan 10 15:42:31.034: INFO: 	Container node-cache ready: true, restart count 0
  Jan 10 15:42:31.034: INFO: vsphere-csi-node-qkfth from kube-system started at 2024-01-10 09:21:38 +0000 UTC (3 container statuses recorded)
  Jan 10 15:42:31.034: INFO: 	Container liveness-probe ready: true, restart count 0
  Jan 10 15:42:31.034: INFO: 	Container node-driver-registrar ready: true, restart count 0
  Jan 10 15:42:31.034: INFO: 	Container vsphere-csi-node ready: true, restart count 0
  Jan 10 15:42:31.034: INFO: prometheus-kube-prometheus-operator-5f847644d6-gkll8 from prometheus started at 2024-01-10 03:09:01 +0000 UTC (1 container statuses recorded)
  Jan 10 15:42:31.034: INFO: 	Container kube-prometheus-stack ready: true, restart count 0
  Jan 10 15:42:31.034: INFO: prometheus-kube-state-metrics-f8b6b59f-g2nrl from prometheus started at 2024-01-10 03:09:01 +0000 UTC (1 container statuses recorded)
  Jan 10 15:42:31.034: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Jan 10 15:42:31.034: INFO: prometheus-prometheus-kube-prometheus-prometheus-0 from prometheus started at 2024-01-10 03:09:07 +0000 UTC (3 container statuses recorded)
  Jan 10 15:42:31.034: INFO: 	Container config-reloader ready: true, restart count 0
  Jan 10 15:42:31.034: INFO: 	Container prometheus ready: false, restart count 152
  Jan 10 15:42:31.034: INFO: 	Container thanos-sidecar ready: true, restart count 0
  Jan 10 15:42:31.034: INFO: prometheus-prometheus-node-exporter-4nknm from prometheus started at 2024-01-10 03:09:01 +0000 UTC (1 container statuses recorded)
  Jan 10 15:42:31.034: INFO: 	Container node-exporter ready: true, restart count 0
  Jan 10 15:42:31.034: INFO: thanos-query-6f697d54b8-sp4fg from prometheus started at 2024-01-10 13:06:38 +0000 UTC (1 container statuses recorded)
  Jan 10 15:42:31.034: INFO: 	Container query ready: true, restart count 0
  Jan 10 15:42:31.034: INFO: sonobuoy-systemd-logs-daemon-set-187fce9e0ddc499b-9m7s6 from sonobuoy started at 2024-01-10 15:26:38 +0000 UTC (2 container statuses recorded)
  Jan 10 15:42:31.034: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jan 10 15:42:31.034: INFO: 	Container systemd-logs ready: true, restart count 0
  Jan 10 15:42:31.034: INFO: traefik-ingress-g4tjs from traefik-ingress started at 2024-01-10 14:39:42 +0000 UTC (1 container statuses recorded)
  Jan 10 15:42:31.034: INFO: 	Container traefik-ingress ready: true, restart count 0
  Jan 10 15:42:31.034: INFO: velero-794b84894f-nwdwf from velero started at 2024-01-10 13:26:26 +0000 UTC (1 container statuses recorded)
  Jan 10 15:42:31.035: INFO: 	Container velero ready: true, restart count 0
  Jan 10 15:42:31.035: INFO: 
  Logging pods the apiserver thinks is on node env1-test-worker-1 before test
  Jan 10 15:42:31.060: INFO: filebeat-filebeat-qtfxn from filebeat started at 2024-01-10 13:26:52 +0000 UTC (1 container statuses recorded)
  Jan 10 15:42:31.060: INFO: 	Container filebeat ready: false, restart count 0
  Jan 10 15:42:31.060: INFO: kube-flannel-jxf5s from kube-system started at 2024-01-09 16:24:09 +0000 UTC (1 container statuses recorded)
  Jan 10 15:42:31.060: INFO: 	Container kube-flannel ready: true, restart count 0
  Jan 10 15:42:31.060: INFO: kube-proxy-78tcd from kube-system started at 2024-01-10 09:13:13 +0000 UTC (1 container statuses recorded)
  Jan 10 15:42:31.060: INFO: 	Container kube-proxy ready: true, restart count 0
  Jan 10 15:42:31.060: INFO: nginx-proxy-env1-test-worker-1 from kube-system started at 2024-01-09 16:38:31 +0000 UTC (1 container statuses recorded)
  Jan 10 15:42:31.060: INFO: 	Container nginx-proxy ready: true, restart count 0
  Jan 10 15:42:31.060: INFO: nodelocaldns-7qx4w from kube-system started at 2024-01-09 15:52:12 +0000 UTC (1 container statuses recorded)
  Jan 10 15:42:31.060: INFO: 	Container node-cache ready: true, restart count 0
  Jan 10 15:42:31.060: INFO: vsphere-csi-node-lr59t from kube-system started at 2024-01-10 09:21:38 +0000 UTC (3 container statuses recorded)
  Jan 10 15:42:31.060: INFO: 	Container liveness-probe ready: true, restart count 0
  Jan 10 15:42:31.061: INFO: 	Container node-driver-registrar ready: true, restart count 0
  Jan 10 15:42:31.061: INFO: 	Container vsphere-csi-node ready: true, restart count 0
  Jan 10 15:42:31.061: INFO: prometheus-prometheus-node-exporter-2wr55 from prometheus started at 2024-01-10 13:26:52 +0000 UTC (1 container statuses recorded)
  Jan 10 15:42:31.061: INFO: 	Container node-exporter ready: true, restart count 0
  Jan 10 15:42:31.061: INFO: sonobuoy from sonobuoy started at 2024-01-10 15:26:36 +0000 UTC (1 container statuses recorded)
  Jan 10 15:42:31.061: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jan 10 15:42:31.061: INFO: sonobuoy-e2e-job-b46f6697883e4f52 from sonobuoy started at 2024-01-10 15:26:37 +0000 UTC (2 container statuses recorded)
  Jan 10 15:42:31.061: INFO: 	Container e2e ready: true, restart count 0
  Jan 10 15:42:31.061: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jan 10 15:42:31.061: INFO: sonobuoy-systemd-logs-daemon-set-187fce9e0ddc499b-nntgv from sonobuoy started at 2024-01-10 15:26:38 +0000 UTC (2 container statuses recorded)
  Jan 10 15:42:31.061: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jan 10 15:42:31.061: INFO: 	Container systemd-logs ready: true, restart count 0
  Jan 10 15:42:31.061: INFO: traefik-ingress-mzrnh from traefik-ingress started at 2024-01-10 13:26:52 +0000 UTC (1 container statuses recorded)
  Jan 10 15:42:31.061: INFO: 	Container traefik-ingress ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 01/10/24 15:42:31.061
  STEP: Explicitly delete pod here to free the resource it takes. @ 01/10/24 15:42:33.112
  STEP: Trying to apply a random label on the found node. @ 01/10/24 15:42:33.152
  STEP: verifying the node has the label kubernetes.io/e2e-c22db72b-f8e0-4133-ae11-e61480f9baf5 42 @ 01/10/24 15:42:33.189
  STEP: Trying to relaunch the pod, now with labels. @ 01/10/24 15:42:33.199
  STEP: removing the label kubernetes.io/e2e-c22db72b-f8e0-4133-ae11-e61480f9baf5 off the node env1-test-worker-1 @ 01/10/24 15:42:35.251
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-c22db72b-f8e0-4133-ae11-e61480f9baf5 @ 01/10/24 15:42:35.323
  Jan 10 15:42:35.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-8870" for this suite. @ 01/10/24 15:42:35.348
• [4.462 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
test/e2e/common/node/expansion.go:155
  STEP: Creating a kubernetes client @ 01/10/24 15:42:35.368
  Jan 10 15:42:35.368: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename var-expansion @ 01/10/24 15:42:35.369
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:42:35.417
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:42:35.425
  Jan 10 15:42:37.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jan 10 15:42:37.487: INFO: Deleting pod "var-expansion-563ec56f-6769-4aab-baf1-15f610cb4a94" in namespace "var-expansion-230"
  Jan 10 15:42:37.502: INFO: Wait up to 5m0s for pod "var-expansion-563ec56f-6769-4aab-baf1-15f610cb4a94" to be fully deleted
  STEP: Destroying namespace "var-expansion-230" for this suite. @ 01/10/24 15:42:39.527
• [4.173 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
test/e2e/node/taints.go:450
  STEP: Creating a kubernetes client @ 01/10/24 15:42:39.542
  Jan 10 15:42:39.542: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename taint-multiple-pods @ 01/10/24 15:42:39.545
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:42:39.601
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:42:39.608
  Jan 10 15:42:39.616: INFO: Waiting up to 1m0s for all nodes to be ready
  Jan 10 15:43:39.727: INFO: Waiting for terminating namespaces to be deleted...
  Jan 10 15:43:39.737: INFO: Starting informer...
  STEP: Starting pods... @ 01/10/24 15:43:39.737
  Jan 10 15:43:40.002: INFO: Pod1 is running on env1-test-worker-1. Tainting Node
  Jan 10 15:43:42.254: INFO: Pod2 is running on env1-test-worker-1. Tainting Node
  STEP: Trying to apply a taint on the Node @ 01/10/24 15:43:42.254
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 01/10/24 15:43:42.297
  STEP: Waiting for Pod1 and Pod2 to be deleted @ 01/10/24 15:43:42.312
  Jan 10 15:43:47.901: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
  Jan 10 15:44:07.979: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
  Jan 10 15:44:07.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 01/10/24 15:44:08.035
  STEP: Destroying namespace "taint-multiple-pods-8968" for this suite. @ 01/10/24 15:44:08.045
• [88.517 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:152
  STEP: Creating a kubernetes client @ 01/10/24 15:44:08.064
  Jan 10 15:44:08.065: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 01/10/24 15:44:08.067
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:44:08.183
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:44:08.19
  STEP: create the container to handle the HTTPGet hook request. @ 01/10/24 15:44:08.213
  STEP: create the pod with lifecycle hook @ 01/10/24 15:44:10.281
  STEP: delete the pod with lifecycle hook @ 01/10/24 15:44:12.35
  STEP: check prestop hook @ 01/10/24 15:44:14.399
  Jan 10 15:44:14.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-3751" for this suite. @ 01/10/24 15:44:14.492
• [6.471 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:137
  STEP: Creating a kubernetes client @ 01/10/24 15:44:14.536
  Jan 10 15:44:14.537: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename emptydir @ 01/10/24 15:44:14.538
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:44:14.594
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:44:14.603
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 01/10/24 15:44:14.617
  STEP: Saw pod success @ 01/10/24 15:44:18.703
  Jan 10 15:44:18.712: INFO: Trying to get logs from node env1-test-worker-1 pod pod-9eed7657-9647-4fe7-8584-1d4b84dd00f6 container test-container: <nil>
  STEP: delete the pod @ 01/10/24 15:44:18.732
  Jan 10 15:44:18.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7892" for this suite. @ 01/10/24 15:44:18.789
• [4.287 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
test/e2e/network/endpointslice.go:104
  STEP: Creating a kubernetes client @ 01/10/24 15:44:18.831
  Jan 10 15:44:18.831: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename endpointslice @ 01/10/24 15:44:18.835
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:44:18.883
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:44:18.891
  Jan 10 15:44:21.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-7480" for this suite. @ 01/10/24 15:44:21.059
• [2.251 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]
test/e2e/apps/statefulset.go:852
  STEP: Creating a kubernetes client @ 01/10/24 15:44:21.091
  Jan 10 15:44:21.091: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename statefulset @ 01/10/24 15:44:21.093
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:44:21.144
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:44:21.16
  STEP: Creating service test in namespace statefulset-6648 @ 01/10/24 15:44:21.179
  STEP: Creating statefulset ss in namespace statefulset-6648 @ 01/10/24 15:44:21.199
  Jan 10 15:44:21.223: INFO: Found 0 stateful pods, waiting for 1
  Jan 10 15:44:31.239: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: getting scale subresource @ 01/10/24 15:44:31.265
  STEP: updating a scale subresource @ 01/10/24 15:44:31.28
  STEP: verifying the statefulset Spec.Replicas was modified @ 01/10/24 15:44:31.299
  STEP: Patch a scale subresource @ 01/10/24 15:44:31.313
  STEP: verifying the statefulset Spec.Replicas was modified @ 01/10/24 15:44:31.349
  Jan 10 15:44:31.376: INFO: Deleting all statefulset in ns statefulset-6648
  Jan 10 15:44:31.386: INFO: Scaling statefulset ss to 0
  Jan 10 15:44:41.493: INFO: Waiting for statefulset status.replicas updated to 0
  Jan 10 15:44:41.501: INFO: Deleting statefulset ss
  Jan 10 15:44:41.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-6648" for this suite. @ 01/10/24 15:44:41.556
• [20.483 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet_etc_hosts.go:64
  STEP: Creating a kubernetes client @ 01/10/24 15:44:41.579
  Jan 10 15:44:41.580: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts @ 01/10/24 15:44:41.582
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:44:41.624
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:44:41.629
  STEP: Setting up the test @ 01/10/24 15:44:41.635
  STEP: Creating hostNetwork=false pod @ 01/10/24 15:44:41.635
  STEP: Creating hostNetwork=true pod @ 01/10/24 15:44:43.713
  STEP: Running the test @ 01/10/24 15:44:45.773
  STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false @ 01/10/24 15:44:45.773
  Jan 10 15:44:45.773: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2577 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 10 15:44:45.773: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  Jan 10 15:44:45.775: INFO: ExecWithOptions: Clientset creation
  Jan 10 15:44:45.775: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2577/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jan 10 15:44:45.929: INFO: Exec stderr: ""
  Jan 10 15:44:45.929: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2577 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 10 15:44:45.929: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  Jan 10 15:44:45.930: INFO: ExecWithOptions: Clientset creation
  Jan 10 15:44:45.931: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2577/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jan 10 15:44:46.078: INFO: Exec stderr: ""
  Jan 10 15:44:46.078: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2577 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 10 15:44:46.078: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  Jan 10 15:44:46.079: INFO: ExecWithOptions: Clientset creation
  Jan 10 15:44:46.079: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2577/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jan 10 15:44:46.227: INFO: Exec stderr: ""
  Jan 10 15:44:46.227: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2577 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 10 15:44:46.227: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  Jan 10 15:44:46.228: INFO: ExecWithOptions: Clientset creation
  Jan 10 15:44:46.229: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2577/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jan 10 15:44:46.369: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount @ 01/10/24 15:44:46.369
  Jan 10 15:44:46.369: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2577 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 10 15:44:46.370: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  Jan 10 15:44:46.371: INFO: ExecWithOptions: Clientset creation
  Jan 10 15:44:46.371: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2577/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Jan 10 15:44:46.540: INFO: Exec stderr: ""
  Jan 10 15:44:46.540: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2577 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 10 15:44:46.540: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  Jan 10 15:44:46.542: INFO: ExecWithOptions: Clientset creation
  Jan 10 15:44:46.542: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2577/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Jan 10 15:44:46.701: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true @ 01/10/24 15:44:46.702
  Jan 10 15:44:46.702: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2577 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 10 15:44:46.702: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  Jan 10 15:44:46.704: INFO: ExecWithOptions: Clientset creation
  Jan 10 15:44:46.705: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2577/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jan 10 15:44:46.867: INFO: Exec stderr: ""
  Jan 10 15:44:46.867: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2577 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 10 15:44:46.867: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  Jan 10 15:44:46.869: INFO: ExecWithOptions: Clientset creation
  Jan 10 15:44:46.869: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2577/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jan 10 15:44:47.010: INFO: Exec stderr: ""
  Jan 10 15:44:47.010: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2577 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 10 15:44:47.010: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  Jan 10 15:44:47.011: INFO: ExecWithOptions: Clientset creation
  Jan 10 15:44:47.011: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2577/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jan 10 15:44:47.154: INFO: Exec stderr: ""
  Jan 10 15:44:47.154: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2577 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 10 15:44:47.154: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  Jan 10 15:44:47.156: INFO: ExecWithOptions: Clientset creation
  Jan 10 15:44:47.156: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2577/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jan 10 15:44:47.308: INFO: Exec stderr: ""
  Jan 10 15:44:47.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "e2e-kubelet-etc-hosts-2577" for this suite. @ 01/10/24 15:44:47.326
• [5.783 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:123
  STEP: Creating a kubernetes client @ 01/10/24 15:44:47.367
  Jan 10 15:44:47.367: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename sysctl @ 01/10/24 15:44:47.369
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:44:47.44
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:44:47.446
  STEP: Creating a pod with one valid and two invalid sysctls @ 01/10/24 15:44:47.454
  Jan 10 15:44:47.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-2234" for this suite. @ 01/10/24 15:44:47.489
• [0.157 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:153
  STEP: Creating a kubernetes client @ 01/10/24 15:44:47.525
  Jan 10 15:44:47.525: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename crd-publish-openapi @ 01/10/24 15:44:47.527
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:44:47.585
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:44:47.59
  Jan 10 15:44:47.597: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 01/10/24 15:44:57.391
  Jan 10 15:44:57.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=crd-publish-openapi-9533 --namespace=crd-publish-openapi-9533 create -f -'
  Jan 10 15:44:59.562: INFO: stderr: ""
  Jan 10 15:44:59.562: INFO: stdout: "e2e-test-crd-publish-openapi-2807-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Jan 10 15:44:59.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=crd-publish-openapi-9533 --namespace=crd-publish-openapi-9533 delete e2e-test-crd-publish-openapi-2807-crds test-cr'
  Jan 10 15:44:59.820: INFO: stderr: ""
  Jan 10 15:44:59.820: INFO: stdout: "e2e-test-crd-publish-openapi-2807-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  Jan 10 15:44:59.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=crd-publish-openapi-9533 --namespace=crd-publish-openapi-9533 apply -f -'
  Jan 10 15:45:02.110: INFO: stderr: ""
  Jan 10 15:45:02.110: INFO: stdout: "e2e-test-crd-publish-openapi-2807-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Jan 10 15:45:02.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=crd-publish-openapi-9533 --namespace=crd-publish-openapi-9533 delete e2e-test-crd-publish-openapi-2807-crds test-cr'
  Jan 10 15:45:02.278: INFO: stderr: ""
  Jan 10 15:45:02.279: INFO: stdout: "e2e-test-crd-publish-openapi-2807-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR without validation schema @ 01/10/24 15:45:02.279
  Jan 10 15:45:02.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=crd-publish-openapi-9533 explain e2e-test-crd-publish-openapi-2807-crds'
  Jan 10 15:45:03.845: INFO: stderr: ""
  Jan 10 15:45:03.846: INFO: stdout: "GROUP:      crd-publish-openapi-test-empty.example.com\nKIND:       e2e-test-crd-publish-openapi-2807-crd\nVERSION:    v1\n\nDESCRIPTION:\n    <empty>\nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n\n"
  Jan 10 15:45:07.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-9533" for this suite. @ 01/10/24 15:45:07.531
• [20.020 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should adopt matching pods on creation [Conformance]
test/e2e/apps/rc.go:94
  STEP: Creating a kubernetes client @ 01/10/24 15:45:07.552
  Jan 10 15:45:07.553: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename replication-controller @ 01/10/24 15:45:07.555
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:45:07.596
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:45:07.601
  STEP: Given a Pod with a 'name' label pod-adoption is created @ 01/10/24 15:45:07.607
  STEP: When a replication controller with a matching selector is created @ 01/10/24 15:45:09.656
  STEP: Then the orphan pod is adopted @ 01/10/24 15:45:09.669
  Jan 10 15:45:10.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-1158" for this suite. @ 01/10/24 15:45:10.697
• [3.160 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]
test/e2e/network/proxy.go:380
  STEP: Creating a kubernetes client @ 01/10/24 15:45:10.724
  Jan 10 15:45:10.725: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename proxy @ 01/10/24 15:45:10.727
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:45:10.759
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:45:10.766
  Jan 10 15:45:10.774: INFO: Creating pod...
  Jan 10 15:45:12.812: INFO: Creating service...
  Jan 10 15:45:12.868: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8536/pods/agnhost/proxy?method=DELETE
  Jan 10 15:45:12.890: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jan 10 15:45:12.890: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8536/pods/agnhost/proxy?method=OPTIONS
  Jan 10 15:45:12.901: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jan 10 15:45:12.901: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8536/pods/agnhost/proxy?method=PATCH
  Jan 10 15:45:12.914: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jan 10 15:45:12.916: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8536/pods/agnhost/proxy?method=POST
  Jan 10 15:45:12.932: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jan 10 15:45:12.932: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8536/pods/agnhost/proxy?method=PUT
  Jan 10 15:45:12.944: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jan 10 15:45:12.946: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8536/services/e2e-proxy-test-service/proxy?method=DELETE
  Jan 10 15:45:12.983: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jan 10 15:45:12.983: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8536/services/e2e-proxy-test-service/proxy?method=OPTIONS
  Jan 10 15:45:13.005: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jan 10 15:45:13.005: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8536/services/e2e-proxy-test-service/proxy?method=PATCH
  Jan 10 15:45:13.027: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jan 10 15:45:13.027: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8536/services/e2e-proxy-test-service/proxy?method=POST
  Jan 10 15:45:13.042: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jan 10 15:45:13.042: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8536/services/e2e-proxy-test-service/proxy?method=PUT
  Jan 10 15:45:13.057: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jan 10 15:45:13.059: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8536/pods/agnhost/proxy?method=GET
  Jan 10 15:45:13.066: INFO: http.Client request:GET StatusCode:301
  Jan 10 15:45:13.066: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8536/services/e2e-proxy-test-service/proxy?method=GET
  Jan 10 15:45:13.079: INFO: http.Client request:GET StatusCode:301
  Jan 10 15:45:13.080: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8536/pods/agnhost/proxy?method=HEAD
  Jan 10 15:45:13.087: INFO: http.Client request:HEAD StatusCode:301
  Jan 10 15:45:13.087: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-8536/services/e2e-proxy-test-service/proxy?method=HEAD
  Jan 10 15:45:13.099: INFO: http.Client request:HEAD StatusCode:301
  Jan 10 15:45:13.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-8536" for this suite. @ 01/10/24 15:45:13.117
• [2.412 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:163
  STEP: Creating a kubernetes client @ 01/10/24 15:45:13.158
  Jan 10 15:45:13.158: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename downward-api @ 01/10/24 15:45:13.16
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:45:13.217
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:45:13.224
  STEP: Creating the pod @ 01/10/24 15:45:13.234
  Jan 10 15:45:15.852: INFO: Successfully updated pod "annotationupdate67fd6306-8089-4cfc-9a5b-f8c23938ab2c"
  Jan 10 15:45:17.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6403" for this suite. @ 01/10/24 15:45:17.916
• [4.775 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:89
  STEP: Creating a kubernetes client @ 01/10/24 15:45:17.939
  Jan 10 15:45:17.939: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename configmap @ 01/10/24 15:45:17.94
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:45:17.986
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:45:17.993
  STEP: Creating configMap with name configmap-test-volume-map-2817b5ca-cb31-430b-aa38-70a2ecc56933 @ 01/10/24 15:45:18.002
  STEP: Creating a pod to test consume configMaps @ 01/10/24 15:45:18.013
  STEP: Saw pod success @ 01/10/24 15:45:22.056
  Jan 10 15:45:22.066: INFO: Trying to get logs from node env1-test-worker-1 pod pod-configmaps-60e60d94-c07d-48c1-bea4-48e2925d0896 container agnhost-container: <nil>
  STEP: delete the pod @ 01/10/24 15:45:22.09
  Jan 10 15:45:22.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3563" for this suite. @ 01/10/24 15:45:22.164
• [4.254 seconds]
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:195
  STEP: Creating a kubernetes client @ 01/10/24 15:45:22.197
  Jan 10 15:45:22.197: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename container-runtime @ 01/10/24 15:45:22.2
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:45:22.245
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:45:22.251
  STEP: create the container @ 01/10/24 15:45:22.257
  W0110 15:45:22.276915      23 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 01/10/24 15:45:22.277
  STEP: get the container status @ 01/10/24 15:45:26.348
  STEP: the container should be terminated @ 01/10/24 15:45:26.365
  STEP: the termination message should be set @ 01/10/24 15:45:26.365
  Jan 10 15:45:26.365: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 01/10/24 15:45:26.366
  Jan 10 15:45:26.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-9609" for this suite. @ 01/10/24 15:45:26.438
• [4.256 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2187
  STEP: Creating a kubernetes client @ 01/10/24 15:45:26.455
  Jan 10 15:45:26.455: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename services @ 01/10/24 15:45:26.457
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:45:26.495
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:45:26.509
  STEP: creating service in namespace services-6266 @ 01/10/24 15:45:26.517
  STEP: creating service affinity-clusterip-transition in namespace services-6266 @ 01/10/24 15:45:26.517
  STEP: creating replication controller affinity-clusterip-transition in namespace services-6266 @ 01/10/24 15:45:26.545
  I0110 15:45:26.561491      23 runners.go:194] Created replication controller with name: affinity-clusterip-transition, namespace: services-6266, replica count: 3
  I0110 15:45:29.613696      23 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jan 10 15:45:29.633: INFO: Creating new exec pod
  Jan 10 15:45:32.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=services-6266 exec execpod-affinitylnd6n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
  Jan 10 15:45:33.053: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
  Jan 10 15:45:33.053: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 10 15:45:33.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=services-6266 exec execpod-affinitylnd6n -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.23.183 80'
  Jan 10 15:45:33.364: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.23.183 80\nConnection to 10.233.23.183 80 port [tcp/http] succeeded!\n"
  Jan 10 15:45:33.364: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 10 15:45:33.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=services-6266 exec execpod-affinitylnd6n -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.23.183:80/ ; done'
  Jan 10 15:45:33.982: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.23.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.23.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.23.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.23.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.23.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.23.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.23.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.23.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.23.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.23.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.23.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.23.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.23.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.23.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.23.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.23.183:80/\n"
  Jan 10 15:45:33.982: INFO: stdout: "\naffinity-clusterip-transition-nzxkx\naffinity-clusterip-transition-5nszl\naffinity-clusterip-transition-rwqwq\naffinity-clusterip-transition-nzxkx\naffinity-clusterip-transition-5nszl\naffinity-clusterip-transition-rwqwq\naffinity-clusterip-transition-nzxkx\naffinity-clusterip-transition-5nszl\naffinity-clusterip-transition-rwqwq\naffinity-clusterip-transition-nzxkx\naffinity-clusterip-transition-5nszl\naffinity-clusterip-transition-rwqwq\naffinity-clusterip-transition-nzxkx\naffinity-clusterip-transition-5nszl\naffinity-clusterip-transition-rwqwq\naffinity-clusterip-transition-nzxkx"
  Jan 10 15:45:33.982: INFO: Received response from host: affinity-clusterip-transition-nzxkx
  Jan 10 15:45:33.982: INFO: Received response from host: affinity-clusterip-transition-5nszl
  Jan 10 15:45:33.982: INFO: Received response from host: affinity-clusterip-transition-rwqwq
  Jan 10 15:45:33.982: INFO: Received response from host: affinity-clusterip-transition-nzxkx
  Jan 10 15:45:33.982: INFO: Received response from host: affinity-clusterip-transition-5nszl
  Jan 10 15:45:33.982: INFO: Received response from host: affinity-clusterip-transition-rwqwq
  Jan 10 15:45:33.982: INFO: Received response from host: affinity-clusterip-transition-nzxkx
  Jan 10 15:45:33.982: INFO: Received response from host: affinity-clusterip-transition-5nszl
  Jan 10 15:45:33.982: INFO: Received response from host: affinity-clusterip-transition-rwqwq
  Jan 10 15:45:33.982: INFO: Received response from host: affinity-clusterip-transition-nzxkx
  Jan 10 15:45:33.982: INFO: Received response from host: affinity-clusterip-transition-5nszl
  Jan 10 15:45:33.982: INFO: Received response from host: affinity-clusterip-transition-rwqwq
  Jan 10 15:45:33.982: INFO: Received response from host: affinity-clusterip-transition-nzxkx
  Jan 10 15:45:33.982: INFO: Received response from host: affinity-clusterip-transition-5nszl
  Jan 10 15:45:33.982: INFO: Received response from host: affinity-clusterip-transition-rwqwq
  Jan 10 15:45:33.982: INFO: Received response from host: affinity-clusterip-transition-nzxkx
  Jan 10 15:45:34.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=services-6266 exec execpod-affinitylnd6n -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.23.183:80/ ; done'
  Jan 10 15:45:34.548: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.23.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.23.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.23.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.23.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.23.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.23.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.23.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.23.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.23.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.23.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.23.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.23.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.23.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.23.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.23.183:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.23.183:80/\n"
  Jan 10 15:45:34.548: INFO: stdout: "\naffinity-clusterip-transition-rwqwq\naffinity-clusterip-transition-rwqwq\naffinity-clusterip-transition-rwqwq\naffinity-clusterip-transition-rwqwq\naffinity-clusterip-transition-rwqwq\naffinity-clusterip-transition-rwqwq\naffinity-clusterip-transition-rwqwq\naffinity-clusterip-transition-rwqwq\naffinity-clusterip-transition-rwqwq\naffinity-clusterip-transition-rwqwq\naffinity-clusterip-transition-rwqwq\naffinity-clusterip-transition-rwqwq\naffinity-clusterip-transition-rwqwq\naffinity-clusterip-transition-rwqwq\naffinity-clusterip-transition-rwqwq\naffinity-clusterip-transition-rwqwq"
  Jan 10 15:45:34.548: INFO: Received response from host: affinity-clusterip-transition-rwqwq
  Jan 10 15:45:34.548: INFO: Received response from host: affinity-clusterip-transition-rwqwq
  Jan 10 15:45:34.548: INFO: Received response from host: affinity-clusterip-transition-rwqwq
  Jan 10 15:45:34.548: INFO: Received response from host: affinity-clusterip-transition-rwqwq
  Jan 10 15:45:34.548: INFO: Received response from host: affinity-clusterip-transition-rwqwq
  Jan 10 15:45:34.548: INFO: Received response from host: affinity-clusterip-transition-rwqwq
  Jan 10 15:45:34.548: INFO: Received response from host: affinity-clusterip-transition-rwqwq
  Jan 10 15:45:34.548: INFO: Received response from host: affinity-clusterip-transition-rwqwq
  Jan 10 15:45:34.548: INFO: Received response from host: affinity-clusterip-transition-rwqwq
  Jan 10 15:45:34.548: INFO: Received response from host: affinity-clusterip-transition-rwqwq
  Jan 10 15:45:34.548: INFO: Received response from host: affinity-clusterip-transition-rwqwq
  Jan 10 15:45:34.548: INFO: Received response from host: affinity-clusterip-transition-rwqwq
  Jan 10 15:45:34.548: INFO: Received response from host: affinity-clusterip-transition-rwqwq
  Jan 10 15:45:34.548: INFO: Received response from host: affinity-clusterip-transition-rwqwq
  Jan 10 15:45:34.548: INFO: Received response from host: affinity-clusterip-transition-rwqwq
  Jan 10 15:45:34.548: INFO: Received response from host: affinity-clusterip-transition-rwqwq
  Jan 10 15:45:34.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jan 10 15:45:34.563: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-6266, will wait for the garbage collector to delete the pods @ 01/10/24 15:45:34.598
  Jan 10 15:45:34.701: INFO: Deleting ReplicationController affinity-clusterip-transition took: 18.26622ms
  Jan 10 15:45:34.802: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.988964ms
  STEP: Destroying namespace "services-6266" for this suite. @ 01/10/24 15:45:37.268
• [10.850 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]
test/e2e/apimachinery/webhook.go:370
  STEP: Creating a kubernetes client @ 01/10/24 15:45:37.306
  Jan 10 15:45:37.306: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename webhook @ 01/10/24 15:45:37.308
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:45:37.359
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:45:37.368
  STEP: Setting up server cert @ 01/10/24 15:45:37.432
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 01/10/24 15:45:38.219
  STEP: Deploying the webhook pod @ 01/10/24 15:45:38.239
  STEP: Wait for the deployment to be ready @ 01/10/24 15:45:38.266
  Jan 10 15:45:38.289: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 01/10/24 15:45:40.32
  STEP: Verifying the service has paired with the endpoint @ 01/10/24 15:45:40.349
  Jan 10 15:45:41.350: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Setting timeout (1s) shorter than webhook latency (5s) @ 01/10/24 15:45:41.36
  STEP: Registering slow webhook via the AdmissionRegistration API @ 01/10/24 15:45:41.36
  STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) @ 01/10/24 15:45:41.407
  STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore @ 01/10/24 15:45:42.427
  STEP: Registering slow webhook via the AdmissionRegistration API @ 01/10/24 15:45:42.427
  STEP: Having no error when timeout is longer than webhook latency @ 01/10/24 15:45:43.51
  STEP: Registering slow webhook via the AdmissionRegistration API @ 01/10/24 15:45:43.51
  STEP: Having no error when timeout is empty (defaulted to 10s in v1) @ 01/10/24 15:45:48.594
  STEP: Registering slow webhook via the AdmissionRegistration API @ 01/10/24 15:45:48.595
  Jan 10 15:45:53.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-794" for this suite. @ 01/10/24 15:45:53.888
  STEP: Destroying namespace "webhook-markers-2541" for this suite. @ 01/10/24 15:45:53.913
• [16.631 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:176
  STEP: Creating a kubernetes client @ 01/10/24 15:45:53.94
  Jan 10 15:45:53.940: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename crd-webhook @ 01/10/24 15:45:53.943
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:45:53.983
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:45:53.992
  STEP: Setting up server cert @ 01/10/24 15:45:54.002
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 01/10/24 15:45:54.894
  STEP: Deploying the custom resource conversion webhook pod @ 01/10/24 15:45:54.905
  STEP: Wait for the deployment to be ready @ 01/10/24 15:45:54.929
  Jan 10 15:45:54.952: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 01/10/24 15:45:56.977
  STEP: Verifying the service has paired with the endpoint @ 01/10/24 15:45:56.994
  Jan 10 15:45:57.995: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Jan 10 15:45:58.005: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Creating a v1 custom resource @ 01/10/24 15:46:05.715
  STEP: Create a v2 custom resource @ 01/10/24 15:46:05.768
  STEP: List CRs in v1 @ 01/10/24 15:46:05.862
  STEP: List CRs in v2 @ 01/10/24 15:46:05.874
  Jan 10 15:46:05.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-9074" for this suite. @ 01/10/24 15:46:06.535
• [12.628 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]
test/e2e/storage/empty_dir_wrapper.go:188
  STEP: Creating a kubernetes client @ 01/10/24 15:46:06.582
  Jan 10 15:46:06.583: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename emptydir-wrapper @ 01/10/24 15:46:06.585
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:46:06.642
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:46:06.654
  STEP: Creating 50 configmaps @ 01/10/24 15:46:06.662
  STEP: Creating RC which spawns configmap-volume pods @ 01/10/24 15:46:07.567
  Jan 10 15:46:07.587: INFO: Pod name wrapped-volume-race-af5bdd39-0285-4d16-a82a-6c19a9ebc89f: Found 0 pods out of 5
  Jan 10 15:46:12.608: INFO: Pod name wrapped-volume-race-af5bdd39-0285-4d16-a82a-6c19a9ebc89f: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 01/10/24 15:46:12.609
  STEP: Creating RC which spawns configmap-volume pods @ 01/10/24 15:46:12.685
  Jan 10 15:46:12.736: INFO: Pod name wrapped-volume-race-e3357fd4-1aaa-425a-9729-c9927e03e65f: Found 0 pods out of 5
  Jan 10 15:46:17.762: INFO: Pod name wrapped-volume-race-e3357fd4-1aaa-425a-9729-c9927e03e65f: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 01/10/24 15:46:17.763
  STEP: Creating RC which spawns configmap-volume pods @ 01/10/24 15:46:17.845
  Jan 10 15:46:17.888: INFO: Pod name wrapped-volume-race-c345b9bc-c4a5-4d97-9d5b-3d1afc050fa0: Found 0 pods out of 5
  Jan 10 15:46:22.908: INFO: Pod name wrapped-volume-race-c345b9bc-c4a5-4d97-9d5b-3d1afc050fa0: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 01/10/24 15:46:22.909
  Jan 10 15:46:22.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController wrapped-volume-race-c345b9bc-c4a5-4d97-9d5b-3d1afc050fa0 in namespace emptydir-wrapper-4562, will wait for the garbage collector to delete the pods @ 01/10/24 15:46:22.973
  Jan 10 15:46:23.055: INFO: Deleting ReplicationController wrapped-volume-race-c345b9bc-c4a5-4d97-9d5b-3d1afc050fa0 took: 19.380742ms
  Jan 10 15:46:23.255: INFO: Terminating ReplicationController wrapped-volume-race-c345b9bc-c4a5-4d97-9d5b-3d1afc050fa0 pods took: 200.628785ms
  STEP: deleting ReplicationController wrapped-volume-race-e3357fd4-1aaa-425a-9729-c9927e03e65f in namespace emptydir-wrapper-4562, will wait for the garbage collector to delete the pods @ 01/10/24 15:46:25.057
  Jan 10 15:46:25.137: INFO: Deleting ReplicationController wrapped-volume-race-e3357fd4-1aaa-425a-9729-c9927e03e65f took: 20.699992ms
  Jan 10 15:46:25.338: INFO: Terminating ReplicationController wrapped-volume-race-e3357fd4-1aaa-425a-9729-c9927e03e65f pods took: 200.534831ms
  STEP: deleting ReplicationController wrapped-volume-race-af5bdd39-0285-4d16-a82a-6c19a9ebc89f in namespace emptydir-wrapper-4562, will wait for the garbage collector to delete the pods @ 01/10/24 15:46:27.339
  Jan 10 15:46:27.435: INFO: Deleting ReplicationController wrapped-volume-race-af5bdd39-0285-4d16-a82a-6c19a9ebc89f took: 15.164828ms
  Jan 10 15:46:27.636: INFO: Terminating ReplicationController wrapped-volume-race-af5bdd39-0285-4d16-a82a-6c19a9ebc89f pods took: 200.657656ms
  STEP: Cleaning up the configMaps @ 01/10/24 15:46:29.337
  STEP: Destroying namespace "emptydir-wrapper-4562" for this suite. @ 01/10/24 15:46:30.247
• [23.684 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
test/e2e/apimachinery/watch.go:257
  STEP: Creating a kubernetes client @ 01/10/24 15:46:30.267
  Jan 10 15:46:30.268: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename watch @ 01/10/24 15:46:30.269
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:46:30.32
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:46:30.329
  STEP: creating a watch on configmaps with a certain label @ 01/10/24 15:46:30.336
  STEP: creating a new configmap @ 01/10/24 15:46:30.338
  STEP: modifying the configmap once @ 01/10/24 15:46:30.349
  STEP: changing the label value of the configmap @ 01/10/24 15:46:30.383
  STEP: Expecting to observe a delete notification for the watched object @ 01/10/24 15:46:30.408
  Jan 10 15:46:30.408: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8171  337e2c29-1dd3-47eb-9ce0-719f36df4b43 186768704 0 2024-01-10 15:46:30 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-01-10 15:46:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jan 10 15:46:30.409: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8171  337e2c29-1dd3-47eb-9ce0-719f36df4b43 186768705 0 2024-01-10 15:46:30 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-01-10 15:46:30 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jan 10 15:46:30.410: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8171  337e2c29-1dd3-47eb-9ce0-719f36df4b43 186768706 0 2024-01-10 15:46:30 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-01-10 15:46:30 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time @ 01/10/24 15:46:30.41
  STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements @ 01/10/24 15:46:30.43
  STEP: changing the label value of the configmap back @ 01/10/24 15:46:40.431
  STEP: modifying the configmap a third time @ 01/10/24 15:46:40.456
  STEP: deleting the configmap @ 01/10/24 15:46:40.476
  STEP: Expecting to observe an add notification for the watched object when the label value was restored @ 01/10/24 15:46:40.488
  Jan 10 15:46:40.488: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8171  337e2c29-1dd3-47eb-9ce0-719f36df4b43 186768842 0 2024-01-10 15:46:30 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-01-10 15:46:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jan 10 15:46:40.488: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8171  337e2c29-1dd3-47eb-9ce0-719f36df4b43 186768843 0 2024-01-10 15:46:30 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-01-10 15:46:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jan 10 15:46:40.488: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8171  337e2c29-1dd3-47eb-9ce0-719f36df4b43 186768844 0 2024-01-10 15:46:30 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2024-01-10 15:46:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jan 10 15:46:40.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-8171" for this suite. @ 01/10/24 15:46:40.499
• [10.249 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:194
  STEP: Creating a kubernetes client @ 01/10/24 15:46:40.52
  Jan 10 15:46:40.520: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename crd-publish-openapi @ 01/10/24 15:46:40.521
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:46:40.576
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:46:40.583
  Jan 10 15:46:40.590: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 01/10/24 15:46:50.137
  Jan 10 15:46:50.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=crd-publish-openapi-8976 --namespace=crd-publish-openapi-8976 create -f -'
  Jan 10 15:46:52.534: INFO: stderr: ""
  Jan 10 15:46:52.535: INFO: stdout: "e2e-test-crd-publish-openapi-734-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Jan 10 15:46:52.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=crd-publish-openapi-8976 --namespace=crd-publish-openapi-8976 delete e2e-test-crd-publish-openapi-734-crds test-cr'
  Jan 10 15:46:52.770: INFO: stderr: ""
  Jan 10 15:46:52.770: INFO: stdout: "e2e-test-crd-publish-openapi-734-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  Jan 10 15:46:52.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=crd-publish-openapi-8976 --namespace=crd-publish-openapi-8976 apply -f -'
  Jan 10 15:46:54.869: INFO: stderr: ""
  Jan 10 15:46:54.869: INFO: stdout: "e2e-test-crd-publish-openapi-734-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Jan 10 15:46:54.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=crd-publish-openapi-8976 --namespace=crd-publish-openapi-8976 delete e2e-test-crd-publish-openapi-734-crds test-cr'
  Jan 10 15:46:55.054: INFO: stderr: ""
  Jan 10 15:46:55.054: INFO: stdout: "e2e-test-crd-publish-openapi-734-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 01/10/24 15:46:55.054
  Jan 10 15:46:55.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=crd-publish-openapi-8976 explain e2e-test-crd-publish-openapi-734-crds'
  Jan 10 15:46:56.974: INFO: stderr: ""
  Jan 10 15:46:56.975: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-at-root.example.com\nKIND:       e2e-test-crd-publish-openapi-734-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties at root for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  Jan 10 15:47:01.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-8976" for this suite. @ 01/10/24 15:47:01.085
• [20.582 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]
test/e2e/common/storage/empty_dir.go:227
  STEP: Creating a kubernetes client @ 01/10/24 15:47:01.108
  Jan 10 15:47:01.108: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename emptydir @ 01/10/24 15:47:01.112
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:47:01.147
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:47:01.153
  STEP: Creating Pod @ 01/10/24 15:47:01.159
  STEP: Reading file content from the nginx-container @ 01/10/24 15:47:03.211
  Jan 10 15:47:03.212: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-7037 PodName:pod-sharedvolume-43d5b82f-3d28-476d-a506-24fd8a49487b ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 10 15:47:03.212: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  Jan 10 15:47:03.214: INFO: ExecWithOptions: Clientset creation
  Jan 10 15:47:03.214: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/emptydir-7037/pods/pod-sharedvolume-43d5b82f-3d28-476d-a506-24fd8a49487b/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
  Jan 10 15:47:03.380: INFO: Exec stderr: ""
  Jan 10 15:47:03.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7037" for this suite. @ 01/10/24 15:47:03.393
• [2.298 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:52
  STEP: Creating a kubernetes client @ 01/10/24 15:47:03.419
  Jan 10 15:47:03.419: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename container-runtime @ 01/10/24 15:47:03.422
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:47:03.475
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:47:03.484
  STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' @ 01/10/24 15:47:03.514
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' @ 01/10/24 15:47:20.725
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition @ 01/10/24 15:47:20.736
  STEP: Container 'terminate-cmd-rpa': should get the expected 'State' @ 01/10/24 15:47:20.757
  STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] @ 01/10/24 15:47:20.757
  STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' @ 01/10/24 15:47:20.807
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' @ 01/10/24 15:47:23.844
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition @ 01/10/24 15:47:24.866
  STEP: Container 'terminate-cmd-rpof': should get the expected 'State' @ 01/10/24 15:47:24.88
  STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] @ 01/10/24 15:47:24.88
  STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' @ 01/10/24 15:47:24.951
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' @ 01/10/24 15:47:25.98
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition @ 01/10/24 15:47:28.004
  STEP: Container 'terminate-cmd-rpn': should get the expected 'State' @ 01/10/24 15:47:28.018
  STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] @ 01/10/24 15:47:28.018
  Jan 10 15:47:28.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-2284" for this suite. @ 01/10/24 15:47:28.082
• [24.683 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
test/e2e/apimachinery/webhook.go:272
  STEP: Creating a kubernetes client @ 01/10/24 15:47:28.102
  Jan 10 15:47:28.102: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename webhook @ 01/10/24 15:47:28.103
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:47:28.14
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:47:28.144
  STEP: Setting up server cert @ 01/10/24 15:47:28.189
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 01/10/24 15:47:29.588
  STEP: Deploying the webhook pod @ 01/10/24 15:47:29.608
  STEP: Wait for the deployment to be ready @ 01/10/24 15:47:29.65
  Jan 10 15:47:29.709: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 01/10/24 15:47:31.734
  STEP: Verifying the service has paired with the endpoint @ 01/10/24 15:47:31.763
  Jan 10 15:47:32.764: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 01/10/24 15:47:32.774
  STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 01/10/24 15:47:32.812
  STEP: Creating a dummy validating-webhook-configuration object @ 01/10/24 15:47:32.847
  STEP: Deleting the validating-webhook-configuration, which should be possible to remove @ 01/10/24 15:47:32.875
  STEP: Creating a dummy mutating-webhook-configuration object @ 01/10/24 15:47:32.889
  STEP: Deleting the mutating-webhook-configuration, which should be possible to remove @ 01/10/24 15:47:32.924
  Jan 10 15:47:32.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1327" for this suite. @ 01/10/24 15:47:33.137
  STEP: Destroying namespace "webhook-markers-1152" for this suite. @ 01/10/24 15:47:33.158
• [5.077 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]
test/e2e/storage/subpath.go:70
  STEP: Creating a kubernetes client @ 01/10/24 15:47:33.18
  Jan 10 15:47:33.180: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename subpath @ 01/10/24 15:47:33.183
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:47:33.212
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:47:33.217
  STEP: Setting up data @ 01/10/24 15:47:33.23
  STEP: Creating pod pod-subpath-test-configmap-2945 @ 01/10/24 15:47:33.257
  STEP: Creating a pod to test atomic-volume-subpath @ 01/10/24 15:47:33.258
  STEP: Saw pod success @ 01/10/24 15:47:57.441
  Jan 10 15:47:57.451: INFO: Trying to get logs from node env1-test-worker-1 pod pod-subpath-test-configmap-2945 container test-container-subpath-configmap-2945: <nil>
  STEP: delete the pod @ 01/10/24 15:47:57.506
  STEP: Deleting pod pod-subpath-test-configmap-2945 @ 01/10/24 15:47:57.559
  Jan 10 15:47:57.560: INFO: Deleting pod "pod-subpath-test-configmap-2945" in namespace "subpath-1445"
  Jan 10 15:47:57.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-1445" for this suite. @ 01/10/24 15:47:57.59
• [24.428 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:74
  STEP: Creating a kubernetes client @ 01/10/24 15:47:57.616
  Jan 10 15:47:57.616: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename configmap @ 01/10/24 15:47:57.617
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:47:57.664
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:47:57.671
  STEP: Creating configMap with name configmap-test-volume-a75da1d3-6f56-4af1-8568-b3ba832fe1cd @ 01/10/24 15:47:57.678
  STEP: Creating a pod to test consume configMaps @ 01/10/24 15:47:57.694
  STEP: Saw pod success @ 01/10/24 15:48:01.743
  Jan 10 15:48:01.751: INFO: Trying to get logs from node env1-test-worker-1 pod pod-configmaps-872a0287-efe9-4e6a-ad27-42c355bd88e1 container agnhost-container: <nil>
  STEP: delete the pod @ 01/10/24 15:48:01.769
  Jan 10 15:48:01.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9333" for this suite. @ 01/10/24 15:48:01.835
• [4.238 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:619
  STEP: Creating a kubernetes client @ 01/10/24 15:48:01.855
  Jan 10 15:48:01.855: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename pods @ 01/10/24 15:48:01.857
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:48:01.895
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:48:01.907
  Jan 10 15:48:01.916: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: creating the pod @ 01/10/24 15:48:01.918
  STEP: submitting the pod to kubernetes @ 01/10/24 15:48:01.918
  Jan 10 15:48:04.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-5096" for this suite. @ 01/10/24 15:48:04.069
• [2.233 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] PreStop should call prestop when killing a pod  [Conformance]
test/e2e/node/pre_stop.go:169
  STEP: Creating a kubernetes client @ 01/10/24 15:48:04.09
  Jan 10 15:48:04.090: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename prestop @ 01/10/24 15:48:04.093
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:48:04.136
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:48:04.145
  STEP: Creating server pod server in namespace prestop-7196 @ 01/10/24 15:48:04.155
  STEP: Waiting for pods to come up. @ 01/10/24 15:48:04.185
  STEP: Creating tester pod tester in namespace prestop-7196 @ 01/10/24 15:48:06.214
  STEP: Deleting pre-stop pod @ 01/10/24 15:48:08.251
  Jan 10 15:48:13.290: INFO: Saw: {
  	"Hostname": "server",
  	"Sent": null,
  	"Received": {
  		"prestop": 1
  	},
  	"Errors": null,
  	"Log": [
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
  	],
  	"StillContactingPeers": true
  }
  Jan 10 15:48:13.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Deleting the server pod @ 01/10/24 15:48:13.301
  STEP: Destroying namespace "prestop-7196" for this suite. @ 01/10/24 15:48:13.331
• [9.276 seconds]
------------------------------
SSS
------------------------------
[sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:164
  STEP: Creating a kubernetes client @ 01/10/24 15:48:13.367
  Jan 10 15:48:13.367: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename security-context @ 01/10/24 15:48:13.369
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:48:13.399
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:48:13.408
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 01/10/24 15:48:13.415
  STEP: Saw pod success @ 01/10/24 15:48:17.476
  Jan 10 15:48:17.486: INFO: Trying to get logs from node env1-test-worker-1 pod security-context-724c07a8-a947-44ea-9088-5aff9aac3b8d container test-container: <nil>
  STEP: delete the pod @ 01/10/24 15:48:17.509
  Jan 10 15:48:17.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-3076" for this suite. @ 01/10/24 15:48:17.586
• [4.233 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
test/e2e/network/proxy.go:286
  STEP: Creating a kubernetes client @ 01/10/24 15:48:17.605
  Jan 10 15:48:17.605: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename proxy @ 01/10/24 15:48:17.607
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:48:17.659
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:48:17.665
  Jan 10 15:48:17.672: INFO: Creating pod...
  Jan 10 15:48:19.746: INFO: Creating service...
  Jan 10 15:48:19.810: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6147/pods/agnhost/proxy/some/path/with/DELETE
  Jan 10 15:48:19.856: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jan 10 15:48:19.856: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6147/pods/agnhost/proxy/some/path/with/GET
  Jan 10 15:48:19.872: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Jan 10 15:48:19.873: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6147/pods/agnhost/proxy/some/path/with/HEAD
  Jan 10 15:48:19.886: INFO: http.Client request:HEAD | StatusCode:200
  Jan 10 15:48:19.886: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6147/pods/agnhost/proxy/some/path/with/OPTIONS
  Jan 10 15:48:19.904: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jan 10 15:48:19.904: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6147/pods/agnhost/proxy/some/path/with/PATCH
  Jan 10 15:48:19.915: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jan 10 15:48:19.915: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6147/pods/agnhost/proxy/some/path/with/POST
  Jan 10 15:48:19.928: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jan 10 15:48:19.928: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6147/pods/agnhost/proxy/some/path/with/PUT
  Jan 10 15:48:19.946: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jan 10 15:48:19.947: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6147/services/test-service/proxy/some/path/with/DELETE
  Jan 10 15:48:19.963: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jan 10 15:48:19.963: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6147/services/test-service/proxy/some/path/with/GET
  Jan 10 15:48:19.978: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Jan 10 15:48:19.979: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6147/services/test-service/proxy/some/path/with/HEAD
  Jan 10 15:48:20.011: INFO: http.Client request:HEAD | StatusCode:200
  Jan 10 15:48:20.011: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6147/services/test-service/proxy/some/path/with/OPTIONS
  Jan 10 15:48:20.032: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jan 10 15:48:20.032: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6147/services/test-service/proxy/some/path/with/PATCH
  Jan 10 15:48:20.045: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jan 10 15:48:20.045: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6147/services/test-service/proxy/some/path/with/POST
  Jan 10 15:48:20.073: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jan 10 15:48:20.074: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-6147/services/test-service/proxy/some/path/with/PUT
  Jan 10 15:48:20.099: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jan 10 15:48:20.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-6147" for this suite. @ 01/10/24 15:48:20.121
• [2.541 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:79
  STEP: Creating a kubernetes client @ 01/10/24 15:48:20.148
  Jan 10 15:48:20.148: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename secrets @ 01/10/24 15:48:20.15
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:48:20.203
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:48:20.215
  STEP: Creating secret with name secret-test-map-db083413-e128-4d4a-b98d-29abb87bc2c7 @ 01/10/24 15:48:20.237
  STEP: Creating a pod to test consume secrets @ 01/10/24 15:48:20.25
  STEP: Saw pod success @ 01/10/24 15:48:24.341
  Jan 10 15:48:24.355: INFO: Trying to get logs from node env1-test-worker-1 pod pod-secrets-534496d5-9d4d-452e-a30d-b3752ea4174c container secret-volume-test: <nil>
  STEP: delete the pod @ 01/10/24 15:48:24.416
  Jan 10 15:48:24.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9664" for this suite. @ 01/10/24 15:48:24.539
• [4.417 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
test/e2e/scheduling/preemption.go:812
  STEP: Creating a kubernetes client @ 01/10/24 15:48:24.568
  Jan 10 15:48:24.568: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename sched-preemption @ 01/10/24 15:48:24.57
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:48:24.627
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:48:24.64
  Jan 10 15:48:24.693: INFO: Waiting up to 1m0s for all nodes to be ready
  Jan 10 15:49:24.798: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 01/10/24 15:49:24.806
  Jan 10 15:49:24.807: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename sched-preemption-path @ 01/10/24 15:49:24.808
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:49:24.854
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:49:24.86
  Jan 10 15:49:24.899: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
  Jan 10 15:49:24.919: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
  Jan 10 15:49:24.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jan 10 15:49:25.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-8598" for this suite. @ 01/10/24 15:49:25.146
  STEP: Destroying namespace "sched-preemption-7376" for this suite. @ 01/10/24 15:49:25.163
• [60.614 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:262
  STEP: Creating a kubernetes client @ 01/10/24 15:49:25.189
  Jan 10 15:49:25.190: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename downward-api @ 01/10/24 15:49:25.192
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:49:25.228
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:49:25.234
  STEP: Creating a pod to test downward API volume plugin @ 01/10/24 15:49:25.241
  STEP: Saw pod success @ 01/10/24 15:49:29.299
  Jan 10 15:49:29.306: INFO: Trying to get logs from node env1-test-worker-1 pod downwardapi-volume-ec70731e-8b63-4840-983b-c1889ed41bd6 container client-container: <nil>
  STEP: delete the pod @ 01/10/24 15:49:29.322
  Jan 10 15:49:29.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9688" for this suite. @ 01/10/24 15:49:29.389
• [4.215 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
test/e2e/network/hostport.go:63
  STEP: Creating a kubernetes client @ 01/10/24 15:49:29.412
  Jan 10 15:49:29.412: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename hostport @ 01/10/24 15:49:29.418
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:49:29.452
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:49:29.458
  STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled @ 01/10/24 15:49:29.476
  STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.61.1.200 on the node which pod1 resides and expect scheduled @ 01/10/24 15:49:41.582
  STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.61.1.200 but use UDP protocol on the node which pod2 resides @ 01/10/24 15:49:43.621
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 @ 01/10/24 15:49:57.757
  Jan 10 15:49:57.757: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.61.1.200 http://127.0.0.1:54323/hostname] Namespace:hostport-4602 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 10 15:49:57.757: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  Jan 10 15:49:57.759: INFO: ExecWithOptions: Clientset creation
  Jan 10 15:49:57.759: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-4602/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.61.1.200+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.61.1.200, port: 54323 @ 01/10/24 15:49:57.933
  Jan 10 15:49:57.933: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.61.1.200:54323/hostname] Namespace:hostport-4602 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 10 15:49:57.933: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  Jan 10 15:49:57.934: INFO: ExecWithOptions: Clientset creation
  Jan 10 15:49:57.935: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-4602/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.61.1.200%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.61.1.200, port: 54323 UDP @ 01/10/24 15:49:58.126
  Jan 10 15:49:58.126: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.61.1.200 54323] Namespace:hostport-4602 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 10 15:49:58.126: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  Jan 10 15:49:58.128: INFO: ExecWithOptions: Clientset creation
  Jan 10 15:49:58.128: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-4602/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.61.1.200+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  Jan 10 15:50:03.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "hostport-4602" for this suite. @ 01/10/24 15:50:03.3
• [33.904 seconds]
------------------------------
[sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]
test/e2e/apps/disruption.go:349
  STEP: Creating a kubernetes client @ 01/10/24 15:50:03.318
  Jan 10 15:50:03.318: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename disruption @ 01/10/24 15:50:03.321
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:50:03.354
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:50:03.361
  STEP: Creating a pdb that targets all three pods in a test replica set @ 01/10/24 15:50:03.366
  STEP: Waiting for the pdb to be processed @ 01/10/24 15:50:03.372
  STEP: First trying to evict a pod which shouldn't be evictable @ 01/10/24 15:50:05.403
  STEP: Waiting for all pods to be running @ 01/10/24 15:50:05.404
  Jan 10 15:50:05.413: INFO: pods: 0 < 3
  STEP: locating a running pod @ 01/10/24 15:50:07.423
  STEP: Updating the pdb to allow a pod to be evicted @ 01/10/24 15:50:07.443
  STEP: Waiting for the pdb to be processed @ 01/10/24 15:50:07.458
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 01/10/24 15:50:07.472
  STEP: Waiting for all pods to be running @ 01/10/24 15:50:07.472
  STEP: Waiting for the pdb to observed all healthy pods @ 01/10/24 15:50:07.479
  STEP: Patching the pdb to disallow a pod to be evicted @ 01/10/24 15:50:07.537
  STEP: Waiting for the pdb to be processed @ 01/10/24 15:50:07.574
  STEP: Waiting for all pods to be running @ 01/10/24 15:50:09.606
  STEP: locating a running pod @ 01/10/24 15:50:09.619
  STEP: Deleting the pdb to allow a pod to be evicted @ 01/10/24 15:50:09.673
  STEP: Waiting for the pdb to be deleted @ 01/10/24 15:50:09.697
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 01/10/24 15:50:09.722
  STEP: Waiting for all pods to be running @ 01/10/24 15:50:09.722
  Jan 10 15:50:09.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-1668" for this suite. @ 01/10/24 15:50:09.793
• [6.507 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]
test/e2e/apps/daemon_set.go:825
  STEP: Creating a kubernetes client @ 01/10/24 15:50:09.825
  Jan 10 15:50:09.826: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename daemonsets @ 01/10/24 15:50:09.827
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:50:09.933
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:50:09.942
  STEP: Creating simple DaemonSet "daemon-set" @ 01/10/24 15:50:09.998
  STEP: Check that daemon pods launch on every node of the cluster. @ 01/10/24 15:50:10.016
  Jan 10 15:50:10.034: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 15:50:10.034: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 15:50:10.035: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 15:50:10.051: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 10 15:50:10.052: INFO: Node env1-test-worker-0 is running 0 daemon pod, expected 1
  Jan 10 15:50:11.069: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 15:50:11.069: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 15:50:11.069: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 15:50:11.081: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 10 15:50:11.082: INFO: Node env1-test-worker-0 is running 0 daemon pod, expected 1
  Jan 10 15:50:12.067: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 15:50:12.068: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 15:50:12.069: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 15:50:12.079: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jan 10 15:50:12.080: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: listing all DaemonSets @ 01/10/24 15:50:12.089
  STEP: DeleteCollection of the DaemonSets @ 01/10/24 15:50:12.099
  STEP: Verify that ReplicaSets have been deleted @ 01/10/24 15:50:12.123
  Jan 10 15:50:12.156: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"186770312"},"items":null}

  Jan 10 15:50:12.171: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"186770314"},"items":[{"metadata":{"name":"daemon-set-6smd4","generateName":"daemon-set-","namespace":"daemonsets-6061","uid":"f90b4b43-e2f7-4e30-987f-15a90e4341d5","resourceVersion":"186770304","creationTimestamp":"2024-01-10T15:50:10Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"39007848-cff7-486d-b9be-751d3a64401a","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2024-01-10T15:50:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"39007848-cff7-486d-b9be-751d3a64401a\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2024-01-10T15:50:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.117\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-nk5kx","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-nk5kx","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"env1-test-worker-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["env1-test-worker-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-10T15:50:10Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-10T15:50:11Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-10T15:50:11Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-10T15:50:10Z"}],"hostIP":"10.61.1.201","podIP":"10.233.68.117","podIPs":[{"ip":"10.233.68.117"}],"startTime":"2024-01-10T15:50:10Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2024-01-10T15:50:10Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://60a7765c77a8a56a42e319c9186dbcc944931428af8ddde415cd309196d038f9","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-ln722","generateName":"daemon-set-","namespace":"daemonsets-6061","uid":"92d87bd4-5d1f-4811-a4eb-c0ac0337ca2f","resourceVersion":"186770314","creationTimestamp":"2024-01-10T15:50:10Z","deletionTimestamp":"2024-01-10T15:50:42Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"39007848-cff7-486d-b9be-751d3a64401a","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2024-01-10T15:50:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"39007848-cff7-486d-b9be-751d3a64401a\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2024-01-10T15:50:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.67.232\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-wfjqt","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-wfjqt","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"env1-test-worker-0","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["env1-test-worker-0"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-10T15:50:10Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-10T15:50:11Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-10T15:50:11Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-01-10T15:50:10Z"}],"hostIP":"10.61.1.200","podIP":"10.233.67.232","podIPs":[{"ip":"10.233.67.232"}],"startTime":"2024-01-10T15:50:10Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2024-01-10T15:50:11Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://e55bb8be2842b2a9aad00508674593644fdaf65a70bc46bc52a814a42ea66537","started":true}],"qosClass":"BestEffort"}}]}

  Jan 10 15:50:12.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-6061" for this suite. @ 01/10/24 15:50:12.218
• [2.408 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:105
  STEP: Creating a kubernetes client @ 01/10/24 15:50:12.238
  Jan 10 15:50:12.239: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename deployment @ 01/10/24 15:50:12.24
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:50:12.276
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:50:12.283
  Jan 10 15:50:12.288: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
  Jan 10 15:50:12.302: INFO: Pod name sample-pod: Found 0 pods out of 1
  Jan 10 15:50:17.313: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 01/10/24 15:50:17.313
  Jan 10 15:50:17.313: INFO: Creating deployment "test-rolling-update-deployment"
  Jan 10 15:50:17.329: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
  Jan 10 15:50:17.351: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
  Jan 10 15:50:19.371: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
  Jan 10 15:50:19.380: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
  Jan 10 15:50:19.412: INFO: Deployment "test-rolling-update-deployment":
  &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-3562  beda355f-f6a7-4e1b-8b2f-9fac96d85c42 186770457 1 2024-01-10 15:50:17 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2024-01-10 15:50:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-10 15:50:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0083df3e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2024-01-10 15:50:17 +0000 UTC,LastTransitionTime:2024-01-10 15:50:17 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-656d657cd8" has successfully progressed.,LastUpdateTime:2024-01-10 15:50:19 +0000 UTC,LastTransitionTime:2024-01-10 15:50:17 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jan 10 15:50:19.421: INFO: New ReplicaSet "test-rolling-update-deployment-656d657cd8" of Deployment "test-rolling-update-deployment":
  &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-656d657cd8  deployment-3562  d1357c6f-ffec-4248-a08b-8d114dfa0a23 186770447 1 2024-01-10 15:50:17 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment beda355f-f6a7-4e1b-8b2f-9fac96d85c42 0xc0083df8e7 0xc0083df8e8}] [] [{kube-controller-manager Update apps/v1 2024-01-10 15:50:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"beda355f-f6a7-4e1b-8b2f-9fac96d85c42\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-10 15:50:19 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 656d657cd8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0083df998 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jan 10 15:50:19.421: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
  Jan 10 15:50:19.421: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-3562  d138519b-d53c-40de-9dde-517bc7755d16 186770456 2 2024-01-10 15:50:12 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment beda355f-f6a7-4e1b-8b2f-9fac96d85c42 0xc0083df7b7 0xc0083df7b8}] [] [{e2e.test Update apps/v1 2024-01-10 15:50:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-10 15:50:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"beda355f-f6a7-4e1b-8b2f-9fac96d85c42\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2024-01-10 15:50:19 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0083df878 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jan 10 15:50:19.430: INFO: Pod "test-rolling-update-deployment-656d657cd8-vgxld" is available:
  &Pod{ObjectMeta:{test-rolling-update-deployment-656d657cd8-vgxld test-rolling-update-deployment-656d657cd8- deployment-3562  11531706-b6fc-4895-86a2-d406fe16ee56 186770446 0 2024-01-10 15:50:17 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-656d657cd8 d1357c6f-ffec-4248-a08b-8d114dfa0a23 0xc00053b677 0xc00053b678}] [] [{kube-controller-manager Update v1 2024-01-10 15:50:17 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d1357c6f-ffec-4248-a08b-8d114dfa0a23\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-10 15:50:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.119\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-72279,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-72279,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 15:50:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 15:50:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 15:50:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 15:50:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.201,PodIP:10.233.68.119,StartTime:2024-01-10 15:50:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-10 15:50:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://68405cfc81f34fcbb0ca86e3bf768fea6b27f41af3e1ec6d47d4c1a0f4e97045,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.119,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 10 15:50:19.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-3562" for this suite. @ 01/10/24 15:50:19.441
• [7.217 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]
test/e2e/apimachinery/garbage_collector.go:379
  STEP: Creating a kubernetes client @ 01/10/24 15:50:19.468
  Jan 10 15:50:19.469: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename gc @ 01/10/24 15:50:19.471
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:50:19.511
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:50:19.519
  STEP: create the rc @ 01/10/24 15:50:19.538
  W0110 15:50:19.550066      23 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 01/10/24 15:50:25.572
  STEP: wait for the rc to be deleted @ 01/10/24 15:50:25.657
  STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods @ 01/10/24 15:50:30.671
  STEP: Gathering metrics @ 01/10/24 15:51:00.701
  Jan 10 15:51:00.939: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jan 10 15:51:00.939: INFO: Deleting pod "simpletest.rc-24srl" in namespace "gc-2181"
  Jan 10 15:51:00.979: INFO: Deleting pod "simpletest.rc-278kg" in namespace "gc-2181"
  Jan 10 15:51:01.040: INFO: Deleting pod "simpletest.rc-2gbjj" in namespace "gc-2181"
  Jan 10 15:51:01.074: INFO: Deleting pod "simpletest.rc-2mxfc" in namespace "gc-2181"
  Jan 10 15:51:01.146: INFO: Deleting pod "simpletest.rc-2p77s" in namespace "gc-2181"
  Jan 10 15:51:01.186: INFO: Deleting pod "simpletest.rc-46wt4" in namespace "gc-2181"
  Jan 10 15:51:01.256: INFO: Deleting pod "simpletest.rc-4mj7m" in namespace "gc-2181"
  Jan 10 15:51:01.315: INFO: Deleting pod "simpletest.rc-4xwnz" in namespace "gc-2181"
  Jan 10 15:51:01.375: INFO: Deleting pod "simpletest.rc-5g5kd" in namespace "gc-2181"
  Jan 10 15:51:01.414: INFO: Deleting pod "simpletest.rc-5j7b6" in namespace "gc-2181"
  Jan 10 15:51:01.453: INFO: Deleting pod "simpletest.rc-68z9x" in namespace "gc-2181"
  Jan 10 15:51:01.500: INFO: Deleting pod "simpletest.rc-6llgv" in namespace "gc-2181"
  Jan 10 15:51:01.566: INFO: Deleting pod "simpletest.rc-6p98s" in namespace "gc-2181"
  Jan 10 15:51:01.622: INFO: Deleting pod "simpletest.rc-6qprk" in namespace "gc-2181"
  Jan 10 15:51:01.667: INFO: Deleting pod "simpletest.rc-7bc4b" in namespace "gc-2181"
  Jan 10 15:51:01.701: INFO: Deleting pod "simpletest.rc-7d6db" in namespace "gc-2181"
  Jan 10 15:51:01.740: INFO: Deleting pod "simpletest.rc-87q9c" in namespace "gc-2181"
  Jan 10 15:51:01.778: INFO: Deleting pod "simpletest.rc-8grhn" in namespace "gc-2181"
  Jan 10 15:51:01.822: INFO: Deleting pod "simpletest.rc-8pnfb" in namespace "gc-2181"
  Jan 10 15:51:01.860: INFO: Deleting pod "simpletest.rc-8rdkt" in namespace "gc-2181"
  Jan 10 15:51:01.893: INFO: Deleting pod "simpletest.rc-8sb7p" in namespace "gc-2181"
  Jan 10 15:51:01.948: INFO: Deleting pod "simpletest.rc-9c4bd" in namespace "gc-2181"
  Jan 10 15:51:01.977: INFO: Deleting pod "simpletest.rc-9gxsv" in namespace "gc-2181"
  Jan 10 15:51:02.008: INFO: Deleting pod "simpletest.rc-9lbfv" in namespace "gc-2181"
  Jan 10 15:51:02.045: INFO: Deleting pod "simpletest.rc-9sxl9" in namespace "gc-2181"
  Jan 10 15:51:02.082: INFO: Deleting pod "simpletest.rc-b4jpg" in namespace "gc-2181"
  Jan 10 15:51:02.133: INFO: Deleting pod "simpletest.rc-b9k6p" in namespace "gc-2181"
  Jan 10 15:51:02.168: INFO: Deleting pod "simpletest.rc-bj5w8" in namespace "gc-2181"
  Jan 10 15:51:02.204: INFO: Deleting pod "simpletest.rc-bk6bc" in namespace "gc-2181"
  Jan 10 15:51:02.257: INFO: Deleting pod "simpletest.rc-cxfq6" in namespace "gc-2181"
  Jan 10 15:51:02.309: INFO: Deleting pod "simpletest.rc-czp26" in namespace "gc-2181"
  Jan 10 15:51:02.368: INFO: Deleting pod "simpletest.rc-ff84j" in namespace "gc-2181"
  Jan 10 15:51:02.402: INFO: Deleting pod "simpletest.rc-fkwmp" in namespace "gc-2181"
  Jan 10 15:51:02.447: INFO: Deleting pod "simpletest.rc-fq4ng" in namespace "gc-2181"
  Jan 10 15:51:02.519: INFO: Deleting pod "simpletest.rc-fqlch" in namespace "gc-2181"
  Jan 10 15:51:02.582: INFO: Deleting pod "simpletest.rc-glbrj" in namespace "gc-2181"
  Jan 10 15:51:02.614: INFO: Deleting pod "simpletest.rc-gqbb5" in namespace "gc-2181"
  Jan 10 15:51:02.651: INFO: Deleting pod "simpletest.rc-gxhl7" in namespace "gc-2181"
  Jan 10 15:51:02.704: INFO: Deleting pod "simpletest.rc-h28gx" in namespace "gc-2181"
  Jan 10 15:51:02.749: INFO: Deleting pod "simpletest.rc-hbqf5" in namespace "gc-2181"
  Jan 10 15:51:02.803: INFO: Deleting pod "simpletest.rc-hbvvw" in namespace "gc-2181"
  Jan 10 15:51:02.834: INFO: Deleting pod "simpletest.rc-hfc4n" in namespace "gc-2181"
  Jan 10 15:51:02.876: INFO: Deleting pod "simpletest.rc-htlpf" in namespace "gc-2181"
  Jan 10 15:51:02.932: INFO: Deleting pod "simpletest.rc-hxwmd" in namespace "gc-2181"
  Jan 10 15:51:02.975: INFO: Deleting pod "simpletest.rc-j2qm7" in namespace "gc-2181"
  Jan 10 15:51:03.013: INFO: Deleting pod "simpletest.rc-j75ll" in namespace "gc-2181"
  Jan 10 15:51:03.047: INFO: Deleting pod "simpletest.rc-j7sgm" in namespace "gc-2181"
  Jan 10 15:51:03.088: INFO: Deleting pod "simpletest.rc-j7wbj" in namespace "gc-2181"
  Jan 10 15:51:03.131: INFO: Deleting pod "simpletest.rc-jqxhx" in namespace "gc-2181"
  Jan 10 15:51:03.172: INFO: Deleting pod "simpletest.rc-jrt4v" in namespace "gc-2181"
  Jan 10 15:51:03.203: INFO: Deleting pod "simpletest.rc-k58p8" in namespace "gc-2181"
  Jan 10 15:51:03.233: INFO: Deleting pod "simpletest.rc-k69tr" in namespace "gc-2181"
  Jan 10 15:51:03.273: INFO: Deleting pod "simpletest.rc-kkj58" in namespace "gc-2181"
  Jan 10 15:51:03.301: INFO: Deleting pod "simpletest.rc-l2vpq" in namespace "gc-2181"
  Jan 10 15:51:03.335: INFO: Deleting pod "simpletest.rc-l6n5p" in namespace "gc-2181"
  Jan 10 15:51:03.364: INFO: Deleting pod "simpletest.rc-l7vdq" in namespace "gc-2181"
  Jan 10 15:51:03.390: INFO: Deleting pod "simpletest.rc-l8kk9" in namespace "gc-2181"
  Jan 10 15:51:03.419: INFO: Deleting pod "simpletest.rc-l928s" in namespace "gc-2181"
  Jan 10 15:51:03.464: INFO: Deleting pod "simpletest.rc-ll64d" in namespace "gc-2181"
  Jan 10 15:51:03.530: INFO: Deleting pod "simpletest.rc-ltxfm" in namespace "gc-2181"
  Jan 10 15:51:03.584: INFO: Deleting pod "simpletest.rc-m2m78" in namespace "gc-2181"
  Jan 10 15:51:03.640: INFO: Deleting pod "simpletest.rc-m2xrm" in namespace "gc-2181"
  Jan 10 15:51:03.679: INFO: Deleting pod "simpletest.rc-m5jr2" in namespace "gc-2181"
  Jan 10 15:51:03.719: INFO: Deleting pod "simpletest.rc-md4lg" in namespace "gc-2181"
  Jan 10 15:51:03.754: INFO: Deleting pod "simpletest.rc-mgrn5" in namespace "gc-2181"
  Jan 10 15:51:03.786: INFO: Deleting pod "simpletest.rc-mkmtg" in namespace "gc-2181"
  Jan 10 15:51:03.812: INFO: Deleting pod "simpletest.rc-mn79z" in namespace "gc-2181"
  Jan 10 15:51:03.846: INFO: Deleting pod "simpletest.rc-q5t6m" in namespace "gc-2181"
  Jan 10 15:51:03.886: INFO: Deleting pod "simpletest.rc-q64pg" in namespace "gc-2181"
  Jan 10 15:51:03.917: INFO: Deleting pod "simpletest.rc-q6fq9" in namespace "gc-2181"
  Jan 10 15:51:03.970: INFO: Deleting pod "simpletest.rc-q8cdz" in namespace "gc-2181"
  Jan 10 15:51:04.002: INFO: Deleting pod "simpletest.rc-qxdhj" in namespace "gc-2181"
  Jan 10 15:51:04.028: INFO: Deleting pod "simpletest.rc-r2bwb" in namespace "gc-2181"
  Jan 10 15:51:04.065: INFO: Deleting pod "simpletest.rc-r4pvp" in namespace "gc-2181"
  Jan 10 15:51:04.092: INFO: Deleting pod "simpletest.rc-rrtrr" in namespace "gc-2181"
  Jan 10 15:51:04.125: INFO: Deleting pod "simpletest.rc-svfn7" in namespace "gc-2181"
  Jan 10 15:51:04.161: INFO: Deleting pod "simpletest.rc-tdg7l" in namespace "gc-2181"
  Jan 10 15:51:04.202: INFO: Deleting pod "simpletest.rc-tdszq" in namespace "gc-2181"
  Jan 10 15:51:04.263: INFO: Deleting pod "simpletest.rc-tnvkp" in namespace "gc-2181"
  Jan 10 15:51:04.350: INFO: Deleting pod "simpletest.rc-tp79t" in namespace "gc-2181"
  Jan 10 15:51:04.404: INFO: Deleting pod "simpletest.rc-ts6r6" in namespace "gc-2181"
  Jan 10 15:51:04.433: INFO: Deleting pod "simpletest.rc-v2s64" in namespace "gc-2181"
  Jan 10 15:51:04.478: INFO: Deleting pod "simpletest.rc-v6pzr" in namespace "gc-2181"
  Jan 10 15:51:04.519: INFO: Deleting pod "simpletest.rc-v8grm" in namespace "gc-2181"
  Jan 10 15:51:04.545: INFO: Deleting pod "simpletest.rc-vdvmz" in namespace "gc-2181"
  Jan 10 15:51:04.602: INFO: Deleting pod "simpletest.rc-wdbx4" in namespace "gc-2181"
  Jan 10 15:51:04.644: INFO: Deleting pod "simpletest.rc-whwvk" in namespace "gc-2181"
  Jan 10 15:51:04.688: INFO: Deleting pod "simpletest.rc-wpxlg" in namespace "gc-2181"
  Jan 10 15:51:04.714: INFO: Deleting pod "simpletest.rc-wqhdd" in namespace "gc-2181"
  Jan 10 15:51:04.752: INFO: Deleting pod "simpletest.rc-wr27d" in namespace "gc-2181"
  Jan 10 15:51:04.789: INFO: Deleting pod "simpletest.rc-wxhq9" in namespace "gc-2181"
  Jan 10 15:51:04.826: INFO: Deleting pod "simpletest.rc-wz7k9" in namespace "gc-2181"
  Jan 10 15:51:04.864: INFO: Deleting pod "simpletest.rc-x29vv" in namespace "gc-2181"
  Jan 10 15:51:04.898: INFO: Deleting pod "simpletest.rc-x57rk" in namespace "gc-2181"
  Jan 10 15:51:04.953: INFO: Deleting pod "simpletest.rc-xbd6n" in namespace "gc-2181"
  Jan 10 15:51:04.990: INFO: Deleting pod "simpletest.rc-xfsp8" in namespace "gc-2181"
  Jan 10 15:51:05.029: INFO: Deleting pod "simpletest.rc-xk6df" in namespace "gc-2181"
  Jan 10 15:51:05.067: INFO: Deleting pod "simpletest.rc-z7q9x" in namespace "gc-2181"
  Jan 10 15:51:05.124: INFO: Deleting pod "simpletest.rc-z856p" in namespace "gc-2181"
  Jan 10 15:51:05.163: INFO: Deleting pod "simpletest.rc-zrmcv" in namespace "gc-2181"
  Jan 10 15:51:05.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-2181" for this suite. @ 01/10/24 15:51:05.227
• [45.777 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:198
  STEP: Creating a kubernetes client @ 01/10/24 15:51:05.246
  Jan 10 15:51:05.246: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename custom-resource-definition @ 01/10/24 15:51:05.247
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:51:05.289
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:51:05.297
  STEP: fetching the /apis discovery document @ 01/10/24 15:51:05.303
  STEP: finding the apiextensions.k8s.io API group in the /apis discovery document @ 01/10/24 15:51:05.305
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document @ 01/10/24 15:51:05.305
  STEP: fetching the /apis/apiextensions.k8s.io discovery document @ 01/10/24 15:51:05.305
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document @ 01/10/24 15:51:05.306
  STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document @ 01/10/24 15:51:05.306
  STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document @ 01/10/24 15:51:05.308
  Jan 10 15:51:05.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-435" for this suite. @ 01/10/24 15:51:05.32
• [0.104 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]
test/e2e/scheduling/preemption.go:224
  STEP: Creating a kubernetes client @ 01/10/24 15:51:05.353
  Jan 10 15:51:05.353: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename sched-preemption @ 01/10/24 15:51:05.355
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:51:05.397
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:51:05.41
  Jan 10 15:51:05.469: INFO: Waiting up to 1m0s for all nodes to be ready
  Jan 10 15:52:05.592: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 01/10/24 15:52:05.6
  Jan 10 15:52:05.695: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Jan 10 15:52:05.717: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Jan 10 15:52:05.783: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Jan 10 15:52:05.808: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 01/10/24 15:52:05.809
  STEP: Run a critical pod that use same resources as that of a lower priority pod @ 01/10/24 15:52:07.883
  Jan 10 15:52:12.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-3243" for this suite. @ 01/10/24 15:52:12.181
• [66.854 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2202
  STEP: Creating a kubernetes client @ 01/10/24 15:52:12.221
  Jan 10 15:52:12.221: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename services @ 01/10/24 15:52:12.224
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:52:12.26
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:52:12.27
  STEP: creating service in namespace services-4464 @ 01/10/24 15:52:12.277
  STEP: creating service affinity-nodeport in namespace services-4464 @ 01/10/24 15:52:12.277
  STEP: creating replication controller affinity-nodeport in namespace services-4464 @ 01/10/24 15:52:12.313
  I0110 15:52:12.327268      23 runners.go:194] Created replication controller with name: affinity-nodeport, namespace: services-4464, replica count: 3
  I0110 15:52:15.378954      23 runners.go:194] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jan 10 15:52:15.419: INFO: Creating new exec pod
  Jan 10 15:52:18.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=services-4464 exec execpod-affinitylpjt5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
  Jan 10 15:52:18.944: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
  Jan 10 15:52:18.944: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 10 15:52:18.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=services-4464 exec execpod-affinitylpjt5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.48.4 80'
  Jan 10 15:52:19.299: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.48.4 80\nConnection to 10.233.48.4 80 port [tcp/http] succeeded!\n"
  Jan 10 15:52:19.300: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 10 15:52:19.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=services-4464 exec execpod-affinitylpjt5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.61.1.200 31592'
  Jan 10 15:52:19.641: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.61.1.200 31592\nConnection to 10.61.1.200 31592 port [tcp/*] succeeded!\n"
  Jan 10 15:52:19.641: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 10 15:52:19.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=services-4464 exec execpod-affinitylpjt5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.61.1.201 31592'
  Jan 10 15:52:19.998: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.61.1.201 31592\nConnection to 10.61.1.201 31592 port [tcp/*] succeeded!\n"
  Jan 10 15:52:19.999: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 10 15:52:19.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=services-4464 exec execpod-affinitylpjt5 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.61.1.200:31592/ ; done'
  Jan 10 15:52:20.544: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:31592/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:31592/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:31592/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:31592/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:31592/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:31592/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:31592/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:31592/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:31592/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:31592/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:31592/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:31592/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:31592/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:31592/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:31592/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.61.1.200:31592/\n"
  Jan 10 15:52:20.545: INFO: stdout: "\naffinity-nodeport-phrx6\naffinity-nodeport-phrx6\naffinity-nodeport-phrx6\naffinity-nodeport-phrx6\naffinity-nodeport-phrx6\naffinity-nodeport-phrx6\naffinity-nodeport-phrx6\naffinity-nodeport-phrx6\naffinity-nodeport-phrx6\naffinity-nodeport-phrx6\naffinity-nodeport-phrx6\naffinity-nodeport-phrx6\naffinity-nodeport-phrx6\naffinity-nodeport-phrx6\naffinity-nodeport-phrx6\naffinity-nodeport-phrx6"
  Jan 10 15:52:20.545: INFO: Received response from host: affinity-nodeport-phrx6
  Jan 10 15:52:20.545: INFO: Received response from host: affinity-nodeport-phrx6
  Jan 10 15:52:20.545: INFO: Received response from host: affinity-nodeport-phrx6
  Jan 10 15:52:20.545: INFO: Received response from host: affinity-nodeport-phrx6
  Jan 10 15:52:20.545: INFO: Received response from host: affinity-nodeport-phrx6
  Jan 10 15:52:20.545: INFO: Received response from host: affinity-nodeport-phrx6
  Jan 10 15:52:20.545: INFO: Received response from host: affinity-nodeport-phrx6
  Jan 10 15:52:20.545: INFO: Received response from host: affinity-nodeport-phrx6
  Jan 10 15:52:20.545: INFO: Received response from host: affinity-nodeport-phrx6
  Jan 10 15:52:20.545: INFO: Received response from host: affinity-nodeport-phrx6
  Jan 10 15:52:20.545: INFO: Received response from host: affinity-nodeport-phrx6
  Jan 10 15:52:20.545: INFO: Received response from host: affinity-nodeport-phrx6
  Jan 10 15:52:20.545: INFO: Received response from host: affinity-nodeport-phrx6
  Jan 10 15:52:20.545: INFO: Received response from host: affinity-nodeport-phrx6
  Jan 10 15:52:20.545: INFO: Received response from host: affinity-nodeport-phrx6
  Jan 10 15:52:20.545: INFO: Received response from host: affinity-nodeport-phrx6
  Jan 10 15:52:20.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jan 10 15:52:20.578: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport in namespace services-4464, will wait for the garbage collector to delete the pods @ 01/10/24 15:52:20.626
  Jan 10 15:52:20.721: INFO: Deleting ReplicationController affinity-nodeport took: 33.084294ms
  Jan 10 15:52:20.822: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.89865ms
  STEP: Destroying namespace "services-4464" for this suite. @ 01/10/24 15:52:22.87
• [10.669 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:99
  STEP: Creating a kubernetes client @ 01/10/24 15:52:22.891
  Jan 10 15:52:22.891: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename configmap @ 01/10/24 15:52:22.893
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:52:22.95
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:52:22.959
  STEP: Creating configMap with name configmap-test-volume-map-53358f1b-b821-4120-a76b-444b848e704a @ 01/10/24 15:52:22.969
  STEP: Creating a pod to test consume configMaps @ 01/10/24 15:52:22.981
  STEP: Saw pod success @ 01/10/24 15:52:27.037
  Jan 10 15:52:27.045: INFO: Trying to get logs from node env1-test-worker-1 pod pod-configmaps-32e38cc7-0d0c-4897-8820-228582503ec7 container agnhost-container: <nil>
  STEP: delete the pod @ 01/10/24 15:52:27.088
  Jan 10 15:52:27.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7752" for this suite. @ 01/10/24 15:52:27.147
• [4.277 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:252
  STEP: Creating a kubernetes client @ 01/10/24 15:52:27.17
  Jan 10 15:52:27.170: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename namespaces @ 01/10/24 15:52:27.172
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:52:27.21
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:52:27.219
  STEP: Creating a test namespace @ 01/10/24 15:52:27.225
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:52:27.276
  STEP: Creating a service in the namespace @ 01/10/24 15:52:27.286
  STEP: Deleting the namespace @ 01/10/24 15:52:27.316
  STEP: Waiting for the namespace to be removed. @ 01/10/24 15:52:27.358
  STEP: Recreating the namespace @ 01/10/24 15:52:34.371
  STEP: Verifying there is no service in the namespace @ 01/10/24 15:52:34.422
  Jan 10 15:52:34.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-6399" for this suite. @ 01/10/24 15:52:34.458
  STEP: Destroying namespace "nsdeletetest-258" for this suite. @ 01/10/24 15:52:34.493
  Jan 10 15:52:34.504: INFO: Namespace nsdeletetest-258 was already deleted
  STEP: Destroying namespace "nsdeletetest-7478" for this suite. @ 01/10/24 15:52:34.505
• [7.359 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]
test/e2e/node/taints.go:290
  STEP: Creating a kubernetes client @ 01/10/24 15:52:34.532
  Jan 10 15:52:34.532: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename taint-single-pod @ 01/10/24 15:52:34.535
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:52:34.633
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:52:34.65
  Jan 10 15:52:34.657: INFO: Waiting up to 1m0s for all nodes to be ready
  Jan 10 15:53:34.744: INFO: Waiting for terminating namespaces to be deleted...
  Jan 10 15:53:34.753: INFO: Starting informer...
  STEP: Starting pod... @ 01/10/24 15:53:34.753
  Jan 10 15:53:34.987: INFO: Pod is running on env1-test-worker-1. Tainting Node
  STEP: Trying to apply a taint on the Node @ 01/10/24 15:53:34.987
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 01/10/24 15:53:35.022
  STEP: Waiting short time to make sure Pod is queued for deletion @ 01/10/24 15:53:35.041
  Jan 10 15:53:35.041: INFO: Pod wasn't evicted. Proceeding
  Jan 10 15:53:35.041: INFO: Removing taint from Node
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 01/10/24 15:53:35.127
  STEP: Waiting some time to make sure that toleration time passed. @ 01/10/24 15:53:35.182
  Jan 10 15:54:50.183: INFO: Pod wasn't evicted. Test successful
  Jan 10 15:54:50.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "taint-single-pod-5853" for this suite. @ 01/10/24 15:54:50.203
• [135.692 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should fail to create secret due to empty secret key [Conformance]
test/e2e/common/node/secrets.go:140
  STEP: Creating a kubernetes client @ 01/10/24 15:54:50.229
  Jan 10 15:54:50.229: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename secrets @ 01/10/24 15:54:50.231
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:54:50.275
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:54:50.285
  STEP: Creating projection with secret that has name secret-emptykey-test-06a7d1fc-48b4-42c8-a860-8ef662ed3604 @ 01/10/24 15:54:50.292
  Jan 10 15:54:50.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6188" for this suite. @ 01/10/24 15:54:50.312
• [0.104 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
test/e2e/network/dns.go:191
  STEP: Creating a kubernetes client @ 01/10/24 15:54:50.335
  Jan 10 15:54:50.335: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename dns @ 01/10/24 15:54:50.337
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:54:50.386
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:54:50.4
  STEP: Creating a test headless service @ 01/10/24 15:54:50.408
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3935 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3935;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3935 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3935;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3935.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3935.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3935.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3935.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3935.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3935.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3935.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3935.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3935.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3935.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3935.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3935.svc;check="$$(dig +notcp +noall +answer +search 120.54.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.54.120_udp@PTR;check="$$(dig +tcp +noall +answer +search 120.54.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.54.120_tcp@PTR;sleep 1; done
   @ 01/10/24 15:54:50.456
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3935 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3935;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3935 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3935;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3935.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3935.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3935.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3935.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3935.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3935.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3935.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3935.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3935.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3935.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3935.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3935.svc;check="$$(dig +notcp +noall +answer +search 120.54.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.54.120_udp@PTR;check="$$(dig +tcp +noall +answer +search 120.54.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.54.120_tcp@PTR;sleep 1; done
   @ 01/10/24 15:54:50.456
  STEP: creating a pod to probe DNS @ 01/10/24 15:54:50.457
  STEP: submitting the pod to kubernetes @ 01/10/24 15:54:50.457
  STEP: retrieving the pod @ 01/10/24 15:54:52.567
  STEP: looking for the results for each expected name from probers @ 01/10/24 15:54:52.58
  Jan 10 15:54:52.592: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3935/dns-test-1fc2b802-7ccb-4111-9f95-c61fef13f829: the server could not find the requested resource (get pods dns-test-1fc2b802-7ccb-4111-9f95-c61fef13f829)
  Jan 10 15:54:52.603: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3935/dns-test-1fc2b802-7ccb-4111-9f95-c61fef13f829: the server could not find the requested resource (get pods dns-test-1fc2b802-7ccb-4111-9f95-c61fef13f829)
  Jan 10 15:54:52.634: INFO: Unable to read wheezy_udp@dns-test-service.dns-3935 from pod dns-3935/dns-test-1fc2b802-7ccb-4111-9f95-c61fef13f829: the server could not find the requested resource (get pods dns-test-1fc2b802-7ccb-4111-9f95-c61fef13f829)
  Jan 10 15:54:52.657: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3935 from pod dns-3935/dns-test-1fc2b802-7ccb-4111-9f95-c61fef13f829: the server could not find the requested resource (get pods dns-test-1fc2b802-7ccb-4111-9f95-c61fef13f829)
  Jan 10 15:54:52.669: INFO: Unable to read wheezy_udp@dns-test-service.dns-3935.svc from pod dns-3935/dns-test-1fc2b802-7ccb-4111-9f95-c61fef13f829: the server could not find the requested resource (get pods dns-test-1fc2b802-7ccb-4111-9f95-c61fef13f829)
  Jan 10 15:54:52.679: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3935.svc from pod dns-3935/dns-test-1fc2b802-7ccb-4111-9f95-c61fef13f829: the server could not find the requested resource (get pods dns-test-1fc2b802-7ccb-4111-9f95-c61fef13f829)
  Jan 10 15:54:52.688: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3935.svc from pod dns-3935/dns-test-1fc2b802-7ccb-4111-9f95-c61fef13f829: the server could not find the requested resource (get pods dns-test-1fc2b802-7ccb-4111-9f95-c61fef13f829)
  Jan 10 15:54:52.697: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3935.svc from pod dns-3935/dns-test-1fc2b802-7ccb-4111-9f95-c61fef13f829: the server could not find the requested resource (get pods dns-test-1fc2b802-7ccb-4111-9f95-c61fef13f829)
  Jan 10 15:54:52.746: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3935/dns-test-1fc2b802-7ccb-4111-9f95-c61fef13f829: the server could not find the requested resource (get pods dns-test-1fc2b802-7ccb-4111-9f95-c61fef13f829)
  Jan 10 15:54:52.755: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3935/dns-test-1fc2b802-7ccb-4111-9f95-c61fef13f829: the server could not find the requested resource (get pods dns-test-1fc2b802-7ccb-4111-9f95-c61fef13f829)
  Jan 10 15:54:52.769: INFO: Unable to read jessie_udp@dns-test-service.dns-3935 from pod dns-3935/dns-test-1fc2b802-7ccb-4111-9f95-c61fef13f829: the server could not find the requested resource (get pods dns-test-1fc2b802-7ccb-4111-9f95-c61fef13f829)
  Jan 10 15:54:52.777: INFO: Unable to read jessie_tcp@dns-test-service.dns-3935 from pod dns-3935/dns-test-1fc2b802-7ccb-4111-9f95-c61fef13f829: the server could not find the requested resource (get pods dns-test-1fc2b802-7ccb-4111-9f95-c61fef13f829)
  Jan 10 15:54:52.787: INFO: Unable to read jessie_udp@dns-test-service.dns-3935.svc from pod dns-3935/dns-test-1fc2b802-7ccb-4111-9f95-c61fef13f829: the server could not find the requested resource (get pods dns-test-1fc2b802-7ccb-4111-9f95-c61fef13f829)
  Jan 10 15:54:52.798: INFO: Unable to read jessie_tcp@dns-test-service.dns-3935.svc from pod dns-3935/dns-test-1fc2b802-7ccb-4111-9f95-c61fef13f829: the server could not find the requested resource (get pods dns-test-1fc2b802-7ccb-4111-9f95-c61fef13f829)
  Jan 10 15:54:52.806: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3935.svc from pod dns-3935/dns-test-1fc2b802-7ccb-4111-9f95-c61fef13f829: the server could not find the requested resource (get pods dns-test-1fc2b802-7ccb-4111-9f95-c61fef13f829)
  Jan 10 15:54:52.822: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3935.svc from pod dns-3935/dns-test-1fc2b802-7ccb-4111-9f95-c61fef13f829: the server could not find the requested resource (get pods dns-test-1fc2b802-7ccb-4111-9f95-c61fef13f829)
  Jan 10 15:54:52.865: INFO: Lookups using dns-3935/dns-test-1fc2b802-7ccb-4111-9f95-c61fef13f829 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3935 wheezy_tcp@dns-test-service.dns-3935 wheezy_udp@dns-test-service.dns-3935.svc wheezy_tcp@dns-test-service.dns-3935.svc wheezy_udp@_http._tcp.dns-test-service.dns-3935.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3935.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3935 jessie_tcp@dns-test-service.dns-3935 jessie_udp@dns-test-service.dns-3935.svc jessie_tcp@dns-test-service.dns-3935.svc jessie_udp@_http._tcp.dns-test-service.dns-3935.svc jessie_tcp@_http._tcp.dns-test-service.dns-3935.svc]

  Jan 10 15:54:58.101: INFO: DNS probes using dns-3935/dns-test-1fc2b802-7ccb-4111-9f95-c61fef13f829 succeeded

  Jan 10 15:54:58.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 01/10/24 15:54:58.117
  STEP: deleting the test service @ 01/10/24 15:54:58.202
  STEP: deleting the test headless service @ 01/10/24 15:54:58.321
  STEP: Destroying namespace "dns-3935" for this suite. @ 01/10/24 15:54:58.395
• [8.083 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]
test/e2e/apimachinery/webhook.go:220
  STEP: Creating a kubernetes client @ 01/10/24 15:54:58.425
  Jan 10 15:54:58.425: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename webhook @ 01/10/24 15:54:58.428
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:54:58.457
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:54:58.464
  STEP: Setting up server cert @ 01/10/24 15:54:58.524
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 01/10/24 15:54:59.575
  STEP: Deploying the webhook pod @ 01/10/24 15:54:59.596
  STEP: Wait for the deployment to be ready @ 01/10/24 15:54:59.626
  Jan 10 15:54:59.645: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 01/10/24 15:55:01.678
  STEP: Verifying the service has paired with the endpoint @ 01/10/24 15:55:01.702
  Jan 10 15:55:02.703: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jan 10 15:55:02.711: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Registering the custom resource webhook via the AdmissionRegistration API @ 01/10/24 15:55:08.245
  STEP: Creating a custom resource that should be denied by the webhook @ 01/10/24 15:55:08.282
  STEP: Creating a custom resource whose deletion would be denied by the webhook @ 01/10/24 15:55:10.35
  STEP: Updating the custom resource with disallowed data should be denied @ 01/10/24 15:55:10.371
  STEP: Deleting the custom resource should be denied @ 01/10/24 15:55:10.392
  STEP: Remove the offending key and value from the custom resource data @ 01/10/24 15:55:10.41
  STEP: Deleting the updated custom resource should be successful @ 01/10/24 15:55:10.434
  Jan 10 15:55:10.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-763" for this suite. @ 01/10/24 15:55:11.184
  STEP: Destroying namespace "webhook-markers-5899" for this suite. @ 01/10/24 15:55:11.207
• [12.809 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Subdomain [Conformance]
test/e2e/network/dns.go:286
  STEP: Creating a kubernetes client @ 01/10/24 15:55:11.24
  Jan 10 15:55:11.240: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename dns @ 01/10/24 15:55:11.242
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:55:11.298
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:55:11.305
  STEP: Creating a test headless service @ 01/10/24 15:55:11.314
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2081.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-2081.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2081.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2081.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2081.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-2081.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2081.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-2081.svc.cluster.local;sleep 1; done
   @ 01/10/24 15:55:11.358
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2081.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-2081.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2081.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-2081.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2081.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-2081.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2081.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-2081.svc.cluster.local;sleep 1; done
   @ 01/10/24 15:55:11.359
  STEP: creating a pod to probe DNS @ 01/10/24 15:55:11.359
  STEP: submitting the pod to kubernetes @ 01/10/24 15:55:11.359
  STEP: retrieving the pod @ 01/10/24 15:55:15.443
  STEP: looking for the results for each expected name from probers @ 01/10/24 15:55:15.452
  Jan 10 15:55:15.464: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2081.svc.cluster.local from pod dns-2081/dns-test-041ca672-be28-4f31-9b09-c4ff068cc877: the server could not find the requested resource (get pods dns-test-041ca672-be28-4f31-9b09-c4ff068cc877)
  Jan 10 15:55:15.486: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2081.svc.cluster.local from pod dns-2081/dns-test-041ca672-be28-4f31-9b09-c4ff068cc877: the server could not find the requested resource (get pods dns-test-041ca672-be28-4f31-9b09-c4ff068cc877)
  Jan 10 15:55:15.497: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2081.svc.cluster.local from pod dns-2081/dns-test-041ca672-be28-4f31-9b09-c4ff068cc877: the server could not find the requested resource (get pods dns-test-041ca672-be28-4f31-9b09-c4ff068cc877)
  Jan 10 15:55:15.506: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2081.svc.cluster.local from pod dns-2081/dns-test-041ca672-be28-4f31-9b09-c4ff068cc877: the server could not find the requested resource (get pods dns-test-041ca672-be28-4f31-9b09-c4ff068cc877)
  Jan 10 15:55:15.515: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2081.svc.cluster.local from pod dns-2081/dns-test-041ca672-be28-4f31-9b09-c4ff068cc877: the server could not find the requested resource (get pods dns-test-041ca672-be28-4f31-9b09-c4ff068cc877)
  Jan 10 15:55:15.524: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2081.svc.cluster.local from pod dns-2081/dns-test-041ca672-be28-4f31-9b09-c4ff068cc877: the server could not find the requested resource (get pods dns-test-041ca672-be28-4f31-9b09-c4ff068cc877)
  Jan 10 15:55:15.535: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2081.svc.cluster.local from pod dns-2081/dns-test-041ca672-be28-4f31-9b09-c4ff068cc877: the server could not find the requested resource (get pods dns-test-041ca672-be28-4f31-9b09-c4ff068cc877)
  Jan 10 15:55:15.550: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2081.svc.cluster.local from pod dns-2081/dns-test-041ca672-be28-4f31-9b09-c4ff068cc877: the server could not find the requested resource (get pods dns-test-041ca672-be28-4f31-9b09-c4ff068cc877)
  Jan 10 15:55:15.550: INFO: Lookups using dns-2081/dns-test-041ca672-be28-4f31-9b09-c4ff068cc877 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2081.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2081.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2081.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2081.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2081.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2081.svc.cluster.local jessie_udp@dns-test-service-2.dns-2081.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2081.svc.cluster.local]

  Jan 10 15:55:20.650: INFO: DNS probes using dns-2081/dns-test-041ca672-be28-4f31-9b09-c4ff068cc877 succeeded

  Jan 10 15:55:20.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 01/10/24 15:55:20.663
  STEP: deleting the test headless service @ 01/10/24 15:55:20.702
  STEP: Destroying namespace "dns-2081" for this suite. @ 01/10/24 15:55:20.746
• [9.534 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:109
  STEP: Creating a kubernetes client @ 01/10/24 15:55:20.779
  Jan 10 15:55:20.779: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename projected @ 01/10/24 15:55:20.781
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:55:20.816
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:55:20.822
  STEP: Creating configMap with name projected-configmap-test-volume-map-35a36344-afbc-41a8-ae67-55589abb4a16 @ 01/10/24 15:55:20.829
  STEP: Creating a pod to test consume configMaps @ 01/10/24 15:55:20.838
  STEP: Saw pod success @ 01/10/24 15:55:24.982
  Jan 10 15:55:24.991: INFO: Trying to get logs from node env1-test-worker-1 pod pod-projected-configmaps-53d9058a-3b19-4176-9c60-015e34e5e735 container agnhost-container: <nil>
  STEP: delete the pod @ 01/10/24 15:55:25.038
  Jan 10 15:55:25.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3978" for this suite. @ 01/10/24 15:55:25.097
• [4.335 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:609
  STEP: Creating a kubernetes client @ 01/10/24 15:55:25.115
  Jan 10 15:55:25.115: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename security-context-test @ 01/10/24 15:55:25.116
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:55:25.161
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:55:25.168
  Jan 10 15:55:29.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-2148" for this suite. @ 01/10/24 15:55:29.275
• [4.184 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:198
  STEP: Creating a kubernetes client @ 01/10/24 15:55:29.314
  Jan 10 15:55:29.314: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename container-probe @ 01/10/24 15:55:29.318
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:55:29.369
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:55:29.375
  STEP: Creating pod liveness-081f5a13-b1a7-4699-b5a0-05fdc768bf21 in namespace container-probe-9004 @ 01/10/24 15:55:29.384
  Jan 10 15:55:31.437: INFO: Started pod liveness-081f5a13-b1a7-4699-b5a0-05fdc768bf21 in namespace container-probe-9004
  STEP: checking the pod's current state and verifying that restartCount is present @ 01/10/24 15:55:31.437
  Jan 10 15:55:31.447: INFO: Initial restart count of pod liveness-081f5a13-b1a7-4699-b5a0-05fdc768bf21 is 0
  Jan 10 15:55:51.580: INFO: Restart count of pod container-probe-9004/liveness-081f5a13-b1a7-4699-b5a0-05fdc768bf21 is now 1 (20.132922891s elapsed)
  Jan 10 15:56:11.705: INFO: Restart count of pod container-probe-9004/liveness-081f5a13-b1a7-4699-b5a0-05fdc768bf21 is now 2 (40.257369675s elapsed)
  Jan 10 15:56:31.813: INFO: Restart count of pod container-probe-9004/liveness-081f5a13-b1a7-4699-b5a0-05fdc768bf21 is now 3 (1m0.365961036s elapsed)
  Jan 10 15:56:51.941: INFO: Restart count of pod container-probe-9004/liveness-081f5a13-b1a7-4699-b5a0-05fdc768bf21 is now 4 (1m20.493537615s elapsed)
  Jan 10 15:57:52.277: INFO: Restart count of pod container-probe-9004/liveness-081f5a13-b1a7-4699-b5a0-05fdc768bf21 is now 5 (2m20.829980532s elapsed)
  Jan 10 15:57:52.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 01/10/24 15:57:52.292
  STEP: Destroying namespace "container-probe-9004" for this suite. @ 01/10/24 15:57:52.322
• [143.023 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:321
  STEP: Creating a kubernetes client @ 01/10/24 15:57:52.34
  Jan 10 15:57:52.340: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename gc @ 01/10/24 15:57:52.342
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:57:52.38
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:57:52.388
  STEP: create the rc @ 01/10/24 15:57:52.395
  W0110 15:57:52.415988      23 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 01/10/24 15:57:57.433
  STEP: wait for all pods to be garbage collected @ 01/10/24 15:57:57.45
  STEP: Gathering metrics @ 01/10/24 15:58:02.464
  Jan 10 15:58:02.691: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jan 10 15:58:02.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-8525" for this suite. @ 01/10/24 15:58:02.706
• [10.382 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]
test/e2e/apps/cronjob.go:70
  STEP: Creating a kubernetes client @ 01/10/24 15:58:02.738
  Jan 10 15:58:02.738: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename cronjob @ 01/10/24 15:58:02.74
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 15:58:02.782
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 15:58:02.788
  STEP: Creating a cronjob @ 01/10/24 15:58:02.794
  STEP: Ensuring more than one job is running at a time @ 01/10/24 15:58:02.808
  STEP: Ensuring at least two running jobs exists by listing jobs explicitly @ 01/10/24 16:00:00.818
  STEP: Removing cronjob @ 01/10/24 16:00:00.829
  Jan 10 16:00:00.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-5006" for this suite. @ 01/10/24 16:00:00.876
• [118.158 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]
test/e2e/apimachinery/resource_quota.go:887
  STEP: Creating a kubernetes client @ 01/10/24 16:00:00.897
  Jan 10 16:00:00.897: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename resourcequota @ 01/10/24 16:00:00.898
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:00:00.962
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:00:00.971
  STEP: Creating a ResourceQuota @ 01/10/24 16:00:00.989
  STEP: Getting a ResourceQuota @ 01/10/24 16:00:01.004
  STEP: Updating a ResourceQuota @ 01/10/24 16:00:01.019
  STEP: Verifying a ResourceQuota was modified @ 01/10/24 16:00:01.036
  STEP: Deleting a ResourceQuota @ 01/10/24 16:00:01.045
  STEP: Verifying the deleted ResourceQuota @ 01/10/24 16:00:01.068
  Jan 10 16:00:01.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5720" for this suite. @ 01/10/24 16:00:01.098
• [0.235 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:45
  STEP: Creating a kubernetes client @ 01/10/24 16:00:01.137
  Jan 10 16:00:01.137: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename downward-api @ 01/10/24 16:00:01.14
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:00:01.207
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:00:01.218
  STEP: Creating a pod to test downward api env vars @ 01/10/24 16:00:01.228
  STEP: Saw pod success @ 01/10/24 16:00:05.288
  Jan 10 16:00:05.295: INFO: Trying to get logs from node env1-test-worker-1 pod downward-api-ad638a76-77b5-4710-9461-b09aee8e06d5 container dapi-container: <nil>
  STEP: delete the pod @ 01/10/24 16:00:05.347
  Jan 10 16:00:05.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9081" for this suite. @ 01/10/24 16:00:05.391
• [4.269 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]
test/e2e/apimachinery/resource_quota.go:806
  STEP: Creating a kubernetes client @ 01/10/24 16:00:05.406
  Jan 10 16:00:05.406: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename resourcequota @ 01/10/24 16:00:05.408
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:00:05.436
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:00:05.442
  STEP: Creating a ResourceQuota with best effort scope @ 01/10/24 16:00:05.448
  STEP: Ensuring ResourceQuota status is calculated @ 01/10/24 16:00:05.458
  STEP: Creating a ResourceQuota with not best effort scope @ 01/10/24 16:00:07.468
  STEP: Ensuring ResourceQuota status is calculated @ 01/10/24 16:00:07.479
  STEP: Creating a best-effort pod @ 01/10/24 16:00:09.488
  STEP: Ensuring resource quota with best effort scope captures the pod usage @ 01/10/24 16:00:09.511
  STEP: Ensuring resource quota with not best effort ignored the pod usage @ 01/10/24 16:00:11.517
  STEP: Deleting the pod @ 01/10/24 16:00:13.527
  STEP: Ensuring resource quota status released the pod usage @ 01/10/24 16:00:13.585
  STEP: Creating a not best-effort pod @ 01/10/24 16:00:15.598
  STEP: Ensuring resource quota with not best effort scope captures the pod usage @ 01/10/24 16:00:15.624
  STEP: Ensuring resource quota with best effort scope ignored the pod usage @ 01/10/24 16:00:17.637
  STEP: Deleting the pod @ 01/10/24 16:00:19.645
  STEP: Ensuring resource quota status released the pod usage @ 01/10/24 16:00:19.693
  Jan 10 16:00:21.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-8996" for this suite. @ 01/10/24 16:00:21.713
• [16.320 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]
test/e2e/apps/replica_set.go:143
  STEP: Creating a kubernetes client @ 01/10/24 16:00:21.729
  Jan 10 16:00:21.729: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename replicaset @ 01/10/24 16:00:21.73
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:00:21.757
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:00:21.768
  STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota @ 01/10/24 16:00:21.772
  Jan 10 16:00:21.795: INFO: Pod name sample-pod: Found 0 pods out of 1
  Jan 10 16:00:26.811: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 01/10/24 16:00:26.811
  STEP: getting scale subresource @ 01/10/24 16:00:26.811
  STEP: updating a scale subresource @ 01/10/24 16:00:26.821
  STEP: verifying the replicaset Spec.Replicas was modified @ 01/10/24 16:00:26.837
  STEP: Patch a scale subresource @ 01/10/24 16:00:26.851
  Jan 10 16:00:26.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-1276" for this suite. @ 01/10/24 16:00:26.921
• [5.229 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
test/e2e/common/node/expansion.go:189
  STEP: Creating a kubernetes client @ 01/10/24 16:00:26.959
  Jan 10 16:00:26.959: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename var-expansion @ 01/10/24 16:00:26.961
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:00:27.045
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:00:27.053
  Jan 10 16:00:29.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jan 10 16:00:29.140: INFO: Deleting pod "var-expansion-842a4109-2af4-47c8-adb8-e3e586967a54" in namespace "var-expansion-1551"
  Jan 10 16:00:29.157: INFO: Wait up to 5m0s for pod "var-expansion-842a4109-2af4-47c8-adb8-e3e586967a54" to be fully deleted
  STEP: Destroying namespace "var-expansion-1551" for this suite. @ 01/10/24 16:00:31.174
• [4.233 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]
test/e2e/apps/daemon_set.go:194
  STEP: Creating a kubernetes client @ 01/10/24 16:00:31.193
  Jan 10 16:00:31.193: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename daemonsets @ 01/10/24 16:00:31.194
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:00:31.227
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:00:31.239
  Jan 10 16:00:31.299: INFO: Creating daemon "daemon-set" with a node selector
  STEP: Initially, daemon pods should not be running on any nodes. @ 01/10/24 16:00:31.311
  Jan 10 16:00:31.323: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 10 16:00:31.323: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Change node label to blue, check that daemon pod is launched. @ 01/10/24 16:00:31.323
  Jan 10 16:00:31.409: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 10 16:00:31.409: INFO: Node env1-test-worker-1 is running 0 daemon pod, expected 1
  Jan 10 16:00:32.426: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 10 16:00:32.426: INFO: Node env1-test-worker-1 is running 0 daemon pod, expected 1
  Jan 10 16:00:33.418: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jan 10 16:00:33.418: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Update the node label to green, and wait for daemons to be unscheduled @ 01/10/24 16:00:33.424
  Jan 10 16:00:33.464: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jan 10 16:00:33.464: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
  Jan 10 16:00:34.472: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 10 16:00:34.472: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate @ 01/10/24 16:00:34.472
  Jan 10 16:00:34.503: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 10 16:00:34.503: INFO: Node env1-test-worker-1 is running 0 daemon pod, expected 1
  Jan 10 16:00:35.511: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 10 16:00:35.511: INFO: Node env1-test-worker-1 is running 0 daemon pod, expected 1
  Jan 10 16:00:36.513: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 10 16:00:36.513: INFO: Node env1-test-worker-1 is running 0 daemon pod, expected 1
  Jan 10 16:00:37.511: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jan 10 16:00:37.512: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 01/10/24 16:00:37.535
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1940, will wait for the garbage collector to delete the pods @ 01/10/24 16:00:37.535
  Jan 10 16:00:37.615: INFO: Deleting DaemonSet.extensions daemon-set took: 12.324157ms
  Jan 10 16:00:37.717: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.198443ms
  Jan 10 16:00:39.325: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 10 16:00:39.325: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jan 10 16:00:39.333: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"186775673"},"items":null}

  Jan 10 16:00:39.339: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"186775673"},"items":null}

  Jan 10 16:00:39.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-1940" for this suite. @ 01/10/24 16:00:39.425
• [8.246 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]
test/e2e/common/node/configmap.go:169
  STEP: Creating a kubernetes client @ 01/10/24 16:00:39.44
  Jan 10 16:00:39.440: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename configmap @ 01/10/24 16:00:39.442
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:00:39.483
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:00:39.488
  STEP: creating a ConfigMap @ 01/10/24 16:00:39.494
  STEP: fetching the ConfigMap @ 01/10/24 16:00:39.505
  STEP: patching the ConfigMap @ 01/10/24 16:00:39.512
  STEP: listing all ConfigMaps in all namespaces with a label selector @ 01/10/24 16:00:39.521
  STEP: deleting the ConfigMap by collection with a label selector @ 01/10/24 16:00:39.532
  STEP: listing all ConfigMaps in test namespace @ 01/10/24 16:00:39.548
  Jan 10 16:00:39.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9315" for this suite. @ 01/10/24 16:00:39.571
• [0.144 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange should list, patch and delete a LimitRange by collection [Conformance]
test/e2e/scheduling/limit_range.go:239
  STEP: Creating a kubernetes client @ 01/10/24 16:00:39.586
  Jan 10 16:00:39.586: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename limitrange @ 01/10/24 16:00:39.588
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:00:39.617
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:00:39.624
  STEP: Creating LimitRange "e2e-limitrange-njbjn" in namespace "limitrange-1997" @ 01/10/24 16:00:39.632
  STEP: Creating another limitRange in another namespace @ 01/10/24 16:00:39.645
  Jan 10 16:00:39.675: INFO: Namespace "e2e-limitrange-njbjn-7881" created
  Jan 10 16:00:39.675: INFO: Creating LimitRange "e2e-limitrange-njbjn" in namespace "e2e-limitrange-njbjn-7881"
  STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-njbjn" @ 01/10/24 16:00:39.684
  Jan 10 16:00:39.690: INFO: Found 2 limitRanges
  STEP: Patching LimitRange "e2e-limitrange-njbjn" in "limitrange-1997" namespace @ 01/10/24 16:00:39.691
  Jan 10 16:00:39.700: INFO: LimitRange "e2e-limitrange-njbjn" has been patched
  STEP: Delete LimitRange "e2e-limitrange-njbjn" by Collection with labelSelector: "e2e-limitrange-njbjn=patched" @ 01/10/24 16:00:39.701
  STEP: Confirm that the limitRange "e2e-limitrange-njbjn" has been deleted @ 01/10/24 16:00:39.718
  Jan 10 16:00:39.718: INFO: Requesting list of LimitRange to confirm quantity
  Jan 10 16:00:39.725: INFO: Found 0 LimitRange with label "e2e-limitrange-njbjn=patched"
  Jan 10 16:00:39.725: INFO: LimitRange "e2e-limitrange-njbjn" has been deleted.
  STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-njbjn" @ 01/10/24 16:00:39.725
  Jan 10 16:00:39.731: INFO: Found 1 limitRange
  Jan 10 16:00:39.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-1997" for this suite. @ 01/10/24 16:00:39.744
  STEP: Destroying namespace "e2e-limitrange-njbjn-7881" for this suite. @ 01/10/24 16:00:39.761
• [0.189 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]
test/e2e/apimachinery/webhook.go:237
  STEP: Creating a kubernetes client @ 01/10/24 16:00:39.782
  Jan 10 16:00:39.782: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename webhook @ 01/10/24 16:00:39.784
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:00:39.816
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:00:39.821
  STEP: Setting up server cert @ 01/10/24 16:00:39.874
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 01/10/24 16:00:40.637
  STEP: Deploying the webhook pod @ 01/10/24 16:00:40.654
  STEP: Wait for the deployment to be ready @ 01/10/24 16:00:40.678
  Jan 10 16:00:40.695: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 01/10/24 16:00:42.721
  STEP: Verifying the service has paired with the endpoint @ 01/10/24 16:00:42.744
  Jan 10 16:00:43.745: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API @ 01/10/24 16:00:43.752
  STEP: create a namespace for the webhook @ 01/10/24 16:00:43.795
  STEP: create a configmap should be unconditionally rejected by the webhook @ 01/10/24 16:00:43.825
  Jan 10 16:00:43.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-801" for this suite. @ 01/10/24 16:00:43.997
  STEP: Destroying namespace "webhook-markers-1763" for this suite. @ 01/10/24 16:00:44.028
  STEP: Destroying namespace "fail-closed-namespace-8476" for this suite. @ 01/10/24 16:00:44.041
• [4.276 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:75
  STEP: Creating a kubernetes client @ 01/10/24 16:00:44.065
  Jan 10 16:00:44.066: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename containers @ 01/10/24 16:00:44.068
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:00:44.11
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:00:44.115
  STEP: Creating a pod to test override command @ 01/10/24 16:00:44.126
  STEP: Saw pod success @ 01/10/24 16:00:48.178
  Jan 10 16:00:48.185: INFO: Trying to get logs from node env1-test-worker-1 pod client-containers-5f650df6-8839-43c7-a600-4b11af570c5c container agnhost-container: <nil>
  STEP: delete the pod @ 01/10/24 16:00:48.208
  Jan 10 16:00:48.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-7011" for this suite. @ 01/10/24 16:00:48.251
• [4.202 seconds]
------------------------------
SSSSS
------------------------------
[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]
test/e2e/architecture/conformance.go:39
  STEP: Creating a kubernetes client @ 01/10/24 16:00:48.268
  Jan 10 16:00:48.268: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename conformance-tests @ 01/10/24 16:00:48.269
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:00:48.295
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:00:48.302
  STEP: Getting node addresses @ 01/10/24 16:00:48.31
  Jan 10 16:00:48.310: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  Jan 10 16:00:48.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "conformance-tests-4083" for this suite. @ 01/10/24 16:00:48.351
• [0.100 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:218
  STEP: Creating a kubernetes client @ 01/10/24 16:00:48.375
  Jan 10 16:00:48.375: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename downward-api @ 01/10/24 16:00:48.377
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:00:48.408
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:00:48.413
  STEP: Creating a pod to test downward api env vars @ 01/10/24 16:00:48.418
  STEP: Saw pod success @ 01/10/24 16:00:52.465
  Jan 10 16:00:52.470: INFO: Trying to get logs from node env1-test-worker-1 pod downward-api-51e60d3b-ab82-49b6-a40c-7afddb43bf12 container dapi-container: <nil>
  STEP: delete the pod @ 01/10/24 16:00:52.489
  Jan 10 16:00:52.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4877" for this suite. @ 01/10/24 16:00:52.534
• [4.173 seconds]
------------------------------
SSS
------------------------------
[sig-node] Probing container should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:134
  STEP: Creating a kubernetes client @ 01/10/24 16:00:52.552
  Jan 10 16:00:52.552: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename container-probe @ 01/10/24 16:00:52.554
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:00:52.592
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:00:52.598
  STEP: Creating pod busybox-c32e1623-6252-41b6-a3fa-7eecb621e5f2 in namespace container-probe-1578 @ 01/10/24 16:00:52.604
  Jan 10 16:00:54.655: INFO: Started pod busybox-c32e1623-6252-41b6-a3fa-7eecb621e5f2 in namespace container-probe-1578
  STEP: checking the pod's current state and verifying that restartCount is present @ 01/10/24 16:00:54.656
  Jan 10 16:00:54.672: INFO: Initial restart count of pod busybox-c32e1623-6252-41b6-a3fa-7eecb621e5f2 is 0
  Jan 10 16:01:44.965: INFO: Restart count of pod container-probe-1578/busybox-c32e1623-6252-41b6-a3fa-7eecb621e5f2 is now 1 (50.2928775s elapsed)
  Jan 10 16:01:44.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 01/10/24 16:01:44.982
  STEP: Destroying namespace "container-probe-1578" for this suite. @ 01/10/24 16:01:45.038
• [52.511 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]
test/e2e/apps/disruption.go:108
  STEP: Creating a kubernetes client @ 01/10/24 16:01:45.065
  Jan 10 16:01:45.065: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename disruption @ 01/10/24 16:01:45.067
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:01:45.134
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:01:45.148
  STEP: creating the pdb @ 01/10/24 16:01:45.159
  STEP: Waiting for the pdb to be processed @ 01/10/24 16:01:45.175
  STEP: updating the pdb @ 01/10/24 16:01:47.21
  STEP: Waiting for the pdb to be processed @ 01/10/24 16:01:47.235
  STEP: patching the pdb @ 01/10/24 16:01:49.257
  STEP: Waiting for the pdb to be processed @ 01/10/24 16:01:49.284
  STEP: Waiting for the pdb to be deleted @ 01/10/24 16:01:51.345
  Jan 10 16:01:51.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-4194" for this suite. @ 01/10/24 16:01:51.381
• [6.339 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:546
  STEP: Creating a kubernetes client @ 01/10/24 16:01:51.411
  Jan 10 16:01:51.411: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename container-probe @ 01/10/24 16:01:51.413
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:01:51.449
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:01:51.457
  STEP: Creating pod test-grpc-0abce4de-75fc-49ba-bd48-2230eca2effd in namespace container-probe-9311 @ 01/10/24 16:01:51.466
  Jan 10 16:01:53.528: INFO: Started pod test-grpc-0abce4de-75fc-49ba-bd48-2230eca2effd in namespace container-probe-9311
  STEP: checking the pod's current state and verifying that restartCount is present @ 01/10/24 16:01:53.528
  Jan 10 16:01:53.534: INFO: Initial restart count of pod test-grpc-0abce4de-75fc-49ba-bd48-2230eca2effd is 0
  Jan 10 16:03:07.981: INFO: Restart count of pod container-probe-9311/test-grpc-0abce4de-75fc-49ba-bd48-2230eca2effd is now 1 (1m14.447524706s elapsed)
  Jan 10 16:03:07.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 01/10/24 16:03:07.996
  STEP: Destroying namespace "container-probe-9311" for this suite. @ 01/10/24 16:03:08.028
• [76.631 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should support CronJob API operations [Conformance]
test/e2e/apps/cronjob.go:324
  STEP: Creating a kubernetes client @ 01/10/24 16:03:08.043
  Jan 10 16:03:08.043: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename cronjob @ 01/10/24 16:03:08.045
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:03:08.09
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:03:08.095
  STEP: Creating a cronjob @ 01/10/24 16:03:08.103
  STEP: creating @ 01/10/24 16:03:08.103
  STEP: getting @ 01/10/24 16:03:08.115
  STEP: listing @ 01/10/24 16:03:08.124
  STEP: watching @ 01/10/24 16:03:08.133
  Jan 10 16:03:08.133: INFO: starting watch
  STEP: cluster-wide listing @ 01/10/24 16:03:08.136
  STEP: cluster-wide watching @ 01/10/24 16:03:08.143
  Jan 10 16:03:08.143: INFO: starting watch
  STEP: patching @ 01/10/24 16:03:08.146
  STEP: updating @ 01/10/24 16:03:08.161
  Jan 10 16:03:08.185: INFO: waiting for watch events with expected annotations
  Jan 10 16:03:08.185: INFO: saw patched and updated annotations
  STEP: patching /status @ 01/10/24 16:03:08.186
  STEP: updating /status @ 01/10/24 16:03:08.202
  STEP: get /status @ 01/10/24 16:03:08.221
  STEP: deleting @ 01/10/24 16:03:08.23
  STEP: deleting a collection @ 01/10/24 16:03:08.263
  Jan 10 16:03:08.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-5855" for this suite. @ 01/10/24 16:03:08.303
• [0.274 seconds]
------------------------------
SSS
------------------------------
[sig-node] Pods should be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:345
  STEP: Creating a kubernetes client @ 01/10/24 16:03:08.321
  Jan 10 16:03:08.322: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename pods @ 01/10/24 16:03:08.325
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:03:08.417
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:03:08.424
  STEP: creating the pod @ 01/10/24 16:03:08.431
  STEP: submitting the pod to kubernetes @ 01/10/24 16:03:08.432
  STEP: verifying the pod is in kubernetes @ 01/10/24 16:03:10.496
  STEP: updating the pod @ 01/10/24 16:03:10.503
  Jan 10 16:03:11.027: INFO: Successfully updated pod "pod-update-a92dc722-d2bf-40cc-aef6-72e45e8d78bc"
  STEP: verifying the updated pod is in kubernetes @ 01/10/24 16:03:11.045
  Jan 10 16:03:11.055: INFO: Pod update OK
  Jan 10 16:03:11.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-6426" for this suite. @ 01/10/24 16:03:11.071
• [2.771 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should update a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:808
  STEP: Creating a kubernetes client @ 01/10/24 16:03:11.095
  Jan 10 16:03:11.095: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename svcaccounts @ 01/10/24 16:03:11.098
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:03:11.153
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:03:11.172
  STEP: Creating ServiceAccount "e2e-sa-74xqd"  @ 01/10/24 16:03:11.184
  Jan 10 16:03:11.196: INFO: AutomountServiceAccountToken: false
  STEP: Updating ServiceAccount "e2e-sa-74xqd"  @ 01/10/24 16:03:11.196
  Jan 10 16:03:11.219: INFO: AutomountServiceAccountToken: true
  Jan 10 16:03:11.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-5081" for this suite. @ 01/10/24 16:03:11.236
• [0.166 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:54
  STEP: Creating a kubernetes client @ 01/10/24 16:03:11.265
  Jan 10 16:03:11.265: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename projected @ 01/10/24 16:03:11.267
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:03:11.311
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:03:11.322
  STEP: Creating a pod to test downward API volume plugin @ 01/10/24 16:03:11.344
  STEP: Saw pod success @ 01/10/24 16:03:15.417
  Jan 10 16:03:15.428: INFO: Trying to get logs from node env1-test-worker-1 pod downwardapi-volume-964d90d3-3ab1-41d0-9e32-7690a3135782 container client-container: <nil>
  STEP: delete the pod @ 01/10/24 16:03:15.474
  Jan 10 16:03:15.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-943" for this suite. @ 01/10/24 16:03:15.562
• [4.327 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:55
  STEP: Creating a kubernetes client @ 01/10/24 16:03:15.599
  Jan 10 16:03:15.599: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename runtimeclass @ 01/10/24 16:03:15.601
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:03:15.67
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:03:15.678
  Jan 10 16:03:15.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-3047" for this suite. @ 01/10/24 16:03:15.742
• [0.180 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
test/e2e/common/node/expansion.go:228
  STEP: Creating a kubernetes client @ 01/10/24 16:03:15.782
  Jan 10 16:03:15.782: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename var-expansion @ 01/10/24 16:03:15.785
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:03:15.835
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:03:15.842
  STEP: creating the pod with failed condition @ 01/10/24 16:03:15.848
  STEP: updating the pod @ 01/10/24 16:05:15.878
  Jan 10 16:05:16.401: INFO: Successfully updated pod "var-expansion-abf3c6c0-7912-4824-a272-e2d7a885153a"
  STEP: waiting for pod running @ 01/10/24 16:05:16.402
  STEP: deleting the pod gracefully @ 01/10/24 16:05:18.422
  Jan 10 16:05:18.422: INFO: Deleting pod "var-expansion-abf3c6c0-7912-4824-a272-e2d7a885153a" in namespace "var-expansion-3603"
  Jan 10 16:05:18.445: INFO: Wait up to 5m0s for pod "var-expansion-abf3c6c0-7912-4824-a272-e2d7a885153a" to be fully deleted
  Jan 10 16:05:50.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-3603" for this suite. @ 01/10/24 16:05:50.719
• [154.950 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a CR with unknown fields for CRD with no validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:286
  STEP: Creating a kubernetes client @ 01/10/24 16:05:50.734
  Jan 10 16:05:50.734: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename field-validation @ 01/10/24 16:05:50.735
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:05:50.777
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:05:50.784
  Jan 10 16:05:50.791: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  Jan 10 16:05:58.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-1939" for this suite. @ 01/10/24 16:05:58.492
• [7.770 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:71
  STEP: Creating a kubernetes client @ 01/10/24 16:05:58.505
  Jan 10 16:05:58.505: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename container-probe @ 01/10/24 16:05:58.507
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:05:58.535
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:05:58.545
  Jan 10 16:06:20.728: INFO: Container started at 2024-01-10 16:05:59 +0000 UTC, pod became ready at 2024-01-10 16:06:18 +0000 UTC
  Jan 10 16:06:20.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-8303" for this suite. @ 01/10/24 16:06:20.744
• [22.259 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:85
  STEP: Creating a kubernetes client @ 01/10/24 16:06:20.767
  Jan 10 16:06:20.768: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename custom-resource-definition @ 01/10/24 16:06:20.771
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:06:20.809
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:06:20.815
  Jan 10 16:06:20.822: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  Jan 10 16:07:22.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-8258" for this suite. @ 01/10/24 16:07:22.681
• [61.937 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should release no longer matching pods [Conformance]
test/e2e/apps/rc.go:103
  STEP: Creating a kubernetes client @ 01/10/24 16:07:22.708
  Jan 10 16:07:22.708: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename replication-controller @ 01/10/24 16:07:22.71
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:07:22.747
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:07:22.754
  STEP: Given a ReplicationController is created @ 01/10/24 16:07:22.759
  STEP: When the matched label of one of its pods change @ 01/10/24 16:07:22.772
  Jan 10 16:07:22.784: INFO: Pod name pod-release: Found 0 pods out of 1
  Jan 10 16:07:27.798: INFO: Pod name pod-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 01/10/24 16:07:27.833
  Jan 10 16:07:28.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-6985" for this suite. @ 01/10/24 16:07:28.881
• [6.197 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:391
  STEP: Creating a kubernetes client @ 01/10/24 16:07:28.91
  Jan 10 16:07:28.910: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename crd-publish-openapi @ 01/10/24 16:07:28.912
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:07:28.966
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:07:28.972
  STEP: set up a multi version CRD @ 01/10/24 16:07:28.978
  Jan 10 16:07:28.979: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: rename a version @ 01/10/24 16:07:43.025
  STEP: check the new version name is served @ 01/10/24 16:07:43.06
  STEP: check the old version name is removed @ 01/10/24 16:07:45.387
  STEP: check the other version is not changed @ 01/10/24 16:07:46.958
  Jan 10 16:07:52.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-8720" for this suite. @ 01/10/24 16:07:52.447
• [23.553 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]
test/e2e/apps/statefulset.go:743
  STEP: Creating a kubernetes client @ 01/10/24 16:07:52.467
  Jan 10 16:07:52.467: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename statefulset @ 01/10/24 16:07:52.469
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:07:52.507
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:07:52.514
  STEP: Creating service test in namespace statefulset-1646 @ 01/10/24 16:07:52.518
  STEP: Looking for a node to schedule stateful set and pod @ 01/10/24 16:07:52.527
  STEP: Creating pod with conflicting port in namespace statefulset-1646 @ 01/10/24 16:07:52.547
  STEP: Waiting until pod test-pod will start running in namespace statefulset-1646 @ 01/10/24 16:07:52.573
  STEP: Creating statefulset with conflicting port in namespace statefulset-1646 @ 01/10/24 16:07:54.597
  STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-1646 @ 01/10/24 16:07:54.611
  Jan 10 16:07:54.665: INFO: Observed stateful pod in namespace: statefulset-1646, name: ss-0, uid: 7944518e-ed84-4599-823c-00d1b25a12be, status phase: Pending. Waiting for statefulset controller to delete.
  Jan 10 16:07:54.721: INFO: Observed stateful pod in namespace: statefulset-1646, name: ss-0, uid: 7944518e-ed84-4599-823c-00d1b25a12be, status phase: Failed. Waiting for statefulset controller to delete.
  Jan 10 16:07:54.802: INFO: Observed stateful pod in namespace: statefulset-1646, name: ss-0, uid: 7944518e-ed84-4599-823c-00d1b25a12be, status phase: Failed. Waiting for statefulset controller to delete.
  Jan 10 16:07:54.813: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-1646
  STEP: Removing pod with conflicting port in namespace statefulset-1646 @ 01/10/24 16:07:54.814
  STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-1646 and will be in running state @ 01/10/24 16:07:54.845
  Jan 10 16:07:56.868: INFO: Deleting all statefulset in ns statefulset-1646
  Jan 10 16:07:56.877: INFO: Scaling statefulset ss to 0
  Jan 10 16:08:06.965: INFO: Waiting for statefulset status.replicas updated to 0
  Jan 10 16:08:06.971: INFO: Deleting statefulset ss
  Jan 10 16:08:07.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-1646" for this suite. @ 01/10/24 16:08:07.017
• [14.563 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:129
  STEP: Creating a kubernetes client @ 01/10/24 16:08:07.038
  Jan 10 16:08:07.038: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename security-context @ 01/10/24 16:08:07.04
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:08:07.086
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:08:07.092
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 01/10/24 16:08:07.099
  STEP: Saw pod success @ 01/10/24 16:08:11.155
  Jan 10 16:08:11.166: INFO: Trying to get logs from node env1-test-worker-1 pod security-context-9b97e4fd-fd02-467b-8885-08b9629a4c4c container test-container: <nil>
  STEP: delete the pod @ 01/10/24 16:08:11.227
  Jan 10 16:08:11.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-7465" for this suite. @ 01/10/24 16:08:11.313
• [4.314 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:479
  STEP: Creating a kubernetes client @ 01/10/24 16:08:11.353
  Jan 10 16:08:11.353: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename gc @ 01/10/24 16:08:11.355
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:08:11.402
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:08:11.409
  STEP: create the deployment @ 01/10/24 16:08:11.415
  W0110 16:08:11.439972      23 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 01/10/24 16:08:11.44
  STEP: delete the deployment @ 01/10/24 16:08:11.962
  STEP: wait for all rs to be garbage collected @ 01/10/24 16:08:11.978
  STEP: expected 0 pods, got 2 pods @ 01/10/24 16:08:11.994
  STEP: Gathering metrics @ 01/10/24 16:08:12.545
  Jan 10 16:08:12.904: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jan 10 16:08:12.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-8272" for this suite. @ 01/10/24 16:08:12.919
• [1.588 seconds]
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]
test/e2e/kubectl/kubectl.go:1735
  STEP: Creating a kubernetes client @ 01/10/24 16:08:12.943
  Jan 10 16:08:12.943: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename kubectl @ 01/10/24 16:08:12.946
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:08:12.996
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:08:13.003
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 01/10/24 16:08:13.007
  Jan 10 16:08:13.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-4217 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Jan 10 16:08:13.197: INFO: stderr: ""
  Jan 10 16:08:13.197: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod is running @ 01/10/24 16:08:13.197
  STEP: verifying the pod e2e-test-httpd-pod was created @ 01/10/24 16:08:18.249
  Jan 10 16:08:18.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-4217 get pod e2e-test-httpd-pod -o json'
  Jan 10 16:08:18.439: INFO: stderr: ""
  Jan 10 16:08:18.439: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2024-01-10T16:08:13Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4217\",\n        \"resourceVersion\": \"186778162\",\n        \"uid\": \"443352bb-67b2-4dff-90db-0cbfbdadd3a0\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-mlgnv\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"env1-test-worker-1\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-mlgnv\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2024-01-10T16:08:13Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2024-01-10T16:08:14Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2024-01-10T16:08:14Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2024-01-10T16:08:13Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://23f4238bf2784f8fe624ddfe2b59a216e57b302039a8cf947cd4bf0ef36505cf\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2024-01-10T16:08:14Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.61.1.201\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.68.207\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.233.68.207\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2024-01-10T16:08:13Z\"\n    }\n}\n"
  STEP: replace the image in the pod @ 01/10/24 16:08:18.439
  Jan 10 16:08:18.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-4217 replace -f -'
  Jan 10 16:08:21.235: INFO: stderr: ""
  Jan 10 16:08:21.235: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 @ 01/10/24 16:08:21.235
  Jan 10 16:08:21.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-4217 delete pods e2e-test-httpd-pod'
  Jan 10 16:08:23.562: INFO: stderr: ""
  Jan 10 16:08:23.562: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Jan 10 16:08:23.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4217" for this suite. @ 01/10/24 16:08:23.58
• [10.658 seconds]
------------------------------
S
------------------------------
[sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]
test/e2e/common/node/pods.go:897
  STEP: Creating a kubernetes client @ 01/10/24 16:08:23.602
  Jan 10 16:08:23.602: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename pods @ 01/10/24 16:08:23.605
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:08:23.654
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:08:23.661
  STEP: creating a Pod with a static label @ 01/10/24 16:08:23.705
  STEP: watching for Pod to be ready @ 01/10/24 16:08:23.729
  Jan 10 16:08:23.733: INFO: observed Pod pod-test in namespace pods-6612 in phase Pending with labels: map[test-pod-static:true] & conditions []
  Jan 10 16:08:23.738: INFO: observed Pod pod-test in namespace pods-6612 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-10 16:08:23 +0000 UTC  }]
  Jan 10 16:08:23.780: INFO: observed Pod pod-test in namespace pods-6612 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-10 16:08:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-10 16:08:23 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-10 16:08:23 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-10 16:08:23 +0000 UTC  }]
  Jan 10 16:08:25.517: INFO: Found Pod pod-test in namespace pods-6612 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-10 16:08:23 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2024-01-10 16:08:25 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2024-01-10 16:08:25 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-10 16:08:23 +0000 UTC  }]
  STEP: patching the Pod with a new Label and updated data @ 01/10/24 16:08:25.527
  STEP: getting the Pod and ensuring that it's patched @ 01/10/24 16:08:25.567
  STEP: replacing the Pod's status Ready condition to False @ 01/10/24 16:08:25.575
  STEP: check the Pod again to ensure its Ready conditions are False @ 01/10/24 16:08:25.605
  STEP: deleting the Pod via a Collection with a LabelSelector @ 01/10/24 16:08:25.606
  STEP: watching for the Pod to be deleted @ 01/10/24 16:08:25.631
  Jan 10 16:08:25.637: INFO: observed event type MODIFIED
  Jan 10 16:08:27.542: INFO: observed event type MODIFIED
  Jan 10 16:08:27.949: INFO: observed event type MODIFIED
  Jan 10 16:08:28.558: INFO: observed event type MODIFIED
  Jan 10 16:08:28.601: INFO: observed event type MODIFIED
  Jan 10 16:08:28.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-6612" for this suite. @ 01/10/24 16:08:28.679
• [5.108 seconds]
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]
test/e2e/apps/statefulset.go:912
  STEP: Creating a kubernetes client @ 01/10/24 16:08:28.711
  Jan 10 16:08:28.711: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename statefulset @ 01/10/24 16:08:28.713
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:08:28.783
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:08:28.791
  STEP: Creating service test in namespace statefulset-636 @ 01/10/24 16:08:28.797
  Jan 10 16:08:28.894: INFO: Found 0 stateful pods, waiting for 1
  Jan 10 16:08:38.911: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: patching the StatefulSet @ 01/10/24 16:08:38.932
  W0110 16:08:38.951093      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Jan 10 16:08:39.011: INFO: Found 1 stateful pods, waiting for 2
  Jan 10 16:08:49.027: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Jan 10 16:08:49.027: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Listing all StatefulSets @ 01/10/24 16:08:49.044
  STEP: Delete all of the StatefulSets @ 01/10/24 16:08:49.053
  STEP: Verify that StatefulSets have been deleted @ 01/10/24 16:08:49.075
  Jan 10 16:08:49.086: INFO: Deleting all statefulset in ns statefulset-636
  Jan 10 16:08:49.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-636" for this suite. @ 01/10/24 16:08:49.21
• [20.519 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:262
  STEP: Creating a kubernetes client @ 01/10/24 16:08:49.234
  Jan 10 16:08:49.234: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename projected @ 01/10/24 16:08:49.236
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:08:49.279
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:08:49.287
  STEP: Creating a pod to test downward API volume plugin @ 01/10/24 16:08:49.296
  STEP: Saw pod success @ 01/10/24 16:08:53.352
  Jan 10 16:08:53.361: INFO: Trying to get logs from node env1-test-worker-1 pod downwardapi-volume-9733d5aa-1b73-4409-b4de-ac80be1a67d1 container client-container: <nil>
  STEP: delete the pod @ 01/10/24 16:08:53.38
  Jan 10 16:08:53.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8972" for this suite. @ 01/10/24 16:08:53.429
• [4.212 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]
test/e2e/storage/subpath.go:60
  STEP: Creating a kubernetes client @ 01/10/24 16:08:53.45
  Jan 10 16:08:53.450: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename subpath @ 01/10/24 16:08:53.451
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:08:53.491
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:08:53.498
  STEP: Setting up data @ 01/10/24 16:08:53.504
  STEP: Creating pod pod-subpath-test-secret-jp59 @ 01/10/24 16:08:53.548
  STEP: Creating a pod to test atomic-volume-subpath @ 01/10/24 16:08:53.548
  STEP: Saw pod success @ 01/10/24 16:09:17.789
  Jan 10 16:09:17.798: INFO: Trying to get logs from node env1-test-worker-1 pod pod-subpath-test-secret-jp59 container test-container-subpath-secret-jp59: <nil>
  STEP: delete the pod @ 01/10/24 16:09:17.815
  STEP: Deleting pod pod-subpath-test-secret-jp59 @ 01/10/24 16:09:17.842
  Jan 10 16:09:17.842: INFO: Deleting pod "pod-subpath-test-secret-jp59" in namespace "subpath-9008"
  Jan 10 16:09:17.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-9008" for this suite. @ 01/10/24 16:09:17.858
• [24.422 seconds]
------------------------------
S
------------------------------
[sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]
test/e2e/kubectl/kubectl.go:1800
  STEP: Creating a kubernetes client @ 01/10/24 16:09:17.873
  Jan 10 16:09:17.873: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename kubectl @ 01/10/24 16:09:17.874
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:09:17.91
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:09:17.916
  STEP: Starting the proxy @ 01/10/24 16:09:17.922
  Jan 10 16:09:17.923: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-8497 proxy --unix-socket=/tmp/kubectl-proxy-unix1976995218/test'
  STEP: retrieving proxy /api/ output @ 01/10/24 16:09:18.047
  Jan 10 16:09:18.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8497" for this suite. @ 01/10/24 16:09:18.06
• [0.200 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:119
  STEP: Creating a kubernetes client @ 01/10/24 16:09:18.076
  Jan 10 16:09:18.076: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename projected @ 01/10/24 16:09:18.078
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:09:18.125
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:09:18.13
  STEP: Creating secret with name projected-secret-test-1fd63dbe-6481-48f3-915c-b81fdb5d2d80 @ 01/10/24 16:09:18.137
  STEP: Creating a pod to test consume secrets @ 01/10/24 16:09:18.147
  STEP: Saw pod success @ 01/10/24 16:09:22.205
  Jan 10 16:09:22.212: INFO: Trying to get logs from node env1-test-worker-1 pod pod-projected-secrets-371e7300-0884-47fc-8d1f-5595e7c4079e container secret-volume-test: <nil>
  STEP: delete the pod @ 01/10/24 16:09:22.231
  Jan 10 16:09:22.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2699" for this suite. @ 01/10/24 16:09:22.287
• [4.236 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]
test/e2e/storage/csistoragecapacity.go:49
  STEP: Creating a kubernetes client @ 01/10/24 16:09:22.316
  Jan 10 16:09:22.316: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename csistoragecapacity @ 01/10/24 16:09:22.318
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:09:22.363
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:09:22.37
  STEP: getting /apis @ 01/10/24 16:09:22.386
  STEP: getting /apis/storage.k8s.io @ 01/10/24 16:09:22.399
  STEP: getting /apis/storage.k8s.io/v1 @ 01/10/24 16:09:22.401
  STEP: creating @ 01/10/24 16:09:22.404
  STEP: watching @ 01/10/24 16:09:22.452
  Jan 10 16:09:22.452: INFO: starting watch
  STEP: getting @ 01/10/24 16:09:22.485
  STEP: listing in namespace @ 01/10/24 16:09:22.492
  STEP: listing across namespaces @ 01/10/24 16:09:22.503
  STEP: patching @ 01/10/24 16:09:22.515
  STEP: updating @ 01/10/24 16:09:22.526
  Jan 10 16:09:22.539: INFO: waiting for watch events with expected annotations in namespace
  Jan 10 16:09:22.540: INFO: waiting for watch events with expected annotations across namespace
  STEP: deleting @ 01/10/24 16:09:22.54
  STEP: deleting a collection @ 01/10/24 16:09:22.593
  Jan 10 16:09:22.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csistoragecapacity-4894" for this suite. @ 01/10/24 16:09:22.677
• [0.379 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:341
  STEP: Creating a kubernetes client @ 01/10/24 16:09:22.697
  Jan 10 16:09:22.697: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename kubectl @ 01/10/24 16:09:22.7
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:09:22.745
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:09:22.756
  STEP: creating a replication controller @ 01/10/24 16:09:22.764
  Jan 10 16:09:22.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-616 create -f -'
  Jan 10 16:09:25.372: INFO: stderr: ""
  Jan 10 16:09:25.373: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 01/10/24 16:09:25.373
  Jan 10 16:09:25.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-616 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jan 10 16:09:25.567: INFO: stderr: ""
  Jan 10 16:09:25.567: INFO: stdout: "update-demo-nautilus-7h58j update-demo-nautilus-f5gzz "
  Jan 10 16:09:25.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-616 get pods update-demo-nautilus-7h58j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jan 10 16:09:25.760: INFO: stderr: ""
  Jan 10 16:09:25.760: INFO: stdout: ""
  Jan 10 16:09:25.760: INFO: update-demo-nautilus-7h58j is created but not running
  Jan 10 16:09:30.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-616 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jan 10 16:09:30.913: INFO: stderr: ""
  Jan 10 16:09:30.913: INFO: stdout: "update-demo-nautilus-7h58j update-demo-nautilus-f5gzz "
  Jan 10 16:09:30.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-616 get pods update-demo-nautilus-7h58j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jan 10 16:09:31.034: INFO: stderr: ""
  Jan 10 16:09:31.034: INFO: stdout: "true"
  Jan 10 16:09:31.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-616 get pods update-demo-nautilus-7h58j -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jan 10 16:09:31.215: INFO: stderr: ""
  Jan 10 16:09:31.215: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jan 10 16:09:31.215: INFO: validating pod update-demo-nautilus-7h58j
  Jan 10 16:09:31.229: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jan 10 16:09:31.230: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jan 10 16:09:31.230: INFO: update-demo-nautilus-7h58j is verified up and running
  Jan 10 16:09:31.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-616 get pods update-demo-nautilus-f5gzz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jan 10 16:09:31.396: INFO: stderr: ""
  Jan 10 16:09:31.396: INFO: stdout: "true"
  Jan 10 16:09:31.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-616 get pods update-demo-nautilus-f5gzz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jan 10 16:09:31.575: INFO: stderr: ""
  Jan 10 16:09:31.575: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jan 10 16:09:31.575: INFO: validating pod update-demo-nautilus-f5gzz
  Jan 10 16:09:31.589: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jan 10 16:09:31.590: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jan 10 16:09:31.590: INFO: update-demo-nautilus-f5gzz is verified up and running
  STEP: using delete to clean up resources @ 01/10/24 16:09:31.59
  Jan 10 16:09:31.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-616 delete --grace-period=0 --force -f -'
  Jan 10 16:09:31.769: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jan 10 16:09:31.769: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Jan 10 16:09:31.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-616 get rc,svc -l name=update-demo --no-headers'
  Jan 10 16:09:32.007: INFO: stderr: "No resources found in kubectl-616 namespace.\n"
  Jan 10 16:09:32.007: INFO: stdout: ""
  Jan 10 16:09:32.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-616 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Jan 10 16:09:32.223: INFO: stderr: ""
  Jan 10 16:09:32.223: INFO: stdout: ""
  Jan 10 16:09:32.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-616" for this suite. @ 01/10/24 16:09:32.238
• [9.553 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]
test/e2e/apimachinery/resource_quota.go:395
  STEP: Creating a kubernetes client @ 01/10/24 16:09:32.252
  Jan 10 16:09:32.252: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename resourcequota @ 01/10/24 16:09:32.254
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:09:32.352
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:09:32.357
  STEP: Counting existing ResourceQuota @ 01/10/24 16:09:32.363
  STEP: Creating a ResourceQuota @ 01/10/24 16:09:37.375
  STEP: Ensuring resource quota status is calculated @ 01/10/24 16:09:37.394
  STEP: Creating a ReplicationController @ 01/10/24 16:09:39.415
  STEP: Ensuring resource quota status captures replication controller creation @ 01/10/24 16:09:39.446
  STEP: Deleting a ReplicationController @ 01/10/24 16:09:41.459
  STEP: Ensuring resource quota status released usage @ 01/10/24 16:09:41.484
  Jan 10 16:09:43.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7426" for this suite. @ 01/10/24 16:09:43.506
• [11.271 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
test/e2e/node/pods.go:163
  STEP: Creating a kubernetes client @ 01/10/24 16:09:43.527
  Jan 10 16:09:43.527: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename pods @ 01/10/24 16:09:43.529
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:09:43.566
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:09:43.572
  STEP: creating the pod @ 01/10/24 16:09:43.578
  STEP: submitting the pod to kubernetes @ 01/10/24 16:09:43.578
  STEP: verifying QOS class is set on the pod @ 01/10/24 16:09:43.595
  Jan 10 16:09:43.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-711" for this suite. @ 01/10/24 16:09:43.634
• [0.141 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:54
  STEP: Creating a kubernetes client @ 01/10/24 16:09:43.672
  Jan 10 16:09:43.672: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename downward-api @ 01/10/24 16:09:43.674
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:09:43.711
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:09:43.716
  STEP: Creating a pod to test downward API volume plugin @ 01/10/24 16:09:43.722
  STEP: Saw pod success @ 01/10/24 16:09:47.781
  Jan 10 16:09:47.789: INFO: Trying to get logs from node env1-test-worker-1 pod downwardapi-volume-987dcda4-516b-4682-851d-83d4b84a848f container client-container: <nil>
  STEP: delete the pod @ 01/10/24 16:09:47.808
  Jan 10 16:09:47.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2688" for this suite. @ 01/10/24 16:09:47.872
• [4.218 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:357
  STEP: Creating a kubernetes client @ 01/10/24 16:09:47.894
  Jan 10 16:09:47.894: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename crd-publish-openapi @ 01/10/24 16:09:47.897
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:09:47.951
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:09:47.957
  STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation @ 01/10/24 16:09:47.963
  Jan 10 16:09:47.964: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  Jan 10 16:09:56.434: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  Jan 10 16:10:16.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-5921" for this suite. @ 01/10/24 16:10:16.362
• [28.485 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
test/e2e/scheduling/limit_range.go:61
  STEP: Creating a kubernetes client @ 01/10/24 16:10:16.386
  Jan 10 16:10:16.386: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename limitrange @ 01/10/24 16:10:16.389
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:10:16.421
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:10:16.429
  STEP: Creating a LimitRange @ 01/10/24 16:10:16.437
  STEP: Setting up watch @ 01/10/24 16:10:16.437
  STEP: Submitting a LimitRange @ 01/10/24 16:10:16.551
  STEP: Verifying LimitRange creation was observed @ 01/10/24 16:10:16.57
  STEP: Fetching the LimitRange to ensure it has proper values @ 01/10/24 16:10:16.57
  Jan 10 16:10:16.577: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Jan 10 16:10:16.578: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with no resource requirements @ 01/10/24 16:10:16.578
  STEP: Ensuring Pod has resource requirements applied from LimitRange @ 01/10/24 16:10:16.595
  Jan 10 16:10:16.615: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Jan 10 16:10:16.616: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with partial resource requirements @ 01/10/24 16:10:16.616
  STEP: Ensuring Pod has merged resource requirements applied from LimitRange @ 01/10/24 16:10:16.649
  Jan 10 16:10:16.672: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
  Jan 10 16:10:16.673: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Failing to create a Pod with less than min resources @ 01/10/24 16:10:16.674
  STEP: Failing to create a Pod with more than max resources @ 01/10/24 16:10:16.681
  STEP: Updating a LimitRange @ 01/10/24 16:10:16.693
  STEP: Verifying LimitRange updating is effective @ 01/10/24 16:10:16.718
  STEP: Creating a Pod with less than former min resources @ 01/10/24 16:10:18.726
  STEP: Failing to create a Pod with more than max resources @ 01/10/24 16:10:18.739
  STEP: Deleting a LimitRange @ 01/10/24 16:10:18.749
  STEP: Verifying the LimitRange was deleted @ 01/10/24 16:10:18.788
  Jan 10 16:10:23.802: INFO: limitRange is already deleted
  STEP: Creating a Pod with more than former max resources @ 01/10/24 16:10:23.802
  Jan 10 16:10:23.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-956" for this suite. @ 01/10/24 16:10:23.845
• [7.477 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services should provide secure master service  [Conformance]
test/e2e/network/service.go:775
  STEP: Creating a kubernetes client @ 01/10/24 16:10:23.864
  Jan 10 16:10:23.864: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename services @ 01/10/24 16:10:23.866
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:10:23.903
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:10:23.908
  Jan 10 16:10:23.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-4205" for this suite. @ 01/10/24 16:10:23.941
• [0.094 seconds]
------------------------------
[sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:45
  STEP: Creating a kubernetes client @ 01/10/24 16:10:23.958
  Jan 10 16:10:23.959: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename configmap @ 01/10/24 16:10:23.961
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:10:23.995
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:10:24.001
  STEP: Creating configMap configmap-8253/configmap-test-1d2b09ec-c12f-465d-9262-390454806e9f @ 01/10/24 16:10:24.006
  STEP: Creating a pod to test consume configMaps @ 01/10/24 16:10:24.016
  STEP: Saw pod success @ 01/10/24 16:10:28.064
  Jan 10 16:10:28.074: INFO: Trying to get logs from node env1-test-worker-1 pod pod-configmaps-b3c0462c-a0d2-46ce-9f50-0e4365a78a0c container env-test: <nil>
  STEP: delete the pod @ 01/10/24 16:10:28.122
  Jan 10 16:10:28.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8253" for this suite. @ 01/10/24 16:10:28.19
• [4.254 seconds]
------------------------------
SSS
------------------------------
[sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
test/e2e/apps/job.go:430
  STEP: Creating a kubernetes client @ 01/10/24 16:10:28.213
  Jan 10 16:10:28.214: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename job @ 01/10/24 16:10:28.216
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:10:28.26
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:10:28.27
  STEP: Creating a job @ 01/10/24 16:10:28.278
  STEP: Ensuring job reaches completions @ 01/10/24 16:10:28.29
  Jan 10 16:10:40.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-8408" for this suite. @ 01/10/24 16:10:40.321
• [12.130 seconds]
------------------------------
SSS
------------------------------
[sig-network] Services should be able to create a functioning NodePort service [Conformance]
test/e2e/network/service.go:1280
  STEP: Creating a kubernetes client @ 01/10/24 16:10:40.344
  Jan 10 16:10:40.345: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename services @ 01/10/24 16:10:40.347
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:10:40.393
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:10:40.401
  STEP: creating service nodeport-test with type=NodePort in namespace services-5626 @ 01/10/24 16:10:40.418
  STEP: creating replication controller nodeport-test in namespace services-5626 @ 01/10/24 16:10:40.451
  I0110 16:10:40.484135      23 runners.go:194] Created replication controller with name: nodeport-test, namespace: services-5626, replica count: 2
  I0110 16:10:43.536801      23 runners.go:194] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jan 10 16:10:43.536: INFO: Creating new exec pod
  Jan 10 16:10:46.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=services-5626 exec execpodj9t8w -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  Jan 10 16:10:47.017: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Jan 10 16:10:47.017: INFO: stdout: "nodeport-test-5v52d"
  Jan 10 16:10:47.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=services-5626 exec execpodj9t8w -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.41.97 80'
  Jan 10 16:10:47.389: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.41.97 80\nConnection to 10.233.41.97 80 port [tcp/http] succeeded!\n"
  Jan 10 16:10:47.389: INFO: stdout: ""
  Jan 10 16:10:48.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=services-5626 exec execpodj9t8w -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.41.97 80'
  Jan 10 16:10:48.760: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.41.97 80\nConnection to 10.233.41.97 80 port [tcp/http] succeeded!\n"
  Jan 10 16:10:48.760: INFO: stdout: ""
  Jan 10 16:10:49.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=services-5626 exec execpodj9t8w -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.41.97 80'
  Jan 10 16:10:49.707: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.41.97 80\nConnection to 10.233.41.97 80 port [tcp/http] succeeded!\n"
  Jan 10 16:10:49.707: INFO: stdout: "nodeport-test-wxktr"
  Jan 10 16:10:49.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=services-5626 exec execpodj9t8w -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.61.1.200 31689'
  Jan 10 16:10:50.059: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.61.1.200 31689\nConnection to 10.61.1.200 31689 port [tcp/*] succeeded!\n"
  Jan 10 16:10:50.059: INFO: stdout: "nodeport-test-5v52d"
  Jan 10 16:10:50.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=services-5626 exec execpodj9t8w -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.61.1.201 31689'
  Jan 10 16:10:50.421: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.61.1.201 31689\nConnection to 10.61.1.201 31689 port [tcp/*] succeeded!\n"
  Jan 10 16:10:50.421: INFO: stdout: "nodeport-test-5v52d"
  Jan 10 16:10:50.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-5626" for this suite. @ 01/10/24 16:10:50.432
• [10.106 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields in both the root and embedded object of a CR [Conformance]
test/e2e/apimachinery/field_validation.go:465
  STEP: Creating a kubernetes client @ 01/10/24 16:10:50.455
  Jan 10 16:10:50.455: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename field-validation @ 01/10/24 16:10:50.457
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:10:50.508
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:10:50.516
  Jan 10 16:10:50.523: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  W0110 16:10:58.185656      23 warnings.go:70] unknown field "alpha"
  W0110 16:10:58.186265      23 warnings.go:70] unknown field "beta"
  W0110 16:10:58.186584      23 warnings.go:70] unknown field "delta"
  W0110 16:10:58.187090      23 warnings.go:70] unknown field "epsilon"
  W0110 16:10:58.187459      23 warnings.go:70] unknown field "gamma"
  Jan 10 16:10:58.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-9464" for this suite. @ 01/10/24 16:10:58.27
• [7.839 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:442
  STEP: Creating a kubernetes client @ 01/10/24 16:10:58.294
  Jan 10 16:10:58.294: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename crd-publish-openapi @ 01/10/24 16:10:58.298
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:10:58.338
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:10:58.346
  STEP: set up a multi version CRD @ 01/10/24 16:10:58.352
  Jan 10 16:10:58.353: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: mark a version not serverd @ 01/10/24 16:11:12.264
  STEP: check the unserved version gets removed @ 01/10/24 16:11:12.313
  STEP: check the other version is not changed @ 01/10/24 16:11:15.716
  Jan 10 16:11:21.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-6520" for this suite. @ 01/10/24 16:11:21.608
• [23.332 seconds]
------------------------------
SS
------------------------------
[sig-network] DNS should provide DNS for the cluster  [Conformance]
test/e2e/network/dns.go:50
  STEP: Creating a kubernetes client @ 01/10/24 16:11:21.629
  Jan 10 16:11:21.629: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename dns @ 01/10/24 16:11:21.632
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:11:21.672
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:11:21.68
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 01/10/24 16:11:21.688
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 01/10/24 16:11:21.688
  STEP: creating a pod to probe DNS @ 01/10/24 16:11:21.689
  STEP: submitting the pod to kubernetes @ 01/10/24 16:11:21.689
  STEP: retrieving the pod @ 01/10/24 16:11:23.754
  STEP: looking for the results for each expected name from probers @ 01/10/24 16:11:23.781
  Jan 10 16:11:23.843: INFO: DNS probes using dns-9547/dns-test-74908069-3461-4049-909d-8c048bd5d139 succeeded

  Jan 10 16:11:23.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 01/10/24 16:11:23.862
  STEP: Destroying namespace "dns-9547" for this suite. @ 01/10/24 16:11:23.917
• [2.330 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:67
  STEP: Creating a kubernetes client @ 01/10/24 16:11:23.96
  Jan 10 16:11:23.960: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename projected @ 01/10/24 16:11:23.962
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:11:24.018
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:11:24.025
  STEP: Creating projection with secret that has name projected-secret-test-8449b3a3-9311-4801-930c-f52a6ae2e95b @ 01/10/24 16:11:24.031
  STEP: Creating a pod to test consume secrets @ 01/10/24 16:11:24.051
  STEP: Saw pod success @ 01/10/24 16:11:28.107
  Jan 10 16:11:28.117: INFO: Trying to get logs from node env1-test-worker-1 pod pod-projected-secrets-1ae7bc17-7968-45b2-874d-3491871ececc container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 01/10/24 16:11:28.157
  Jan 10 16:11:28.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5110" for this suite. @ 01/10/24 16:11:28.212
• [4.276 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:76
  STEP: Creating a kubernetes client @ 01/10/24 16:11:28.237
  Jan 10 16:11:28.237: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename var-expansion @ 01/10/24 16:11:28.239
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:11:28.28
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:11:28.291
  STEP: Creating a pod to test substitution in container's command @ 01/10/24 16:11:28.3
  STEP: Saw pod success @ 01/10/24 16:11:32.373
  Jan 10 16:11:32.380: INFO: Trying to get logs from node env1-test-worker-1 pod var-expansion-ee0063d6-95fc-42c3-86ba-04a4b23f6461 container dapi-container: <nil>
  STEP: delete the pod @ 01/10/24 16:11:32.395
  Jan 10 16:11:32.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-1152" for this suite. @ 01/10/24 16:11:32.439
• [4.215 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:56
  STEP: Creating a kubernetes client @ 01/10/24 16:11:32.455
  Jan 10 16:11:32.455: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename projected @ 01/10/24 16:11:32.457
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:11:32.508
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:11:32.515
  STEP: Creating projection with secret that has name projected-secret-test-426c1db5-acd2-4a86-9ca3-426d2705cf78 @ 01/10/24 16:11:32.521
  STEP: Creating a pod to test consume secrets @ 01/10/24 16:11:32.53
  STEP: Saw pod success @ 01/10/24 16:11:36.586
  Jan 10 16:11:36.594: INFO: Trying to get logs from node env1-test-worker-1 pod pod-projected-secrets-c410f960-aee7-4e76-bfd0-a4d7ccc72b31 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 01/10/24 16:11:36.61
  Jan 10 16:11:36.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-641" for this suite. @ 01/10/24 16:11:36.675
• [4.235 seconds]
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]
test/e2e/kubectl/kubectl.go:1341
  STEP: Creating a kubernetes client @ 01/10/24 16:11:36.692
  Jan 10 16:11:36.692: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename kubectl @ 01/10/24 16:11:36.694
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:11:36.733
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:11:36.746
  Jan 10 16:11:36.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-6252 create -f -'
  Jan 10 16:11:39.704: INFO: stderr: ""
  Jan 10 16:11:39.704: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  Jan 10 16:11:39.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-6252 create -f -'
  Jan 10 16:11:42.291: INFO: stderr: ""
  Jan 10 16:11:42.291: INFO: stdout: "service/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 01/10/24 16:11:42.291
  Jan 10 16:11:43.302: INFO: Selector matched 1 pods for map[app:agnhost]
  Jan 10 16:11:43.302: INFO: Found 1 / 1
  Jan 10 16:11:43.302: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Jan 10 16:11:43.308: INFO: Selector matched 1 pods for map[app:agnhost]
  Jan 10 16:11:43.308: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jan 10 16:11:43.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-6252 describe pod agnhost-primary-6sgfw'
  Jan 10 16:11:43.530: INFO: stderr: ""
  Jan 10 16:11:43.530: INFO: stdout: "Name:             agnhost-primary-6sgfw\nNamespace:        kubectl-6252\nPriority:         0\nService Account:  default\nNode:             env1-test-worker-1/10.61.1.201\nStart Time:       Wed, 10 Jan 2024 16:11:39 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.233.68.228\nIPs:\n  IP:           10.233.68.228\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://3fbc0af7d711e84c3424a277fa188696e4ac8aa901d093ce5d19624bcb5ae643\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 10 Jan 2024 16:11:40 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-pnqqq (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-pnqqq:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-6252/agnhost-primary-6sgfw to env1-test-worker-1\n  Normal  Pulled     3s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    3s    kubelet            Created container agnhost-primary\n  Normal  Started    3s    kubelet            Started container agnhost-primary\n"
  Jan 10 16:11:43.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-6252 describe rc agnhost-primary'
  Jan 10 16:11:43.800: INFO: stderr: ""
  Jan 10 16:11:43.800: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-6252\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: agnhost-primary-6sgfw\n"
  Jan 10 16:11:43.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-6252 describe service agnhost-primary'
  Jan 10 16:11:43.990: INFO: stderr: ""
  Jan 10 16:11:43.990: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-6252\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.233.15.44\nIPs:               10.233.15.44\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.233.68.228:6379\nSession Affinity:  None\nEvents:            <none>\n"
  Jan 10 16:11:44.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-6252 describe node env1-test-master-0'
  Jan 10 16:11:44.319: INFO: stderr: ""
  Jan 10 16:11:44.319: INFO: stdout: "Name:               env1-test-master-0\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=vsphere-vm.cpu-2.mem-7gb.os-ubuntu\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=env1-test-master-0\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\n                    node.kubernetes.io/instance-type=vsphere-vm.cpu-2.mem-7gb.os-ubuntu\nAnnotations:        alpha.kubernetes.io/provided-node-ip: 10.61.1.197\n                    csi.volume.kubernetes.io/nodeid: {\"csi.vsphere.vmware.com\":\"42086e21-1067-e398-4609-6a5784f3d5d6\"}\n                    flannel.alpha.coreos.com/backend-data: {\"VNI\":4096,\"VtepMAC\":\"26:d1:5d:16:e7:71\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 10.61.1.197\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 30 Mar 2022 13:31:56 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  env1-test-master-0\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 10 Jan 2024 16:11:40 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Tue, 09 Jan 2024 16:25:58 +0000   Tue, 09 Jan 2024 16:25:58 +0000   FlannelIsUp                  Flannel is running on this node\n  MemoryPressure       False   Wed, 10 Jan 2024 16:11:40 +0000   Tue, 09 Jan 2024 15:50:24 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 10 Jan 2024 16:11:40 +0000   Tue, 09 Jan 2024 15:50:24 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 10 Jan 2024 16:11:40 +0000   Tue, 09 Jan 2024 15:50:24 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 10 Jan 2024 16:11:40 +0000   Tue, 09 Jan 2024 15:53:19 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  Hostname:    env1-test-master-0\n  InternalIP:  10.61.1.197\n  ExternalIP:  10.61.1.197\nCapacity:\n  cpu:                2\n  ephemeral-storage:  60795672Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8054876Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  56029291223\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             7952476Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 4284f3e04f9c47a4b59e54ce697ed9a6\n  System UUID:                42086e21-1067-e398-4609-6a5784f3d5d6\n  Boot ID:                    24165872-ba8d-4771-a127-74ef58934caf\n  Kernel Version:             5.4.0-67-generic\n  OS Image:                   Ubuntu 20.04.2 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.7.5\n  Kubelet Version:            v1.27.5\n  Kube-Proxy Version:         v1.27.5\nPodCIDR:                      10.233.64.0/24\nPodCIDRs:                     10.233.64.0/24\nProviderID:                   vsphere://42086e21-1067-e398-4609-6a5784f3d5d6\nNon-terminated Pods:          (13 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  filebeat                    filebeat-filebeat-4p44c                                    200m (10%)    1 (50%)     200Mi (2%)       1000Mi (12%)   14h\n  kube-system                 dns-autoscaler-7f7b458498-ncsvl                            20m (1%)      0 (0%)      10Mi (0%)        0 (0%)         24h\n  kube-system                 kube-apiserver-env1-test-master-0                          250m (12%)    0 (0%)      0 (0%)           0 (0%)         24h\n  kube-system                 kube-controller-manager-env1-test-master-0                 200m (10%)    0 (0%)      0 (0%)           0 (0%)         24h\n  kube-system                 kube-flannel-pxzh2                                         150m (7%)     300m (15%)  64M (0%)         500M (6%)      23h\n  kube-system                 kube-proxy-tvjwn                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         6h58m\n  kube-system                 kube-scheduler-env1-test-master-0                          100m (5%)     0 (0%)      0 (0%)           0 (0%)         24h\n  kube-system                 nodelocaldns-llks8                                         100m (5%)     0 (0%)      70Mi (0%)        200Mi (2%)     24h\n  kube-system                 vsphere-cloud-controller-manager-gbzjw                     200m (10%)    0 (0%)      0 (0%)           0 (0%)         111m\n  kube-system                 vsphere-csi-controller-c6bb68754-hphk6                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         6h50m\n  kube-system                 vsphere-csi-node-9wghb                                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         6h50m\n  prometheus                  prometheus-prometheus-node-exporter-mktvs                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         13h\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-187fce9e0ddc499b-ltck5    0 (0%)        0 (0%)      0 (0%)           0 (0%)         45m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests       Limits\n  --------           --------       ------\n  cpu                1220m (61%)    1300m (65%)\n  memory             349220Ki (4%)  1758291200 (21%)\n  ephemeral-storage  0 (0%)         0 (0%)\n  hugepages-1Gi      0 (0%)         0 (0%)\n  hugepages-2Mi      0 (0%)         0 (0%)\nEvents:              <none>\n"
  Jan 10 16:11:44.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-6252 describe namespace kubectl-6252'
  Jan 10 16:11:44.538: INFO: stderr: ""
  Jan 10 16:11:44.538: INFO: stdout: "Name:         kubectl-6252\nLabels:       e2e-framework=kubectl\n              e2e-run=59d9f44f-6074-423b-8ef4-6590885bb6d0\n              kubernetes.io/metadata.name=kubectl-6252\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
  Jan 10 16:11:44.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6252" for this suite. @ 01/10/24 16:11:44.556
• [7.889 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:57
  STEP: Creating a kubernetes client @ 01/10/24 16:11:44.587
  Jan 10 16:11:44.587: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename projected @ 01/10/24 16:11:44.589
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:11:44.629
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:11:44.637
  STEP: Creating configMap with name projected-configmap-test-volume-68495fc1-f833-453e-8ea2-0f4a05cb3b36 @ 01/10/24 16:11:44.647
  STEP: Creating a pod to test consume configMaps @ 01/10/24 16:11:44.659
  STEP: Saw pod success @ 01/10/24 16:11:48.715
  Jan 10 16:11:48.724: INFO: Trying to get logs from node env1-test-worker-1 pod pod-projected-configmaps-2f69e6b3-b4f9-4ed1-b8c7-47ee41ab9f8f container agnhost-container: <nil>
  STEP: delete the pod @ 01/10/24 16:11:48.741
  Jan 10 16:11:48.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7909" for this suite. @ 01/10/24 16:11:48.821
• [4.264 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]
test/e2e/apimachinery/webhook.go:284
  STEP: Creating a kubernetes client @ 01/10/24 16:11:48.86
  Jan 10 16:11:48.860: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename webhook @ 01/10/24 16:11:48.862
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:11:48.921
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:11:48.928
  STEP: Setting up server cert @ 01/10/24 16:11:49.01
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 01/10/24 16:11:50.596
  STEP: Deploying the webhook pod @ 01/10/24 16:11:50.646
  STEP: Wait for the deployment to be ready @ 01/10/24 16:11:50.685
  Jan 10 16:11:50.721: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 01/10/24 16:11:52.748
  STEP: Verifying the service has paired with the endpoint @ 01/10/24 16:11:52.775
  Jan 10 16:11:53.777: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jan 10 16:11:53.787: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-46-crds.webhook.example.com via the AdmissionRegistration API @ 01/10/24 16:11:59.317
  STEP: Creating a custom resource that should be mutated by the webhook @ 01/10/24 16:11:59.375
  Jan 10 16:12:01.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3737" for this suite. @ 01/10/24 16:12:02.298
  STEP: Destroying namespace "webhook-markers-2314" for this suite. @ 01/10/24 16:12:02.326
• [13.491 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
test/e2e/apimachinery/aggregator.go:92
  STEP: Creating a kubernetes client @ 01/10/24 16:12:02.367
  Jan 10 16:12:02.367: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename aggregator @ 01/10/24 16:12:02.368
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:12:02.434
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:12:02.442
  Jan 10 16:12:02.452: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Registering the sample API server. @ 01/10/24 16:12:02.453
  Jan 10 16:12:03.173: INFO: Found ClusterRoles; assuming RBAC is enabled.
  Jan 10 16:12:03.315: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
  Jan 10 16:12:05.431: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 10, 16, 12, 3, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 10, 16, 12, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 10, 16, 12, 3, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 10, 16, 12, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jan 10 16:12:07.450: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 10, 16, 12, 3, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 10, 16, 12, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 10, 16, 12, 3, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 10, 16, 12, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jan 10 16:12:09.441: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 10, 16, 12, 3, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 10, 16, 12, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 10, 16, 12, 3, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 10, 16, 12, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jan 10 16:12:11.447: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 10, 16, 12, 3, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 10, 16, 12, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 10, 16, 12, 3, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 10, 16, 12, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jan 10 16:12:13.439: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 10, 16, 12, 3, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 10, 16, 12, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 10, 16, 12, 3, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 10, 16, 12, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jan 10 16:12:15.443: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 10, 16, 12, 3, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 10, 16, 12, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 10, 16, 12, 3, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 10, 16, 12, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jan 10 16:12:17.439: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 10, 16, 12, 3, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 10, 16, 12, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 10, 16, 12, 3, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 10, 16, 12, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jan 10 16:12:19.441: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 10, 16, 12, 3, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 10, 16, 12, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 10, 16, 12, 3, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 10, 16, 12, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jan 10 16:12:21.442: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 10, 16, 12, 3, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 10, 16, 12, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 10, 16, 12, 3, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 10, 16, 12, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jan 10 16:12:23.442: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 10, 16, 12, 3, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 10, 16, 12, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 10, 16, 12, 3, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 10, 16, 12, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jan 10 16:12:25.443: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2024, time.January, 10, 16, 12, 3, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 10, 16, 12, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 10, 16, 12, 3, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 10, 16, 12, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jan 10 16:12:27.589: INFO: Waited 127.651627ms for the sample-apiserver to be ready to handle requests.
  STEP: Read Status for v1alpha1.wardle.example.com @ 01/10/24 16:12:27.673
  STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' @ 01/10/24 16:12:27.68
  STEP: List APIServices @ 01/10/24 16:12:27.692
  Jan 10 16:12:27.710: INFO: Found v1alpha1.wardle.example.com in APIServiceList
  STEP: Adding a label to the APIService @ 01/10/24 16:12:27.71
  Jan 10 16:12:27.736: INFO: APIService labels: map[e2e-apiservice:patched]
  STEP: Updating APIService Status @ 01/10/24 16:12:27.736
  Jan 10 16:12:27.779: INFO: updatedStatus.Conditions: []v1.APIServiceCondition{v1.APIServiceCondition{Type:"Available", Status:"True", LastTransitionTime:time.Date(2024, time.January, 10, 16, 12, 27, 0, time.Local), Reason:"Passed", Message:"all checks passed"}, v1.APIServiceCondition{Type:"StatusUpdated", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: Confirm that v1alpha1.wardle.example.com /status was updated @ 01/10/24 16:12:27.779
  Jan 10 16:12:27.798: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {Available True 2024-01-10 16:12:27 +0000 UTC Passed all checks passed}
  Jan 10 16:12:27.798: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jan 10 16:12:27.798: INFO: Found updated status condition for v1alpha1.wardle.example.com
  STEP: Replace APIService v1alpha1.wardle.example.com @ 01/10/24 16:12:27.798
  Jan 10 16:12:27.823: INFO: Found updated apiService label for "v1alpha1.wardle.example.com"
  STEP: Delete APIService "dynamic-flunder-256211492" @ 01/10/24 16:12:27.823
  STEP: Recreating test-flunder before removing endpoint via deleteCollection @ 01/10/24 16:12:27.852
  STEP: Read v1alpha1.wardle.example.com /status before patching it @ 01/10/24 16:12:27.876
  STEP: Patch APIService Status @ 01/10/24 16:12:27.886
  STEP: Confirm that v1alpha1.wardle.example.com /status was patched @ 01/10/24 16:12:27.898
  Jan 10 16:12:27.908: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {Available True 2024-01-10 16:12:27 +0000 UTC Passed all checks passed}
  Jan 10 16:12:27.908: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jan 10 16:12:27.908: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC E2E Set by e2e test}
  Jan 10 16:12:27.908: INFO: Found patched status condition for v1alpha1.wardle.example.com
  STEP: APIService deleteCollection with labelSelector: "e2e-apiservice=patched" @ 01/10/24 16:12:27.908
  STEP: Confirm that the generated APIService has been deleted @ 01/10/24 16:12:27.922
  Jan 10 16:12:27.922: INFO: Requesting list of APIServices to confirm quantity
  Jan 10 16:12:27.932: INFO: Found 0 APIService with label "e2e-apiservice=patched"
  Jan 10 16:12:27.932: INFO: APIService v1alpha1.wardle.example.com has been deleted.
  Jan 10 16:12:28.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "aggregator-3274" for this suite. @ 01/10/24 16:12:28.266
• [25.919 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:107
  STEP: Creating a kubernetes client @ 01/10/24 16:12:28.298
  Jan 10 16:12:28.299: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename emptydir @ 01/10/24 16:12:28.304
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:12:28.373
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:12:28.39
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 01/10/24 16:12:28.397
  STEP: Saw pod success @ 01/10/24 16:12:32.45
  Jan 10 16:12:32.458: INFO: Trying to get logs from node env1-test-worker-1 pod pod-5bef8321-c5d1-423e-9b1c-38b4002197b0 container test-container: <nil>
  STEP: delete the pod @ 01/10/24 16:12:32.479
  Jan 10 16:12:32.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1281" for this suite. @ 01/10/24 16:12:32.531
• [4.250 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:250
  STEP: Creating a kubernetes client @ 01/10/24 16:12:32.551
  Jan 10 16:12:32.551: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename projected @ 01/10/24 16:12:32.554
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:12:32.616
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:12:32.624
  STEP: Creating a pod to test downward API volume plugin @ 01/10/24 16:12:32.63
  STEP: Saw pod success @ 01/10/24 16:12:36.683
  Jan 10 16:12:36.695: INFO: Trying to get logs from node env1-test-worker-1 pod downwardapi-volume-1dc6e784-aed6-41c5-89d0-3441e11e8571 container client-container: <nil>
  STEP: delete the pod @ 01/10/24 16:12:36.717
  Jan 10 16:12:36.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-305" for this suite. @ 01/10/24 16:12:36.771
• [4.241 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:99
  STEP: Creating a kubernetes client @ 01/10/24 16:12:36.801
  Jan 10 16:12:36.801: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename secrets @ 01/10/24 16:12:36.803
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:12:36.845
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:12:36.861
  STEP: Creating secret with name secret-test-92bc4440-4d66-4c2c-a9f8-ca84874cc3f8 @ 01/10/24 16:12:36.913
  STEP: Creating a pod to test consume secrets @ 01/10/24 16:12:36.926
  STEP: Saw pod success @ 01/10/24 16:12:40.992
  Jan 10 16:12:41.001: INFO: Trying to get logs from node env1-test-worker-1 pod pod-secrets-aecb5bae-82f5-4675-a291-9949498c6ddf container secret-volume-test: <nil>
  STEP: delete the pod @ 01/10/24 16:12:41.024
  Jan 10 16:12:41.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-2018" for this suite. @ 01/10/24 16:12:41.093
  STEP: Destroying namespace "secret-namespace-947" for this suite. @ 01/10/24 16:12:41.113
• [4.330 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:131
  STEP: Creating a kubernetes client @ 01/10/24 16:12:41.134
  Jan 10 16:12:41.134: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename projected @ 01/10/24 16:12:41.136
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:12:41.18
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:12:41.192
  STEP: Creating the pod @ 01/10/24 16:12:41.203
  Jan 10 16:12:43.824: INFO: Successfully updated pod "labelsupdate271b0b45-917e-4f05-bba8-ec55883d2289"
  Jan 10 16:12:45.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8653" for this suite. @ 01/10/24 16:12:45.881
• [4.763 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]
test/e2e/apps/replica_set.go:176
  STEP: Creating a kubernetes client @ 01/10/24 16:12:45.904
  Jan 10 16:12:45.904: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename replicaset @ 01/10/24 16:12:45.907
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:12:45.952
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:12:45.959
  STEP: Create a Replicaset @ 01/10/24 16:12:45.979
  STEP: Verify that the required pods have come up. @ 01/10/24 16:12:45.998
  Jan 10 16:12:46.010: INFO: Pod name sample-pod: Found 0 pods out of 1
  Jan 10 16:12:51.023: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 01/10/24 16:12:51.023
  STEP: Getting /status @ 01/10/24 16:12:51.023
  Jan 10 16:12:51.036: INFO: Replicaset test-rs has Conditions: []
  STEP: updating the Replicaset Status @ 01/10/24 16:12:51.036
  Jan 10 16:12:51.068: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the ReplicaSet status to be updated @ 01/10/24 16:12:51.068
  Jan 10 16:12:51.073: INFO: Observed &ReplicaSet event: ADDED
  Jan 10 16:12:51.073: INFO: Observed &ReplicaSet event: MODIFIED
  Jan 10 16:12:51.074: INFO: Observed &ReplicaSet event: MODIFIED
  Jan 10 16:12:51.075: INFO: Observed &ReplicaSet event: MODIFIED
  Jan 10 16:12:51.075: INFO: Found replicaset test-rs in namespace replicaset-4781 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jan 10 16:12:51.075: INFO: Replicaset test-rs has an updated status
  STEP: patching the Replicaset Status @ 01/10/24 16:12:51.075
  Jan 10 16:12:51.075: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Jan 10 16:12:51.095: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Replicaset status to be patched @ 01/10/24 16:12:51.095
  Jan 10 16:12:51.102: INFO: Observed &ReplicaSet event: ADDED
  Jan 10 16:12:51.103: INFO: Observed &ReplicaSet event: MODIFIED
  Jan 10 16:12:51.103: INFO: Observed &ReplicaSet event: MODIFIED
  Jan 10 16:12:51.104: INFO: Observed &ReplicaSet event: MODIFIED
  Jan 10 16:12:51.104: INFO: Observed replicaset test-rs in namespace replicaset-4781 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jan 10 16:12:51.105: INFO: Observed &ReplicaSet event: MODIFIED
  Jan 10 16:12:51.105: INFO: Found replicaset test-rs in namespace replicaset-4781 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
  Jan 10 16:12:51.105: INFO: Replicaset test-rs has a patched status
  Jan 10 16:12:51.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-4781" for this suite. @ 01/10/24 16:12:51.118
• [5.233 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]
test/e2e/apimachinery/watch.go:334
  STEP: Creating a kubernetes client @ 01/10/24 16:12:51.137
  Jan 10 16:12:51.137: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename watch @ 01/10/24 16:12:51.139
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:12:51.178
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:12:51.184
  STEP: getting a starting resourceVersion @ 01/10/24 16:12:51.191
  STEP: starting a background goroutine to produce watch events @ 01/10/24 16:12:51.2
  STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order @ 01/10/24 16:12:51.2
  Jan 10 16:12:53.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-3126" for this suite. @ 01/10/24 16:12:54.002
• [2.923 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
test/e2e/apimachinery/garbage_collector.go:538
  STEP: Creating a kubernetes client @ 01/10/24 16:12:54.069
  Jan 10 16:12:54.070: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename gc @ 01/10/24 16:12:54.072
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:12:54.111
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:12:54.121
  STEP: create the deployment @ 01/10/24 16:12:54.133
  W0110 16:12:54.145922      23 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 01/10/24 16:12:54.146
  STEP: delete the deployment @ 01/10/24 16:12:54.386
  STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs @ 01/10/24 16:12:54.437
  STEP: Gathering metrics @ 01/10/24 16:12:55.011
  Jan 10 16:12:55.310: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jan 10 16:12:55.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-8290" for this suite. @ 01/10/24 16:12:55.332
• [1.302 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:215
  STEP: Creating a kubernetes client @ 01/10/24 16:12:55.378
  Jan 10 16:12:55.378: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename projected @ 01/10/24 16:12:55.38
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:12:55.435
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:12:55.442
  STEP: Creating secret with name s-test-opt-del-3dbf3411-0218-4062-b5dc-181a0c9523ce @ 01/10/24 16:12:55.46
  STEP: Creating secret with name s-test-opt-upd-02678b61-d1e1-4b7b-b5db-1bb38d0803b2 @ 01/10/24 16:12:55.477
  STEP: Creating the pod @ 01/10/24 16:12:55.491
  STEP: Deleting secret s-test-opt-del-3dbf3411-0218-4062-b5dc-181a0c9523ce @ 01/10/24 16:12:57.607
  STEP: Updating secret s-test-opt-upd-02678b61-d1e1-4b7b-b5db-1bb38d0803b2 @ 01/10/24 16:12:57.621
  STEP: Creating secret with name s-test-opt-create-2c2f9f3c-0dc8-48bc-bd46-9c3872a033d4 @ 01/10/24 16:12:57.634
  STEP: waiting to observe update in volume @ 01/10/24 16:12:57.646
  Jan 10 16:14:04.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1227" for this suite. @ 01/10/24 16:14:04.603
• [69.249 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
test/e2e/storage/csi_inline.go:46
  STEP: Creating a kubernetes client @ 01/10/24 16:14:04.634
  Jan 10 16:14:04.634: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename csiinlinevolumes @ 01/10/24 16:14:04.637
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:14:04.681
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:14:04.703
  STEP: creating @ 01/10/24 16:14:04.715
  STEP: getting @ 01/10/24 16:14:04.749
  STEP: listing @ 01/10/24 16:14:04.765
  STEP: deleting @ 01/10/24 16:14:04.774
  Jan 10 16:14:04.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-3328" for this suite. @ 01/10/24 16:14:04.842
• [0.231 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
test/e2e/apps/statefulset.go:701
  STEP: Creating a kubernetes client @ 01/10/24 16:14:04.868
  Jan 10 16:14:04.868: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename statefulset @ 01/10/24 16:14:04.87
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:14:04.908
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:14:04.915
  STEP: Creating service test in namespace statefulset-4699 @ 01/10/24 16:14:04.921
  STEP: Creating stateful set ss in namespace statefulset-4699 @ 01/10/24 16:14:04.938
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4699 @ 01/10/24 16:14:04.951
  Jan 10 16:14:04.962: INFO: Found 0 stateful pods, waiting for 1
  Jan 10 16:14:14.973: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod @ 01/10/24 16:14:14.974
  Jan 10 16:14:14.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=statefulset-4699 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jan 10 16:14:15.287: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jan 10 16:14:15.287: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jan 10 16:14:15.287: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jan 10 16:14:15.297: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  Jan 10 16:14:25.311: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jan 10 16:14:25.311: INFO: Waiting for statefulset status.replicas updated to 0
  Jan 10 16:14:25.360: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
  Jan 10 16:14:25.360: INFO: ss-0  env1-test-worker-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-10 16:14:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-10 16:14:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-10 16:14:15 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-10 16:14:05 +0000 UTC  }]
  Jan 10 16:14:25.360: INFO: 
  Jan 10 16:14:25.360: INFO: StatefulSet ss has not reached scale 3, at 1
  Jan 10 16:14:26.370: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.991020252s
  Jan 10 16:14:27.379: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.981259563s
  Jan 10 16:14:28.389: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.971313777s
  Jan 10 16:14:29.398: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.961911646s
  Jan 10 16:14:30.408: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.952590066s
  Jan 10 16:14:31.423: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.942362113s
  Jan 10 16:14:32.434: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.927103366s
  Jan 10 16:14:33.447: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.916532474s
  Jan 10 16:14:34.461: INFO: Verifying statefulset ss doesn't scale past 3 for another 903.659216ms
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4699 @ 01/10/24 16:14:35.461
  Jan 10 16:14:35.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=statefulset-4699 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jan 10 16:14:35.846: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jan 10 16:14:35.846: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jan 10 16:14:35.846: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jan 10 16:14:35.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=statefulset-4699 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jan 10 16:14:36.186: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Jan 10 16:14:36.186: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jan 10 16:14:36.186: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jan 10 16:14:36.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=statefulset-4699 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jan 10 16:14:36.535: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Jan 10 16:14:36.535: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jan 10 16:14:36.535: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jan 10 16:14:36.544: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Jan 10 16:14:36.544: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Jan 10 16:14:36.544: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Scale down will not halt with unhealthy stateful pod @ 01/10/24 16:14:36.545
  Jan 10 16:14:36.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=statefulset-4699 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jan 10 16:14:36.901: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jan 10 16:14:36.901: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jan 10 16:14:36.901: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jan 10 16:14:36.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=statefulset-4699 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jan 10 16:14:37.171: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jan 10 16:14:37.171: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jan 10 16:14:37.171: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jan 10 16:14:37.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=statefulset-4699 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jan 10 16:14:37.512: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jan 10 16:14:37.512: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jan 10 16:14:37.512: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jan 10 16:14:37.512: INFO: Waiting for statefulset status.replicas updated to 0
  Jan 10 16:14:37.521: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
  Jan 10 16:14:47.551: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jan 10 16:14:47.551: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Jan 10 16:14:47.551: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Jan 10 16:14:47.585: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
  Jan 10 16:14:47.585: INFO: ss-0  env1-test-worker-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-10 16:14:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-10 16:14:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-10 16:14:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-10 16:14:05 +0000 UTC  }]
  Jan 10 16:14:47.586: INFO: ss-1  env1-test-worker-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-10 16:14:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-10 16:14:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-10 16:14:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-10 16:14:25 +0000 UTC  }]
  Jan 10 16:14:47.586: INFO: ss-2  env1-test-worker-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-10 16:14:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-10 16:14:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-10 16:14:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-10 16:14:25 +0000 UTC  }]
  Jan 10 16:14:47.586: INFO: 
  Jan 10 16:14:47.586: INFO: StatefulSet ss has not reached scale 0, at 3
  Jan 10 16:14:48.601: INFO: POD   NODE                PHASE      GRACE  CONDITIONS
  Jan 10 16:14:48.602: INFO: ss-0  env1-test-worker-1  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-10 16:14:05 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-10 16:14:37 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-10 16:14:37 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-10 16:14:05 +0000 UTC  }]
  Jan 10 16:14:48.603: INFO: ss-1  env1-test-worker-0  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-10 16:14:25 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-10 16:14:37 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-10 16:14:37 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-10 16:14:25 +0000 UTC  }]
  Jan 10 16:14:48.603: INFO: ss-2  env1-test-worker-1  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2024-01-10 16:14:25 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2024-01-10 16:14:37 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2024-01-10 16:14:37 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2024-01-10 16:14:25 +0000 UTC  }]
  Jan 10 16:14:48.604: INFO: 
  Jan 10 16:14:48.604: INFO: StatefulSet ss has not reached scale 0, at 3
  Jan 10 16:14:49.613: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.96733513s
  Jan 10 16:14:50.628: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.958339073s
  Jan 10 16:14:51.638: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.943456109s
  Jan 10 16:14:52.649: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.93431878s
  Jan 10 16:14:53.660: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.922221625s
  Jan 10 16:14:54.672: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.912395594s
  Jan 10 16:14:55.681: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.900006004s
  Jan 10 16:14:56.690: INFO: Verifying statefulset ss doesn't scale past 0 for another 890.692308ms
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4699 @ 01/10/24 16:14:57.691
  Jan 10 16:14:57.703: INFO: Scaling statefulset ss to 0
  Jan 10 16:14:57.727: INFO: Waiting for statefulset status.replicas updated to 0
  Jan 10 16:14:57.734: INFO: Deleting all statefulset in ns statefulset-4699
  Jan 10 16:14:57.740: INFO: Scaling statefulset ss to 0
  Jan 10 16:14:57.768: INFO: Waiting for statefulset status.replicas updated to 0
  Jan 10 16:14:57.775: INFO: Deleting statefulset ss
  Jan 10 16:14:57.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-4699" for this suite. @ 01/10/24 16:14:57.822
• [52.969 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]
test/e2e/apimachinery/webhook.go:331
  STEP: Creating a kubernetes client @ 01/10/24 16:14:57.839
  Jan 10 16:14:57.839: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename webhook @ 01/10/24 16:14:57.84
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:14:57.879
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:14:57.886
  STEP: Setting up server cert @ 01/10/24 16:14:57.955
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 01/10/24 16:14:58.45
  STEP: Deploying the webhook pod @ 01/10/24 16:14:58.465
  STEP: Wait for the deployment to be ready @ 01/10/24 16:14:58.494
  Jan 10 16:14:58.509: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 01/10/24 16:15:00.546
  STEP: Verifying the service has paired with the endpoint @ 01/10/24 16:15:00.587
  Jan 10 16:15:01.587: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jan 10 16:15:01.596: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5338-crds.webhook.example.com via the AdmissionRegistration API @ 01/10/24 16:15:07.122
  STEP: Creating a custom resource that should be mutated by the webhook @ 01/10/24 16:15:07.166
  Jan 10 16:15:09.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5472" for this suite. @ 01/10/24 16:15:10.155
  STEP: Destroying namespace "webhook-markers-8056" for this suite. @ 01/10/24 16:15:10.17
• [12.346 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:74
  STEP: Creating a kubernetes client @ 01/10/24 16:15:10.186
  Jan 10 16:15:10.186: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename projected @ 01/10/24 16:15:10.188
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:15:10.239
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:15:10.249
  STEP: Creating configMap with name projected-configmap-test-volume-22c402b4-8b53-4b4f-b87b-7c04642d1e48 @ 01/10/24 16:15:10.262
  STEP: Creating a pod to test consume configMaps @ 01/10/24 16:15:10.281
  STEP: Saw pod success @ 01/10/24 16:15:14.356
  Jan 10 16:15:14.366: INFO: Trying to get logs from node env1-test-worker-1 pod pod-projected-configmaps-255e8ccd-d716-44c9-b78e-41cf610ba832 container agnhost-container: <nil>
  STEP: delete the pod @ 01/10/24 16:15:14.403
  Jan 10 16:15:14.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3703" for this suite. @ 01/10/24 16:15:14.577
• [4.429 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:248
  STEP: Creating a kubernetes client @ 01/10/24 16:15:14.616
  Jan 10 16:15:14.616: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename container-runtime @ 01/10/24 16:15:14.62
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:15:14.672
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:15:14.686
  STEP: create the container @ 01/10/24 16:15:14.693
  W0110 16:15:14.729348      23 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 01/10/24 16:15:14.729
  STEP: get the container status @ 01/10/24 16:15:17.772
  STEP: the container should be terminated @ 01/10/24 16:15:17.79
  STEP: the termination message should be set @ 01/10/24 16:15:17.791
  Jan 10 16:15:17.791: INFO: Expected: &{OK} to match Container's Termination Message: OK --
  STEP: delete the container @ 01/10/24 16:15:17.792
  Jan 10 16:15:17.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-6767" for this suite. @ 01/10/24 16:15:17.86
• [3.262 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:486
  STEP: Creating a kubernetes client @ 01/10/24 16:15:17.889
  Jan 10 16:15:17.890: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename security-context-test @ 01/10/24 16:15:17.892
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:15:17.944
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:15:17.953
  Jan 10 16:15:22.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-5037" for this suite. @ 01/10/24 16:15:22.061
• [4.190 seconds]
------------------------------
SS
------------------------------
[sig-apps] ReplicaSet Replace and Patch tests [Conformance]
test/e2e/apps/replica_set.go:154
  STEP: Creating a kubernetes client @ 01/10/24 16:15:22.08
  Jan 10 16:15:22.080: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename replicaset @ 01/10/24 16:15:22.082
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:15:22.149
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:15:22.155
  Jan 10 16:15:22.193: INFO: Pod name sample-pod: Found 0 pods out of 1
  Jan 10 16:15:27.206: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 01/10/24 16:15:27.206
  STEP: Scaling up "test-rs" replicaset  @ 01/10/24 16:15:27.206
  Jan 10 16:15:27.230: INFO: Updating replica set "test-rs"
  STEP: patching the ReplicaSet @ 01/10/24 16:15:27.231
  W0110 16:15:27.274687      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Jan 10 16:15:27.278: INFO: observed ReplicaSet test-rs in namespace replicaset-7595 with ReadyReplicas 1, AvailableReplicas 1
  Jan 10 16:15:27.323: INFO: observed ReplicaSet test-rs in namespace replicaset-7595 with ReadyReplicas 1, AvailableReplicas 1
  Jan 10 16:15:27.408: INFO: observed ReplicaSet test-rs in namespace replicaset-7595 with ReadyReplicas 1, AvailableReplicas 1
  Jan 10 16:15:27.437: INFO: observed ReplicaSet test-rs in namespace replicaset-7595 with ReadyReplicas 1, AvailableReplicas 1
  Jan 10 16:15:29.117: INFO: observed ReplicaSet test-rs in namespace replicaset-7595 with ReadyReplicas 2, AvailableReplicas 2
  Jan 10 16:15:29.194: INFO: observed Replicaset test-rs in namespace replicaset-7595 with ReadyReplicas 3 found true
  Jan 10 16:15:29.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-7595" for this suite. @ 01/10/24 16:15:29.215
• [7.155 seconds]
------------------------------
SS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:109
  STEP: Creating a kubernetes client @ 01/10/24 16:15:29.237
  Jan 10 16:15:29.237: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename configmap @ 01/10/24 16:15:29.239
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:15:29.281
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:15:29.288
  STEP: Creating configMap with name configmap-test-volume-map-9296aa81-0c91-439b-86ba-4dc19f9d2b7a @ 01/10/24 16:15:29.294
  STEP: Creating a pod to test consume configMaps @ 01/10/24 16:15:29.304
  STEP: Saw pod success @ 01/10/24 16:15:33.381
  Jan 10 16:15:33.388: INFO: Trying to get logs from node env1-test-worker-1 pod pod-configmaps-89b0a2e7-3539-4911-a9e0-1738292bf64d container agnhost-container: <nil>
  STEP: delete the pod @ 01/10/24 16:15:33.403
  Jan 10 16:15:33.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6552" for this suite. @ 01/10/24 16:15:33.455
• [4.236 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown and duplicate fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:64
  STEP: Creating a kubernetes client @ 01/10/24 16:15:33.474
  Jan 10 16:15:33.474: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename field-validation @ 01/10/24 16:15:33.476
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:15:33.516
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:15:33.524
  STEP: apply creating a deployment @ 01/10/24 16:15:33.532
  Jan 10 16:15:33.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-1578" for this suite. @ 01/10/24 16:15:33.593
• [0.133 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:99
  STEP: Creating a kubernetes client @ 01/10/24 16:15:33.616
  Jan 10 16:15:33.616: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename projected @ 01/10/24 16:15:33.618
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:15:33.674
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:15:33.679
  STEP: Creating configMap with name projected-configmap-test-volume-map-923c3b0c-9430-44ef-9f11-e2b0d0eb4e7d @ 01/10/24 16:15:33.686
  STEP: Creating a pod to test consume configMaps @ 01/10/24 16:15:33.696
  STEP: Saw pod success @ 01/10/24 16:15:37.77
  Jan 10 16:15:37.783: INFO: Trying to get logs from node env1-test-worker-1 pod pod-projected-configmaps-a7bf6c88-d0a9-4554-a1ed-c1ca27506839 container agnhost-container: <nil>
  STEP: delete the pod @ 01/10/24 16:15:37.807
  Jan 10 16:15:37.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4866" for this suite. @ 01/10/24 16:15:37.873
• [4.271 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]
test/e2e/common/node/configmap.go:138
  STEP: Creating a kubernetes client @ 01/10/24 16:15:37.887
  Jan 10 16:15:37.887: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename configmap @ 01/10/24 16:15:37.889
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:15:37.939
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:15:37.947
  STEP: Creating configMap that has name configmap-test-emptyKey-af1a09c5-e4b3-4eb8-a86a-29d2adbdc4de @ 01/10/24 16:15:37.952
  Jan 10 16:15:37.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2959" for this suite. @ 01/10/24 16:15:37.97
• [0.113 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl logs logs should be able to retrieve and filter logs  [Conformance]
test/e2e/kubectl/logs.go:114
  STEP: Creating a kubernetes client @ 01/10/24 16:15:38.008
  Jan 10 16:15:38.008: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename kubectl-logs @ 01/10/24 16:15:38.01
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:15:38.061
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:15:38.067
  STEP: creating an pod @ 01/10/24 16:15:38.075
  Jan 10 16:15:38.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-logs-535 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
  Jan 10 16:15:38.272: INFO: stderr: ""
  Jan 10 16:15:38.272: INFO: stdout: "pod/logs-generator created\n"
  STEP: Waiting for log generator to start. @ 01/10/24 16:15:38.272
  Jan 10 16:15:38.272: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
  Jan 10 16:15:40.302: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
  STEP: checking for a matching strings @ 01/10/24 16:15:40.302
  Jan 10 16:15:40.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-logs-535 logs logs-generator logs-generator'
  Jan 10 16:15:40.528: INFO: stderr: ""
  Jan 10 16:15:40.528: INFO: stdout: "I0110 16:15:39.166265       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/7k4z 367\nI0110 16:15:39.366555       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/8pp 579\nI0110 16:15:39.567240       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/z4v 555\nI0110 16:15:39.766580       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/pbtp 548\nI0110 16:15:39.966391       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/2ftz 581\nI0110 16:15:40.166842       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/k2gz 296\nI0110 16:15:40.366433       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/rpq 568\n"
  STEP: limiting log lines @ 01/10/24 16:15:40.528
  Jan 10 16:15:40.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-logs-535 logs logs-generator logs-generator --tail=1'
  Jan 10 16:15:40.709: INFO: stderr: ""
  Jan 10 16:15:40.709: INFO: stdout: "I0110 16:15:40.566858       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/2d9 238\n"
  Jan 10 16:15:40.709: INFO: got output "I0110 16:15:40.566858       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/2d9 238\n"
  STEP: limiting log bytes @ 01/10/24 16:15:40.709
  Jan 10 16:15:40.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-logs-535 logs logs-generator logs-generator --limit-bytes=1'
  Jan 10 16:15:40.893: INFO: stderr: ""
  Jan 10 16:15:40.893: INFO: stdout: "I"
  Jan 10 16:15:40.893: INFO: got output "I"
  STEP: exposing timestamps @ 01/10/24 16:15:40.893
  Jan 10 16:15:40.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-logs-535 logs logs-generator logs-generator --tail=1 --timestamps'
  Jan 10 16:15:41.085: INFO: stderr: ""
  Jan 10 16:15:41.085: INFO: stdout: "2024-01-10T16:15:40.966962700Z I0110 16:15:40.966610       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/scxt 532\n"
  Jan 10 16:15:41.085: INFO: got output "2024-01-10T16:15:40.966962700Z I0110 16:15:40.966610       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/scxt 532\n"
  STEP: restricting to a time range @ 01/10/24 16:15:41.085
  Jan 10 16:15:43.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-logs-535 logs logs-generator logs-generator --since=1s'
  Jan 10 16:15:43.818: INFO: stderr: ""
  Jan 10 16:15:43.818: INFO: stdout: "I0110 16:15:42.967123       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/wtx5 427\nI0110 16:15:43.166596       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/default/pods/94r 593\nI0110 16:15:43.366967       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/kube-system/pods/kgq 328\nI0110 16:15:43.566468       1 logs_generator.go:76] 22 GET /api/v1/namespaces/ns/pods/wh7 403\nI0110 16:15:43.767540       1 logs_generator.go:76] 23 POST /api/v1/namespaces/kube-system/pods/4r9v 226\n"
  Jan 10 16:15:43.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-logs-535 logs logs-generator logs-generator --since=24h'
  Jan 10 16:15:44.023: INFO: stderr: ""
  Jan 10 16:15:44.023: INFO: stdout: "I0110 16:15:39.166265       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/7k4z 367\nI0110 16:15:39.366555       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/8pp 579\nI0110 16:15:39.567240       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/z4v 555\nI0110 16:15:39.766580       1 logs_generator.go:76] 3 GET /api/v1/namespaces/kube-system/pods/pbtp 548\nI0110 16:15:39.966391       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/2ftz 581\nI0110 16:15:40.166842       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/k2gz 296\nI0110 16:15:40.366433       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/rpq 568\nI0110 16:15:40.566858       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/2d9 238\nI0110 16:15:40.768525       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/2cs9 327\nI0110 16:15:40.966610       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/scxt 532\nI0110 16:15:41.166992       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/6ps 403\nI0110 16:15:41.366532       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/sgn 226\nI0110 16:15:41.567071       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/9sd 455\nI0110 16:15:41.766973       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/6m7 474\nI0110 16:15:41.966401       1 logs_generator.go:76] 14 POST /api/v1/namespaces/kube-system/pods/2b8 597\nI0110 16:15:42.167118       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/84z 518\nI0110 16:15:42.366584       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/vklc 277\nI0110 16:15:42.567321       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/r7v9 347\nI0110 16:15:42.766581       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/jxc9 573\nI0110 16:15:42.967123       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/wtx5 427\nI0110 16:15:43.166596       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/default/pods/94r 593\nI0110 16:15:43.366967       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/kube-system/pods/kgq 328\nI0110 16:15:43.566468       1 logs_generator.go:76] 22 GET /api/v1/namespaces/ns/pods/wh7 403\nI0110 16:15:43.767540       1 logs_generator.go:76] 23 POST /api/v1/namespaces/kube-system/pods/4r9v 226\nI0110 16:15:43.966389       1 logs_generator.go:76] 24 PUT /api/v1/namespaces/ns/pods/2m9 570\n"
  Jan 10 16:15:44.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-logs-535 delete pod logs-generator'
  Jan 10 16:15:45.278: INFO: stderr: ""
  Jan 10 16:15:45.278: INFO: stdout: "pod \"logs-generator\" deleted\n"
  Jan 10 16:15:45.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-logs-535" for this suite. @ 01/10/24 16:15:45.29
• [7.307 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should get and update a ReplicationController scale [Conformance]
test/e2e/apps/rc.go:424
  STEP: Creating a kubernetes client @ 01/10/24 16:15:45.317
  Jan 10 16:15:45.317: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename replication-controller @ 01/10/24 16:15:45.319
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:15:45.369
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:15:45.374
  STEP: Creating ReplicationController "e2e-rc-2294q" @ 01/10/24 16:15:45.381
  Jan 10 16:15:45.401: INFO: Get Replication Controller "e2e-rc-2294q" to confirm replicas
  Jan 10 16:15:46.420: INFO: Get Replication Controller "e2e-rc-2294q" to confirm replicas
  Jan 10 16:15:46.430: INFO: Found 1 replicas for "e2e-rc-2294q" replication controller
  STEP: Getting scale subresource for ReplicationController "e2e-rc-2294q" @ 01/10/24 16:15:46.43
  STEP: Updating a scale subresource @ 01/10/24 16:15:46.44
  STEP: Verifying replicas where modified for replication controller "e2e-rc-2294q" @ 01/10/24 16:15:46.458
  Jan 10 16:15:46.459: INFO: Get Replication Controller "e2e-rc-2294q" to confirm replicas
  Jan 10 16:15:47.495: INFO: Get Replication Controller "e2e-rc-2294q" to confirm replicas
  Jan 10 16:15:47.503: INFO: Found 2 replicas for "e2e-rc-2294q" replication controller
  Jan 10 16:15:47.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-9995" for this suite. @ 01/10/24 16:15:47.52
• [2.219 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:78
  STEP: Creating a kubernetes client @ 01/10/24 16:15:47.54
  Jan 10 16:15:47.540: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename projected @ 01/10/24 16:15:47.542
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:15:47.588
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:15:47.592
  STEP: Creating projection with secret that has name projected-secret-test-map-ffca2f17-4d29-4b17-bcd4-fcc216281566 @ 01/10/24 16:15:47.6
  STEP: Creating a pod to test consume secrets @ 01/10/24 16:15:47.62
  STEP: Saw pod success @ 01/10/24 16:15:51.715
  Jan 10 16:15:51.725: INFO: Trying to get logs from node env1-test-worker-1 pod pod-projected-secrets-370a8979-1b34-4dc9-a8be-bae87afa0f51 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 01/10/24 16:15:51.744
  Jan 10 16:15:51.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4255" for this suite. @ 01/10/24 16:15:51.816
• [4.297 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/rc.go:69
  STEP: Creating a kubernetes client @ 01/10/24 16:15:51.84
  Jan 10 16:15:51.841: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename replication-controller @ 01/10/24 16:15:51.842
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:15:51.876
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:15:51.882
  STEP: Creating replication controller my-hostname-basic-18386237-f3f0-483a-9ff6-d1f0e7f4393c @ 01/10/24 16:15:51.889
  Jan 10 16:15:51.926: INFO: Pod name my-hostname-basic-18386237-f3f0-483a-9ff6-d1f0e7f4393c: Found 0 pods out of 1
  Jan 10 16:15:56.941: INFO: Pod name my-hostname-basic-18386237-f3f0-483a-9ff6-d1f0e7f4393c: Found 1 pods out of 1
  Jan 10 16:15:56.941: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-18386237-f3f0-483a-9ff6-d1f0e7f4393c" are running
  Jan 10 16:15:56.952: INFO: Pod "my-hostname-basic-18386237-f3f0-483a-9ff6-d1f0e7f4393c-m2p2f" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-01-10 16:15:51 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-01-10 16:15:53 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-01-10 16:15:53 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-01-10 16:15:51 +0000 UTC Reason: Message:}])
  Jan 10 16:15:56.953: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 01/10/24 16:15:56.954
  Jan 10 16:15:56.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-4069" for this suite. @ 01/10/24 16:15:56.995
• [5.168 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should support configurable pod DNS nameservers [Conformance]
test/e2e/network/dns.go:407
  STEP: Creating a kubernetes client @ 01/10/24 16:15:57.015
  Jan 10 16:15:57.015: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename dns @ 01/10/24 16:15:57.017
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:15:57.062
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:15:57.069
  STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... @ 01/10/24 16:15:57.077
  Jan 10 16:15:57.102: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-1675  98434abc-7dd5-4619-af23-acda3984ea62 186782032 0 2024-01-10 16:15:57 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2024-01-10 16:15:57 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-27cng,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-27cng,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  STEP: Verifying customized DNS suffix list is configured on pod... @ 01/10/24 16:15:59.133
  Jan 10 16:15:59.133: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-1675 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 10 16:15:59.133: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  Jan 10 16:15:59.135: INFO: ExecWithOptions: Clientset creation
  Jan 10 16:15:59.135: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-1675/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  STEP: Verifying customized DNS server is configured on pod... @ 01/10/24 16:15:59.328
  Jan 10 16:15:59.328: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-1675 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 10 16:15:59.328: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  Jan 10 16:15:59.329: INFO: ExecWithOptions: Clientset creation
  Jan 10 16:15:59.330: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-1675/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jan 10 16:15:59.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jan 10 16:15:59.544: INFO: Deleting pod test-dns-nameservers...
  STEP: Destroying namespace "dns-1675" for this suite. @ 01/10/24 16:15:59.609
• [2.645 seconds]
------------------------------
[sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:47
  STEP: Creating a kubernetes client @ 01/10/24 16:15:59.66
  Jan 10 16:15:59.660: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename var-expansion @ 01/10/24 16:15:59.662
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:15:59.721
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:15:59.729
  STEP: Creating a pod to test env composition @ 01/10/24 16:15:59.737
  STEP: Saw pod success @ 01/10/24 16:16:03.8
  Jan 10 16:16:03.811: INFO: Trying to get logs from node env1-test-worker-1 pod var-expansion-8239050b-07cd-4ee8-a665-fef7aa02b6a6 container dapi-container: <nil>
  STEP: delete the pod @ 01/10/24 16:16:03.831
  Jan 10 16:16:03.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-1862" for this suite. @ 01/10/24 16:16:03.892
• [4.254 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/replica_set.go:111
  STEP: Creating a kubernetes client @ 01/10/24 16:16:03.917
  Jan 10 16:16:03.917: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename replicaset @ 01/10/24 16:16:03.919
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:16:03.961
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:16:03.969
  Jan 10 16:16:03.976: INFO: Creating ReplicaSet my-hostname-basic-b4252a94-e496-4578-863b-702358b9b422
  Jan 10 16:16:04.004: INFO: Pod name my-hostname-basic-b4252a94-e496-4578-863b-702358b9b422: Found 0 pods out of 1
  Jan 10 16:16:09.016: INFO: Pod name my-hostname-basic-b4252a94-e496-4578-863b-702358b9b422: Found 1 pods out of 1
  Jan 10 16:16:09.016: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-b4252a94-e496-4578-863b-702358b9b422" is running
  Jan 10 16:16:09.030: INFO: Pod "my-hostname-basic-b4252a94-e496-4578-863b-702358b9b422-54djs" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-01-10 16:16:04 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-01-10 16:16:05 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-01-10 16:16:05 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-01-10 16:16:04 +0000 UTC Reason: Message:}])
  Jan 10 16:16:09.030: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 01/10/24 16:16:09.03
  Jan 10 16:16:09.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-4273" for this suite. @ 01/10/24 16:16:09.094
• [5.207 seconds]
------------------------------
[sig-api-machinery] FieldValidation should detect duplicates in a CR when preserving unknown fields [Conformance]
test/e2e/apimachinery/field_validation.go:610
  STEP: Creating a kubernetes client @ 01/10/24 16:16:09.126
  Jan 10 16:16:09.126: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename field-validation @ 01/10/24 16:16:09.129
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:16:09.179
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:16:09.186
  Jan 10 16:16:09.199: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  W0110 16:16:16.930756      23 warnings.go:70] unknown field "alpha"
  W0110 16:16:16.930822      23 warnings.go:70] unknown field "beta"
  W0110 16:16:16.930835      23 warnings.go:70] unknown field "delta"
  W0110 16:16:16.930846      23 warnings.go:70] unknown field "epsilon"
  W0110 16:16:16.930857      23 warnings.go:70] unknown field "gamma"
  Jan 10 16:16:16.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-814" for this suite. @ 01/10/24 16:16:17.021
• [7.912 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]
test/e2e/network/service.go:1416
  STEP: Creating a kubernetes client @ 01/10/24 16:16:17.04
  Jan 10 16:16:17.040: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename services @ 01/10/24 16:16:17.042
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:16:17.076
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:16:17.083
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-123 @ 01/10/24 16:16:17.092
  STEP: changing the ExternalName service to type=ClusterIP @ 01/10/24 16:16:17.108
  STEP: creating replication controller externalname-service in namespace services-123 @ 01/10/24 16:16:17.149
  I0110 16:16:17.163974      23 runners.go:194] Created replication controller with name: externalname-service, namespace: services-123, replica count: 2
  I0110 16:16:20.215296      23 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jan 10 16:16:20.215: INFO: Creating new exec pod
  Jan 10 16:16:23.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=services-123 exec execpodmd45d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Jan 10 16:16:23.608: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Jan 10 16:16:23.609: INFO: stdout: "externalname-service-6w9b9"
  Jan 10 16:16:23.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=services-123 exec execpodmd45d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.59.161 80'
  Jan 10 16:16:23.949: INFO: stderr: "+ nc -v -t -w 2 10.233.59.161 80\n+ echo hostName\nConnection to 10.233.59.161 80 port [tcp/http] succeeded!\n"
  Jan 10 16:16:23.949: INFO: stdout: ""
  Jan 10 16:16:24.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=services-123 exec execpodmd45d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.59.161 80'
  Jan 10 16:16:25.260: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.59.161 80\nConnection to 10.233.59.161 80 port [tcp/http] succeeded!\n"
  Jan 10 16:16:25.260: INFO: stdout: "externalname-service-6w9b9"
  Jan 10 16:16:25.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jan 10 16:16:25.285: INFO: Cleaning up the ExternalName to ClusterIP test service
  STEP: Destroying namespace "services-123" for this suite. @ 01/10/24 16:16:25.36
• [8.340 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:141
  STEP: Creating a kubernetes client @ 01/10/24 16:16:25.382
  Jan 10 16:16:25.382: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename crd-webhook @ 01/10/24 16:16:25.385
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:16:25.428
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:16:25.436
  STEP: Setting up server cert @ 01/10/24 16:16:25.446
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 01/10/24 16:16:26.917
  STEP: Deploying the custom resource conversion webhook pod @ 01/10/24 16:16:26.941
  STEP: Wait for the deployment to be ready @ 01/10/24 16:16:26.968
  Jan 10 16:16:27.006: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 01/10/24 16:16:29.038
  STEP: Verifying the service has paired with the endpoint @ 01/10/24 16:16:29.059
  Jan 10 16:16:30.060: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Jan 10 16:16:30.070: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Creating a v1 custom resource @ 01/10/24 16:16:37.778
  STEP: v2 custom resource should be converted @ 01/10/24 16:16:37.789
  Jan 10 16:16:37.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-4410" for this suite. @ 01/10/24 16:16:38.542
• [13.196 seconds]
------------------------------
[sig-node] Probing container should *not* be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:523
  STEP: Creating a kubernetes client @ 01/10/24 16:16:38.578
  Jan 10 16:16:38.578: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename container-probe @ 01/10/24 16:16:38.579
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:16:38.656
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:16:38.665
  STEP: Creating pod test-grpc-bdd5cc6e-b228-4971-baba-26852220b48d in namespace container-probe-2309 @ 01/10/24 16:16:38.677
  Jan 10 16:16:40.742: INFO: Started pod test-grpc-bdd5cc6e-b228-4971-baba-26852220b48d in namespace container-probe-2309
  STEP: checking the pod's current state and verifying that restartCount is present @ 01/10/24 16:16:40.742
  Jan 10 16:16:40.750: INFO: Initial restart count of pod test-grpc-bdd5cc6e-b228-4971-baba-26852220b48d is 0
  Jan 10 16:20:42.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 01/10/24 16:20:42.079
  STEP: Destroying namespace "container-probe-2309" for this suite. @ 01/10/24 16:20:42.112
• [243.568 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:95
  STEP: Creating a kubernetes client @ 01/10/24 16:20:42.152
  Jan 10 16:20:42.152: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename var-expansion @ 01/10/24 16:20:42.163
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:20:42.208
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:20:42.214
  STEP: Creating a pod to test substitution in container's args @ 01/10/24 16:20:42.22
  STEP: Saw pod success @ 01/10/24 16:20:46.299
  Jan 10 16:20:46.307: INFO: Trying to get logs from node env1-test-worker-1 pod var-expansion-d433e924-fe3e-4f73-96e7-93b608fb865a container dapi-container: <nil>
  STEP: delete the pod @ 01/10/24 16:20:46.358
  Jan 10 16:20:46.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-3544" for this suite. @ 01/10/24 16:20:46.407
• [4.271 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:232
  STEP: Creating a kubernetes client @ 01/10/24 16:20:46.44
  Jan 10 16:20:46.441: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename container-runtime @ 01/10/24 16:20:46.444
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:20:46.491
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:20:46.499
  STEP: create the container @ 01/10/24 16:20:46.505
  W0110 16:20:46.528885      23 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 01/10/24 16:20:46.529
  STEP: get the container status @ 01/10/24 16:20:49.585
  STEP: the container should be terminated @ 01/10/24 16:20:49.592
  STEP: the termination message should be set @ 01/10/24 16:20:49.593
  Jan 10 16:20:49.593: INFO: Expected: &{} to match Container's Termination Message:  --
  STEP: delete the container @ 01/10/24 16:20:49.593
  Jan 10 16:20:49.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-7623" for this suite. @ 01/10/24 16:20:49.656
• [3.234 seconds]
------------------------------
S
------------------------------
[sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]
test/e2e/common/node/ephemeral_containers.go:46
  STEP: Creating a kubernetes client @ 01/10/24 16:20:49.677
  Jan 10 16:20:49.677: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename ephemeral-containers-test @ 01/10/24 16:20:49.68
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:20:49.72
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:20:49.727
  STEP: creating a target pod @ 01/10/24 16:20:49.733
  STEP: adding an ephemeral container @ 01/10/24 16:20:51.791
  STEP: checking pod container endpoints @ 01/10/24 16:20:53.84
  Jan 10 16:20:53.840: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-9789 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 10 16:20:53.840: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  Jan 10 16:20:53.842: INFO: ExecWithOptions: Clientset creation
  Jan 10 16:20:53.842: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/ephemeral-containers-test-9789/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
  Jan 10 16:20:53.981: INFO: Exec stderr: ""
  Jan 10 16:20:54.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ephemeral-containers-test-9789" for this suite. @ 01/10/24 16:20:54.015
• [4.360 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:354
  STEP: Creating a kubernetes client @ 01/10/24 16:20:54.041
  Jan 10 16:20:54.041: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename kubectl @ 01/10/24 16:20:54.043
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:20:54.082
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:20:54.092
  STEP: creating a replication controller @ 01/10/24 16:20:54.099
  Jan 10 16:20:54.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-8344 create -f -'
  Jan 10 16:20:57.476: INFO: stderr: ""
  Jan 10 16:20:57.476: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 01/10/24 16:20:57.477
  Jan 10 16:20:57.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-8344 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jan 10 16:20:57.623: INFO: stderr: ""
  Jan 10 16:20:57.623: INFO: stdout: "update-demo-nautilus-c9n9c update-demo-nautilus-x9rjc "
  Jan 10 16:20:57.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-8344 get pods update-demo-nautilus-c9n9c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jan 10 16:20:57.771: INFO: stderr: ""
  Jan 10 16:20:57.772: INFO: stdout: ""
  Jan 10 16:20:57.772: INFO: update-demo-nautilus-c9n9c is created but not running
  Jan 10 16:21:02.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-8344 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jan 10 16:21:02.966: INFO: stderr: ""
  Jan 10 16:21:02.966: INFO: stdout: "update-demo-nautilus-c9n9c update-demo-nautilus-x9rjc "
  Jan 10 16:21:02.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-8344 get pods update-demo-nautilus-c9n9c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jan 10 16:21:03.143: INFO: stderr: ""
  Jan 10 16:21:03.143: INFO: stdout: "true"
  Jan 10 16:21:03.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-8344 get pods update-demo-nautilus-c9n9c -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jan 10 16:21:03.328: INFO: stderr: ""
  Jan 10 16:21:03.328: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jan 10 16:21:03.328: INFO: validating pod update-demo-nautilus-c9n9c
  Jan 10 16:21:03.343: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jan 10 16:21:03.343: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jan 10 16:21:03.344: INFO: update-demo-nautilus-c9n9c is verified up and running
  Jan 10 16:21:03.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-8344 get pods update-demo-nautilus-x9rjc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jan 10 16:21:03.530: INFO: stderr: ""
  Jan 10 16:21:03.530: INFO: stdout: "true"
  Jan 10 16:21:03.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-8344 get pods update-demo-nautilus-x9rjc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jan 10 16:21:03.696: INFO: stderr: ""
  Jan 10 16:21:03.696: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jan 10 16:21:03.696: INFO: validating pod update-demo-nautilus-x9rjc
  Jan 10 16:21:03.708: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jan 10 16:21:03.708: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jan 10 16:21:03.708: INFO: update-demo-nautilus-x9rjc is verified up and running
  STEP: scaling down the replication controller @ 01/10/24 16:21:03.709
  Jan 10 16:21:03.713: INFO: scanned /root for discovery docs: <nil>
  Jan 10 16:21:03.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-8344 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
  Jan 10 16:21:04.941: INFO: stderr: ""
  Jan 10 16:21:04.941: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 01/10/24 16:21:04.941
  Jan 10 16:21:04.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-8344 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jan 10 16:21:05.133: INFO: stderr: ""
  Jan 10 16:21:05.133: INFO: stdout: "update-demo-nautilus-c9n9c "
  Jan 10 16:21:05.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-8344 get pods update-demo-nautilus-c9n9c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jan 10 16:21:05.295: INFO: stderr: ""
  Jan 10 16:21:05.295: INFO: stdout: "true"
  Jan 10 16:21:05.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-8344 get pods update-demo-nautilus-c9n9c -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jan 10 16:21:05.453: INFO: stderr: ""
  Jan 10 16:21:05.453: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jan 10 16:21:05.453: INFO: validating pod update-demo-nautilus-c9n9c
  Jan 10 16:21:05.462: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jan 10 16:21:05.462: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jan 10 16:21:05.463: INFO: update-demo-nautilus-c9n9c is verified up and running
  STEP: scaling up the replication controller @ 01/10/24 16:21:05.463
  Jan 10 16:21:05.467: INFO: scanned /root for discovery docs: <nil>
  Jan 10 16:21:05.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-8344 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
  Jan 10 16:21:06.716: INFO: stderr: ""
  Jan 10 16:21:06.716: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 01/10/24 16:21:06.716
  Jan 10 16:21:06.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-8344 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jan 10 16:21:06.933: INFO: stderr: ""
  Jan 10 16:21:06.933: INFO: stdout: "update-demo-nautilus-c9n9c update-demo-nautilus-cwjzb "
  Jan 10 16:21:06.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-8344 get pods update-demo-nautilus-c9n9c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jan 10 16:21:07.100: INFO: stderr: ""
  Jan 10 16:21:07.100: INFO: stdout: "true"
  Jan 10 16:21:07.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-8344 get pods update-demo-nautilus-c9n9c -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jan 10 16:21:07.293: INFO: stderr: ""
  Jan 10 16:21:07.293: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jan 10 16:21:07.293: INFO: validating pod update-demo-nautilus-c9n9c
  Jan 10 16:21:07.302: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jan 10 16:21:07.302: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jan 10 16:21:07.303: INFO: update-demo-nautilus-c9n9c is verified up and running
  Jan 10 16:21:07.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-8344 get pods update-demo-nautilus-cwjzb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jan 10 16:21:07.459: INFO: stderr: ""
  Jan 10 16:21:07.460: INFO: stdout: "true"
  Jan 10 16:21:07.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-8344 get pods update-demo-nautilus-cwjzb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jan 10 16:21:07.611: INFO: stderr: ""
  Jan 10 16:21:07.612: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jan 10 16:21:07.612: INFO: validating pod update-demo-nautilus-cwjzb
  Jan 10 16:21:07.621: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jan 10 16:21:07.621: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jan 10 16:21:07.621: INFO: update-demo-nautilus-cwjzb is verified up and running
  STEP: using delete to clean up resources @ 01/10/24 16:21:07.621
  Jan 10 16:21:07.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-8344 delete --grace-period=0 --force -f -'
  Jan 10 16:21:07.792: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jan 10 16:21:07.792: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Jan 10 16:21:07.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-8344 get rc,svc -l name=update-demo --no-headers'
  Jan 10 16:21:07.988: INFO: stderr: "No resources found in kubectl-8344 namespace.\n"
  Jan 10 16:21:07.988: INFO: stdout: ""
  Jan 10 16:21:07.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-8344 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Jan 10 16:21:08.142: INFO: stderr: ""
  Jan 10 16:21:08.143: INFO: stdout: ""
  Jan 10 16:21:08.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8344" for this suite. @ 01/10/24 16:21:08.156
• [14.137 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]
test/e2e/apimachinery/namespace.go:303
  STEP: Creating a kubernetes client @ 01/10/24 16:21:08.186
  Jan 10 16:21:08.186: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename namespaces @ 01/10/24 16:21:08.189
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:21:08.231
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:21:08.243
  STEP: Read namespace status @ 01/10/24 16:21:08.249
  Jan 10 16:21:08.262: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
  STEP: Patch namespace status @ 01/10/24 16:21:08.262
  Jan 10 16:21:08.287: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
  STEP: Update namespace status @ 01/10/24 16:21:08.287
  Jan 10 16:21:08.310: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
  Jan 10 16:21:08.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-5363" for this suite. @ 01/10/24 16:21:08.326
• [0.162 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:194
  STEP: Creating a kubernetes client @ 01/10/24 16:21:08.369
  Jan 10 16:21:08.369: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename projected @ 01/10/24 16:21:08.371
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:21:08.408
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:21:08.415
  STEP: Creating a pod to test downward API volume plugin @ 01/10/24 16:21:08.423
  STEP: Saw pod success @ 01/10/24 16:21:12.483
  Jan 10 16:21:12.491: INFO: Trying to get logs from node env1-test-worker-1 pod downwardapi-volume-a03f07b8-1945-4616-a14f-2eab169c15e7 container client-container: <nil>
  STEP: delete the pod @ 01/10/24 16:21:12.521
  Jan 10 16:21:12.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6900" for this suite. @ 01/10/24 16:21:12.589
• [4.234 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply an invalid CR with extra properties for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:344
  STEP: Creating a kubernetes client @ 01/10/24 16:21:12.604
  Jan 10 16:21:12.604: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename field-validation @ 01/10/24 16:21:12.606
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:21:12.658
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:21:12.666
  Jan 10 16:21:12.673: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  W0110 16:21:12.675772      23 field_validation.go:417] props: &JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{spec: {  <nil>  object   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[cronSpec:{  <nil>  string   nil <nil> false <nil> false <nil> <nil> ^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$ <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} foo:{  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} ports:{  <nil>  array   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] &JSONSchemaPropsOrArray{Schema:&JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[containerPort protocol],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{containerPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostIP: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},name: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},protocol: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},},JSONSchemas:[]JSONSchemaProps{},} [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [containerPort protocol] 0xc000a9de80 <nil> []}] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},}
  W0110 16:21:20.318749      23 warnings.go:70] unknown field "alpha"
  W0110 16:21:20.318783      23 warnings.go:70] unknown field "beta"
  W0110 16:21:20.318792      23 warnings.go:70] unknown field "delta"
  W0110 16:21:20.318800      23 warnings.go:70] unknown field "epsilon"
  W0110 16:21:20.318807      23 warnings.go:70] unknown field "gamma"
  Jan 10 16:21:20.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-4737" for this suite. @ 01/10/24 16:21:20.451
• [7.865 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]
test/e2e/kubectl/kubectl.go:1027
  STEP: Creating a kubernetes client @ 01/10/24 16:21:20.476
  Jan 10 16:21:20.476: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename kubectl @ 01/10/24 16:21:20.479
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:21:20.532
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:21:20.537
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 01/10/24 16:21:20.547
  Jan 10 16:21:20.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-755 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Jan 10 16:21:20.737: INFO: stderr: ""
  Jan 10 16:21:20.737: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: replace the image in the pod with server-side dry-run @ 01/10/24 16:21:20.737
  Jan 10 16:21:20.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-755 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
  Jan 10 16:21:20.962: INFO: stderr: ""
  Jan 10 16:21:20.962: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 01/10/24 16:21:20.962
  Jan 10 16:21:20.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-755 delete pods e2e-test-httpd-pod'
  Jan 10 16:21:22.863: INFO: stderr: ""
  Jan 10 16:21:22.863: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Jan 10 16:21:22.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-755" for this suite. @ 01/10/24 16:21:22.876
• [2.426 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should support rollover [Conformance]
test/e2e/apps/deployment.go:132
  STEP: Creating a kubernetes client @ 01/10/24 16:21:22.904
  Jan 10 16:21:22.904: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename deployment @ 01/10/24 16:21:22.905
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:21:22.945
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:21:22.951
  Jan 10 16:21:22.983: INFO: Pod name rollover-pod: Found 0 pods out of 1
  Jan 10 16:21:28.000: INFO: Pod name rollover-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 01/10/24 16:21:28
  Jan 10 16:21:28.000: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
  Jan 10 16:21:30.009: INFO: Creating deployment "test-rollover-deployment"
  Jan 10 16:21:30.031: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
  Jan 10 16:21:32.058: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
  Jan 10 16:21:32.071: INFO: Ensure that both replica sets have 1 created replica
  Jan 10 16:21:32.087: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
  Jan 10 16:21:32.113: INFO: Updating deployment test-rollover-deployment
  Jan 10 16:21:32.113: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
  Jan 10 16:21:34.146: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
  Jan 10 16:21:34.195: INFO: Make sure deployment "test-rollover-deployment" is complete
  Jan 10 16:21:34.220: INFO: all replica sets need to contain the pod-template-hash label
  Jan 10 16:21:34.222: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.January, 10, 16, 21, 30, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 10, 16, 21, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 10, 16, 21, 33, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 10, 16, 21, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jan 10 16:21:36.244: INFO: all replica sets need to contain the pod-template-hash label
  Jan 10 16:21:36.245: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.January, 10, 16, 21, 30, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 10, 16, 21, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 10, 16, 21, 33, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 10, 16, 21, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jan 10 16:21:38.243: INFO: all replica sets need to contain the pod-template-hash label
  Jan 10 16:21:38.244: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.January, 10, 16, 21, 30, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 10, 16, 21, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 10, 16, 21, 33, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 10, 16, 21, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jan 10 16:21:40.244: INFO: all replica sets need to contain the pod-template-hash label
  Jan 10 16:21:40.245: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.January, 10, 16, 21, 30, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 10, 16, 21, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 10, 16, 21, 33, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 10, 16, 21, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jan 10 16:21:42.252: INFO: all replica sets need to contain the pod-template-hash label
  Jan 10 16:21:42.253: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2024, time.January, 10, 16, 21, 30, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 10, 16, 21, 30, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2024, time.January, 10, 16, 21, 33, 0, time.Local), LastTransitionTime:time.Date(2024, time.January, 10, 16, 21, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jan 10 16:21:44.260: INFO: 
  Jan 10 16:21:44.260: INFO: Ensure that both old replica sets have no replicas
  Jan 10 16:21:44.296: INFO: Deployment "test-rollover-deployment":
  &Deployment{ObjectMeta:{test-rollover-deployment  deployment-7601  176a1914-4506-4edc-89d5-107652c7696f 186783981 2 2024-01-10 16:21:30 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2024-01-10 16:21:32 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-10 16:21:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00053a5d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2024-01-10 16:21:30 +0000 UTC,LastTransitionTime:2024-01-10 16:21:30 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-57777854c9" has successfully progressed.,LastUpdateTime:2024-01-10 16:21:44 +0000 UTC,LastTransitionTime:2024-01-10 16:21:30 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jan 10 16:21:44.306: INFO: New ReplicaSet "test-rollover-deployment-57777854c9" of Deployment "test-rollover-deployment":
  &ReplicaSet{ObjectMeta:{test-rollover-deployment-57777854c9  deployment-7601  c7293a00-c544-4632-b409-b71147010f30 186783969 2 2024-01-10 16:21:32 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 176a1914-4506-4edc-89d5-107652c7696f 0xc004c72377 0xc004c72378}] [] [{kube-controller-manager Update apps/v1 2024-01-10 16:21:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"176a1914-4506-4edc-89d5-107652c7696f\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-10 16:21:43 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 57777854c9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c72428 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jan 10 16:21:44.307: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
  Jan 10 16:21:44.307: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-7601  742af660-19d8-4fcd-9dc2-b4a4fda0f588 186783979 2 2024-01-10 16:21:22 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 176a1914-4506-4edc-89d5-107652c7696f 0xc004c72247 0xc004c72248}] [] [{e2e.test Update apps/v1 2024-01-10 16:21:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-10 16:21:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"176a1914-4506-4edc-89d5-107652c7696f\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2024-01-10 16:21:44 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004c72308 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jan 10 16:21:44.307: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-58779b56b4  deployment-7601  eb47f2ff-7a31-4a62-8263-1a8b58cd020c 186783909 2 2024-01-10 16:21:30 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 176a1914-4506-4edc-89d5-107652c7696f 0xc004c72497 0xc004c72498}] [] [{kube-controller-manager Update apps/v1 2024-01-10 16:21:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"176a1914-4506-4edc-89d5-107652c7696f\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-10 16:21:32 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 58779b56b4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c72548 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jan 10 16:21:44.323: INFO: Pod "test-rollover-deployment-57777854c9-4r62s" is available:
  &Pod{ObjectMeta:{test-rollover-deployment-57777854c9-4r62s test-rollover-deployment-57777854c9- deployment-7601  2d6c66a7-c7b3-448d-92d9-a92466610876 186783924 0 2024-01-10 16:21:32 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [{apps/v1 ReplicaSet test-rollover-deployment-57777854c9 c7293a00-c544-4632-b409-b71147010f30 0xc00053a9b7 0xc00053a9b8}] [] [{kube-controller-manager Update v1 2024-01-10 16:21:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c7293a00-c544-4632-b409-b71147010f30\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-10 16:21:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.18\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7dmmb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7dmmb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:21:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:21:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:21:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:21:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.201,PodIP:10.233.68.18,StartTime:2024-01-10 16:21:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-10 16:21:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://6e0a7e72e82e5c3401fff95caa4f466fe070bc752e90505d90830656dff58968,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.18,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 10 16:21:44.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-7601" for this suite. @ 01/10/24 16:21:44.351
• [21.465 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]
test/e2e/apps/statefulset.go:316
  STEP: Creating a kubernetes client @ 01/10/24 16:21:44.372
  Jan 10 16:21:44.372: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename statefulset @ 01/10/24 16:21:44.373
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:21:44.464
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:21:44.474
  STEP: Creating service test in namespace statefulset-3333 @ 01/10/24 16:21:44.492
  STEP: Creating a new StatefulSet @ 01/10/24 16:21:44.512
  Jan 10 16:21:44.546: INFO: Found 0 stateful pods, waiting for 3
  Jan 10 16:21:54.562: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Jan 10 16:21:54.562: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Jan 10 16:21:54.563: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  Jan 10 16:21:54.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=statefulset-3333 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jan 10 16:21:54.899: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jan 10 16:21:54.899: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jan 10 16:21:54.899: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 01/10/24 16:22:04.94
  Jan 10 16:22:04.973: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 01/10/24 16:22:04.973
  STEP: Updating Pods in reverse ordinal order @ 01/10/24 16:22:15.024
  Jan 10 16:22:15.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=statefulset-3333 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jan 10 16:22:15.365: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jan 10 16:22:15.365: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jan 10 16:22:15.365: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  STEP: Rolling back to a previous revision @ 01/10/24 16:22:25.418
  Jan 10 16:22:25.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=statefulset-3333 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jan 10 16:22:25.732: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jan 10 16:22:25.732: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jan 10 16:22:25.732: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jan 10 16:22:35.818: INFO: Updating stateful set ss2
  STEP: Rolling back update in reverse ordinal order @ 01/10/24 16:22:45.861
  Jan 10 16:22:45.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=statefulset-3333 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jan 10 16:22:46.183: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jan 10 16:22:46.183: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jan 10 16:22:46.183: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jan 10 16:22:56.246: INFO: Deleting all statefulset in ns statefulset-3333
  Jan 10 16:22:56.253: INFO: Scaling statefulset ss2 to 0
  Jan 10 16:23:06.293: INFO: Waiting for statefulset status.replicas updated to 0
  Jan 10 16:23:06.302: INFO: Deleting statefulset ss2
  Jan 10 16:23:06.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-3333" for this suite. @ 01/10/24 16:23:06.42
• [82.068 seconds]
------------------------------
SSS
------------------------------
[sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]
test/e2e/network/service.go:1493
  STEP: Creating a kubernetes client @ 01/10/24 16:23:06.441
  Jan 10 16:23:06.441: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename services @ 01/10/24 16:23:06.443
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:23:06.484
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:23:06.494
  STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-2497 @ 01/10/24 16:23:06.501
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 01/10/24 16:23:06.537
  STEP: creating service externalsvc in namespace services-2497 @ 01/10/24 16:23:06.538
  STEP: creating replication controller externalsvc in namespace services-2497 @ 01/10/24 16:23:06.578
  I0110 16:23:06.612168      23 runners.go:194] Created replication controller with name: externalsvc, namespace: services-2497, replica count: 2
  I0110 16:23:09.663731      23 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the ClusterIP service to type=ExternalName @ 01/10/24 16:23:09.672
  Jan 10 16:23:09.703: INFO: Creating new exec pod
  Jan 10 16:23:11.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=services-2497 exec execpod7bg6p -- /bin/sh -x -c nslookup clusterip-service.services-2497.svc.cluster.local'
  Jan 10 16:23:12.158: INFO: stderr: "+ nslookup clusterip-service.services-2497.svc.cluster.local\n"
  Jan 10 16:23:12.158: INFO: stdout: "Server:\t\t169.254.25.10\nAddress:\t169.254.25.10#53\n\nclusterip-service.services-2497.svc.cluster.local\tcanonical name = externalsvc.services-2497.svc.cluster.local.\nName:\texternalsvc.services-2497.svc.cluster.local\nAddress: 10.233.60.164\n\n"
  Jan 10 16:23:12.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-2497, will wait for the garbage collector to delete the pods @ 01/10/24 16:23:12.177
  Jan 10 16:23:12.253: INFO: Deleting ReplicationController externalsvc took: 18.889046ms
  Jan 10 16:23:12.354: INFO: Terminating ReplicationController externalsvc pods took: 101.106171ms
  Jan 10 16:23:14.715: INFO: Cleaning up the ClusterIP to ExternalName test service
  STEP: Destroying namespace "services-2497" for this suite. @ 01/10/24 16:23:14.779
• [8.363 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]
test/e2e/apimachinery/webhook.go:209
  STEP: Creating a kubernetes client @ 01/10/24 16:23:14.808
  Jan 10 16:23:14.808: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename webhook @ 01/10/24 16:23:14.81
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:23:14.846
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:23:14.855
  STEP: Setting up server cert @ 01/10/24 16:23:14.911
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 01/10/24 16:23:16.329
  STEP: Deploying the webhook pod @ 01/10/24 16:23:16.353
  STEP: Wait for the deployment to be ready @ 01/10/24 16:23:16.381
  Jan 10 16:23:16.414: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 01/10/24 16:23:18.445
  STEP: Verifying the service has paired with the endpoint @ 01/10/24 16:23:18.506
  Jan 10 16:23:19.507: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 01/10/24 16:23:19.516
  STEP: create a pod @ 01/10/24 16:23:19.554
  STEP: 'kubectl attach' the pod, should be denied by the webhook @ 01/10/24 16:23:21.604
  Jan 10 16:23:21.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=webhook-9858 attach --namespace=webhook-9858 to-be-attached-pod -i -c=container1'
  Jan 10 16:23:21.840: INFO: rc: 1
  Jan 10 16:23:21.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9858" for this suite. @ 01/10/24 16:23:22.026
  STEP: Destroying namespace "webhook-markers-8963" for this suite. @ 01/10/24 16:23:22.049
• [7.266 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Job should manage the lifecycle of a job [Conformance]
test/e2e/apps/job.go:713
  STEP: Creating a kubernetes client @ 01/10/24 16:23:22.089
  Jan 10 16:23:22.089: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename job @ 01/10/24 16:23:22.092
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:23:22.136
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:23:22.147
  STEP: Creating a suspended job @ 01/10/24 16:23:22.17
  STEP: Patching the Job @ 01/10/24 16:23:22.189
  STEP: Watching for Job to be patched @ 01/10/24 16:23:22.213
  Jan 10 16:23:22.218: INFO: Event ADDED observed for Job e2e-ckpgm in namespace job-8704 with labels: map[e2e-job-label:e2e-ckpgm] and annotations: map[batch.kubernetes.io/job-tracking:]
  Jan 10 16:23:22.219: INFO: Event MODIFIED found for Job e2e-ckpgm in namespace job-8704 with labels: map[e2e-ckpgm:patched e2e-job-label:e2e-ckpgm] and annotations: map[batch.kubernetes.io/job-tracking:]
  STEP: Updating the job @ 01/10/24 16:23:22.219
  STEP: Watching for Job to be updated @ 01/10/24 16:23:22.284
  Jan 10 16:23:22.294: INFO: Event MODIFIED found for Job e2e-ckpgm in namespace job-8704 with labels: map[e2e-ckpgm:patched e2e-job-label:e2e-ckpgm] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jan 10 16:23:22.296: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
  STEP: Listing all Jobs with LabelSelector @ 01/10/24 16:23:22.297
  Jan 10 16:23:22.308: INFO: Job: e2e-ckpgm as labels: map[e2e-ckpgm:patched e2e-job-label:e2e-ckpgm]
  STEP: Waiting for job to complete @ 01/10/24 16:23:22.308
  STEP: Delete a job collection with a labelselector @ 01/10/24 16:23:30.316
  STEP: Watching for Job to be deleted @ 01/10/24 16:23:30.336
  Jan 10 16:23:30.341: INFO: Event MODIFIED observed for Job e2e-ckpgm in namespace job-8704 with labels: map[e2e-ckpgm:patched e2e-job-label:e2e-ckpgm] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jan 10 16:23:30.341: INFO: Event MODIFIED observed for Job e2e-ckpgm in namespace job-8704 with labels: map[e2e-ckpgm:patched e2e-job-label:e2e-ckpgm] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jan 10 16:23:30.342: INFO: Event MODIFIED observed for Job e2e-ckpgm in namespace job-8704 with labels: map[e2e-ckpgm:patched e2e-job-label:e2e-ckpgm] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jan 10 16:23:30.342: INFO: Event MODIFIED observed for Job e2e-ckpgm in namespace job-8704 with labels: map[e2e-ckpgm:patched e2e-job-label:e2e-ckpgm] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jan 10 16:23:30.342: INFO: Event MODIFIED observed for Job e2e-ckpgm in namespace job-8704 with labels: map[e2e-ckpgm:patched e2e-job-label:e2e-ckpgm] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jan 10 16:23:30.342: INFO: Event DELETED found for Job e2e-ckpgm in namespace job-8704 with labels: map[e2e-ckpgm:patched e2e-job-label:e2e-ckpgm] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  STEP: Relist jobs to confirm deletion @ 01/10/24 16:23:30.343
  Jan 10 16:23:30.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-8704" for this suite. @ 01/10/24 16:23:30.379
• [8.356 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]
test/e2e/storage/subpath.go:106
  STEP: Creating a kubernetes client @ 01/10/24 16:23:30.454
  Jan 10 16:23:30.454: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename subpath @ 01/10/24 16:23:30.457
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:23:30.533
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:23:30.539
  STEP: Setting up data @ 01/10/24 16:23:30.545
  STEP: Creating pod pod-subpath-test-projected-wnjg @ 01/10/24 16:23:30.568
  STEP: Creating a pod to test atomic-volume-subpath @ 01/10/24 16:23:30.569
  STEP: Saw pod success @ 01/10/24 16:23:54.733
  Jan 10 16:23:54.740: INFO: Trying to get logs from node env1-test-worker-1 pod pod-subpath-test-projected-wnjg container test-container-subpath-projected-wnjg: <nil>
  STEP: delete the pod @ 01/10/24 16:23:54.778
  STEP: Deleting pod pod-subpath-test-projected-wnjg @ 01/10/24 16:23:54.809
  Jan 10 16:23:54.809: INFO: Deleting pod "pod-subpath-test-projected-wnjg" in namespace "subpath-8859"
  Jan 10 16:23:54.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-8859" for this suite. @ 01/10/24 16:23:54.829
• [24.387 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should delete a job [Conformance]
test/e2e/apps/job.go:485
  STEP: Creating a kubernetes client @ 01/10/24 16:23:54.843
  Jan 10 16:23:54.843: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename job @ 01/10/24 16:23:54.844
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:23:54.874
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:23:54.879
  STEP: Creating a job @ 01/10/24 16:23:54.885
  STEP: Ensuring active pods == parallelism @ 01/10/24 16:23:54.898
  STEP: delete a job @ 01/10/24 16:23:56.914
  STEP: deleting Job.batch foo in namespace job-9759, will wait for the garbage collector to delete the pods @ 01/10/24 16:23:56.914
  Jan 10 16:23:56.989: INFO: Deleting Job.batch foo took: 16.809461ms
  Jan 10 16:23:57.090: INFO: Terminating Job.batch foo pods took: 100.819445ms
  STEP: Ensuring job was deleted @ 01/10/24 16:24:30.091
  Jan 10 16:24:30.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-9759" for this suite. @ 01/10/24 16:24:30.113
• [35.286 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:107
  STEP: Creating a kubernetes client @ 01/10/24 16:24:30.13
  Jan 10 16:24:30.130: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename pod-network-test @ 01/10/24 16:24:30.132
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:24:30.177
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:24:30.183
  STEP: Performing setup for networking test in namespace pod-network-test-6787 @ 01/10/24 16:24:30.188
  STEP: creating a selector @ 01/10/24 16:24:30.188
  STEP: Creating the service pods in kubernetes @ 01/10/24 16:24:30.188
  Jan 10 16:24:30.188: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 01/10/24 16:24:52.413
  Jan 10 16:24:54.489: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
  Jan 10 16:24:54.489: INFO: Going to poll 10.233.67.50 on port 8083 at least 0 times, with a maximum of 34 tries before failing
  Jan 10 16:24:54.504: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.67.50:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6787 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 10 16:24:54.504: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  Jan 10 16:24:54.505: INFO: ExecWithOptions: Clientset creation
  Jan 10 16:24:54.505: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-6787/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.67.50%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jan 10 16:24:54.676: INFO: Found all 1 expected endpoints: [netserver-0]
  Jan 10 16:24:54.677: INFO: Going to poll 10.233.68.36 on port 8083 at least 0 times, with a maximum of 34 tries before failing
  Jan 10 16:24:54.686: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.68.36:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6787 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 10 16:24:54.687: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  Jan 10 16:24:54.688: INFO: ExecWithOptions: Clientset creation
  Jan 10 16:24:54.689: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-6787/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.68.36%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jan 10 16:24:54.834: INFO: Found all 1 expected endpoints: [netserver-1]
  Jan 10 16:24:54.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-6787" for this suite. @ 01/10/24 16:24:54.848
• [24.736 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:222
  STEP: Creating a kubernetes client @ 01/10/24 16:24:54.869
  Jan 10 16:24:54.869: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename downward-api @ 01/10/24 16:24:54.871
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:24:54.924
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:24:54.932
  STEP: Creating a pod to test downward API volume plugin @ 01/10/24 16:24:54.944
  STEP: Saw pod success @ 01/10/24 16:24:58.999
  Jan 10 16:24:59.012: INFO: Trying to get logs from node env1-test-worker-1 pod downwardapi-volume-8b35d1dc-c276-43e8-b41b-a0f0bded99a7 container client-container: <nil>
  STEP: delete the pod @ 01/10/24 16:24:59.029
  Jan 10 16:24:59.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8716" for this suite. @ 01/10/24 16:24:59.078
• [4.233 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:84
  STEP: Creating a kubernetes client @ 01/10/24 16:24:59.109
  Jan 10 16:24:59.109: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename pod-network-test @ 01/10/24 16:24:59.114
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:24:59.156
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:24:59.164
  STEP: Performing setup for networking test in namespace pod-network-test-6212 @ 01/10/24 16:24:59.169
  STEP: creating a selector @ 01/10/24 16:24:59.169
  STEP: Creating the service pods in kubernetes @ 01/10/24 16:24:59.169
  Jan 10 16:24:59.169: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 01/10/24 16:25:35.475
  Jan 10 16:25:37.529: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
  Jan 10 16:25:37.529: INFO: Breadth first check of 10.233.67.51 on host 10.61.1.200...
  Jan 10 16:25:37.536: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.68.40:9080/dial?request=hostname&protocol=http&host=10.233.67.51&port=8083&tries=1'] Namespace:pod-network-test-6212 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 10 16:25:37.536: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  Jan 10 16:25:37.539: INFO: ExecWithOptions: Clientset creation
  Jan 10 16:25:37.539: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-6212/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.68.40%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.67.51%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jan 10 16:25:37.707: INFO: Waiting for responses: map[]
  Jan 10 16:25:37.707: INFO: reached 10.233.67.51 after 0/1 tries
  Jan 10 16:25:37.707: INFO: Breadth first check of 10.233.68.39 on host 10.61.1.201...
  Jan 10 16:25:37.716: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.68.40:9080/dial?request=hostname&protocol=http&host=10.233.68.39&port=8083&tries=1'] Namespace:pod-network-test-6212 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 10 16:25:37.716: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  Jan 10 16:25:37.718: INFO: ExecWithOptions: Clientset creation
  Jan 10 16:25:37.718: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-6212/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.68.40%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.68.39%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jan 10 16:25:37.875: INFO: Waiting for responses: map[]
  Jan 10 16:25:37.876: INFO: reached 10.233.68.39 after 0/1 tries
  Jan 10 16:25:37.876: INFO: Going to retry 0 out of 2 pods....
  Jan 10 16:25:37.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-6212" for this suite. @ 01/10/24 16:25:37.892
• [38.805 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:47
  STEP: Creating a kubernetes client @ 01/10/24 16:25:37.918
  Jan 10 16:25:37.919: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename configmap @ 01/10/24 16:25:37.922
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:25:37.986
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:25:37.995
  STEP: Creating configMap with name configmap-test-volume-3ba36bcc-23b2-4ff7-807d-f8837beb4dea @ 01/10/24 16:25:38.005
  STEP: Creating a pod to test consume configMaps @ 01/10/24 16:25:38.019
  STEP: Saw pod success @ 01/10/24 16:25:42.085
  Jan 10 16:25:42.093: INFO: Trying to get logs from node env1-test-worker-1 pod pod-configmaps-d65c2e7c-bcc1-49b4-b317-8a4a395fade0 container agnhost-container: <nil>
  STEP: delete the pod @ 01/10/24 16:25:42.116
  Jan 10 16:25:42.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2611" for this suite. @ 01/10/24 16:25:42.182
• [4.282 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:117
  STEP: Creating a kubernetes client @ 01/10/24 16:25:42.204
  Jan 10 16:25:42.204: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename emptydir @ 01/10/24 16:25:42.207
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:25:42.271
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:25:42.284
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 01/10/24 16:25:42.291
  STEP: Saw pod success @ 01/10/24 16:25:46.349
  Jan 10 16:25:46.359: INFO: Trying to get logs from node env1-test-worker-1 pod pod-39e175aa-0867-412a-9100-08c2004511d3 container test-container: <nil>
  STEP: delete the pod @ 01/10/24 16:25:46.382
  Jan 10 16:25:46.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1479" for this suite. @ 01/10/24 16:25:46.455
• [4.281 seconds]
------------------------------
SS
------------------------------
[sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:528
  STEP: Creating a kubernetes client @ 01/10/24 16:25:46.486
  Jan 10 16:25:46.486: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename security-context-test @ 01/10/24 16:25:46.489
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:25:46.528
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:25:46.536
  Jan 10 16:25:50.671: INFO: Got logs for pod "busybox-privileged-false-7dc9fbba-0065-4262-b111-3b05481574c4": "ip: RTNETLINK answers: Operation not permitted\n"
  Jan 10 16:25:50.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-6834" for this suite. @ 01/10/24 16:25:50.685
• [4.215 seconds]
------------------------------
[sig-node] PodTemplates should delete a collection of pod templates [Conformance]
test/e2e/common/node/podtemplates.go:122
  STEP: Creating a kubernetes client @ 01/10/24 16:25:50.702
  Jan 10 16:25:50.702: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename podtemplate @ 01/10/24 16:25:50.704
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:25:50.742
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:25:50.749
  STEP: Create set of pod templates @ 01/10/24 16:25:50.756
  Jan 10 16:25:50.770: INFO: created test-podtemplate-1
  Jan 10 16:25:50.784: INFO: created test-podtemplate-2
  Jan 10 16:25:50.795: INFO: created test-podtemplate-3
  STEP: get a list of pod templates with a label in the current namespace @ 01/10/24 16:25:50.795
  STEP: delete collection of pod templates @ 01/10/24 16:25:50.806
  Jan 10 16:25:50.806: INFO: requesting DeleteCollection of pod templates
  STEP: check that the list of pod templates matches the requested quantity @ 01/10/24 16:25:50.841
  Jan 10 16:25:50.841: INFO: requesting list of pod templates to confirm quantity
  Jan 10 16:25:50.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-7887" for this suite. @ 01/10/24 16:25:50.864
• [0.188 seconds]
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]
test/e2e/kubectl/kubectl.go:1315
  STEP: Creating a kubernetes client @ 01/10/24 16:25:50.894
  Jan 10 16:25:50.894: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename kubectl @ 01/10/24 16:25:50.898
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:25:50.942
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:25:50.948
  STEP: validating cluster-info @ 01/10/24 16:25:50.955
  Jan 10 16:25:50.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-7329 cluster-info'
  Jan 10 16:25:51.144: INFO: stderr: ""
  Jan 10 16:25:51.144: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
  Jan 10 16:25:51.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7329" for this suite. @ 01/10/24 16:25:51.159
• [0.283 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]
test/e2e/common/node/runtimeclass.go:189
  STEP: Creating a kubernetes client @ 01/10/24 16:25:51.179
  Jan 10 16:25:51.179: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename runtimeclass @ 01/10/24 16:25:51.181
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:25:51.272
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:25:51.278
  STEP: getting /apis @ 01/10/24 16:25:51.285
  STEP: getting /apis/node.k8s.io @ 01/10/24 16:25:51.3
  STEP: getting /apis/node.k8s.io/v1 @ 01/10/24 16:25:51.304
  STEP: creating @ 01/10/24 16:25:51.308
  STEP: watching @ 01/10/24 16:25:51.378
  Jan 10 16:25:51.378: INFO: starting watch
  STEP: getting @ 01/10/24 16:25:51.393
  STEP: listing @ 01/10/24 16:25:51.404
  STEP: patching @ 01/10/24 16:25:51.412
  STEP: updating @ 01/10/24 16:25:51.426
  Jan 10 16:25:51.441: INFO: waiting for watch events with expected annotations
  STEP: deleting @ 01/10/24 16:25:51.443
  STEP: deleting a collection @ 01/10/24 16:25:51.473
  Jan 10 16:25:51.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-8011" for this suite. @ 01/10/24 16:25:51.547
• [0.391 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]
test/e2e/apimachinery/table_conversion.go:154
  STEP: Creating a kubernetes client @ 01/10/24 16:25:51.58
  Jan 10 16:25:51.580: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename tables @ 01/10/24 16:25:51.583
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:25:51.63
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:25:51.637
  Jan 10 16:25:51.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "tables-7684" for this suite. @ 01/10/24 16:25:51.661
• [0.095 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:243
  STEP: Creating a kubernetes client @ 01/10/24 16:25:51.681
  Jan 10 16:25:51.681: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename namespaces @ 01/10/24 16:25:51.683
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:25:51.732
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:25:51.737
  STEP: Creating a test namespace @ 01/10/24 16:25:51.742
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:25:51.833
  STEP: Creating a pod in the namespace @ 01/10/24 16:25:51.841
  STEP: Waiting for the pod to have running status @ 01/10/24 16:25:51.877
  STEP: Deleting the namespace @ 01/10/24 16:25:53.903
  STEP: Waiting for the namespace to be removed. @ 01/10/24 16:25:53.931
  STEP: Recreating the namespace @ 01/10/24 16:26:06.95
  STEP: Verifying there are no pods in the namespace @ 01/10/24 16:26:06.995
  Jan 10 16:26:07.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-9163" for this suite. @ 01/10/24 16:26:07.02
  STEP: Destroying namespace "nsdeletetest-280" for this suite. @ 01/10/24 16:26:07.033
  Jan 10 16:26:07.043: INFO: Namespace nsdeletetest-280 was already deleted
  STEP: Destroying namespace "nsdeletetest-6519" for this suite. @ 01/10/24 16:26:07.043
• [15.386 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
test/e2e/apimachinery/garbage_collector.go:713
  STEP: Creating a kubernetes client @ 01/10/24 16:26:07.074
  Jan 10 16:26:07.074: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename gc @ 01/10/24 16:26:07.076
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:26:07.119
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:26:07.127
  STEP: create the rc1 @ 01/10/24 16:26:07.146
  STEP: create the rc2 @ 01/10/24 16:26:07.167
  STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well @ 01/10/24 16:26:13.196
  STEP: delete the rc simpletest-rc-to-be-deleted @ 01/10/24 16:26:14.428
  STEP: wait for the rc to be deleted @ 01/10/24 16:26:14.449
  Jan 10 16:26:19.478: INFO: 66 pods remaining
  Jan 10 16:26:19.478: INFO: 65 pods has nil DeletionTimestamp
  Jan 10 16:26:19.478: INFO: 
  STEP: Gathering metrics @ 01/10/24 16:26:24.503
  Jan 10 16:26:24.805: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jan 10 16:26:24.805: INFO: Deleting pod "simpletest-rc-to-be-deleted-29dxv" in namespace "gc-1782"
  Jan 10 16:26:24.851: INFO: Deleting pod "simpletest-rc-to-be-deleted-2q2sb" in namespace "gc-1782"
  Jan 10 16:26:24.883: INFO: Deleting pod "simpletest-rc-to-be-deleted-46xkq" in namespace "gc-1782"
  Jan 10 16:26:24.933: INFO: Deleting pod "simpletest-rc-to-be-deleted-49b72" in namespace "gc-1782"
  Jan 10 16:26:24.962: INFO: Deleting pod "simpletest-rc-to-be-deleted-4c2k7" in namespace "gc-1782"
  Jan 10 16:26:24.993: INFO: Deleting pod "simpletest-rc-to-be-deleted-4hcxl" in namespace "gc-1782"
  Jan 10 16:26:25.021: INFO: Deleting pod "simpletest-rc-to-be-deleted-4l5wp" in namespace "gc-1782"
  Jan 10 16:26:25.059: INFO: Deleting pod "simpletest-rc-to-be-deleted-52c26" in namespace "gc-1782"
  Jan 10 16:26:25.095: INFO: Deleting pod "simpletest-rc-to-be-deleted-54vfz" in namespace "gc-1782"
  Jan 10 16:26:25.122: INFO: Deleting pod "simpletest-rc-to-be-deleted-599dc" in namespace "gc-1782"
  Jan 10 16:26:25.170: INFO: Deleting pod "simpletest-rc-to-be-deleted-5j9md" in namespace "gc-1782"
  Jan 10 16:26:25.211: INFO: Deleting pod "simpletest-rc-to-be-deleted-5l4h5" in namespace "gc-1782"
  Jan 10 16:26:25.246: INFO: Deleting pod "simpletest-rc-to-be-deleted-5nkcz" in namespace "gc-1782"
  Jan 10 16:26:25.283: INFO: Deleting pod "simpletest-rc-to-be-deleted-5p84x" in namespace "gc-1782"
  Jan 10 16:26:25.350: INFO: Deleting pod "simpletest-rc-to-be-deleted-6nhrm" in namespace "gc-1782"
  Jan 10 16:26:25.402: INFO: Deleting pod "simpletest-rc-to-be-deleted-6s2xl" in namespace "gc-1782"
  Jan 10 16:26:25.475: INFO: Deleting pod "simpletest-rc-to-be-deleted-6scww" in namespace "gc-1782"
  Jan 10 16:26:25.519: INFO: Deleting pod "simpletest-rc-to-be-deleted-6wxzv" in namespace "gc-1782"
  Jan 10 16:26:25.569: INFO: Deleting pod "simpletest-rc-to-be-deleted-76rzs" in namespace "gc-1782"
  Jan 10 16:26:25.617: INFO: Deleting pod "simpletest-rc-to-be-deleted-7c7gr" in namespace "gc-1782"
  Jan 10 16:26:25.673: INFO: Deleting pod "simpletest-rc-to-be-deleted-7m5t2" in namespace "gc-1782"
  Jan 10 16:26:25.711: INFO: Deleting pod "simpletest-rc-to-be-deleted-7xpfc" in namespace "gc-1782"
  Jan 10 16:26:25.750: INFO: Deleting pod "simpletest-rc-to-be-deleted-876xt" in namespace "gc-1782"
  Jan 10 16:26:25.795: INFO: Deleting pod "simpletest-rc-to-be-deleted-9kbqb" in namespace "gc-1782"
  Jan 10 16:26:25.865: INFO: Deleting pod "simpletest-rc-to-be-deleted-9lrg2" in namespace "gc-1782"
  Jan 10 16:26:25.908: INFO: Deleting pod "simpletest-rc-to-be-deleted-9n5wh" in namespace "gc-1782"
  Jan 10 16:26:25.958: INFO: Deleting pod "simpletest-rc-to-be-deleted-9xtlp" in namespace "gc-1782"
  Jan 10 16:26:25.999: INFO: Deleting pod "simpletest-rc-to-be-deleted-b2w96" in namespace "gc-1782"
  Jan 10 16:26:26.035: INFO: Deleting pod "simpletest-rc-to-be-deleted-b457m" in namespace "gc-1782"
  Jan 10 16:26:26.084: INFO: Deleting pod "simpletest-rc-to-be-deleted-c79qv" in namespace "gc-1782"
  Jan 10 16:26:26.124: INFO: Deleting pod "simpletest-rc-to-be-deleted-ccgtg" in namespace "gc-1782"
  Jan 10 16:26:26.156: INFO: Deleting pod "simpletest-rc-to-be-deleted-cd2kj" in namespace "gc-1782"
  Jan 10 16:26:26.203: INFO: Deleting pod "simpletest-rc-to-be-deleted-cjpxc" in namespace "gc-1782"
  Jan 10 16:26:26.248: INFO: Deleting pod "simpletest-rc-to-be-deleted-ctnvr" in namespace "gc-1782"
  Jan 10 16:26:26.285: INFO: Deleting pod "simpletest-rc-to-be-deleted-djxr6" in namespace "gc-1782"
  Jan 10 16:26:26.331: INFO: Deleting pod "simpletest-rc-to-be-deleted-dldz5" in namespace "gc-1782"
  Jan 10 16:26:26.364: INFO: Deleting pod "simpletest-rc-to-be-deleted-flmwh" in namespace "gc-1782"
  Jan 10 16:26:26.411: INFO: Deleting pod "simpletest-rc-to-be-deleted-g576l" in namespace "gc-1782"
  Jan 10 16:26:26.457: INFO: Deleting pod "simpletest-rc-to-be-deleted-g77jk" in namespace "gc-1782"
  Jan 10 16:26:26.503: INFO: Deleting pod "simpletest-rc-to-be-deleted-gfzdw" in namespace "gc-1782"
  Jan 10 16:26:26.532: INFO: Deleting pod "simpletest-rc-to-be-deleted-ggcx5" in namespace "gc-1782"
  Jan 10 16:26:26.572: INFO: Deleting pod "simpletest-rc-to-be-deleted-h86sj" in namespace "gc-1782"
  Jan 10 16:26:26.601: INFO: Deleting pod "simpletest-rc-to-be-deleted-hch5t" in namespace "gc-1782"
  Jan 10 16:26:26.647: INFO: Deleting pod "simpletest-rc-to-be-deleted-hdjls" in namespace "gc-1782"
  Jan 10 16:26:26.688: INFO: Deleting pod "simpletest-rc-to-be-deleted-j6d7p" in namespace "gc-1782"
  Jan 10 16:26:26.756: INFO: Deleting pod "simpletest-rc-to-be-deleted-k5bkm" in namespace "gc-1782"
  Jan 10 16:26:26.814: INFO: Deleting pod "simpletest-rc-to-be-deleted-kbpx9" in namespace "gc-1782"
  Jan 10 16:26:26.857: INFO: Deleting pod "simpletest-rc-to-be-deleted-kn6qf" in namespace "gc-1782"
  Jan 10 16:26:26.927: INFO: Deleting pod "simpletest-rc-to-be-deleted-kn85n" in namespace "gc-1782"
  Jan 10 16:26:26.957: INFO: Deleting pod "simpletest-rc-to-be-deleted-kqgs6" in namespace "gc-1782"
  Jan 10 16:26:27.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-1782" for this suite. @ 01/10/24 16:26:27.051
• [20.006 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]
test/e2e/apps/rc.go:85
  STEP: Creating a kubernetes client @ 01/10/24 16:26:27.081
  Jan 10 16:26:27.081: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename replication-controller @ 01/10/24 16:26:27.083
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:26:27.133
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:26:27.139
  Jan 10 16:26:27.149: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
  STEP: Creating rc "condition-test" that asks for more than the allowed pod quota @ 01/10/24 16:26:28.191
  STEP: Checking rc "condition-test" has the desired failure condition set @ 01/10/24 16:26:28.209
  STEP: Scaling down rc "condition-test" to satisfy pod quota @ 01/10/24 16:26:29.24
  Jan 10 16:26:29.275: INFO: Updating replication controller "condition-test"
  STEP: Checking rc "condition-test" has no failure condition set @ 01/10/24 16:26:29.276
  Jan 10 16:26:30.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-9145" for this suite. @ 01/10/24 16:26:30.311
• [3.253 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:147
  STEP: Creating a kubernetes client @ 01/10/24 16:26:30.336
  Jan 10 16:26:30.337: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename emptydir @ 01/10/24 16:26:30.338
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:26:30.394
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:26:30.407
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 01/10/24 16:26:30.414
  STEP: Saw pod success @ 01/10/24 16:26:34.536
  Jan 10 16:26:34.550: INFO: Trying to get logs from node env1-test-worker-1 pod pod-db98a203-bd0a-45f7-9d05-b10f7c2028a1 container test-container: <nil>
  STEP: delete the pod @ 01/10/24 16:26:34.57
  Jan 10 16:26:34.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-480" for this suite. @ 01/10/24 16:26:34.622
• [4.310 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]
test/e2e/apimachinery/resource_quota.go:451
  STEP: Creating a kubernetes client @ 01/10/24 16:26:34.647
  Jan 10 16:26:34.647: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename resourcequota @ 01/10/24 16:26:34.648
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:26:34.711
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:26:34.718
  STEP: Counting existing ResourceQuota @ 01/10/24 16:26:34.725
  STEP: Creating a ResourceQuota @ 01/10/24 16:26:39.734
  STEP: Ensuring resource quota status is calculated @ 01/10/24 16:26:39.748
  STEP: Creating a ReplicaSet @ 01/10/24 16:26:41.759
  STEP: Ensuring resource quota status captures replicaset creation @ 01/10/24 16:26:41.791
  STEP: Deleting a ReplicaSet @ 01/10/24 16:26:43.801
  STEP: Ensuring resource quota status released usage @ 01/10/24 16:26:43.815
  Jan 10 16:26:45.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-123" for this suite. @ 01/10/24 16:26:45.84
• [11.209 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:187
  STEP: Creating a kubernetes client @ 01/10/24 16:26:45.86
  Jan 10 16:26:45.860: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename emptydir @ 01/10/24 16:26:45.863
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:26:45.905
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:26:45.912
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 01/10/24 16:26:45.918
  STEP: Saw pod success @ 01/10/24 16:26:49.98
  Jan 10 16:26:49.988: INFO: Trying to get logs from node env1-test-worker-1 pod pod-1622ea73-37fc-4f82-8f82-3cc131d78532 container test-container: <nil>
  STEP: delete the pod @ 01/10/24 16:26:50.004
  Jan 10 16:26:50.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-977" for this suite. @ 01/10/24 16:26:50.053
• [4.207 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:222
  STEP: Creating a kubernetes client @ 01/10/24 16:26:50.067
  Jan 10 16:26:50.068: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename projected @ 01/10/24 16:26:50.069
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:26:50.116
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:26:50.122
  STEP: Creating a pod to test downward API volume plugin @ 01/10/24 16:26:50.129
  STEP: Saw pod success @ 01/10/24 16:26:54.207
  Jan 10 16:26:54.221: INFO: Trying to get logs from node env1-test-worker-1 pod downwardapi-volume-ceacee69-1fd6-498a-a9c6-ed61166d1ac8 container client-container: <nil>
  STEP: delete the pod @ 01/10/24 16:26:54.246
  Jan 10 16:26:54.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8771" for this suite. @ 01/10/24 16:26:54.362
• [4.329 seconds]
------------------------------
SS
------------------------------
[sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:227
  STEP: Creating a kubernetes client @ 01/10/24 16:26:54.398
  Jan 10 16:26:54.398: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename pods @ 01/10/24 16:26:54.4
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:26:54.455
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:26:54.472
  STEP: creating the pod @ 01/10/24 16:26:54.48
  STEP: setting up watch @ 01/10/24 16:26:54.48
  STEP: submitting the pod to kubernetes @ 01/10/24 16:26:54.593
  STEP: verifying the pod is in kubernetes @ 01/10/24 16:26:54.654
  STEP: verifying pod creation was observed @ 01/10/24 16:26:54.665
  STEP: deleting the pod gracefully @ 01/10/24 16:26:56.717
  STEP: verifying pod deletion was observed @ 01/10/24 16:26:56.747
  Jan 10 16:26:58.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-8174" for this suite. @ 01/10/24 16:26:58.412
• [4.035 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should patch a secret [Conformance]
test/e2e/common/node/secrets.go:154
  STEP: Creating a kubernetes client @ 01/10/24 16:26:58.446
  Jan 10 16:26:58.447: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename secrets @ 01/10/24 16:26:58.449
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:26:58.499
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:26:58.508
  STEP: creating a secret @ 01/10/24 16:26:58.515
  STEP: listing secrets in all namespaces to ensure that there are more than zero @ 01/10/24 16:26:58.527
  STEP: patching the secret @ 01/10/24 16:26:58.601
  STEP: deleting the secret using a LabelSelector @ 01/10/24 16:26:58.622
  STEP: listing secrets in all namespaces, searching for label name and value in patch @ 01/10/24 16:26:58.659
  Jan 10 16:26:58.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-1342" for this suite. @ 01/10/24 16:26:58.745
• [0.331 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events should manage the lifecycle of an event [Conformance]
test/e2e/instrumentation/core_events.go:57
  STEP: Creating a kubernetes client @ 01/10/24 16:26:58.791
  Jan 10 16:26:58.791: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename events @ 01/10/24 16:26:58.794
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:26:58.896
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:26:58.902
  STEP: creating a test event @ 01/10/24 16:26:58.91
  STEP: listing all events in all namespaces @ 01/10/24 16:26:58.919
  STEP: patching the test event @ 01/10/24 16:26:58.933
  STEP: fetching the test event @ 01/10/24 16:26:58.959
  STEP: updating the test event @ 01/10/24 16:26:58.968
  STEP: getting the test event @ 01/10/24 16:26:58.991
  STEP: deleting the test event @ 01/10/24 16:26:59.001
  STEP: listing all events in all namespaces @ 01/10/24 16:26:59.018
  Jan 10 16:26:59.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-2025" for this suite. @ 01/10/24 16:26:59.045
• [0.266 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should find a service from listing all namespaces [Conformance]
test/e2e/network/service.go:3113
  STEP: Creating a kubernetes client @ 01/10/24 16:26:59.058
  Jan 10 16:26:59.058: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename services @ 01/10/24 16:26:59.06
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:26:59.101
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:26:59.111
  STEP: fetching services @ 01/10/24 16:26:59.12
  Jan 10 16:26:59.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-7547" for this suite. @ 01/10/24 16:26:59.142
• [0.103 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]
test/e2e/apimachinery/webhook.go:249
  STEP: Creating a kubernetes client @ 01/10/24 16:26:59.162
  Jan 10 16:26:59.162: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename webhook @ 01/10/24 16:26:59.164
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:26:59.208
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:26:59.214
  STEP: Setting up server cert @ 01/10/24 16:26:59.274
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 01/10/24 16:27:00.839
  STEP: Deploying the webhook pod @ 01/10/24 16:27:00.86
  STEP: Wait for the deployment to be ready @ 01/10/24 16:27:00.888
  Jan 10 16:27:00.919: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 01/10/24 16:27:02.949
  STEP: Verifying the service has paired with the endpoint @ 01/10/24 16:27:02.99
  Jan 10 16:27:03.991: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating configmap webhook via the AdmissionRegistration API @ 01/10/24 16:27:04.009
  STEP: create a configmap that should be updated by the webhook @ 01/10/24 16:27:04.053
  Jan 10 16:27:04.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1683" for this suite. @ 01/10/24 16:27:04.466
  STEP: Destroying namespace "webhook-markers-5800" for this suite. @ 01/10/24 16:27:04.534
• [5.426 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]
test/e2e/apps/rc.go:112
  STEP: Creating a kubernetes client @ 01/10/24 16:27:04.602
  Jan 10 16:27:04.602: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename replication-controller @ 01/10/24 16:27:04.606
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:27:04.893
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:27:04.899
  STEP: creating a ReplicationController @ 01/10/24 16:27:04.918
  STEP: waiting for RC to be added @ 01/10/24 16:27:04.931
  STEP: waiting for available Replicas @ 01/10/24 16:27:04.931
  STEP: patching ReplicationController @ 01/10/24 16:27:06.434
  STEP: waiting for RC to be modified @ 01/10/24 16:27:06.457
  STEP: patching ReplicationController status @ 01/10/24 16:27:06.458
  STEP: waiting for RC to be modified @ 01/10/24 16:27:06.473
  STEP: waiting for available Replicas @ 01/10/24 16:27:06.476
  STEP: fetching ReplicationController status @ 01/10/24 16:27:06.487
  STEP: patching ReplicationController scale @ 01/10/24 16:27:06.497
  STEP: waiting for RC to be modified @ 01/10/24 16:27:06.509
  STEP: waiting for ReplicationController's scale to be the max amount @ 01/10/24 16:27:06.509
  STEP: fetching ReplicationController; ensuring that it's patched @ 01/10/24 16:27:07.932
  STEP: updating ReplicationController status @ 01/10/24 16:27:07.944
  STEP: waiting for RC to be modified @ 01/10/24 16:27:07.963
  STEP: listing all ReplicationControllers @ 01/10/24 16:27:07.964
  STEP: checking that ReplicationController has expected values @ 01/10/24 16:27:07.976
  STEP: deleting ReplicationControllers by collection @ 01/10/24 16:27:07.976
  STEP: waiting for ReplicationController to have a DELETED watchEvent @ 01/10/24 16:27:08.004
  Jan 10 16:27:08.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0110 16:27:08.118896      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "replication-controller-3493" for this suite. @ 01/10/24 16:27:08.138
• [3.556 seconds]
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:207
  STEP: Creating a kubernetes client @ 01/10/24 16:27:08.161
  Jan 10 16:27:08.161: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename emptydir @ 01/10/24 16:27:08.164
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:27:08.202
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:27:08.209
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 01/10/24 16:27:08.214
  E0110 16:27:09.119243      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:27:10.119643      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:27:11.119843      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:27:12.120217      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/10/24 16:27:12.271
  Jan 10 16:27:12.280: INFO: Trying to get logs from node env1-test-worker-1 pod pod-6d5f0027-7493-4015-a125-e2c3165745b7 container test-container: <nil>
  STEP: delete the pod @ 01/10/24 16:27:12.298
  Jan 10 16:27:12.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8310" for this suite. @ 01/10/24 16:27:12.352
• [4.217 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]
test/e2e/apps/cronjob.go:97
  STEP: Creating a kubernetes client @ 01/10/24 16:27:12.38
  Jan 10 16:27:12.380: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename cronjob @ 01/10/24 16:27:12.382
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:27:12.418
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:27:12.425
  STEP: Creating a suspended cronjob @ 01/10/24 16:27:12.457
  STEP: Ensuring no jobs are scheduled @ 01/10/24 16:27:12.483
  E0110 16:27:13.121075      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:27:14.121756      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:27:15.122689      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:27:16.122971      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:27:17.123348      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:27:18.124633      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:27:19.124921      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:27:20.125983      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:27:21.126228      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:27:22.126230      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:27:23.127332      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:27:24.127650      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:27:25.127784      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:27:26.128174      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:27:27.128397      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:27:28.129374      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:27:29.130440      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:27:30.130547      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:27:31.130816      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:27:32.131468      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:27:33.131890      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:27:34.132528      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:27:35.132605      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:27:36.132896      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:27:37.133256      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:27:38.133936      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:27:39.134416      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:27:40.134531      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:27:41.134701      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:27:42.135206      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:27:43.135599      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:27:44.135789      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:27:45.136062      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:27:46.136271      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:27:47.136609      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:27:48.136724      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:27:49.137693      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:27:50.138076      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:27:51.138417      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:27:52.138861      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:27:53.139992      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:27:54.140231      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:27:55.140291      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:27:56.140459      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:27:57.140870      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:27:58.141289      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:27:59.141656      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:00.141821      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:01.142121      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:02.142709      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:03.143333      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:04.143999      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:05.144521      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:06.145119      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:07.146036      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:08.146269      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:09.146720      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:10.146849      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:11.147142      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:12.148489      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:13.148472      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:14.149592      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:15.149854      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:16.150097      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:17.150806      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:18.151152      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:19.151107      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:20.151287      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:21.151625      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:22.151891      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:23.152054      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:24.152417      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:25.152797      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:26.153057      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:27.153359      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:28.153893      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:29.154174      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:30.154822      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:31.154785      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:32.155314      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:33.155848      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:34.156575      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:35.157220      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:36.157617      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:37.158106      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:38.158980      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:39.159774      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:40.159988      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:41.160514      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:42.161511      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:43.162208      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:44.162874      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:45.163103      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:46.163554      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:47.164657      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:48.165136      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:49.165601      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:50.166022      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:51.166247      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:52.166732      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:53.167810      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:54.168382      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:55.169554      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:56.170277      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:57.170667      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:58.170901      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:28:59.171120      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:00.171370      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:01.171602      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:02.171840      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:03.172919      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:04.173377      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:05.173665      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:06.173823      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:07.174756      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:08.174861      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:09.175458      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:10.175822      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:11.176092      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:12.176719      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:13.176937      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:14.177575      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:15.178064      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:16.179172      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:17.179388      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:18.179393      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:19.180482      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:20.180590      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:21.181521      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:22.181966      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:23.182368      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:24.182395      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:25.182802      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:26.183056      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:27.183324      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:28.183228      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:29.183629      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:30.183739      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:31.184234      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:32.184926      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:33.185514      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:34.185863      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:35.186132      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:36.186626      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:37.186875      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:38.187201      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:39.187254      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:40.187583      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:41.187841      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:42.188099      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:43.188379      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:44.188930      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:45.189219      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:46.189820      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:47.189836      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:48.190212      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:49.190648      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:50.191155      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:51.191436      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:52.191584      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:53.192018      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:54.192343      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:55.193547      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:56.194121      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:57.195035      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:58.195578      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:29:59.196027      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:00.196729      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:01.197041      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:02.197439      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:03.197600      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:04.197924      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:05.198512      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:06.198667      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:07.199327      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:08.199760      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:09.200519      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:10.200892      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:11.201768      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:12.202296      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:13.203284      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:14.203655      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:15.203809      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:16.204475      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:17.204922      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:18.205716      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:19.205957      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:20.206268      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:21.206633      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:22.206657      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:23.207768      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:24.208754      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:25.209550      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:26.210169      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:27.210104      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:28.210815      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:29.211218      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:30.211631      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:31.212097      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:32.212518      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:33.213131      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:34.213501      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:35.213822      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:36.214187      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:37.214639      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:38.215868      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:39.215952      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:40.216547      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:41.216841      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:42.216945      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:43.217057      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:44.217596      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:45.218064      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:46.218045      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:47.218582      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:48.219476      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:49.219829      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:50.220466      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:51.220630      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:52.221280      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:53.222377      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:54.223100      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:55.223425      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:56.224062      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:57.224850      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:58.225451      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:30:59.226351      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:00.226794      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:01.227841      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:02.228096      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:03.228239      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:04.229440      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:05.229858      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:06.230094      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:07.230216      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:08.230741      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:09.231059      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:10.231122      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:11.231482      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:12.231589      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:13.231818      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:14.232319      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:15.232678      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:16.233415      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:17.234415      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:18.234584      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:19.234788      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:20.234980      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:21.235298      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:22.235424      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:23.235701      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:24.236032      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:25.236386      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:26.236526      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:27.236720      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:28.237105      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:29.237955      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:30.238366      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:31.239074      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:32.239270      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:33.239616      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:34.240368      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:35.240490      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:36.241075      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:37.241355      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:38.241980      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:39.242116      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:40.243015      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:41.244002      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:42.244391      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:43.244789      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:44.244871      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:45.245212      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:46.245900      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:47.246097      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:48.246637      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:49.246838      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:50.247913      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:51.248228      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:52.248336      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:53.248674      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:54.249236      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:55.249905      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:56.250739      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:57.250829      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:58.251094      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:31:59.251442      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:32:00.251337      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:32:01.251642      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:32:02.252147      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:32:03.252864      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:32:04.253563      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:32:05.253624      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:32:06.254474      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:32:07.254525      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:32:08.255267      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:32:09.255285      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:32:10.257046      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:32:11.257223      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:32:12.257912      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring no job exists by listing jobs explicitly @ 01/10/24 16:32:12.501
  STEP: Removing cronjob @ 01/10/24 16:32:12.511
  Jan 10 16:32:12.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-4150" for this suite. @ 01/10/24 16:32:12.539
• [300.179 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:95
  STEP: Creating a kubernetes client @ 01/10/24 16:32:12.568
  Jan 10 16:32:12.568: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename secrets @ 01/10/24 16:32:12.57
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:32:12.655
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:32:12.664
  STEP: creating secret secrets-3536/secret-test-19d78119-1204-4f27-9b23-be58e3e7b3e8 @ 01/10/24 16:32:12.67
  STEP: Creating a pod to test consume secrets @ 01/10/24 16:32:12.689
  E0110 16:32:13.258172      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:32:14.258636      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:32:15.258907      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:32:16.259299      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/10/24 16:32:16.762
  Jan 10 16:32:16.789: INFO: Trying to get logs from node env1-test-worker-1 pod pod-configmaps-7c364dc2-42f6-48e7-a462-7ddc974386d2 container env-test: <nil>
  STEP: delete the pod @ 01/10/24 16:32:16.842
  Jan 10 16:32:16.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-3536" for this suite. @ 01/10/24 16:32:16.897
• [4.366 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:69
  STEP: Creating a kubernetes client @ 01/10/24 16:32:16.935
  Jan 10 16:32:16.935: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename crd-publish-openapi @ 01/10/24 16:32:16.937
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:32:16.99
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:32:16.996
  Jan 10 16:32:17.007: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  E0110 16:32:17.260289      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:32:18.260719      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:32:19.260932      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:32:20.261540      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:32:21.261919      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:32:22.262052      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:32:23.262328      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:32:24.263118      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:32:25.263669      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:32:26.264663      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with known and required properties @ 01/10/24 16:32:26.623
  Jan 10 16:32:26.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=crd-publish-openapi-1622 --namespace=crd-publish-openapi-1622 create -f -'
  E0110 16:32:27.265431      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:32:28.265594      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:32:28.955: INFO: stderr: ""
  Jan 10 16:32:28.955: INFO: stdout: "e2e-test-crd-publish-openapi-8458-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Jan 10 16:32:28.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=crd-publish-openapi-1622 --namespace=crd-publish-openapi-1622 delete e2e-test-crd-publish-openapi-8458-crds test-foo'
  Jan 10 16:32:29.161: INFO: stderr: ""
  Jan 10 16:32:29.161: INFO: stdout: "e2e-test-crd-publish-openapi-8458-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  Jan 10 16:32:29.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=crd-publish-openapi-1622 --namespace=crd-publish-openapi-1622 apply -f -'
  E0110 16:32:29.266402      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:32:30.267008      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:32:31.267419      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:32:31.405: INFO: stderr: ""
  Jan 10 16:32:31.405: INFO: stdout: "e2e-test-crd-publish-openapi-8458-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Jan 10 16:32:31.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=crd-publish-openapi-1622 --namespace=crd-publish-openapi-1622 delete e2e-test-crd-publish-openapi-8458-crds test-foo'
  Jan 10 16:32:31.564: INFO: stderr: ""
  Jan 10 16:32:31.564: INFO: stdout: "e2e-test-crd-publish-openapi-8458-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values @ 01/10/24 16:32:31.565
  Jan 10 16:32:31.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=crd-publish-openapi-1622 --namespace=crd-publish-openapi-1622 create -f -'
  E0110 16:32:32.267390      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:32:32.859: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema @ 01/10/24 16:32:32.859
  Jan 10 16:32:32.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=crd-publish-openapi-1622 --namespace=crd-publish-openapi-1622 create -f -'
  E0110 16:32:33.268230      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:32:33.295: INFO: rc: 1
  Jan 10 16:32:33.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=crd-publish-openapi-1622 --namespace=crd-publish-openapi-1622 apply -f -'
  Jan 10 16:32:33.793: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request without required properties @ 01/10/24 16:32:33.793
  Jan 10 16:32:33.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=crd-publish-openapi-1622 --namespace=crd-publish-openapi-1622 create -f -'
  E0110 16:32:34.269063      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:32:34.382: INFO: rc: 1
  Jan 10 16:32:34.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=crd-publish-openapi-1622 --namespace=crd-publish-openapi-1622 apply -f -'
  Jan 10 16:32:34.980: INFO: rc: 1
  STEP: kubectl explain works to explain CR properties @ 01/10/24 16:32:34.98
  Jan 10 16:32:34.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=crd-publish-openapi-1622 explain e2e-test-crd-publish-openapi-8458-crds'
  E0110 16:32:35.269661      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:32:35.493: INFO: stderr: ""
  Jan 10 16:32:35.494: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-8458-crd\nVERSION:    v1\n\nDESCRIPTION:\n    Foo CRD for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Foo\n\n  status\t<Object>\n    Status of Foo\n\n\n"
  STEP: kubectl explain works to explain CR properties recursively @ 01/10/24 16:32:35.495
  Jan 10 16:32:35.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=crd-publish-openapi-1622 explain e2e-test-crd-publish-openapi-8458-crds.metadata'
  Jan 10 16:32:36.006: INFO: stderr: ""
  Jan 10 16:32:36.006: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-8458-crd\nVERSION:    v1\n\nFIELD: metadata <ObjectMeta>\n\nDESCRIPTION:\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ObjectMeta is metadata that all persisted resources must have, which\n    includes all objects users must create.\n    \nFIELDS:\n  annotations\t<map[string]string>\n    Annotations is an unstructured key value map stored with a resource that may\n    be set by external tools to store and retrieve arbitrary metadata. They are\n    not queryable and should be preserved when modifying objects. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations\n\n  creationTimestamp\t<string>\n    CreationTimestamp is a timestamp representing the server time when this\n    object was created. It is not guaranteed to be set in happens-before order\n    across separate operations. Clients may not set this value. It is\n    represented in RFC3339 form and is in UTC.\n    \n    Populated by the system. Read-only. Null for lists. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  deletionGracePeriodSeconds\t<integer>\n    Number of seconds allowed for this object to gracefully terminate before it\n    will be removed from the system. Only set when deletionTimestamp is also\n    set. May only be shortened. Read-only.\n\n  deletionTimestamp\t<string>\n    DeletionTimestamp is RFC 3339 date and time at which this resource will be\n    deleted. This field is set by the server when a graceful deletion is\n    requested by the user, and is not directly settable by a client. The\n    resource is expected to be deleted (no longer visible from resource lists,\n    and not reachable by name) after the time in this field, once the finalizers\n    list is empty. As long as the finalizers list contains items, deletion is\n    blocked. Once the deletionTimestamp is set, this value may not be unset or\n    be set further into the future, although it may be shortened or the resource\n    may be deleted prior to this time. For example, a user may request that a\n    pod is deleted in 30 seconds. The Kubelet will react by sending a graceful\n    termination signal to the containers in the pod. After that 30 seconds, the\n    Kubelet will send a hard termination signal (SIGKILL) to the container and\n    after cleanup, remove the pod from the API. In the presence of network\n    partitions, this object may still exist after this timestamp, until an\n    administrator or automated process can determine the resource is fully\n    terminated. If not set, graceful deletion of the object has not been\n    requested.\n    \n    Populated by the system when a graceful deletion is requested. Read-only.\n    More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  finalizers\t<[]string>\n    Must be empty before the object is deleted from the registry. Each entry is\n    an identifier for the responsible component that will remove the entry from\n    the list. If the deletionTimestamp of the object is non-nil, entries in this\n    list can only be removed. Finalizers may be processed and removed in any\n    order.  Order is NOT enforced because it introduces significant risk of\n    stuck finalizers. finalizers is a shared field, any actor with permission\n    can reorder it. If the finalizer list is processed in order, then this can\n    lead to a situation in which the component responsible for the first\n    finalizer in the list is waiting for a signal (field value, external system,\n    or other) produced by a component responsible for a finalizer later in the\n    list, resulting in a deadlock. Without enforced ordering finalizers are free\n    to order amongst themselves and are not vulnerable to ordering changes in\n    the list.\n\n  generateName\t<string>\n    GenerateName is an optional prefix, used by the server, to generate a unique\n    name ONLY IF the Name field has not been provided. If this field is used,\n    the name returned to the client will be different than the name passed. This\n    value will also be combined with a unique suffix. The provided value has the\n    same validation rules as the Name field, and may be truncated by the length\n    of the suffix required to make the value unique on the server.\n    \n    If this field is specified and the generated name exists, the server will\n    return a 409.\n    \n    Applied only if Name is not specified. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n  generation\t<integer>\n    A sequence number representing a specific generation of the desired state.\n    Populated by the system. Read-only.\n\n  labels\t<map[string]string>\n    Map of string keys and values that can be used to organize and categorize\n    (scope and select) objects. May match selectors of replication controllers\n    and services. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/labels\n\n  managedFields\t<[]ManagedFieldsEntry>\n    ManagedFields maps workflow-id and version to the set of fields that are\n    managed by that workflow. This is mostly for internal housekeeping, and\n    users typically shouldn't need to set or understand this field. A workflow\n    can be the user's name, a controller's name, or the name of a specific apply\n    path like \"ci-cd\". The set of fields is always in the version that the\n    workflow used when modifying the object.\n\n  name\t<string>\n    Name must be unique within a namespace. Is required when creating resources,\n    although some resources may allow a client to request the generation of an\n    appropriate name automatically. Name is primarily intended for creation\n    idempotence and configuration definition. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#names\n\n  namespace\t<string>\n    Namespace defines the space within which each name must be unique. An empty\n    namespace is equivalent to the \"default\" namespace, but \"default\" is the\n    canonical representation. Not all objects are required to be scoped to a\n    namespace - the value of this field for those objects will be empty.\n    \n    Must be a DNS_LABEL. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces\n\n  ownerReferences\t<[]OwnerReference>\n    List of objects depended by this object. If ALL objects in the list have\n    been deleted, this object will be garbage collected. If this object is\n    managed by a controller, then an entry in this list will point to this\n    controller, with the controller field set to true. There cannot be more than\n    one managing controller.\n\n  resourceVersion\t<string>\n    An opaque value that represents the internal version of this object that can\n    be used by clients to determine when objects have changed. May be used for\n    optimistic concurrency, change detection, and the watch operation on a\n    resource or set of resources. Clients must treat these values as opaque and\n    passed unmodified back to the server. They may only be valid for a\n    particular resource or set of resources.\n    \n    Populated by the system. Read-only. Value must be treated as opaque by\n    clients and . More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n  selfLink\t<string>\n    Deprecated: selfLink is a legacy read-only field that is no longer populated\n    by the system.\n\n  uid\t<string>\n    UID is the unique in time and space value for this object. It is typically\n    generated by the server on successful creation of a resource and is not\n    allowed to change on PUT operations.\n    \n    Populated by the system. Read-only. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#uids\n\n\n"
  Jan 10 16:32:36.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=crd-publish-openapi-1622 explain e2e-test-crd-publish-openapi-8458-crds.spec'
  E0110 16:32:36.269887      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:32:36.607: INFO: stderr: ""
  Jan 10 16:32:36.607: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-8458-crd\nVERSION:    v1\n\nFIELD: spec <Object>\n\nDESCRIPTION:\n    Specification of Foo\n    \nFIELDS:\n  bars\t<[]Object>\n    List of Bars and their specs.\n\n\n"
  Jan 10 16:32:36.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=crd-publish-openapi-1622 explain e2e-test-crd-publish-openapi-8458-crds.spec.bars'
  Jan 10 16:32:37.169: INFO: stderr: ""
  Jan 10 16:32:37.169: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-8458-crd\nVERSION:    v1\n\nFIELD: bars <[]Object>\n\nDESCRIPTION:\n    List of Bars and their specs.\n    \nFIELDS:\n  age\t<string>\n    Age of Bar.\n\n  bazs\t<[]string>\n    List of Bazs.\n\n  feeling\t<string>\n    Whether Bar is feeling great.\n\n  name\t<string> -required-\n    Name of Bar.\n\n\n"
  STEP: kubectl explain works to return error when explain is called on property that doesn't exist @ 01/10/24 16:32:37.169
  Jan 10 16:32:37.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=crd-publish-openapi-1622 explain e2e-test-crd-publish-openapi-8458-crds.spec.bars2'
  E0110 16:32:37.270899      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:32:37.690: INFO: rc: 1
  E0110 16:32:38.271507      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:32:39.272180      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:32:40.272532      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:32:41.274072      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:32:41.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-1622" for this suite. @ 01/10/24 16:32:42.025
• [25.110 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:89
  STEP: Creating a kubernetes client @ 01/10/24 16:32:42.054
  Jan 10 16:32:42.054: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename secrets @ 01/10/24 16:32:42.058
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:32:42.098
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:32:42.107
  STEP: Creating secret with name secret-test-map-99defd2b-0980-4cac-9904-9de41aa9d5f5 @ 01/10/24 16:32:42.114
  STEP: Creating a pod to test consume secrets @ 01/10/24 16:32:42.127
  E0110 16:32:42.274234      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:32:43.275075      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:32:44.275491      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:32:45.275671      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/10/24 16:32:46.181
  Jan 10 16:32:46.192: INFO: Trying to get logs from node env1-test-worker-1 pod pod-secrets-689907b4-236f-4ca2-bcd5-d93e1f4fb63a container secret-volume-test: <nil>
  STEP: delete the pod @ 01/10/24 16:32:46.236
  E0110 16:32:46.276347      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:32:46.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5233" for this suite. @ 01/10/24 16:32:46.296
• [4.267 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should delete old replica sets [Conformance]
test/e2e/apps/deployment.go:122
  STEP: Creating a kubernetes client @ 01/10/24 16:32:46.327
  Jan 10 16:32:46.327: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename deployment @ 01/10/24 16:32:46.331
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:32:46.374
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:32:46.381
  Jan 10 16:32:46.414: INFO: Pod name cleanup-pod: Found 0 pods out of 1
  E0110 16:32:47.276591      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:32:48.277253      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:32:49.277368      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:32:50.277485      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:32:51.277781      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:32:51.426: INFO: Pod name cleanup-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 01/10/24 16:32:51.426
  Jan 10 16:32:51.426: INFO: Creating deployment test-cleanup-deployment
  STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up @ 01/10/24 16:32:51.469
  Jan 10 16:32:51.509: INFO: Deployment "test-cleanup-deployment":
  &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-8159  360012d8-5631-4264-8544-a7a34ebb91ad 186790019 1 2024-01-10 16:32:51 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2024-01-10 16:32:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007f869a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

  Jan 10 16:32:51.524: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
  Jan 10 16:32:51.524: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
  Jan 10 16:32:51.524: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-8159  a7a4a72a-ae3a-4a68-a354-02eb4bb39ca3 186790020 1 2024-01-10 16:32:46 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 360012d8-5631-4264-8544-a7a34ebb91ad 0xc007f86e67 0xc007f86e68}] [] [{e2e.test Update apps/v1 2024-01-10 16:32:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-10 16:32:47 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2024-01-10 16:32:51 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"360012d8-5631-4264-8544-a7a34ebb91ad\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc007f86f38 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jan 10 16:32:51.538: INFO: Pod "test-cleanup-controller-szsks" is available:
  &Pod{ObjectMeta:{test-cleanup-controller-szsks test-cleanup-controller- deployment-8159  963a8580-5348-4076-84bf-9cdd5598c317 186790000 0 2024-01-10 16:32:46 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller a7a4a72a-ae3a-4a68-a354-02eb4bb39ca3 0xc007f87247 0xc007f87248}] [] [{kube-controller-manager Update v1 2024-01-10 16:32:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a7a4a72a-ae3a-4a68-a354-02eb4bb39ca3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-10 16:32:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.106\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-99zwc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-99zwc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.201,PodIP:10.233.68.106,StartTime:2024-01-10 16:32:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-10 16:32:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://8c882709162d4c4fd5dcc07abaeb1d2266d840ca571c4aab0f1da73ce657980c,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.106,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 10 16:32:51.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-8159" for this suite. @ 01/10/24 16:32:51.56
• [5.260 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should support proportional scaling [Conformance]
test/e2e/apps/deployment.go:160
  STEP: Creating a kubernetes client @ 01/10/24 16:32:51.588
  Jan 10 16:32:51.588: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename deployment @ 01/10/24 16:32:51.59
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:32:51.718
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:32:51.742
  Jan 10 16:32:51.759: INFO: Creating deployment "webserver-deployment"
  Jan 10 16:32:51.776: INFO: Waiting for observed generation 1
  E0110 16:32:52.278787      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:32:53.279032      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:32:53.812: INFO: Waiting for all required pods to come up
  Jan 10 16:32:53.824: INFO: Pod name httpd: Found 10 pods out of 10
  STEP: ensuring each pod is running @ 01/10/24 16:32:53.824
  E0110 16:32:54.279719      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:32:55.280116      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:32:55.862: INFO: Waiting for deployment "webserver-deployment" to complete
  Jan 10 16:32:55.875: INFO: Updating deployment "webserver-deployment" with a non-existent image
  Jan 10 16:32:55.896: INFO: Updating deployment webserver-deployment
  Jan 10 16:32:55.896: INFO: Waiting for observed generation 2
  E0110 16:32:56.280756      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:32:57.281261      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:32:57.926: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
  Jan 10 16:32:57.933: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
  Jan 10 16:32:57.941: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Jan 10 16:32:57.969: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
  Jan 10 16:32:57.969: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
  Jan 10 16:32:57.982: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Jan 10 16:32:58.023: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
  Jan 10 16:32:58.024: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
  Jan 10 16:32:58.079: INFO: Updating deployment webserver-deployment
  Jan 10 16:32:58.079: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
  Jan 10 16:32:58.117: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
  E0110 16:32:58.282475      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:32:59.283296      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:33:00.148: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
  Jan 10 16:33:00.163: INFO: Deployment "webserver-deployment":
  &Deployment{ObjectMeta:{webserver-deployment  deployment-1288  e792a4c8-21bb-42e7-86f3-3e94f7bd4c93 186790345 3 2024-01-10 16:32:51 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2024-01-10 16:32:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-10 16:32:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004ace4d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2024-01-10 16:32:58 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-7b75d79cf5" is progressing.,LastUpdateTime:2024-01-10 16:32:58 +0000 UTC,LastTransitionTime:2024-01-10 16:32:51 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

  Jan 10 16:33:00.172: INFO: New ReplicaSet "webserver-deployment-7b75d79cf5" of Deployment "webserver-deployment":
  &ReplicaSet{ObjectMeta:{webserver-deployment-7b75d79cf5  deployment-1288  84b6cc3d-44a3-450e-8ce3-423953412bc1 186790332 3 2024-01-10 16:32:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment e792a4c8-21bb-42e7-86f3-3e94f7bd4c93 0xc005189357 0xc005189358}] [] [{kube-controller-manager Update apps/v1 2024-01-10 16:32:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e792a4c8-21bb-42e7-86f3-3e94f7bd4c93\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-10 16:32:58 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7b75d79cf5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051893f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jan 10 16:33:00.172: INFO: All old ReplicaSets of Deployment "webserver-deployment":
  Jan 10 16:33:00.172: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-67bd4bf6dc  deployment-1288  a762287e-4c3f-4172-9acc-6a277d3e5ec8 186790330 3 2024-01-10 16:32:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment e792a4c8-21bb-42e7-86f3-3e94f7bd4c93 0xc005189267 0xc005189268}] [] [{kube-controller-manager Update apps/v1 2024-01-10 16:32:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e792a4c8-21bb-42e7-86f3-3e94f7bd4c93\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-10 16:32:58 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051892f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
  Jan 10 16:33:00.193: INFO: Pod "webserver-deployment-67bd4bf6dc-46s5n" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-46s5n webserver-deployment-67bd4bf6dc- deployment-1288  801be7fc-0198-4610-965b-15f74244a68f 186790144 0 2024-01-10 16:32:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a762287e-4c3f-4172-9acc-6a277d3e5ec8 0xc004ace8a7 0xc004ace8a8}] [] [{kube-controller-manager Update v1 2024-01-10 16:32:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a762287e-4c3f-4172-9acc-6a277d3e5ec8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-10 16:32:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.67.106\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-shxmp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-shxmp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.200,PodIP:10.233.67.106,StartTime:2024-01-10 16:32:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-10 16:32:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://71c15303e5fa8e154705ce5201b2608fd345fd2c3277791bbe5baa548b090d6b,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.67.106,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 10 16:33:00.193: INFO: Pod "webserver-deployment-67bd4bf6dc-4dkz5" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-4dkz5 webserver-deployment-67bd4bf6dc- deployment-1288  89339e97-eda8-4ed6-bc28-9a4c2809f63e 186790338 0 2024-01-10 16:32:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a762287e-4c3f-4172-9acc-6a277d3e5ec8 0xc004acea97 0xc004acea98}] [] [{kube-controller-manager Update v1 2024-01-10 16:32:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a762287e-4c3f-4172-9acc-6a277d3e5ec8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-10 16:32:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fngxn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fngxn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.200,PodIP:,StartTime:2024-01-10 16:32:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 10 16:33:00.194: INFO: Pod "webserver-deployment-67bd4bf6dc-6228r" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-6228r webserver-deployment-67bd4bf6dc- deployment-1288  97f4f7a8-3c65-41ea-898a-2ecb5e3e4061 186790325 0 2024-01-10 16:32:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a762287e-4c3f-4172-9acc-6a277d3e5ec8 0xc004acec57 0xc004acec58}] [] [{kube-controller-manager Update v1 2024-01-10 16:32:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a762287e-4c3f-4172-9acc-6a277d3e5ec8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-10 16:32:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5mcbd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5mcbd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.200,PodIP:,StartTime:2024-01-10 16:32:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 10 16:33:00.194: INFO: Pod "webserver-deployment-67bd4bf6dc-6ptwj" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-6ptwj webserver-deployment-67bd4bf6dc- deployment-1288  244b75c1-4703-4fbb-91f6-f528a66e18ae 186790368 0 2024-01-10 16:32:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a762287e-4c3f-4172-9acc-6a277d3e5ec8 0xc004acee27 0xc004acee28}] [] [{kube-controller-manager Update v1 2024-01-10 16:32:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a762287e-4c3f-4172-9acc-6a277d3e5ec8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-10 16:32:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d9mjc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d9mjc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.201,PodIP:,StartTime:2024-01-10 16:32:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 10 16:33:00.194: INFO: Pod "webserver-deployment-67bd4bf6dc-7wpw6" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-7wpw6 webserver-deployment-67bd4bf6dc- deployment-1288  ad3d4842-bae7-4c4d-acc3-bb6b05b73b4b 186790363 0 2024-01-10 16:32:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a762287e-4c3f-4172-9acc-6a277d3e5ec8 0xc004acefe7 0xc004acefe8}] [] [{kube-controller-manager Update v1 2024-01-10 16:32:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a762287e-4c3f-4172-9acc-6a277d3e5ec8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-10 16:32:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9b8v8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9b8v8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.201,PodIP:,StartTime:2024-01-10 16:32:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 10 16:33:00.195: INFO: Pod "webserver-deployment-67bd4bf6dc-db6sx" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-db6sx webserver-deployment-67bd4bf6dc- deployment-1288  0104dc87-6faa-49f8-8b89-252b59132fe7 186790155 0 2024-01-10 16:32:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a762287e-4c3f-4172-9acc-6a277d3e5ec8 0xc004acf1a7 0xc004acf1a8}] [] [{kube-controller-manager Update v1 2024-01-10 16:32:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a762287e-4c3f-4172-9acc-6a277d3e5ec8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-10 16:32:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.108\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hv75p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hv75p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.201,PodIP:10.233.68.108,StartTime:2024-01-10 16:32:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-10 16:32:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://b870c22faab3579c0d6924deac14fb015a5e38271f44db25593e7ac3e429f153,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.108,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 10 16:33:00.195: INFO: Pod "webserver-deployment-67bd4bf6dc-fx75c" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-fx75c webserver-deployment-67bd4bf6dc- deployment-1288  21f2b502-d960-4e77-ba59-9e20d2337bf7 186790161 0 2024-01-10 16:32:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a762287e-4c3f-4172-9acc-6a277d3e5ec8 0xc004acf3a7 0xc004acf3a8}] [] [{kube-controller-manager Update v1 2024-01-10 16:32:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a762287e-4c3f-4172-9acc-6a277d3e5ec8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-10 16:32:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.112\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w5st9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w5st9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.201,PodIP:10.233.68.112,StartTime:2024-01-10 16:32:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-10 16:32:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://29c9d11d0ed5e45c92e6345cbfa29eeb9562d24ddad66bb8683419f95ee4b2db,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.112,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 10 16:33:00.195: INFO: Pod "webserver-deployment-67bd4bf6dc-fzlj4" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-fzlj4 webserver-deployment-67bd4bf6dc- deployment-1288  6ea851fe-ffd1-40f2-ab4b-aa30e50f7319 186790335 0 2024-01-10 16:32:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a762287e-4c3f-4172-9acc-6a277d3e5ec8 0xc004acf587 0xc004acf588}] [] [{kube-controller-manager Update v1 2024-01-10 16:32:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a762287e-4c3f-4172-9acc-6a277d3e5ec8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-10 16:32:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ksz24,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ksz24,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.200,PodIP:,StartTime:2024-01-10 16:32:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 10 16:33:00.196: INFO: Pod "webserver-deployment-67bd4bf6dc-jhhps" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-jhhps webserver-deployment-67bd4bf6dc- deployment-1288  4b0010df-0529-4f51-996e-ca89e0287291 186790158 0 2024-01-10 16:32:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a762287e-4c3f-4172-9acc-6a277d3e5ec8 0xc004acf757 0xc004acf758}] [] [{kube-controller-manager Update v1 2024-01-10 16:32:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a762287e-4c3f-4172-9acc-6a277d3e5ec8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-10 16:32:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.110\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6f7w9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6f7w9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.201,PodIP:10.233.68.110,StartTime:2024-01-10 16:32:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-10 16:32:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://9764d11a4f35c75a0e9c7105f083f99642e944ddee40c16f4bc3516ff7f02af0,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.110,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 10 16:33:00.196: INFO: Pod "webserver-deployment-67bd4bf6dc-km94s" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-km94s webserver-deployment-67bd4bf6dc- deployment-1288  97a97d7b-e29d-49f8-ba26-f032af549e1b 186790136 0 2024-01-10 16:32:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a762287e-4c3f-4172-9acc-6a277d3e5ec8 0xc004acf937 0xc004acf938}] [] [{kube-controller-manager Update v1 2024-01-10 16:32:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a762287e-4c3f-4172-9acc-6a277d3e5ec8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-10 16:32:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.67.105\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8sqxx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8sqxx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.200,PodIP:10.233.67.105,StartTime:2024-01-10 16:32:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-10 16:32:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e913979c632d6b5fd5788e517fc6aabd7ae778f69dd3fce61aa0f039ed18abc6,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.67.105,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 10 16:33:00.197: INFO: Pod "webserver-deployment-67bd4bf6dc-l2ppj" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-l2ppj webserver-deployment-67bd4bf6dc- deployment-1288  b6be2aaa-6888-4e50-81ba-c4df13544a91 186790360 0 2024-01-10 16:32:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a762287e-4c3f-4172-9acc-6a277d3e5ec8 0xc004acfb27 0xc004acfb28}] [] [{kube-controller-manager Update v1 2024-01-10 16:32:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a762287e-4c3f-4172-9acc-6a277d3e5ec8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-10 16:32:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7bhl5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7bhl5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.201,PodIP:,StartTime:2024-01-10 16:32:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 10 16:33:00.197: INFO: Pod "webserver-deployment-67bd4bf6dc-l6827" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-l6827 webserver-deployment-67bd4bf6dc- deployment-1288  419def19-ae33-4de8-a1a6-7b860998a81b 186790347 0 2024-01-10 16:32:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a762287e-4c3f-4172-9acc-6a277d3e5ec8 0xc004acfce7 0xc004acfce8}] [] [{kube-controller-manager Update v1 2024-01-10 16:32:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a762287e-4c3f-4172-9acc-6a277d3e5ec8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-10 16:32:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5sd9b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5sd9b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.200,PodIP:,StartTime:2024-01-10 16:32:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 10 16:33:00.197: INFO: Pod "webserver-deployment-67bd4bf6dc-n5q9k" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-n5q9k webserver-deployment-67bd4bf6dc- deployment-1288  94a3b43b-0fec-4a98-8811-36e939b0aed5 186790142 0 2024-01-10 16:32:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a762287e-4c3f-4172-9acc-6a277d3e5ec8 0xc004acfeb7 0xc004acfeb8}] [] [{kube-controller-manager Update v1 2024-01-10 16:32:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a762287e-4c3f-4172-9acc-6a277d3e5ec8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-10 16:32:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.67.107\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fhtlv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fhtlv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.200,PodIP:10.233.67.107,StartTime:2024-01-10 16:32:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-10 16:32:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://b3341e1d72257954f64c8fc105d5949bd6139f111e4390d6fe1b3a32b0aa2851,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.67.107,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 10 16:33:00.197: INFO: Pod "webserver-deployment-67bd4bf6dc-rb2pv" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-rb2pv webserver-deployment-67bd4bf6dc- deployment-1288  50b9c213-1267-4e89-ba89-173947282d9a 186790334 0 2024-01-10 16:32:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a762287e-4c3f-4172-9acc-6a277d3e5ec8 0xc004b120a7 0xc004b120a8}] [] [{kube-controller-manager Update v1 2024-01-10 16:32:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a762287e-4c3f-4172-9acc-6a277d3e5ec8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-10 16:32:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cbkhp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cbkhp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.201,PodIP:,StartTime:2024-01-10 16:32:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 10 16:33:00.198: INFO: Pod "webserver-deployment-67bd4bf6dc-skqcd" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-skqcd webserver-deployment-67bd4bf6dc- deployment-1288  9883a824-127a-42d3-a9d0-35eebc23b3c0 186790168 0 2024-01-10 16:32:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a762287e-4c3f-4172-9acc-6a277d3e5ec8 0xc004b12267 0xc004b12268}] [] [{kube-controller-manager Update v1 2024-01-10 16:32:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a762287e-4c3f-4172-9acc-6a277d3e5ec8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-10 16:32:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.111\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kp4lx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kp4lx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.201,PodIP:10.233.68.111,StartTime:2024-01-10 16:32:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-10 16:32:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://799434a045bab287e2be082fcc3cc0a7948a51eb5f1a5752bf7e79a3695a43a0,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.111,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 10 16:33:00.198: INFO: Pod "webserver-deployment-67bd4bf6dc-tmh69" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-tmh69 webserver-deployment-67bd4bf6dc- deployment-1288  198f6c50-318d-47e9-9e9c-26c10aba5e4c 186790374 0 2024-01-10 16:32:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a762287e-4c3f-4172-9acc-6a277d3e5ec8 0xc004b12477 0xc004b12478}] [] [{kube-controller-manager Update v1 2024-01-10 16:32:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a762287e-4c3f-4172-9acc-6a277d3e5ec8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-10 16:32:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zthv7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zthv7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.201,PodIP:,StartTime:2024-01-10 16:32:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 10 16:33:00.198: INFO: Pod "webserver-deployment-67bd4bf6dc-tthrc" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-tthrc webserver-deployment-67bd4bf6dc- deployment-1288  6b6bc18b-0742-419a-aed8-d36b9dad129b 186790298 0 2024-01-10 16:32:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a762287e-4c3f-4172-9acc-6a277d3e5ec8 0xc004b12657 0xc004b12658}] [] [{kube-controller-manager Update v1 2024-01-10 16:32:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a762287e-4c3f-4172-9acc-6a277d3e5ec8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-10 16:32:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w5dcw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w5dcw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.200,PodIP:,StartTime:2024-01-10 16:32:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 10 16:33:00.204: INFO: Pod "webserver-deployment-67bd4bf6dc-xnjd5" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-xnjd5 webserver-deployment-67bd4bf6dc- deployment-1288  fe141135-0780-490a-af93-f03331006f64 186790349 0 2024-01-10 16:32:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a762287e-4c3f-4172-9acc-6a277d3e5ec8 0xc004b12837 0xc004b12838}] [] [{kube-controller-manager Update v1 2024-01-10 16:32:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a762287e-4c3f-4172-9acc-6a277d3e5ec8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-10 16:32:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wbmcs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wbmcs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.201,PodIP:,StartTime:2024-01-10 16:32:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 10 16:33:00.205: INFO: Pod "webserver-deployment-67bd4bf6dc-xwcnp" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-xwcnp webserver-deployment-67bd4bf6dc- deployment-1288  c493c602-7ece-43ed-b6aa-79a5dc79ea55 186790346 0 2024-01-10 16:32:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a762287e-4c3f-4172-9acc-6a277d3e5ec8 0xc004b12a37 0xc004b12a38}] [] [{kube-controller-manager Update v1 2024-01-10 16:32:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a762287e-4c3f-4172-9acc-6a277d3e5ec8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-10 16:32:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pghhx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pghhx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.200,PodIP:,StartTime:2024-01-10 16:32:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 10 16:33:00.206: INFO: Pod "webserver-deployment-67bd4bf6dc-z8hk4" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-z8hk4 webserver-deployment-67bd4bf6dc- deployment-1288  60517eb2-a8c8-489e-8805-4fb17eef7588 186790148 0 2024-01-10 16:32:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc a762287e-4c3f-4172-9acc-6a277d3e5ec8 0xc004b12c07 0xc004b12c08}] [] [{kube-controller-manager Update v1 2024-01-10 16:32:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a762287e-4c3f-4172-9acc-6a277d3e5ec8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-10 16:32:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.67.108\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h4g6n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h4g6n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.200,PodIP:10.233.67.108,StartTime:2024-01-10 16:32:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-10 16:32:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://cda545f884ded2a55a11330c71f024736c9a11f89b90bbe57d5f6c1372ebe5c7,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.67.108,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 10 16:33:00.206: INFO: Pod "webserver-deployment-7b75d79cf5-2l42l" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-2l42l webserver-deployment-7b75d79cf5- deployment-1288  38c61d00-33a7-4856-89dc-4de51e4e1ff9 186790352 0 2024-01-10 16:32:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 84b6cc3d-44a3-450e-8ce3-423953412bc1 0xc004b12df7 0xc004b12df8}] [] [{kube-controller-manager Update v1 2024-01-10 16:32:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"84b6cc3d-44a3-450e-8ce3-423953412bc1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-10 16:32:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ttqzt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ttqzt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.201,PodIP:,StartTime:2024-01-10 16:32:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 10 16:33:00.206: INFO: Pod "webserver-deployment-7b75d79cf5-5xhnc" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-5xhnc webserver-deployment-7b75d79cf5- deployment-1288  84ae8cbd-afb7-4915-b3b1-11ae0f2ded65 186790355 0 2024-01-10 16:32:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 84b6cc3d-44a3-450e-8ce3-423953412bc1 0xc004b13017 0xc004b13018}] [] [{kube-controller-manager Update v1 2024-01-10 16:32:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"84b6cc3d-44a3-450e-8ce3-423953412bc1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-10 16:32:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ll494,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ll494,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.201,PodIP:,StartTime:2024-01-10 16:32:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 10 16:33:00.207: INFO: Pod "webserver-deployment-7b75d79cf5-flfcw" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-flfcw webserver-deployment-7b75d79cf5- deployment-1288  02d254e3-2d7f-42c4-a646-fbd66a0f9aa3 186790314 0 2024-01-10 16:32:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 84b6cc3d-44a3-450e-8ce3-423953412bc1 0xc004b13207 0xc004b13208}] [] [{kube-controller-manager Update v1 2024-01-10 16:32:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"84b6cc3d-44a3-450e-8ce3-423953412bc1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-10 16:32:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tr9w4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tr9w4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.200,PodIP:,StartTime:2024-01-10 16:32:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 10 16:33:00.207: INFO: Pod "webserver-deployment-7b75d79cf5-gckp5" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-gckp5 webserver-deployment-7b75d79cf5- deployment-1288  4ba28735-2072-462d-8fcd-6d69f2200252 186790371 0 2024-01-10 16:32:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 84b6cc3d-44a3-450e-8ce3-423953412bc1 0xc004b13407 0xc004b13408}] [] [{kube-controller-manager Update v1 2024-01-10 16:32:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"84b6cc3d-44a3-450e-8ce3-423953412bc1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-10 16:32:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vj2k2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vj2k2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.201,PodIP:,StartTime:2024-01-10 16:32:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 10 16:33:00.207: INFO: Pod "webserver-deployment-7b75d79cf5-h6xz4" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-h6xz4 webserver-deployment-7b75d79cf5- deployment-1288  18839447-7ca1-43dc-b843-f8c08bff287e 186790401 0 2024-01-10 16:32:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 84b6cc3d-44a3-450e-8ce3-423953412bc1 0xc004b13627 0xc004b13628}] [] [{kube-controller-manager Update v1 2024-01-10 16:32:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"84b6cc3d-44a3-450e-8ce3-423953412bc1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-10 16:33:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.113\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nk9fd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nk9fd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.201,PodIP:10.233.68.113,StartTime:2024-01-10 16:32:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.113,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 10 16:33:00.208: INFO: Pod "webserver-deployment-7b75d79cf5-lgz9j" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-lgz9j webserver-deployment-7b75d79cf5- deployment-1288  aab19ac2-4a1b-429b-bb6d-313fe60a544d 186790376 0 2024-01-10 16:32:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 84b6cc3d-44a3-450e-8ce3-423953412bc1 0xc004b13847 0xc004b13848}] [] [{kube-controller-manager Update v1 2024-01-10 16:32:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"84b6cc3d-44a3-450e-8ce3-423953412bc1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-10 16:32:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zkf78,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zkf78,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.201,PodIP:,StartTime:2024-01-10 16:32:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 10 16:33:00.208: INFO: Pod "webserver-deployment-7b75d79cf5-mf9qv" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-mf9qv webserver-deployment-7b75d79cf5- deployment-1288  5ea1fd70-e815-499c-963d-7b26ef42ca83 186790350 0 2024-01-10 16:32:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 84b6cc3d-44a3-450e-8ce3-423953412bc1 0xc004b13a37 0xc004b13a38}] [] [{kube-controller-manager Update v1 2024-01-10 16:32:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"84b6cc3d-44a3-450e-8ce3-423953412bc1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-10 16:32:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9d6tn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9d6tn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.200,PodIP:,StartTime:2024-01-10 16:32:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 10 16:33:00.209: INFO: Pod "webserver-deployment-7b75d79cf5-mlgts" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-mlgts webserver-deployment-7b75d79cf5- deployment-1288  b8872d37-689f-40d0-9f76-b4b34ec8ef99 186790206 0 2024-01-10 16:32:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 84b6cc3d-44a3-450e-8ce3-423953412bc1 0xc004b13c17 0xc004b13c18}] [] [{kube-controller-manager Update v1 2024-01-10 16:32:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"84b6cc3d-44a3-450e-8ce3-423953412bc1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-10 16:32:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v8nw8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v8nw8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.201,PodIP:,StartTime:2024-01-10 16:32:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 10 16:33:00.209: INFO: Pod "webserver-deployment-7b75d79cf5-n7tv6" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-n7tv6 webserver-deployment-7b75d79cf5- deployment-1288  557451d0-d317-49eb-b2fd-37bc8763841a 186790223 0 2024-01-10 16:32:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 84b6cc3d-44a3-450e-8ce3-423953412bc1 0xc004b13e07 0xc004b13e08}] [] [{kube-controller-manager Update v1 2024-01-10 16:32:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"84b6cc3d-44a3-450e-8ce3-423953412bc1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-10 16:32:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9stgb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9stgb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.200,PodIP:,StartTime:2024-01-10 16:32:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 10 16:33:00.209: INFO: Pod "webserver-deployment-7b75d79cf5-nf5vf" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-nf5vf webserver-deployment-7b75d79cf5- deployment-1288  5f552a9e-ca4d-4d11-a7ac-8c2208146a49 186790391 0 2024-01-10 16:32:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 84b6cc3d-44a3-450e-8ce3-423953412bc1 0xc004b13fe7 0xc004b13fe8}] [] [{kube-controller-manager Update v1 2024-01-10 16:32:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"84b6cc3d-44a3-450e-8ce3-423953412bc1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-10 16:32:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.67.109\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gbwtw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gbwtw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.200,PodIP:10.233.67.109,StartTime:2024-01-10 16:32:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.67.109,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 10 16:33:00.209: INFO: Pod "webserver-deployment-7b75d79cf5-pg7fh" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-pg7fh webserver-deployment-7b75d79cf5- deployment-1288  782d4d3e-b282-4f3a-9e75-548e86371ddc 186790317 0 2024-01-10 16:32:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 84b6cc3d-44a3-450e-8ce3-423953412bc1 0xc00517a1f7 0xc00517a1f8}] [] [{kube-controller-manager Update v1 2024-01-10 16:32:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"84b6cc3d-44a3-450e-8ce3-423953412bc1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-10 16:32:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tdm5d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tdm5d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.200,PodIP:,StartTime:2024-01-10 16:32:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 10 16:33:00.210: INFO: Pod "webserver-deployment-7b75d79cf5-s4grn" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-s4grn webserver-deployment-7b75d79cf5- deployment-1288  ee794669-fc3b-4227-9127-f947178d629b 186790230 0 2024-01-10 16:32:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 84b6cc3d-44a3-450e-8ce3-423953412bc1 0xc00517a3e7 0xc00517a3e8}] [] [{kube-controller-manager Update v1 2024-01-10 16:32:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"84b6cc3d-44a3-450e-8ce3-423953412bc1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-10 16:32:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wb922,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wb922,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.201,PodIP:,StartTime:2024-01-10 16:32:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 10 16:33:00.210: INFO: Pod "webserver-deployment-7b75d79cf5-w9vvb" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-w9vvb webserver-deployment-7b75d79cf5- deployment-1288  a5365527-fb1d-41ed-9012-0136ab86e0e0 186790280 0 2024-01-10 16:32:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 84b6cc3d-44a3-450e-8ce3-423953412bc1 0xc00517a5f7 0xc00517a5f8}] [] [{kube-controller-manager Update v1 2024-01-10 16:32:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"84b6cc3d-44a3-450e-8ce3-423953412bc1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-10 16:32:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wkr95,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wkr95,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:32:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.200,PodIP:,StartTime:2024-01-10 16:32:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 10 16:33:00.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-1288" for this suite. @ 01/10/24 16:33:00.223
• [8.649 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:309
  STEP: Creating a kubernetes client @ 01/10/24 16:33:00.239
  Jan 10 16:33:00.239: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename crd-publish-openapi @ 01/10/24 16:33:00.24
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:33:00.267
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:33:00.273
  STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation @ 01/10/24 16:33:00.278
  Jan 10 16:33:00.279: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  E0110 16:33:00.289622      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:33:01.290402      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:33:02.291299      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:33:03.292142      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:33:04.292588      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:33:05.292813      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:33:06.293201      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:33:07.293766      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:33:08.293602      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:33:09.294379      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:33:10.295081      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:33:11.295835      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:33:12.296172      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:33:13.297138      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:33:14.298527      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:33:15.300046      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:33:16.300884      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:33:17.301065      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:33:18.301762      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:33:19.302639      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation @ 01/10/24 16:33:19.565
  Jan 10 16:33:19.567: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  E0110 16:33:20.303127      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:33:21.303759      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:33:22.304255      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:33:23.305363      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:33:24.305557      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:33:25.306421      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:33:26.306919      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:33:27.307090      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:33:28.307935      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:33:28.315: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  E0110 16:33:29.309173      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:33:30.309825      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:33:31.309986      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:33:32.310084      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:33:33.310852      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:33:34.312963      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:33:35.313524      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:33:36.315364      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:33:37.316246      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:33:38.316933      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:33:39.317983      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:33:40.319014      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:33:41.319847      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:33:42.320542      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:33:43.320686      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:33:44.321695      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:33:45.322416      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:33:46.323472      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:33:46.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-9905" for this suite. @ 01/10/24 16:33:46.586
• [46.372 seconds]
------------------------------
[sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]
test/e2e/network/service.go:1533
  STEP: Creating a kubernetes client @ 01/10/24 16:33:46.612
  Jan 10 16:33:46.612: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename services @ 01/10/24 16:33:46.617
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:33:46.676
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:33:46.681
  STEP: creating a service nodeport-service with the type=NodePort in namespace services-7674 @ 01/10/24 16:33:46.69
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 01/10/24 16:33:46.77
  STEP: creating service externalsvc in namespace services-7674 @ 01/10/24 16:33:46.771
  STEP: creating replication controller externalsvc in namespace services-7674 @ 01/10/24 16:33:46.839
  I0110 16:33:46.906733      23 runners.go:194] Created replication controller with name: externalsvc, namespace: services-7674, replica count: 2
  E0110 16:33:47.323788      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:33:48.324061      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:33:49.324834      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0110 16:33:49.958516      23 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the NodePort service to type=ExternalName @ 01/10/24 16:33:49.968
  Jan 10 16:33:50.014: INFO: Creating new exec pod
  E0110 16:33:50.325549      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:33:51.326001      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:33:52.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=services-7674 exec execpodzqpm2 -- /bin/sh -x -c nslookup nodeport-service.services-7674.svc.cluster.local'
  E0110 16:33:52.326822      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:33:52.541: INFO: stderr: "+ nslookup nodeport-service.services-7674.svc.cluster.local\n"
  Jan 10 16:33:52.541: INFO: stdout: "Server:\t\t169.254.25.10\nAddress:\t169.254.25.10#53\n\nnodeport-service.services-7674.svc.cluster.local\tcanonical name = externalsvc.services-7674.svc.cluster.local.\nName:\texternalsvc.services-7674.svc.cluster.local\nAddress: 10.233.28.113\n\n"
  Jan 10 16:33:52.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-7674, will wait for the garbage collector to delete the pods @ 01/10/24 16:33:52.556
  Jan 10 16:33:52.640: INFO: Deleting ReplicationController externalsvc took: 23.783937ms
  Jan 10 16:33:52.741: INFO: Terminating ReplicationController externalsvc pods took: 101.142175ms
  E0110 16:33:53.327893      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:33:54.328044      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:33:54.881: INFO: Cleaning up the NodePort to ExternalName test service
  STEP: Destroying namespace "services-7674" for this suite. @ 01/10/24 16:33:54.953
• [8.380 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]
test/e2e/common/node/expansion.go:300
  STEP: Creating a kubernetes client @ 01/10/24 16:33:54.994
  Jan 10 16:33:54.994: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename var-expansion @ 01/10/24 16:33:54.996
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:33:55.069
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:33:55.074
  STEP: creating the pod @ 01/10/24 16:33:55.078
  STEP: waiting for pod running @ 01/10/24 16:33:55.096
  E0110 16:33:55.329109      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:33:56.330018      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating a file in subpath @ 01/10/24 16:33:57.118
  Jan 10 16:33:57.125: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-8003 PodName:var-expansion-6438a576-5369-4847-97b6-30f9d63c4341 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 10 16:33:57.125: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  Jan 10 16:33:57.126: INFO: ExecWithOptions: Clientset creation
  Jan 10 16:33:57.127: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-8003/pods/var-expansion-6438a576-5369-4847-97b6-30f9d63c4341/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: test for file in mounted path @ 01/10/24 16:33:57.253
  Jan 10 16:33:57.261: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-8003 PodName:var-expansion-6438a576-5369-4847-97b6-30f9d63c4341 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 10 16:33:57.261: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  Jan 10 16:33:57.262: INFO: ExecWithOptions: Clientset creation
  Jan 10 16:33:57.262: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-8003/pods/var-expansion-6438a576-5369-4847-97b6-30f9d63c4341/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  E0110 16:33:57.330271      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: updating the annotation value @ 01/10/24 16:33:57.414
  Jan 10 16:33:57.958: INFO: Successfully updated pod "var-expansion-6438a576-5369-4847-97b6-30f9d63c4341"
  STEP: waiting for annotated pod running @ 01/10/24 16:33:57.959
  STEP: deleting the pod gracefully @ 01/10/24 16:33:57.967
  Jan 10 16:33:57.968: INFO: Deleting pod "var-expansion-6438a576-5369-4847-97b6-30f9d63c4341" in namespace "var-expansion-8003"
  Jan 10 16:33:58.004: INFO: Wait up to 5m0s for pod "var-expansion-6438a576-5369-4847-97b6-30f9d63c4341" to be fully deleted
  E0110 16:33:58.331242      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:33:59.332220      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:00.332463      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:01.333389      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:02.333575      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:03.334178      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:04.334919      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:05.335651      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:06.335780      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:07.336670      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:08.336831      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:09.337900      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:10.337935      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:11.338890      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:12.339598      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:13.340273      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:14.341217      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:15.341667      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:16.341879      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:17.342836      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:18.343531      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:19.344061      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:20.345135      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:21.345973      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:22.346536      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:23.347051      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:24.347753      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:25.348235      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:26.348487      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:27.348509      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:28.349574      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:29.350596      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:34:30.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-8003" for this suite. @ 01/10/24 16:34:30.239
• [35.268 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]
test/e2e/common/node/expansion.go:115
  STEP: Creating a kubernetes client @ 01/10/24 16:34:30.263
  Jan 10 16:34:30.263: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename var-expansion @ 01/10/24 16:34:30.266
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:34:30.312
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:34:30.322
  STEP: Creating a pod to test substitution in volume subpath @ 01/10/24 16:34:30.329
  E0110 16:34:30.351694      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:31.352956      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:32.353627      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:33.353918      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:34.354496      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/10/24 16:34:34.385
  Jan 10 16:34:34.399: INFO: Trying to get logs from node env1-test-worker-1 pod var-expansion-3fa99cbe-e65c-4ab0-b3f2-ecc395e7b1be container dapi-container: <nil>
  STEP: delete the pod @ 01/10/24 16:34:34.478
  Jan 10 16:34:34.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-6559" for this suite. @ 01/10/24 16:34:34.571
• [4.361 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:240
  STEP: Creating a kubernetes client @ 01/10/24 16:34:34.627
  Jan 10 16:34:34.628: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename configmap @ 01/10/24 16:34:34.63
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:34:34.69
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:34:34.698
  STEP: Creating configMap with name cm-test-opt-del-d2e6c859-9fff-4849-ac80-1eac2ba407a1 @ 01/10/24 16:34:34.726
  STEP: Creating configMap with name cm-test-opt-upd-ade051c6-c74e-40a1-914a-ad7c43a14e4a @ 01/10/24 16:34:34.736
  STEP: Creating the pod @ 01/10/24 16:34:34.749
  E0110 16:34:35.354660      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:36.355265      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:37.355697      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:38.356401      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting configmap cm-test-opt-del-d2e6c859-9fff-4849-ac80-1eac2ba407a1 @ 01/10/24 16:34:38.918
  STEP: Updating configmap cm-test-opt-upd-ade051c6-c74e-40a1-914a-ad7c43a14e4a @ 01/10/24 16:34:38.944
  STEP: Creating configMap with name cm-test-opt-create-ab89d380-c92b-41df-a6ad-a24f520c3c11 @ 01/10/24 16:34:38.958
  STEP: waiting to observe update in volume @ 01/10/24 16:34:38.974
  E0110 16:34:39.356955      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:40.357069      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:41.357895      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:42.358475      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:43.358568      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:44.359112      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:45.360224      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:46.360803      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:47.361055      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:48.361232      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:49.362235      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:50.362371      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:51.363084      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:52.363188      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:53.364238      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:54.364636      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:55.364882      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:56.365597      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:57.366341      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:58.366701      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:34:59.367633      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:00.367846      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:01.368738      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:02.368919      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:03.368969      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:04.369249      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:05.369446      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:06.370011      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:07.371617      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:08.372367      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:09.373660      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:10.374630      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:11.374963      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:12.375891      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:13.376141      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:14.377253      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:15.377446      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:16.378308      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:17.378484      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:18.379179      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:19.380134      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:20.381066      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:21.382290      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:22.382387      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:23.382338      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:24.383240      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:25.383599      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:26.383748      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:27.383997      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:28.384449      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:29.385187      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:30.385454      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:31.386126      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:32.386235      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:33.386497      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:34.386683      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:35.386783      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:36.387107      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:37.387290      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:38.387375      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:39.388636      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:40.388739      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:41.389154      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:42.389274      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:43.389577      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:44.389990      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:45.389942      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:46.391171      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:47.391807      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:48.392728      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:49.393697      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:50.394142      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:51.394719      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:52.394934      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:53.395909      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:54.396934      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:55.397970      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:56.398142      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:57.398519      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:58.399064      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:35:59.399404      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:00.400237      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:01.401060      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:02.401622      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:03.402137      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:04.402484      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:05.403164      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:06.403884      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:07.404643      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:08.404854      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:09.405359      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:10.405655      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:36:10.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5139" for this suite. @ 01/10/24 16:36:10.469
• [95.870 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]
test/e2e/kubectl/kubectl.go:1673
  STEP: Creating a kubernetes client @ 01/10/24 16:36:10.501
  Jan 10 16:36:10.501: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename kubectl @ 01/10/24 16:36:10.504
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:36:10.559
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:36:10.566
  Jan 10 16:36:10.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-7357 version'
  Jan 10 16:36:10.738: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
  Jan 10 16:36:10.738: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.1\", GitCommit:\"4c9411232e10168d7b050c49a1b59f6df9d7ea4b\", GitTreeState:\"clean\", BuildDate:\"2023-04-14T13:21:19Z\", GoVersion:\"go1.20.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v5.0.1\nServer Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.5\", GitCommit:\"93e0d7146fb9c3e9f68aa41b2b4265b2fcdb0a4c\", GitTreeState:\"clean\", BuildDate:\"2023-08-24T00:42:11Z\", GoVersion:\"go1.20.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
  Jan 10 16:36:10.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7357" for this suite. @ 01/10/24 16:36:10.751
• [0.266 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
test/e2e/auth/service_accounts.go:529
  STEP: Creating a kubernetes client @ 01/10/24 16:36:10.769
  Jan 10 16:36:10.769: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename svcaccounts @ 01/10/24 16:36:10.771
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:36:10.807
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:36:10.814
  Jan 10 16:36:10.853: INFO: created pod
  E0110 16:36:11.406121      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:12.406788      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:13.407310      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:14.407992      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/10/24 16:36:14.907
  E0110 16:36:15.408172      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:16.409009      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:17.409483      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:18.409598      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:19.410817      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:20.411632      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:21.412027      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:22.412533      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:23.413357      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:24.413544      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:25.413711      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:26.413862      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:27.414324      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:28.414756      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:29.414898      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:30.414948      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:31.415290      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:32.416054      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:33.416453      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:34.417260      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:35.417784      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:36.418253      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:37.418630      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:38.418853      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:39.419952      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:40.420563      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:41.421001      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:42.421427      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:43.421920      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:44.422443      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:36:44.907: INFO: polling logs
  Jan 10 16:36:44.937: INFO: Pod logs: 
  I0110 16:36:11.765695       1 log.go:198] OK: Got token
  I0110 16:36:11.765775       1 log.go:198] validating with in-cluster discovery
  I0110 16:36:11.766333       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
  I0110 16:36:11.766402       1 log.go:198] Full, not-validated claims: 
  openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-5625:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1704905171, NotBefore:1704904571, IssuedAt:1704904571, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5625", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"548b85ec-28c0-4cc1-9286-830a37344870"}}}
  I0110 16:36:11.808311       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
  I0110 16:36:11.827373       1 log.go:198] OK: Validated signature on JWT
  I0110 16:36:11.827727       1 log.go:198] OK: Got valid claims from token!
  I0110 16:36:11.827792       1 log.go:198] Full, validated claims: 
  &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-5625:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1704905171, NotBefore:1704904571, IssuedAt:1704904571, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5625", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"548b85ec-28c0-4cc1-9286-830a37344870"}}}

  Jan 10 16:36:44.937: INFO: completed pod
  Jan 10 16:36:44.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-5625" for this suite. @ 01/10/24 16:36:44.984
• [34.239 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]
test/e2e/apimachinery/discovery.go:122
  STEP: Creating a kubernetes client @ 01/10/24 16:36:45.015
  Jan 10 16:36:45.015: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename discovery @ 01/10/24 16:36:45.017
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:36:45.073
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:36:45.083
  STEP: Setting up server cert @ 01/10/24 16:36:45.096
  E0110 16:36:45.422179      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:46.422415      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:47.422778      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:36:47.553: INFO: Checking APIGroup: apiregistration.k8s.io
  Jan 10 16:36:47.555: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
  Jan 10 16:36:47.555: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
  Jan 10 16:36:47.555: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
  Jan 10 16:36:47.555: INFO: Checking APIGroup: apps
  Jan 10 16:36:47.558: INFO: PreferredVersion.GroupVersion: apps/v1
  Jan 10 16:36:47.558: INFO: Versions found [{apps/v1 v1}]
  Jan 10 16:36:47.558: INFO: apps/v1 matches apps/v1
  Jan 10 16:36:47.558: INFO: Checking APIGroup: events.k8s.io
  Jan 10 16:36:47.560: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
  Jan 10 16:36:47.560: INFO: Versions found [{events.k8s.io/v1 v1}]
  Jan 10 16:36:47.560: INFO: events.k8s.io/v1 matches events.k8s.io/v1
  Jan 10 16:36:47.560: INFO: Checking APIGroup: authentication.k8s.io
  Jan 10 16:36:47.563: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
  Jan 10 16:36:47.563: INFO: Versions found [{authentication.k8s.io/v1 v1}]
  Jan 10 16:36:47.563: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
  Jan 10 16:36:47.563: INFO: Checking APIGroup: authorization.k8s.io
  Jan 10 16:36:47.565: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
  Jan 10 16:36:47.565: INFO: Versions found [{authorization.k8s.io/v1 v1}]
  Jan 10 16:36:47.565: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
  Jan 10 16:36:47.565: INFO: Checking APIGroup: autoscaling
  Jan 10 16:36:47.571: INFO: PreferredVersion.GroupVersion: autoscaling/v2
  Jan 10 16:36:47.571: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
  Jan 10 16:36:47.571: INFO: autoscaling/v2 matches autoscaling/v2
  Jan 10 16:36:47.571: INFO: Checking APIGroup: batch
  Jan 10 16:36:47.574: INFO: PreferredVersion.GroupVersion: batch/v1
  Jan 10 16:36:47.574: INFO: Versions found [{batch/v1 v1}]
  Jan 10 16:36:47.574: INFO: batch/v1 matches batch/v1
  Jan 10 16:36:47.574: INFO: Checking APIGroup: certificates.k8s.io
  Jan 10 16:36:47.576: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
  Jan 10 16:36:47.576: INFO: Versions found [{certificates.k8s.io/v1 v1}]
  Jan 10 16:36:47.576: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
  Jan 10 16:36:47.576: INFO: Checking APIGroup: networking.k8s.io
  Jan 10 16:36:47.579: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
  Jan 10 16:36:47.579: INFO: Versions found [{networking.k8s.io/v1 v1}]
  Jan 10 16:36:47.579: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
  Jan 10 16:36:47.579: INFO: Checking APIGroup: policy
  Jan 10 16:36:47.582: INFO: PreferredVersion.GroupVersion: policy/v1
  Jan 10 16:36:47.582: INFO: Versions found [{policy/v1 v1}]
  Jan 10 16:36:47.582: INFO: policy/v1 matches policy/v1
  Jan 10 16:36:47.582: INFO: Checking APIGroup: rbac.authorization.k8s.io
  Jan 10 16:36:47.585: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
  Jan 10 16:36:47.585: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
  Jan 10 16:36:47.585: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
  Jan 10 16:36:47.585: INFO: Checking APIGroup: storage.k8s.io
  Jan 10 16:36:47.588: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
  Jan 10 16:36:47.588: INFO: Versions found [{storage.k8s.io/v1 v1}]
  Jan 10 16:36:47.588: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
  Jan 10 16:36:47.588: INFO: Checking APIGroup: admissionregistration.k8s.io
  Jan 10 16:36:47.590: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
  Jan 10 16:36:47.590: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
  Jan 10 16:36:47.590: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
  Jan 10 16:36:47.590: INFO: Checking APIGroup: apiextensions.k8s.io
  Jan 10 16:36:47.593: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
  Jan 10 16:36:47.593: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
  Jan 10 16:36:47.593: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
  Jan 10 16:36:47.593: INFO: Checking APIGroup: scheduling.k8s.io
  Jan 10 16:36:47.596: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
  Jan 10 16:36:47.596: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
  Jan 10 16:36:47.596: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
  Jan 10 16:36:47.596: INFO: Checking APIGroup: coordination.k8s.io
  Jan 10 16:36:47.598: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
  Jan 10 16:36:47.598: INFO: Versions found [{coordination.k8s.io/v1 v1}]
  Jan 10 16:36:47.598: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
  Jan 10 16:36:47.598: INFO: Checking APIGroup: node.k8s.io
  Jan 10 16:36:47.600: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
  Jan 10 16:36:47.600: INFO: Versions found [{node.k8s.io/v1 v1}]
  Jan 10 16:36:47.600: INFO: node.k8s.io/v1 matches node.k8s.io/v1
  Jan 10 16:36:47.600: INFO: Checking APIGroup: discovery.k8s.io
  Jan 10 16:36:47.602: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
  Jan 10 16:36:47.603: INFO: Versions found [{discovery.k8s.io/v1 v1}]
  Jan 10 16:36:47.603: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
  Jan 10 16:36:47.603: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
  Jan 10 16:36:47.604: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
  Jan 10 16:36:47.604: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
  Jan 10 16:36:47.604: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
  Jan 10 16:36:47.604: INFO: Checking APIGroup: monitoring.coreos.com
  Jan 10 16:36:47.606: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
  Jan 10 16:36:47.606: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
  Jan 10 16:36:47.606: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
  Jan 10 16:36:47.606: INFO: Checking APIGroup: velero.io
  Jan 10 16:36:47.609: INFO: PreferredVersion.GroupVersion: velero.io/v1
  Jan 10 16:36:47.609: INFO: Versions found [{velero.io/v1 v1}]
  Jan 10 16:36:47.609: INFO: velero.io/v1 matches velero.io/v1
  Jan 10 16:36:47.609: INFO: Checking APIGroup: cns.vmware.com
  Jan 10 16:36:47.612: INFO: PreferredVersion.GroupVersion: cns.vmware.com/v1alpha1
  Jan 10 16:36:47.612: INFO: Versions found [{cns.vmware.com/v1alpha1 v1alpha1}]
  Jan 10 16:36:47.612: INFO: cns.vmware.com/v1alpha1 matches cns.vmware.com/v1alpha1
  Jan 10 16:36:47.612: INFO: Checking APIGroup: traefik.containo.us
  Jan 10 16:36:47.616: INFO: PreferredVersion.GroupVersion: traefik.containo.us/v1alpha1
  Jan 10 16:36:47.616: INFO: Versions found [{traefik.containo.us/v1alpha1 v1alpha1}]
  Jan 10 16:36:47.616: INFO: traefik.containo.us/v1alpha1 matches traefik.containo.us/v1alpha1
  Jan 10 16:36:47.616: INFO: Checking APIGroup: metrics.k8s.io
  Jan 10 16:36:47.619: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
  Jan 10 16:36:47.619: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
  Jan 10 16:36:47.619: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
  Jan 10 16:36:47.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "discovery-8657" for this suite. @ 01/10/24 16:36:47.633
• [2.636 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]
test/e2e/kubectl/kubectl.go:996
  STEP: Creating a kubernetes client @ 01/10/24 16:36:47.658
  Jan 10 16:36:47.659: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename kubectl @ 01/10/24 16:36:47.662
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:36:47.7
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:36:47.711
  STEP: create deployment with httpd image @ 01/10/24 16:36:47.72
  Jan 10 16:36:47.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-1888 create -f -'
  E0110 16:36:48.423952      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:49.424377      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:50.426074      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:36:50.452: INFO: stderr: ""
  Jan 10 16:36:50.453: INFO: stdout: "deployment.apps/httpd-deployment created\n"
  STEP: verify diff finds difference between live and declared image @ 01/10/24 16:36:50.453
  Jan 10 16:36:50.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-1888 diff -f -'
  E0110 16:36:51.425904      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:52.426362      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:36:52.635: INFO: rc: 1
  Jan 10 16:36:52.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-1888 delete -f -'
  Jan 10 16:36:52.826: INFO: stderr: ""
  Jan 10 16:36:52.826: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
  Jan 10 16:36:52.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1888" for this suite. @ 01/10/24 16:36:52.86
• [5.219 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:272
  STEP: Creating a kubernetes client @ 01/10/24 16:36:52.877
  Jan 10 16:36:52.878: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename namespaces @ 01/10/24 16:36:52.879
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:36:52.951
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:36:52.964
  STEP: creating a Namespace @ 01/10/24 16:36:52.98
  STEP: patching the Namespace @ 01/10/24 16:36:53.026
  STEP: get the Namespace and ensuring it has the label @ 01/10/24 16:36:53.046
  Jan 10 16:36:53.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-3099" for this suite. @ 01/10/24 16:36:53.079
  STEP: Destroying namespace "nspatchtest-8351c14a-c37b-48d5-9916-ef057abef007-8661" for this suite. @ 01/10/24 16:36:53.1
• [0.244 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:157
  STEP: Creating a kubernetes client @ 01/10/24 16:36:53.137
  Jan 10 16:36:53.137: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename emptydir @ 01/10/24 16:36:53.139
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:36:53.197
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:36:53.205
  STEP: Creating a pod to test emptydir volume type on node default medium @ 01/10/24 16:36:53.212
  E0110 16:36:53.426735      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:54.427734      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:55.428419      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:56.428672      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/10/24 16:36:57.288
  Jan 10 16:36:57.295: INFO: Trying to get logs from node env1-test-worker-1 pod pod-5d3c0f09-bde3-423e-a476-e047d3af49f3 container test-container: <nil>
  STEP: delete the pod @ 01/10/24 16:36:57.32
  Jan 10 16:36:57.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5394" for this suite. @ 01/10/24 16:36:57.377
• [4.254 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]
test/e2e/network/endpointslice.go:68
  STEP: Creating a kubernetes client @ 01/10/24 16:36:57.392
  Jan 10 16:36:57.394: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename endpointslice @ 01/10/24 16:36:57.396
  E0110 16:36:57.428644      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:36:57.435
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:36:57.441
  Jan 10 16:36:57.472: INFO: Endpoints addresses: [10.61.1.197 10.61.1.198 10.61.1.199] , ports: [6443]
  Jan 10 16:36:57.472: INFO: EndpointSlices addresses: [10.61.1.197 10.61.1.198 10.61.1.199] , ports: [6443]
  Jan 10 16:36:57.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-8715" for this suite. @ 01/10/24 16:36:57.489
• [0.113 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]
test/e2e/apps/disruption.go:164
  STEP: Creating a kubernetes client @ 01/10/24 16:36:57.507
  Jan 10 16:36:57.507: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename disruption @ 01/10/24 16:36:57.509
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:36:57.582
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:36:57.591
  STEP: Waiting for the pdb to be processed @ 01/10/24 16:36:57.616
  E0110 16:36:58.429975      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:36:59.430627      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating PodDisruptionBudget status @ 01/10/24 16:36:59.641
  STEP: Waiting for all pods to be running @ 01/10/24 16:36:59.683
  Jan 10 16:36:59.719: INFO: running pods: 0 < 1
  E0110 16:37:00.431623      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:37:01.432129      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: locating a running pod @ 01/10/24 16:37:01.731
  STEP: Waiting for the pdb to be processed @ 01/10/24 16:37:01.756
  STEP: Patching PodDisruptionBudget status @ 01/10/24 16:37:01.777
  STEP: Waiting for the pdb to be processed @ 01/10/24 16:37:01.8
  Jan 10 16:37:01.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-937" for this suite. @ 01/10/24 16:37:01.82
• [4.331 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events should delete a collection of events [Conformance]
test/e2e/instrumentation/core_events.go:175
  STEP: Creating a kubernetes client @ 01/10/24 16:37:01.84
  Jan 10 16:37:01.840: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename events @ 01/10/24 16:37:01.841
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:37:01.875
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:37:01.881
  STEP: Create set of events @ 01/10/24 16:37:01.886
  Jan 10 16:37:01.913: INFO: created test-event-1
  Jan 10 16:37:01.920: INFO: created test-event-2
  Jan 10 16:37:01.930: INFO: created test-event-3
  STEP: get a list of Events with a label in the current namespace @ 01/10/24 16:37:01.93
  STEP: delete collection of events @ 01/10/24 16:37:01.936
  Jan 10 16:37:01.936: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 01/10/24 16:37:01.975
  Jan 10 16:37:01.976: INFO: requesting list of events to confirm quantity
  Jan 10 16:37:01.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-5437" for this suite. @ 01/10/24 16:37:01.993
• [0.166 seconds]
------------------------------
SSSS
------------------------------
[sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]
test/e2e/network/dns.go:117
  STEP: Creating a kubernetes client @ 01/10/24 16:37:02.006
  Jan 10 16:37:02.006: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename dns @ 01/10/24 16:37:02.008
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:37:02.048
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:37:02.055
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2672.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-2672.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
   @ 01/10/24 16:37:02.061
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2672.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-2672.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
   @ 01/10/24 16:37:02.061
  STEP: creating a pod to probe /etc/hosts @ 01/10/24 16:37:02.061
  STEP: submitting the pod to kubernetes @ 01/10/24 16:37:02.061
  E0110 16:37:02.432205      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:37:03.432519      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 01/10/24 16:37:04.101
  STEP: looking for the results for each expected name from probers @ 01/10/24 16:37:04.109
  Jan 10 16:37:04.156: INFO: DNS probes using dns-2672/dns-test-5a746ac8-fabe-46cf-a2c9-91dbf0424a09 succeeded

  Jan 10 16:37:04.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 01/10/24 16:37:04.177
  STEP: Destroying namespace "dns-2672" for this suite. @ 01/10/24 16:37:04.221
• [2.244 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:163
  STEP: Creating a kubernetes client @ 01/10/24 16:37:04.257
  Jan 10 16:37:04.257: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename projected @ 01/10/24 16:37:04.259
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:37:04.333
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:37:04.357
  STEP: Creating the pod @ 01/10/24 16:37:04.365
  E0110 16:37:04.433037      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:37:05.434249      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:37:06.434242      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:37:07.027: INFO: Successfully updated pod "annotationupdatea9b92fa1-1370-489d-af18-d8c769994496"
  E0110 16:37:07.434398      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:37:08.434942      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:37:09.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8387" for this suite. @ 01/10/24 16:37:09.145
• [4.905 seconds]
------------------------------
SSS
------------------------------
[sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]
test/e2e/apps/controller_revision.go:124
  STEP: Creating a kubernetes client @ 01/10/24 16:37:09.163
  Jan 10 16:37:09.163: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename controllerrevisions @ 01/10/24 16:37:09.166
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:37:09.215
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:37:09.225
  STEP: Creating DaemonSet "e2e-jcvps-daemon-set" @ 01/10/24 16:37:09.354
  STEP: Check that daemon pods launch on every node of the cluster. @ 01/10/24 16:37:09.377
  Jan 10 16:37:09.387: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 16:37:09.388: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 16:37:09.388: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 16:37:09.395: INFO: Number of nodes with available pods controlled by daemonset e2e-jcvps-daemon-set: 0
  Jan 10 16:37:09.395: INFO: Node env1-test-worker-0 is running 0 daemon pod, expected 1
  E0110 16:37:09.435786      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:37:10.409: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 16:37:10.409: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 16:37:10.410: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 16:37:10.422: INFO: Number of nodes with available pods controlled by daemonset e2e-jcvps-daemon-set: 0
  Jan 10 16:37:10.423: INFO: Node env1-test-worker-0 is running 0 daemon pod, expected 1
  E0110 16:37:10.436722      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:37:11.417: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 16:37:11.417: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 16:37:11.418: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 16:37:11.430: INFO: Number of nodes with available pods controlled by daemonset e2e-jcvps-daemon-set: 2
  Jan 10 16:37:11.430: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-jcvps-daemon-set
  E0110 16:37:11.437582      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Confirm DaemonSet "e2e-jcvps-daemon-set" successfully created with "daemonset-name=e2e-jcvps-daemon-set" label @ 01/10/24 16:37:11.44
  STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-jcvps-daemon-set" @ 01/10/24 16:37:11.483
  Jan 10 16:37:11.510: INFO: Located ControllerRevision: "e2e-jcvps-daemon-set-b9cb47dcf"
  STEP: Patching ControllerRevision "e2e-jcvps-daemon-set-b9cb47dcf" @ 01/10/24 16:37:11.528
  Jan 10 16:37:11.549: INFO: e2e-jcvps-daemon-set-b9cb47dcf has been patched
  STEP: Create a new ControllerRevision @ 01/10/24 16:37:11.549
  Jan 10 16:37:11.582: INFO: Created ControllerRevision: e2e-jcvps-daemon-set-598fdbbf55
  STEP: Confirm that there are two ControllerRevisions @ 01/10/24 16:37:11.582
  Jan 10 16:37:11.582: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jan 10 16:37:11.593: INFO: Found 2 ControllerRevisions
  STEP: Deleting ControllerRevision "e2e-jcvps-daemon-set-b9cb47dcf" @ 01/10/24 16:37:11.593
  STEP: Confirm that there is only one ControllerRevision @ 01/10/24 16:37:11.613
  Jan 10 16:37:11.613: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jan 10 16:37:11.624: INFO: Found 1 ControllerRevisions
  STEP: Updating ControllerRevision "e2e-jcvps-daemon-set-598fdbbf55" @ 01/10/24 16:37:11.635
  Jan 10 16:37:11.659: INFO: e2e-jcvps-daemon-set-598fdbbf55 has been updated
  STEP: Generate another ControllerRevision by patching the Daemonset @ 01/10/24 16:37:11.659
  W0110 16:37:11.678613      23 warnings.go:70] unknown field "updateStrategy"
  STEP: Confirm that there are two ControllerRevisions @ 01/10/24 16:37:11.678
  Jan 10 16:37:11.678: INFO: Requesting list of ControllerRevisions to confirm quantity
  E0110 16:37:12.438434      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:37:12.699: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jan 10 16:37:12.710: INFO: Found 2 ControllerRevisions
  STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-jcvps-daemon-set-598fdbbf55=updated" @ 01/10/24 16:37:12.71
  STEP: Confirm that there is only one ControllerRevision @ 01/10/24 16:37:12.735
  Jan 10 16:37:12.735: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jan 10 16:37:12.745: INFO: Found 1 ControllerRevisions
  Jan 10 16:37:12.758: INFO: ControllerRevision "e2e-jcvps-daemon-set-566c649fd6" has revision 3
  STEP: Deleting DaemonSet "e2e-jcvps-daemon-set" @ 01/10/24 16:37:12.767
  STEP: deleting DaemonSet.extensions e2e-jcvps-daemon-set in namespace controllerrevisions-2640, will wait for the garbage collector to delete the pods @ 01/10/24 16:37:12.767
  Jan 10 16:37:12.846: INFO: Deleting DaemonSet.extensions e2e-jcvps-daemon-set took: 19.510211ms
  Jan 10 16:37:12.947: INFO: Terminating DaemonSet.extensions e2e-jcvps-daemon-set pods took: 101.306026ms
  E0110 16:37:13.438699      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:37:14.439868      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:37:14.670: INFO: Number of nodes with available pods controlled by daemonset e2e-jcvps-daemon-set: 0
  Jan 10 16:37:14.670: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-jcvps-daemon-set
  Jan 10 16:37:14.695: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"186792149"},"items":null}

  Jan 10 16:37:14.722: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"186792150"},"items":null}

  Jan 10 16:37:14.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "controllerrevisions-2640" for this suite. @ 01/10/24 16:37:14.817
• [5.692 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:174
  STEP: Creating a kubernetes client @ 01/10/24 16:37:14.86
  Jan 10 16:37:14.860: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename projected @ 01/10/24 16:37:14.863
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:37:14.91
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:37:14.916
  STEP: Creating configMap with name cm-test-opt-del-872a5c0a-e353-45bc-b01e-1e5784f3eccc @ 01/10/24 16:37:14.946
  STEP: Creating configMap with name cm-test-opt-upd-acef8b50-e2ec-4bd8-a89d-ed8b4310eef8 @ 01/10/24 16:37:14.966
  STEP: Creating the pod @ 01/10/24 16:37:14.979
  E0110 16:37:15.440928      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:37:16.441730      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting configmap cm-test-opt-del-872a5c0a-e353-45bc-b01e-1e5784f3eccc @ 01/10/24 16:37:17.108
  STEP: Updating configmap cm-test-opt-upd-acef8b50-e2ec-4bd8-a89d-ed8b4310eef8 @ 01/10/24 16:37:17.13
  STEP: Creating configMap with name cm-test-opt-create-0b95a955-b7dc-4d3a-9ccc-e672771567d7 @ 01/10/24 16:37:17.147
  STEP: waiting to observe update in volume @ 01/10/24 16:37:17.162
  E0110 16:37:17.442819      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:37:18.443837      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:37:19.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5372" for this suite. @ 01/10/24 16:37:19.257
• [4.419 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]
test/e2e/apimachinery/resource_quota.go:693
  STEP: Creating a kubernetes client @ 01/10/24 16:37:19.282
  Jan 10 16:37:19.284: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename resourcequota @ 01/10/24 16:37:19.289
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:37:19.323
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:37:19.331
  STEP: Creating a ResourceQuota with terminating scope @ 01/10/24 16:37:19.338
  STEP: Ensuring ResourceQuota status is calculated @ 01/10/24 16:37:19.349
  E0110 16:37:19.444604      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:37:20.445872      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota with not terminating scope @ 01/10/24 16:37:21.363
  STEP: Ensuring ResourceQuota status is calculated @ 01/10/24 16:37:21.393
  E0110 16:37:21.446489      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:37:22.446916      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a long running pod @ 01/10/24 16:37:23.405
  E0110 16:37:23.447680      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with not terminating scope captures the pod usage @ 01/10/24 16:37:23.449
  E0110 16:37:24.447858      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:37:25.448116      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with terminating scope ignored the pod usage @ 01/10/24 16:37:25.459
  E0110 16:37:26.449096      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:37:27.449546      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 01/10/24 16:37:27.474
  STEP: Ensuring resource quota status released the pod usage @ 01/10/24 16:37:27.523
  E0110 16:37:28.449883      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:37:29.450000      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a terminating pod @ 01/10/24 16:37:29.539
  STEP: Ensuring resource quota with terminating scope captures the pod usage @ 01/10/24 16:37:29.564
  E0110 16:37:30.450345      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:37:31.450908      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with not terminating scope ignored the pod usage @ 01/10/24 16:37:31.583
  E0110 16:37:32.451454      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:37:33.452145      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 01/10/24 16:37:33.59
  STEP: Ensuring resource quota status released the pod usage @ 01/10/24 16:37:33.624
  E0110 16:37:34.452584      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:37:35.453558      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:37:35.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-1449" for this suite. @ 01/10/24 16:37:35.658
• [16.397 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]
test/e2e/auth/service_accounts.go:161
  STEP: Creating a kubernetes client @ 01/10/24 16:37:35.684
  Jan 10 16:37:35.684: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename svcaccounts @ 01/10/24 16:37:35.686
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:37:35.732
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:37:35.739
  Jan 10 16:37:35.789: INFO: created pod pod-service-account-defaultsa
  Jan 10 16:37:35.789: INFO: pod pod-service-account-defaultsa service account token volume mount: true
  Jan 10 16:37:35.812: INFO: created pod pod-service-account-mountsa
  Jan 10 16:37:35.813: INFO: pod pod-service-account-mountsa service account token volume mount: true
  Jan 10 16:37:35.851: INFO: created pod pod-service-account-nomountsa
  Jan 10 16:37:35.851: INFO: pod pod-service-account-nomountsa service account token volume mount: false
  Jan 10 16:37:35.868: INFO: created pod pod-service-account-defaultsa-mountspec
  Jan 10 16:37:35.868: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
  Jan 10 16:37:35.902: INFO: created pod pod-service-account-mountsa-mountspec
  Jan 10 16:37:35.902: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
  Jan 10 16:37:35.917: INFO: created pod pod-service-account-nomountsa-mountspec
  Jan 10 16:37:35.917: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
  Jan 10 16:37:35.939: INFO: created pod pod-service-account-defaultsa-nomountspec
  Jan 10 16:37:35.939: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
  Jan 10 16:37:35.965: INFO: created pod pod-service-account-mountsa-nomountspec
  Jan 10 16:37:35.966: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
  Jan 10 16:37:35.984: INFO: created pod pod-service-account-nomountsa-nomountspec
  Jan 10 16:37:35.985: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
  Jan 10 16:37:35.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-6407" for this suite. @ 01/10/24 16:37:36.015
• [0.375 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:131
  STEP: Creating a kubernetes client @ 01/10/24 16:37:36.07
  Jan 10 16:37:36.070: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename downward-api @ 01/10/24 16:37:36.074
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:37:36.134
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:37:36.148
  STEP: Creating the pod @ 01/10/24 16:37:36.156
  E0110 16:37:36.455907      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:37:37.456258      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:37:38.456594      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:37:38.762: INFO: Successfully updated pod "labelsupdate69b72d32-ed64-4b48-9ac9-cb06937526d0"
  E0110 16:37:39.456975      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:37:40.457330      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:37:40.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9653" for this suite. @ 01/10/24 16:37:40.82
• [4.769 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Lease lease API should be available [Conformance]
test/e2e/common/node/lease.go:72
  STEP: Creating a kubernetes client @ 01/10/24 16:37:40.842
  Jan 10 16:37:40.842: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename lease-test @ 01/10/24 16:37:40.845
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:37:40.886
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:37:40.892
  Jan 10 16:37:41.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "lease-test-8650" for this suite. @ 01/10/24 16:37:41.111
• [0.293 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]
test/e2e/common/node/podtemplates.go:53
  STEP: Creating a kubernetes client @ 01/10/24 16:37:41.135
  Jan 10 16:37:41.135: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename podtemplate @ 01/10/24 16:37:41.137
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:37:41.221
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:37:41.24
  Jan 10 16:37:41.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-4416" for this suite. @ 01/10/24 16:37:41.436
  E0110 16:37:41.458279      23 retrywatcher.go:130] "Watch failed" err="context canceled"
• [0.327 seconds]
------------------------------
SS
------------------------------
[sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]
test/e2e/apps/job.go:513
  STEP: Creating a kubernetes client @ 01/10/24 16:37:41.464
  Jan 10 16:37:41.465: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename job @ 01/10/24 16:37:41.466
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:37:41.522
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:37:41.53
  STEP: Creating a job @ 01/10/24 16:37:41.537
  STEP: Ensuring active pods == parallelism @ 01/10/24 16:37:41.56
  E0110 16:37:42.461586      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:37:43.462723      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Orphaning one of the Job's Pods @ 01/10/24 16:37:43.584
  Jan 10 16:37:44.158: INFO: Successfully updated pod "adopt-release-bpbp4"
  STEP: Checking that the Job readopts the Pod @ 01/10/24 16:37:44.158
  E0110 16:37:44.462858      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:37:45.463617      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing the labels from the Job's Pod @ 01/10/24 16:37:46.181
  E0110 16:37:46.464839      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:37:46.733: INFO: Successfully updated pod "adopt-release-bpbp4"
  STEP: Checking that the Job releases the Pod @ 01/10/24 16:37:46.733
  E0110 16:37:47.464946      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:37:48.465734      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:37:48.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-3537" for this suite. @ 01/10/24 16:37:48.813
• [7.371 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:255
  STEP: Creating a kubernetes client @ 01/10/24 16:37:48.839
  Jan 10 16:37:48.839: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename init-container @ 01/10/24 16:37:48.843
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:37:48.926
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:37:48.935
  STEP: creating the pod @ 01/10/24 16:37:48.942
  Jan 10 16:37:48.943: INFO: PodSpec: initContainers in spec.initContainers
  E0110 16:37:49.466638      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:37:50.466837      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:37:51.467762      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:37:52.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-4721" for this suite. @ 01/10/24 16:37:52.221
• [3.412 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:69
  STEP: Creating a kubernetes client @ 01/10/24 16:37:52.254
  Jan 10 16:37:52.254: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename projected @ 01/10/24 16:37:52.256
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:37:52.324
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:37:52.331
  STEP: Creating a pod to test downward API volume plugin @ 01/10/24 16:37:52.336
  E0110 16:37:52.468451      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:37:53.469095      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:37:54.469692      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:37:55.469866      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/10/24 16:37:56.42
  Jan 10 16:37:56.437: INFO: Trying to get logs from node env1-test-worker-1 pod downwardapi-volume-fb9e3824-0df7-4904-9481-54cf73bb93e6 container client-container: <nil>
  STEP: delete the pod @ 01/10/24 16:37:56.456
  E0110 16:37:56.470220      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:37:56.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9479" for this suite. @ 01/10/24 16:37:56.513
• [4.278 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should patch a pod status [Conformance]
test/e2e/common/node/pods.go:1084
  STEP: Creating a kubernetes client @ 01/10/24 16:37:56.537
  Jan 10 16:37:56.537: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename pods @ 01/10/24 16:37:56.539
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:37:56.595
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:37:56.603
  STEP: Create a pod @ 01/10/24 16:37:56.612
  E0110 16:37:57.470661      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:37:58.471651      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching /status @ 01/10/24 16:37:58.659
  Jan 10 16:37:58.686: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
  Jan 10 16:37:58.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9785" for this suite. @ 01/10/24 16:37:58.699
• [2.179 seconds]
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]
test/e2e/apps/daemon_set.go:864
  STEP: Creating a kubernetes client @ 01/10/24 16:37:58.716
  Jan 10 16:37:58.716: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename daemonsets @ 01/10/24 16:37:58.719
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:37:58.76
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:37:58.769
  STEP: Creating simple DaemonSet "daemon-set" @ 01/10/24 16:37:58.885
  STEP: Check that daemon pods launch on every node of the cluster. @ 01/10/24 16:37:58.908
  Jan 10 16:37:58.929: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 16:37:58.930: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 16:37:58.930: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 16:37:58.944: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 10 16:37:58.944: INFO: Node env1-test-worker-0 is running 0 daemon pod, expected 1
  E0110 16:37:59.473985      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:37:59.960: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 16:37:59.961: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 16:37:59.961: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 16:37:59.970: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 10 16:37:59.971: INFO: Node env1-test-worker-0 is running 0 daemon pod, expected 1
  E0110 16:38:00.474584      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:38:00.960: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 16:38:00.961: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 16:38:00.961: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 16:38:00.971: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jan 10 16:38:00.971: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Getting /status @ 01/10/24 16:38:00.979
  Jan 10 16:38:01.005: INFO: Daemon Set daemon-set has Conditions: []
  STEP: updating the DaemonSet Status @ 01/10/24 16:38:01.005
  Jan 10 16:38:01.035: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the daemon set status to be updated @ 01/10/24 16:38:01.035
  Jan 10 16:38:01.039: INFO: Observed &DaemonSet event: ADDED
  Jan 10 16:38:01.039: INFO: Observed &DaemonSet event: MODIFIED
  Jan 10 16:38:01.040: INFO: Observed &DaemonSet event: MODIFIED
  Jan 10 16:38:01.040: INFO: Observed &DaemonSet event: MODIFIED
  Jan 10 16:38:01.040: INFO: Found daemon set daemon-set in namespace daemonsets-2457 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jan 10 16:38:01.040: INFO: Daemon set daemon-set has an updated status
  STEP: patching the DaemonSet Status @ 01/10/24 16:38:01.041
  STEP: watching for the daemon set status to be patched @ 01/10/24 16:38:01.064
  Jan 10 16:38:01.075: INFO: Observed &DaemonSet event: ADDED
  Jan 10 16:38:01.075: INFO: Observed &DaemonSet event: MODIFIED
  Jan 10 16:38:01.077: INFO: Observed &DaemonSet event: MODIFIED
  Jan 10 16:38:01.078: INFO: Observed &DaemonSet event: MODIFIED
  Jan 10 16:38:01.078: INFO: Observed daemon set daemon-set in namespace daemonsets-2457 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jan 10 16:38:01.078: INFO: Observed &DaemonSet event: MODIFIED
  Jan 10 16:38:01.078: INFO: Found daemon set daemon-set in namespace daemonsets-2457 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
  Jan 10 16:38:01.078: INFO: Daemon set daemon-set has a patched status
  STEP: Deleting DaemonSet "daemon-set" @ 01/10/24 16:38:01.088
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2457, will wait for the garbage collector to delete the pods @ 01/10/24 16:38:01.088
  Jan 10 16:38:01.164: INFO: Deleting DaemonSet.extensions daemon-set took: 14.851491ms
  Jan 10 16:38:01.265: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.369153ms
  E0110 16:38:01.475492      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:38:02.476432      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:38:02.773: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 10 16:38:02.773: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jan 10 16:38:02.781: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"186792781"},"items":null}

  Jan 10 16:38:02.787: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"186792781"},"items":null}

  Jan 10 16:38:02.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-2457" for this suite. @ 01/10/24 16:38:02.824
• [4.119 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:423
  STEP: Creating a kubernetes client @ 01/10/24 16:38:02.841
  Jan 10 16:38:02.841: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename configmap @ 01/10/24 16:38:02.842
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:38:02.901
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:38:02.907
  STEP: Creating configMap with name configmap-test-volume-628d95e7-6799-4f54-939e-a414f25a3a33 @ 01/10/24 16:38:02.913
  STEP: Creating a pod to test consume configMaps @ 01/10/24 16:38:02.93
  E0110 16:38:03.477206      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:38:04.477703      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:38:05.477732      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:38:06.478373      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/10/24 16:38:06.984
  Jan 10 16:38:06.997: INFO: Trying to get logs from node env1-test-worker-1 pod pod-configmaps-e2263a9f-bdac-4180-9cce-c7815af8134f container configmap-volume-test: <nil>
  STEP: delete the pod @ 01/10/24 16:38:07.021
  Jan 10 16:38:07.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9508" for this suite. @ 01/10/24 16:38:07.087
• [4.275 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:194
  STEP: Creating a kubernetes client @ 01/10/24 16:38:07.123
  Jan 10 16:38:07.123: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename downward-api @ 01/10/24 16:38:07.125
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:38:07.174
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:38:07.181
  STEP: Creating a pod to test downward API volume plugin @ 01/10/24 16:38:07.19
  E0110 16:38:07.478475      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:38:08.479548      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:38:09.480110      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:38:10.480315      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/10/24 16:38:11.256
  Jan 10 16:38:11.266: INFO: Trying to get logs from node env1-test-worker-1 pod downwardapi-volume-3b6da485-7578-4e9f-b6c2-859541da3377 container client-container: <nil>
  STEP: delete the pod @ 01/10/24 16:38:11.285
  Jan 10 16:38:11.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4771" for this suite. @ 01/10/24 16:38:11.34
• [4.243 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]
test/e2e/auth/service_accounts.go:78
  STEP: Creating a kubernetes client @ 01/10/24 16:38:11.375
  Jan 10 16:38:11.376: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename svcaccounts @ 01/10/24 16:38:11.379
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:38:11.425
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:38:11.436
  E0110 16:38:11.480894      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:38:12.480964      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:38:13.482125      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: reading a file in the container @ 01/10/24 16:38:13.511
  Jan 10 16:38:13.511: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3070 pod-service-account-b659c267-502a-4733-a993-3098897b5670 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
  STEP: reading a file in the container @ 01/10/24 16:38:13.83
  Jan 10 16:38:13.830: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3070 pod-service-account-b659c267-502a-4733-a993-3098897b5670 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
  STEP: reading a file in the container @ 01/10/24 16:38:14.16
  Jan 10 16:38:14.160: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3070 pod-service-account-b659c267-502a-4733-a993-3098897b5670 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
  E0110 16:38:14.482674      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:38:14.521: INFO: Got root ca configmap in namespace "svcaccounts-3070"
  Jan 10 16:38:14.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-3070" for this suite. @ 01/10/24 16:38:14.547
• [3.196 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]
test/e2e/apimachinery/watch.go:191
  STEP: Creating a kubernetes client @ 01/10/24 16:38:14.572
  Jan 10 16:38:14.572: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename watch @ 01/10/24 16:38:14.574
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:38:14.646
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:38:14.655
  STEP: creating a watch on configmaps @ 01/10/24 16:38:14.662
  STEP: creating a new configmap @ 01/10/24 16:38:14.664
  STEP: modifying the configmap once @ 01/10/24 16:38:14.678
  STEP: closing the watch once it receives two notifications @ 01/10/24 16:38:14.696
  Jan 10 16:38:14.697: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2907  82f85a98-6bb0-47e9-9876-11f68d79e2af 186792920 0 2024-01-10 16:38:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2024-01-10 16:38:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jan 10 16:38:14.698: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2907  82f85a98-6bb0-47e9-9876-11f68d79e2af 186792921 0 2024-01-10 16:38:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2024-01-10 16:38:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time, while the watch is closed @ 01/10/24 16:38:14.699
  STEP: creating a new watch on configmaps from the last resource version observed by the first watch @ 01/10/24 16:38:14.742
  STEP: deleting the configmap @ 01/10/24 16:38:14.744
  STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed @ 01/10/24 16:38:14.765
  Jan 10 16:38:14.765: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2907  82f85a98-6bb0-47e9-9876-11f68d79e2af 186792922 0 2024-01-10 16:38:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2024-01-10 16:38:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jan 10 16:38:14.766: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2907  82f85a98-6bb0-47e9-9876-11f68d79e2af 186792923 0 2024-01-10 16:38:14 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2024-01-10 16:38:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jan 10 16:38:14.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-2907" for this suite. @ 01/10/24 16:38:14.784
• [0.233 seconds]
------------------------------
S
------------------------------
[sig-apps] Job should apply changes to a job status [Conformance]
test/e2e/apps/job.go:642
  STEP: Creating a kubernetes client @ 01/10/24 16:38:14.805
  Jan 10 16:38:14.805: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename job @ 01/10/24 16:38:14.806
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:38:14.848
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:38:14.855
  STEP: Creating a job @ 01/10/24 16:38:14.862
  STEP: Ensure pods equal to parallelism count is attached to the job @ 01/10/24 16:38:14.875
  E0110 16:38:15.482939      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:38:16.483498      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching /status @ 01/10/24 16:38:16.885
  STEP: updating /status @ 01/10/24 16:38:16.899
  STEP: get /status @ 01/10/24 16:38:16.99
  Jan 10 16:38:17.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-3057" for this suite. @ 01/10/24 16:38:17.02
• [2.230 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:124
  STEP: Creating a kubernetes client @ 01/10/24 16:38:17.04
  Jan 10 16:38:17.040: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename configmap @ 01/10/24 16:38:17.042
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:38:17.082
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:38:17.089
  STEP: Creating configMap with name configmap-test-upd-a464773c-9f7f-4612-8897-9ba885fd3032 @ 01/10/24 16:38:17.108
  STEP: Creating the pod @ 01/10/24 16:38:17.121
  E0110 16:38:17.484118      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:38:18.484660      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating configmap configmap-test-upd-a464773c-9f7f-4612-8897-9ba885fd3032 @ 01/10/24 16:38:19.213
  STEP: waiting to observe update in volume @ 01/10/24 16:38:19.23
  E0110 16:38:19.485154      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:38:20.485891      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:38:21.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1039" for this suite. @ 01/10/24 16:38:21.319
• [4.329 seconds]
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]
test/e2e/kubectl/kubectl.go:1640
  STEP: Creating a kubernetes client @ 01/10/24 16:38:21.369
  Jan 10 16:38:21.369: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename kubectl @ 01/10/24 16:38:21.372
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:38:21.446
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:38:21.455
  STEP: creating Agnhost RC @ 01/10/24 16:38:21.466
  Jan 10 16:38:21.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-5602 create -f -'
  E0110 16:38:21.486163      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:38:22.311: INFO: stderr: ""
  Jan 10 16:38:22.311: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 01/10/24 16:38:22.311
  E0110 16:38:22.486650      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:38:23.330: INFO: Selector matched 1 pods for map[app:agnhost]
  Jan 10 16:38:23.330: INFO: Found 0 / 1
  E0110 16:38:23.487801      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:38:24.332: INFO: Selector matched 1 pods for map[app:agnhost]
  Jan 10 16:38:24.332: INFO: Found 1 / 1
  Jan 10 16:38:24.332: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  STEP: patching all pods @ 01/10/24 16:38:24.332
  Jan 10 16:38:24.358: INFO: Selector matched 1 pods for map[app:agnhost]
  Jan 10 16:38:24.358: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jan 10 16:38:24.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-5602 patch pod agnhost-primary-fxtgv -p {"metadata":{"annotations":{"x":"y"}}}'
  E0110 16:38:24.488270      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:38:24.652: INFO: stderr: ""
  Jan 10 16:38:24.652: INFO: stdout: "pod/agnhost-primary-fxtgv patched\n"
  STEP: checking annotations @ 01/10/24 16:38:24.652
  Jan 10 16:38:24.672: INFO: Selector matched 1 pods for map[app:agnhost]
  Jan 10 16:38:24.672: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jan 10 16:38:24.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5602" for this suite. @ 01/10/24 16:38:24.696
• [3.361 seconds]
------------------------------
SS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:168
  STEP: Creating a kubernetes client @ 01/10/24 16:38:24.731
  Jan 10 16:38:24.731: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 01/10/24 16:38:24.733
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:38:24.784
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:38:24.791
  STEP: create the container to handle the HTTPGet hook request. @ 01/10/24 16:38:24.814
  E0110 16:38:25.489715      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:38:26.491671      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 01/10/24 16:38:26.877
  E0110 16:38:27.491805      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:38:28.492209      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:38:29.492366      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:38:30.492993      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:38:31.494081      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:38:32.494788      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:38:33.494983      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:38:34.496094      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:38:35.496220      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:38:36.496600      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:38:37.497772      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:38:38.497777      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:38:39.498452      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:38:40.499035      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:38:41.499042      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:38:42.499317      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:38:43.499657      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:38:44.500071      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check poststart hook @ 01/10/24 16:38:45.028
  STEP: delete the pod with lifecycle hook @ 01/10/24 16:38:45.078
  E0110 16:38:45.501523      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:38:46.501674      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:38:47.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-640" for this suite. @ 01/10/24 16:38:47.134
• [22.419 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:89
  STEP: Creating a kubernetes client @ 01/10/24 16:38:47.165
  Jan 10 16:38:47.166: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename projected @ 01/10/24 16:38:47.168
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:38:47.225
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:38:47.234
  STEP: Creating configMap with name projected-configmap-test-volume-map-97386249-652e-4f87-8c87-e7a0f7f1e464 @ 01/10/24 16:38:47.241
  STEP: Creating a pod to test consume configMaps @ 01/10/24 16:38:47.255
  E0110 16:38:47.501824      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:38:48.502697      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:38:49.503563      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:38:50.504523      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/10/24 16:38:51.329
  Jan 10 16:38:51.353: INFO: Trying to get logs from node env1-test-worker-1 pod pod-projected-configmaps-12e71910-f630-4270-bb12-553a3430bbe8 container agnhost-container: <nil>
  STEP: delete the pod @ 01/10/24 16:38:51.381
  Jan 10 16:38:51.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7920" for this suite. @ 01/10/24 16:38:51.456
• [4.316 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:156
  STEP: Creating a kubernetes client @ 01/10/24 16:38:51.491
  Jan 10 16:38:51.491: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename runtimeclass @ 01/10/24 16:38:51.493
  E0110 16:38:51.504774      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:38:51.55
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:38:51.557
  STEP: Deleting RuntimeClass runtimeclass-160-delete-me @ 01/10/24 16:38:51.589
  STEP: Waiting for the RuntimeClass to disappear @ 01/10/24 16:38:51.616
  Jan 10 16:38:51.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-160" for this suite. @ 01/10/24 16:38:51.682
• [0.232 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:213
  STEP: Creating a kubernetes client @ 01/10/24 16:38:51.724
  Jan 10 16:38:51.724: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 01/10/24 16:38:51.726
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:38:51.767
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:38:51.775
  STEP: create the container to handle the HTTPGet hook request. @ 01/10/24 16:38:51.793
  E0110 16:38:52.505191      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:38:53.505864      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:38:54.505801      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:38:55.506365      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 01/10/24 16:38:55.855
  E0110 16:38:56.506977      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:38:57.507518      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod with lifecycle hook @ 01/10/24 16:38:57.899
  E0110 16:38:58.509831      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:38:59.508344      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:39:00.508760      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:39:01.509213      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check prestop hook @ 01/10/24 16:39:01.975
  Jan 10 16:39:01.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-6350" for this suite. @ 01/10/24 16:39:02.003
• [10.295 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:57
  STEP: Creating a kubernetes client @ 01/10/24 16:39:02.022
  Jan 10 16:39:02.022: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename configmap @ 01/10/24 16:39:02.024
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:39:02.065
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:39:02.072
  STEP: Creating configMap with name configmap-test-volume-8ac4a6a6-20d9-463e-80e3-38921b0046e0 @ 01/10/24 16:39:02.078
  STEP: Creating a pod to test consume configMaps @ 01/10/24 16:39:02.092
  E0110 16:39:02.509485      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:39:03.509833      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:39:04.510471      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:39:05.510925      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/10/24 16:39:06.156
  Jan 10 16:39:06.163: INFO: Trying to get logs from node env1-test-worker-1 pod pod-configmaps-0f58abe3-5d2b-4bc7-84e2-aa46e8eb65fe container agnhost-container: <nil>
  STEP: delete the pod @ 01/10/24 16:39:06.178
  Jan 10 16:39:06.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9182" for this suite. @ 01/10/24 16:39:06.236
• [4.228 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:89
  STEP: Creating a kubernetes client @ 01/10/24 16:39:06.253
  Jan 10 16:39:06.253: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename containers @ 01/10/24 16:39:06.255
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:39:06.292
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:39:06.298
  STEP: Creating a pod to test override all @ 01/10/24 16:39:06.304
  E0110 16:39:06.511753      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:39:07.512548      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:39:08.517920      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:39:09.518590      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/10/24 16:39:10.37
  Jan 10 16:39:10.382: INFO: Trying to get logs from node env1-test-worker-1 pod client-containers-1ee7ff1f-8bc9-48cd-af66-a88ef77c7c79 container agnhost-container: <nil>
  STEP: delete the pod @ 01/10/24 16:39:10.405
  Jan 10 16:39:10.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-8809" for this suite. @ 01/10/24 16:39:10.476
• [4.252 seconds]
------------------------------
SSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:347
  STEP: Creating a kubernetes client @ 01/10/24 16:39:10.507
  Jan 10 16:39:10.507: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename security-context-test @ 01/10/24 16:39:10.509
  E0110 16:39:10.518868      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:39:10.562
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:39:10.572
  E0110 16:39:11.520394      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:39:12.520752      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:39:13.521285      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:39:14.522355      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:39:14.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-457" for this suite. @ 01/10/24 16:39:14.656
• [4.168 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:177
  STEP: Creating a kubernetes client @ 01/10/24 16:39:14.677
  Jan 10 16:39:14.677: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename emptydir @ 01/10/24 16:39:14.678
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:39:14.732
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:39:14.746
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 01/10/24 16:39:14.77
  E0110 16:39:15.523103      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:39:16.523434      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:39:17.524080      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:39:18.524650      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/10/24 16:39:18.838
  Jan 10 16:39:18.849: INFO: Trying to get logs from node env1-test-worker-1 pod pod-b1379fd7-c3be-49e1-bf9a-7ba63da5b642 container test-container: <nil>
  STEP: delete the pod @ 01/10/24 16:39:18.867
  Jan 10 16:39:18.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4403" for this suite. @ 01/10/24 16:39:18.932
• [4.275 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]
test/e2e/apimachinery/webhook.go:260
  STEP: Creating a kubernetes client @ 01/10/24 16:39:18.952
  Jan 10 16:39:18.952: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename webhook @ 01/10/24 16:39:18.954
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:39:19.012
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:39:19.022
  STEP: Setting up server cert @ 01/10/24 16:39:19.093
  E0110 16:39:19.525029      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:39:20.525653      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:39:21.526751      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 01/10/24 16:39:21.611
  STEP: Deploying the webhook pod @ 01/10/24 16:39:21.633
  STEP: Wait for the deployment to be ready @ 01/10/24 16:39:21.676
  Jan 10 16:39:21.697: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0110 16:39:22.527730      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:39:23.527899      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 01/10/24 16:39:23.725
  STEP: Verifying the service has paired with the endpoint @ 01/10/24 16:39:23.769
  E0110 16:39:24.528252      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:39:24.769: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating pod webhook via the AdmissionRegistration API @ 01/10/24 16:39:24.785
  STEP: create a pod that should be updated by the webhook @ 01/10/24 16:39:24.829
  Jan 10 16:39:24.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8182" for this suite. @ 01/10/24 16:39:25.068
  STEP: Destroying namespace "webhook-markers-645" for this suite. @ 01/10/24 16:39:25.094
• [6.168 seconds]
------------------------------
SS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:57
  STEP: Creating a kubernetes client @ 01/10/24 16:39:25.122
  Jan 10 16:39:25.123: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename secrets @ 01/10/24 16:39:25.126
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:39:25.167
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:39:25.176
  STEP: Creating secret with name secret-test-c05fd120-c141-4b43-96a0-9a72192cdd06 @ 01/10/24 16:39:25.182
  STEP: Creating a pod to test consume secrets @ 01/10/24 16:39:25.199
  E0110 16:39:25.529295      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:39:26.530273      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:39:27.530990      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:39:28.531204      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/10/24 16:39:29.25
  Jan 10 16:39:29.263: INFO: Trying to get logs from node env1-test-worker-1 pod pod-secrets-5ae76c14-3f02-4f2d-b53e-a7bcaecd7516 container secret-volume-test: <nil>
  STEP: delete the pod @ 01/10/24 16:39:29.282
  Jan 10 16:39:29.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9865" for this suite. @ 01/10/24 16:39:29.339
• [4.241 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:268
  STEP: Creating a kubernetes client @ 01/10/24 16:39:29.365
  Jan 10 16:39:29.365: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename downward-api @ 01/10/24 16:39:29.368
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:39:29.45
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:39:29.458
  STEP: Creating a pod to test downward api env vars @ 01/10/24 16:39:29.464
  E0110 16:39:29.531713      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:39:30.532256      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:39:31.532882      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:39:32.533067      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:39:33.533832      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/10/24 16:39:33.535
  Jan 10 16:39:33.541: INFO: Trying to get logs from node env1-test-worker-1 pod downward-api-a61f63da-dd04-4d2d-bd0c-f18ac51fce81 container dapi-container: <nil>
  STEP: delete the pod @ 01/10/24 16:39:33.558
  Jan 10 16:39:33.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-262" for this suite. @ 01/10/24 16:39:33.618
• [4.276 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:647
  STEP: Creating a kubernetes client @ 01/10/24 16:39:33.672
  Jan 10 16:39:33.672: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename svcaccounts @ 01/10/24 16:39:33.674
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:39:33.716
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:39:33.721
  STEP: creating a ServiceAccount @ 01/10/24 16:39:33.73
  STEP: watching for the ServiceAccount to be added @ 01/10/24 16:39:33.764
  STEP: patching the ServiceAccount @ 01/10/24 16:39:33.776
  STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) @ 01/10/24 16:39:33.803
  STEP: deleting the ServiceAccount @ 01/10/24 16:39:33.82
  Jan 10 16:39:33.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-7654" for this suite. @ 01/10/24 16:39:33.878
• [0.233 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:145
  STEP: Creating a kubernetes client @ 01/10/24 16:39:33.914
  Jan 10 16:39:33.914: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename custom-resource-definition @ 01/10/24 16:39:33.916
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:39:33.967
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:39:33.975
  Jan 10 16:39:33.982: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  E0110 16:39:34.534047      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:39:35.534908      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:39:36.534893      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:39:37.535351      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:39:38.536063      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:39:39.537027      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:39:39.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-6484" for this suite. @ 01/10/24 16:39:39.674
• [5.787 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]
test/e2e/storage/subpath.go:80
  STEP: Creating a kubernetes client @ 01/10/24 16:39:39.718
  Jan 10 16:39:39.719: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename subpath @ 01/10/24 16:39:39.723
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:39:39.785
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:39:39.791
  STEP: Setting up data @ 01/10/24 16:39:39.797
  STEP: Creating pod pod-subpath-test-configmap-qmsf @ 01/10/24 16:39:39.833
  STEP: Creating a pod to test atomic-volume-subpath @ 01/10/24 16:39:39.833
  E0110 16:39:40.538111      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:39:41.538441      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:39:42.539592      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:39:43.540174      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:39:44.540987      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:39:45.541663      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:39:46.542114      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:39:47.542769      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:39:48.547860      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:39:49.546958      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:39:50.547787      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:39:51.548114      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:39:52.549267      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:39:53.550123      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:39:54.550955      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:39:55.551401      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:39:56.551493      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:39:57.552193      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:39:58.553132      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:39:59.553294      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:40:00.553631      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:40:01.553886      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:40:02.554780      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:40:03.555176      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/10/24 16:40:04.021
  Jan 10 16:40:04.029: INFO: Trying to get logs from node env1-test-worker-1 pod pod-subpath-test-configmap-qmsf container test-container-subpath-configmap-qmsf: <nil>
  STEP: delete the pod @ 01/10/24 16:40:04.046
  STEP: Deleting pod pod-subpath-test-configmap-qmsf @ 01/10/24 16:40:04.085
  Jan 10 16:40:04.085: INFO: Deleting pod "pod-subpath-test-configmap-qmsf" in namespace "subpath-7412"
  Jan 10 16:40:04.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-7412" for this suite. @ 01/10/24 16:40:04.105
• [24.406 seconds]
------------------------------
SSSS
------------------------------
[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2165
  STEP: Creating a kubernetes client @ 01/10/24 16:40:04.126
  Jan 10 16:40:04.126: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename services @ 01/10/24 16:40:04.128
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:40:04.17
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:40:04.182
  STEP: creating service in namespace services-1940 @ 01/10/24 16:40:04.188
  STEP: creating service affinity-clusterip in namespace services-1940 @ 01/10/24 16:40:04.189
  STEP: creating replication controller affinity-clusterip in namespace services-1940 @ 01/10/24 16:40:04.244
  I0110 16:40:04.264791      23 runners.go:194] Created replication controller with name: affinity-clusterip, namespace: services-1940, replica count: 3
  E0110 16:40:04.556916      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:40:05.556401      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:40:06.557216      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0110 16:40:07.316359      23 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jan 10 16:40:07.333: INFO: Creating new exec pod
  E0110 16:40:07.557749      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:40:08.559056      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:40:09.559208      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:40:10.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=services-1940 exec execpod-affinitykxg54 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
  E0110 16:40:10.559984      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:40:10.763: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
  Jan 10 16:40:10.763: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 10 16:40:10.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=services-1940 exec execpod-affinitykxg54 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.48.116 80'
  Jan 10 16:40:11.090: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.48.116 80\nConnection to 10.233.48.116 80 port [tcp/http] succeeded!\n"
  Jan 10 16:40:11.090: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 10 16:40:11.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=services-1940 exec execpod-affinitykxg54 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.48.116:80/ ; done'
  E0110 16:40:11.561101      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:40:11.607: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.116:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.116:80/\n"
  Jan 10 16:40:11.607: INFO: stdout: "\naffinity-clusterip-w7xlf\naffinity-clusterip-w7xlf\naffinity-clusterip-w7xlf\naffinity-clusterip-w7xlf\naffinity-clusterip-w7xlf\naffinity-clusterip-w7xlf\naffinity-clusterip-w7xlf\naffinity-clusterip-w7xlf\naffinity-clusterip-w7xlf\naffinity-clusterip-w7xlf\naffinity-clusterip-w7xlf\naffinity-clusterip-w7xlf\naffinity-clusterip-w7xlf\naffinity-clusterip-w7xlf\naffinity-clusterip-w7xlf\naffinity-clusterip-w7xlf"
  Jan 10 16:40:11.607: INFO: Received response from host: affinity-clusterip-w7xlf
  Jan 10 16:40:11.607: INFO: Received response from host: affinity-clusterip-w7xlf
  Jan 10 16:40:11.607: INFO: Received response from host: affinity-clusterip-w7xlf
  Jan 10 16:40:11.607: INFO: Received response from host: affinity-clusterip-w7xlf
  Jan 10 16:40:11.607: INFO: Received response from host: affinity-clusterip-w7xlf
  Jan 10 16:40:11.607: INFO: Received response from host: affinity-clusterip-w7xlf
  Jan 10 16:40:11.607: INFO: Received response from host: affinity-clusterip-w7xlf
  Jan 10 16:40:11.607: INFO: Received response from host: affinity-clusterip-w7xlf
  Jan 10 16:40:11.607: INFO: Received response from host: affinity-clusterip-w7xlf
  Jan 10 16:40:11.607: INFO: Received response from host: affinity-clusterip-w7xlf
  Jan 10 16:40:11.607: INFO: Received response from host: affinity-clusterip-w7xlf
  Jan 10 16:40:11.607: INFO: Received response from host: affinity-clusterip-w7xlf
  Jan 10 16:40:11.607: INFO: Received response from host: affinity-clusterip-w7xlf
  Jan 10 16:40:11.607: INFO: Received response from host: affinity-clusterip-w7xlf
  Jan 10 16:40:11.607: INFO: Received response from host: affinity-clusterip-w7xlf
  Jan 10 16:40:11.607: INFO: Received response from host: affinity-clusterip-w7xlf
  Jan 10 16:40:11.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jan 10 16:40:11.618: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip in namespace services-1940, will wait for the garbage collector to delete the pods @ 01/10/24 16:40:11.647
  Jan 10 16:40:11.727: INFO: Deleting ReplicationController affinity-clusterip took: 18.848823ms
  Jan 10 16:40:11.828: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.98232ms
  E0110 16:40:12.561968      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:40:13.562770      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-1940" for this suite. @ 01/10/24 16:40:14.496
• [10.395 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]
test/e2e/apimachinery/webhook.go:118
  STEP: Creating a kubernetes client @ 01/10/24 16:40:14.523
  Jan 10 16:40:14.523: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename webhook @ 01/10/24 16:40:14.525
  E0110 16:40:14.563681      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:40:14.578
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:40:14.587
  STEP: Setting up server cert @ 01/10/24 16:40:14.664
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 01/10/24 16:40:15.3
  STEP: Deploying the webhook pod @ 01/10/24 16:40:15.318
  STEP: Wait for the deployment to be ready @ 01/10/24 16:40:15.35
  Jan 10 16:40:15.381: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0110 16:40:15.564304      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:40:16.564313      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 01/10/24 16:40:17.408
  STEP: Verifying the service has paired with the endpoint @ 01/10/24 16:40:17.429
  E0110 16:40:17.564821      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:40:18.429: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: fetching the /apis discovery document @ 01/10/24 16:40:18.438
  STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document @ 01/10/24 16:40:18.441
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document @ 01/10/24 16:40:18.441
  STEP: fetching the /apis/admissionregistration.k8s.io discovery document @ 01/10/24 16:40:18.441
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document @ 01/10/24 16:40:18.443
  STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document @ 01/10/24 16:40:18.444
  STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document @ 01/10/24 16:40:18.446
  Jan 10 16:40:18.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0110 16:40:18.565713      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-3844" for this suite. @ 01/10/24 16:40:18.634
  STEP: Destroying namespace "webhook-markers-5182" for this suite. @ 01/10/24 16:40:18.659
• [4.166 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]
test/e2e/apimachinery/crd_watch.go:51
  STEP: Creating a kubernetes client @ 01/10/24 16:40:18.711
  Jan 10 16:40:18.711: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename crd-watch @ 01/10/24 16:40:18.715
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:40:18.76
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:40:18.77
  Jan 10 16:40:18.782: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  E0110 16:40:19.565969      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:40:20.566701      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:40:21.567848      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:40:22.568158      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:40:23.568879      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:40:24.569821      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:40:25.570220      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating first CR  @ 01/10/24 16:40:26.429
  Jan 10 16:40:26.440: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-01-10T16:40:26Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-01-10T16:40:26Z]] name:name1 resourceVersion:186794201 uid:d09c5932-5a15-4b64-8dfe-d6871bab4287] num:map[num1:9223372036854775807 num2:1000000]]}
  E0110 16:40:26.570320      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:40:27.570702      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:40:28.570896      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:40:29.571491      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:40:30.571755      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:40:31.572497      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:40:32.573048      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:40:33.573428      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:40:34.574467      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:40:35.575427      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating second CR @ 01/10/24 16:40:36.441
  Jan 10 16:40:36.456: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-01-10T16:40:36Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-01-10T16:40:36Z]] name:name2 resourceVersion:186794241 uid:5ef44326-96ae-46b0-a0b5-04a8ae81d89e] num:map[num1:9223372036854775807 num2:1000000]]}
  E0110 16:40:36.575641      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:40:37.576186      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:40:38.579877      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:40:39.580411      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:40:40.580934      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:40:41.581546      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:40:42.581641      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:40:43.581977      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:40:44.582771      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:40:45.583092      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Modifying first CR @ 01/10/24 16:40:46.456
  Jan 10 16:40:46.479: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-01-10T16:40:26Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-01-10T16:40:46Z]] name:name1 resourceVersion:186794281 uid:d09c5932-5a15-4b64-8dfe-d6871bab4287] num:map[num1:9223372036854775807 num2:1000000]]}
  E0110 16:40:46.583170      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:40:47.583844      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:40:48.583876      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:40:49.584852      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:40:50.585932      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:40:51.586070      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:40:52.586814      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:40:53.587348      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:40:54.588269      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:40:55.588813      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Modifying second CR @ 01/10/24 16:40:56.48
  Jan 10 16:40:56.504: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-01-10T16:40:36Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-01-10T16:40:56Z]] name:name2 resourceVersion:186794320 uid:5ef44326-96ae-46b0-a0b5-04a8ae81d89e] num:map[num1:9223372036854775807 num2:1000000]]}
  E0110 16:40:56.589884      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:40:57.590510      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:40:58.590668      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:40:59.590771      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:41:00.590945      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:41:01.591901      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:41:02.591445      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:41:03.593407      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:41:04.592572      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:41:05.592698      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting first CR @ 01/10/24 16:41:06.504
  Jan 10 16:41:06.526: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-01-10T16:40:26Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-01-10T16:40:46Z]] name:name1 resourceVersion:186794358 uid:d09c5932-5a15-4b64-8dfe-d6871bab4287] num:map[num1:9223372036854775807 num2:1000000]]}
  E0110 16:41:06.593447      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:41:07.594600      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:41:08.594683      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:41:09.595435      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:41:10.595747      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:41:11.596077      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:41:12.596161      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:41:13.596725      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:41:14.597037      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:41:15.597739      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting second CR @ 01/10/24 16:41:16.527
  Jan 10 16:41:16.552: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2024-01-10T16:40:36Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2024-01-10T16:40:56Z]] name:name2 resourceVersion:186794397 uid:5ef44326-96ae-46b0-a0b5-04a8ae81d89e] num:map[num1:9223372036854775807 num2:1000000]]}
  E0110 16:41:16.598558      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:41:17.598820      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:41:18.598931      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:41:19.599705      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:41:20.600159      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:41:21.600719      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:41:22.601144      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:41:23.601105      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:41:24.601803      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:41:25.602013      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:41:26.602088      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:41:27.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-watch-8013" for this suite. @ 01/10/24 16:41:27.111
• [68.436 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:85
  STEP: Creating a kubernetes client @ 01/10/24 16:41:27.149
  Jan 10 16:41:27.149: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename projected @ 01/10/24 16:41:27.152
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:41:27.202
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:41:27.21
  STEP: Creating a pod to test downward API volume plugin @ 01/10/24 16:41:27.219
  E0110 16:41:27.602736      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:41:28.603076      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:41:29.604005      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:41:30.604559      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/10/24 16:41:31.273
  Jan 10 16:41:31.285: INFO: Trying to get logs from node env1-test-worker-1 pod downwardapi-volume-8ff892c2-2b98-49ff-a65c-7b11b3e70339 container client-container: <nil>
  STEP: delete the pod @ 01/10/24 16:41:31.305
  Jan 10 16:41:31.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9881" for this suite. @ 01/10/24 16:41:31.4
• [4.267 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]
test/e2e/kubectl/kubectl.go:1775
  STEP: Creating a kubernetes client @ 01/10/24 16:41:31.421
  Jan 10 16:41:31.422: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename kubectl @ 01/10/24 16:41:31.424
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:41:31.48
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:41:31.49
  STEP: starting the proxy server @ 01/10/24 16:41:31.499
  Jan 10 16:41:31.501: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-3053 proxy -p 0 --disable-filter'
  E0110 16:41:31.605583      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: curling proxy /api/ output @ 01/10/24 16:41:31.662
  Jan 10 16:41:31.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3053" for this suite. @ 01/10/24 16:41:31.702
• [0.298 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:645
  STEP: Creating a kubernetes client @ 01/10/24 16:41:31.724
  Jan 10 16:41:31.724: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename webhook @ 01/10/24 16:41:31.726
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:41:31.776
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:41:31.782
  STEP: Setting up server cert @ 01/10/24 16:41:31.852
  E0110 16:41:32.606674      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 01/10/24 16:41:32.624
  STEP: Deploying the webhook pod @ 01/10/24 16:41:32.646
  STEP: Wait for the deployment to be ready @ 01/10/24 16:41:32.694
  Jan 10 16:41:32.738: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0110 16:41:33.606850      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:41:34.608853      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 01/10/24 16:41:34.76
  STEP: Verifying the service has paired with the endpoint @ 01/10/24 16:41:34.782
  E0110 16:41:35.609118      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:41:35.782: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 01/10/24 16:41:35.953
  STEP: Creating a configMap that should be mutated @ 01/10/24 16:41:35.988
  STEP: Deleting the collection of validation webhooks @ 01/10/24 16:41:36.085
  STEP: Creating a configMap that should not be mutated @ 01/10/24 16:41:36.234
  Jan 10 16:41:36.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8765" for this suite. @ 01/10/24 16:41:36.468
  STEP: Destroying namespace "webhook-markers-6236" for this suite. @ 01/10/24 16:41:36.497
• [4.819 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]
test/e2e/apimachinery/resource_quota.go:161
  STEP: Creating a kubernetes client @ 01/10/24 16:41:36.552
  Jan 10 16:41:36.553: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename resourcequota @ 01/10/24 16:41:36.556
  E0110 16:41:36.609532      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:41:36.684
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:41:36.693
  STEP: Discovering how many secrets are in namespace by default @ 01/10/24 16:41:36.7
  E0110 16:41:37.609986      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:41:38.611194      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:41:39.612143      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:41:40.612287      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:41:41.612618      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Counting existing ResourceQuota @ 01/10/24 16:41:41.712
  E0110 16:41:42.612859      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:41:43.613070      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:41:44.613931      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:41:45.614510      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:41:46.614680      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 01/10/24 16:41:46.731
  STEP: Ensuring resource quota status is calculated @ 01/10/24 16:41:46.751
  E0110 16:41:47.615284      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:41:48.615963      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Secret @ 01/10/24 16:41:48.768
  STEP: Ensuring resource quota status captures secret creation @ 01/10/24 16:41:48.803
  E0110 16:41:49.616713      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:41:50.617614      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a secret @ 01/10/24 16:41:50.814
  STEP: Ensuring resource quota status released usage @ 01/10/24 16:41:50.831
  E0110 16:41:51.618039      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:41:52.618678      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:41:52.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6477" for this suite. @ 01/10/24 16:41:52.86
• [16.328 seconds]
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]
test/e2e/kubectl/kubectl.go:396
  STEP: Creating a kubernetes client @ 01/10/24 16:41:52.883
  Jan 10 16:41:52.883: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename kubectl @ 01/10/24 16:41:52.885
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:41:52.976
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:41:52.984
  STEP: creating all guestbook components @ 01/10/24 16:41:52.992
  Jan 10 16:41:52.992: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-replica
    labels:
      app: agnhost
      role: replica
      tier: backend
  spec:
    ports:
    - port: 6379
    selector:
      app: agnhost
      role: replica
      tier: backend

  Jan 10 16:41:52.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-9115 create -f -'
  E0110 16:41:53.619334      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:41:54.226: INFO: stderr: ""
  Jan 10 16:41:54.226: INFO: stdout: "service/agnhost-replica created\n"
  Jan 10 16:41:54.226: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-primary
    labels:
      app: agnhost
      role: primary
      tier: backend
  spec:
    ports:
    - port: 6379
      targetPort: 6379
    selector:
      app: agnhost
      role: primary
      tier: backend

  Jan 10 16:41:54.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-9115 create -f -'
  E0110 16:41:54.620604      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:41:55.328: INFO: stderr: ""
  Jan 10 16:41:55.329: INFO: stdout: "service/agnhost-primary created\n"
  Jan 10 16:41:55.329: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: frontend
    labels:
      app: guestbook
      tier: frontend
  spec:
    # if your cluster supports it, uncomment the following to automatically create
    # an external load-balanced IP for the frontend service.
    # type: LoadBalancer
    ports:
    - port: 80
    selector:
      app: guestbook
      tier: frontend

  Jan 10 16:41:55.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-9115 create -f -'
  E0110 16:41:55.620855      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:41:56.620936      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:41:57.621117      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:41:58.096: INFO: stderr: ""
  Jan 10 16:41:58.096: INFO: stdout: "service/frontend created\n"
  Jan 10 16:41:58.096: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: frontend
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: guestbook
        tier: frontend
    template:
      metadata:
        labels:
          app: guestbook
          tier: frontend
      spec:
        containers:
        - name: guestbook-frontend
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--backend-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 80

  Jan 10 16:41:58.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-9115 create -f -'
  E0110 16:41:58.621730      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:41:58.793: INFO: stderr: ""
  Jan 10 16:41:58.793: INFO: stdout: "deployment.apps/frontend created\n"
  Jan 10 16:41:58.794: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-primary
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: agnhost
        role: primary
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: primary
          tier: backend
      spec:
        containers:
        - name: primary
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Jan 10 16:41:58.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-9115 create -f -'
  Jan 10 16:41:59.510: INFO: stderr: ""
  Jan 10 16:41:59.510: INFO: stdout: "deployment.apps/agnhost-primary created\n"
  Jan 10 16:41:59.510: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-replica
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: agnhost
        role: replica
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: replica
          tier: backend
      spec:
        containers:
        - name: replica
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Jan 10 16:41:59.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-9115 create -f -'
  E0110 16:41:59.621983      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:42:00.353: INFO: stderr: ""
  Jan 10 16:42:00.354: INFO: stdout: "deployment.apps/agnhost-replica created\n"
  STEP: validating guestbook app @ 01/10/24 16:42:00.354
  Jan 10 16:42:00.354: INFO: Waiting for all frontend pods to be Running.
  E0110 16:42:00.622837      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:42:01.623461      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:42:02.623681      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:42:03.624042      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:42:04.625012      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:42:05.404: INFO: Waiting for frontend to serve content.
  Jan 10 16:42:05.425: INFO: Trying to add a new entry to the guestbook.
  Jan 10 16:42:05.449: INFO: Verifying that added entry can be retrieved.
  STEP: using delete to clean up resources @ 01/10/24 16:42:05.468
  Jan 10 16:42:05.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-9115 delete --grace-period=0 --force -f -'
  E0110 16:42:05.625530      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:42:05.692: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jan 10 16:42:05.692: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
  STEP: using delete to clean up resources @ 01/10/24 16:42:05.692
  Jan 10 16:42:05.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-9115 delete --grace-period=0 --force -f -'
  Jan 10 16:42:05.956: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jan 10 16:42:05.956: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 01/10/24 16:42:05.957
  Jan 10 16:42:05.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-9115 delete --grace-period=0 --force -f -'
  Jan 10 16:42:06.264: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jan 10 16:42:06.264: INFO: stdout: "service \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 01/10/24 16:42:06.264
  Jan 10 16:42:06.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-9115 delete --grace-period=0 --force -f -'
  Jan 10 16:42:06.455: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jan 10 16:42:06.455: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 01/10/24 16:42:06.455
  Jan 10 16:42:06.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-9115 delete --grace-period=0 --force -f -'
  E0110 16:42:06.626461      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:42:06.683: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jan 10 16:42:06.683: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 01/10/24 16:42:06.683
  Jan 10 16:42:06.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-9115 delete --grace-period=0 --force -f -'
  Jan 10 16:42:06.873: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jan 10 16:42:06.873: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
  Jan 10 16:42:06.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9115" for this suite. @ 01/10/24 16:42:06.886
• [14.024 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:95
  STEP: Creating a kubernetes client @ 01/10/24 16:42:06.908
  Jan 10 16:42:06.908: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename pod-network-test @ 01/10/24 16:42:06.909
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:42:06.952
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:42:06.96
  STEP: Performing setup for networking test in namespace pod-network-test-9876 @ 01/10/24 16:42:06.964
  STEP: creating a selector @ 01/10/24 16:42:06.964
  STEP: Creating the service pods in kubernetes @ 01/10/24 16:42:06.964
  Jan 10 16:42:06.964: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0110 16:42:07.626557      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:42:08.627223      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:42:09.628048      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:42:10.628705      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:42:11.629220      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:42:12.629574      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:42:13.632058      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:42:14.632926      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:42:15.633406      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:42:16.633602      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:42:17.633860      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:42:18.635411      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 01/10/24 16:42:19.14
  E0110 16:42:19.636504      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:42:20.637157      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:42:21.202: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
  Jan 10 16:42:21.202: INFO: Breadth first check of 10.233.67.132 on host 10.61.1.200...
  Jan 10 16:42:21.212: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.68.182:9080/dial?request=hostname&protocol=udp&host=10.233.67.132&port=8081&tries=1'] Namespace:pod-network-test-9876 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 10 16:42:21.212: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  Jan 10 16:42:21.213: INFO: ExecWithOptions: Clientset creation
  Jan 10 16:42:21.213: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9876/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.68.182%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.67.132%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jan 10 16:42:21.396: INFO: Waiting for responses: map[]
  Jan 10 16:42:21.396: INFO: reached 10.233.67.132 after 0/1 tries
  Jan 10 16:42:21.396: INFO: Breadth first check of 10.233.68.181 on host 10.61.1.201...
  Jan 10 16:42:21.406: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.68.182:9080/dial?request=hostname&protocol=udp&host=10.233.68.181&port=8081&tries=1'] Namespace:pod-network-test-9876 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jan 10 16:42:21.406: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  Jan 10 16:42:21.407: INFO: ExecWithOptions: Clientset creation
  Jan 10 16:42:21.407: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9876/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.68.182%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.68.181%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jan 10 16:42:21.592: INFO: Waiting for responses: map[]
  Jan 10 16:42:21.592: INFO: reached 10.233.68.181 after 0/1 tries
  Jan 10 16:42:21.592: INFO: Going to retry 0 out of 2 pods....
  Jan 10 16:42:21.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-9876" for this suite. @ 01/10/24 16:42:21.622
  E0110 16:42:21.637650      23 retrywatcher.go:130] "Watch failed" err="context canceled"
• [14.747 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] FieldValidation should create/apply a valid CR for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:168
  STEP: Creating a kubernetes client @ 01/10/24 16:42:21.657
  Jan 10 16:42:21.657: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename field-validation @ 01/10/24 16:42:21.659
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:42:21.704
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:42:21.711
  Jan 10 16:42:21.717: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  E0110 16:42:22.638683      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:42:23.639601      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:42:24.640503      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:42:25.641021      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:42:26.641479      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:42:27.642843      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:42:28.644088      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0110 16:42:29.589249      23 warnings.go:70] unknown field "alpha"
  W0110 16:42:29.589540      23 warnings.go:70] unknown field "beta"
  W0110 16:42:29.589660      23 warnings.go:70] unknown field "delta"
  W0110 16:42:29.589781      23 warnings.go:70] unknown field "epsilon"
  W0110 16:42:29.589901      23 warnings.go:70] unknown field "gamma"
  E0110 16:42:29.644481      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:42:29.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-666" for this suite. @ 01/10/24 16:42:29.699
• [8.062 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:236
  STEP: Creating a kubernetes client @ 01/10/24 16:42:29.719
  Jan 10 16:42:29.719: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename projected @ 01/10/24 16:42:29.721
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:42:29.825
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:42:29.831
  STEP: Creating a pod to test downward API volume plugin @ 01/10/24 16:42:29.838
  E0110 16:42:30.644775      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:42:31.645453      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:42:32.645521      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:42:33.646081      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/10/24 16:42:33.902
  Jan 10 16:42:33.911: INFO: Trying to get logs from node env1-test-worker-1 pod downwardapi-volume-bf7fdfaf-4aba-42d6-a54c-fdda114639f2 container client-container: <nil>
  STEP: delete the pod @ 01/10/24 16:42:33.939
  Jan 10 16:42:34.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1480" for this suite. @ 01/10/24 16:42:34.041
• [4.347 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]
test/e2e/apps/replica_set.go:165
  STEP: Creating a kubernetes client @ 01/10/24 16:42:34.066
  Jan 10 16:42:34.067: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename replicaset @ 01/10/24 16:42:34.068
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:42:34.121
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:42:34.129
  STEP: Create a ReplicaSet @ 01/10/24 16:42:34.14
  STEP: Verify that the required pods have come up @ 01/10/24 16:42:34.158
  Jan 10 16:42:34.167: INFO: Pod name sample-pod: Found 0 pods out of 3
  E0110 16:42:34.647106      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:42:35.647667      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:42:36.648681      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:42:37.649049      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:42:38.649736      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:42:39.184: INFO: Pod name sample-pod: Found 3 pods out of 3
  STEP: ensuring each pod is running @ 01/10/24 16:42:39.184
  Jan 10 16:42:39.192: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
  STEP: Listing all ReplicaSets @ 01/10/24 16:42:39.192
  STEP: DeleteCollection of the ReplicaSets @ 01/10/24 16:42:39.207
  STEP: After DeleteCollection verify that ReplicaSets have been deleted @ 01/10/24 16:42:39.235
  Jan 10 16:42:39.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-9058" for this suite. @ 01/10/24 16:42:39.342
• [5.351 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:183
  STEP: Creating a kubernetes client @ 01/10/24 16:42:39.42
  Jan 10 16:42:39.420: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename container-probe @ 01/10/24 16:42:39.422
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:42:39.511
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:42:39.52
  STEP: Creating pod liveness-1092598d-d7f9-4263-95a1-e6f448d8ed64 in namespace container-probe-2477 @ 01/10/24 16:42:39.53
  E0110 16:42:39.650592      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:42:40.651390      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:42:41.598: INFO: Started pod liveness-1092598d-d7f9-4263-95a1-e6f448d8ed64 in namespace container-probe-2477
  STEP: checking the pod's current state and verifying that restartCount is present @ 01/10/24 16:42:41.598
  Jan 10 16:42:41.609: INFO: Initial restart count of pod liveness-1092598d-d7f9-4263-95a1-e6f448d8ed64 is 0
  E0110 16:42:41.652206      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:42:42.652796      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:42:43.653968      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:42:44.654970      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:42:45.655484      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:42:46.656632      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:42:47.657641      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:42:48.657665      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:42:49.658401      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:42:50.658628      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:42:51.659784      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:42:52.660802      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:42:53.661019      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:42:54.662101      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:42:55.662188      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:42:56.662377      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:42:57.662615      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:42:58.663233      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:42:59.663651      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:00.663881      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:01.664162      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:02.664919      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:03.664721      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:04.665851      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:05.666470      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:06.667233      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:07.668182      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:08.668564      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:09.668762      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:10.669043      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:11.669671      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:12.669661      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:13.669870      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:14.670347      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:15.670574      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:16.671064      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:17.671241      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:18.671449      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:19.671834      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:20.672110      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:21.672878      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:22.672812      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:23.673038      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:24.673810      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:25.674912      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:26.674996      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:27.675752      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:28.675833      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:29.676599      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:30.676958      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:31.677170      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:32.677577      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:33.677834      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:34.677928      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:35.678172      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:36.678368      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:37.678594      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:38.678792      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:39.679693      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:40.680009      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:41.680155      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:42.680490      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:43.680556      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:44.680864      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:45.681470      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:46.682494      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:47.682920      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:48.683885      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:49.684570      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:50.684645      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:51.685185      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:52.686145      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:53.686672      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:54.686950      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:55.686983      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:56.688271      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:57.689218      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:58.689664      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:43:59.690733      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:00.691265      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:01.691271      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:02.691447      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:03.691963      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:04.692610      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:05.692739      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:06.693765      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:07.694682      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:08.694793      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:09.695917      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:10.696910      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:11.697387      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:12.698187      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:13.698710      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:14.699643      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:15.700750      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:16.701699      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:17.701915      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:18.701974      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:19.702970      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:20.703463      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:21.704211      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:22.705165      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:23.705501      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:24.706120      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:25.706975      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:26.707797      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:27.708061      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:28.708699      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:29.708902      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:30.709819      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:31.710000      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:32.710663      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:33.711392      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:34.711732      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:35.711991      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:36.712110      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:37.712353      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:38.712765      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:39.713134      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:40.713838      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:41.714079      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:42.715117      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:43.715316      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:44.716338      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:45.716466      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:46.717189      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:47.717703      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:48.718670      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:49.719660      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:50.720349      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:51.720713      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:52.720932      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:53.721649      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:54.721860      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:55.723007      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:56.723953      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:57.724063      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:58.724827      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:44:59.725243      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:00.725509      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:01.725684      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:02.725892      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:03.726917      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:04.726942      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:05.727840      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:06.728602      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:07.728689      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:08.728853      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:09.729581      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:10.730071      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:11.730551      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:12.730819      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:13.731260      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:14.732146      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:15.732304      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:16.732483      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:17.732693      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:18.733581      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:19.734480      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:20.735893      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:21.736083      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:22.736993      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:23.737441      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:24.737767      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:25.738159      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:26.738717      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:27.738850      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:28.738980      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:29.739880      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:30.740760      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:31.741687      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:32.742192      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:33.742731      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:34.743536      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:35.743899      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:36.744378      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:37.744950      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:38.745727      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:39.745688      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:40.746763      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:41.747424      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:42.748611      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:43.749793      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:44.750366      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:45.750892      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:46.751189      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:47.751759      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:48.751998      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:49.753393      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:50.753195      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:51.753549      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:52.754565      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:53.754710      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:54.755585      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:55.755901      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:56.756369      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:57.757043      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:58.757376      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:45:59.757577      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:00.758233      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:01.758763      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:02.759365      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:03.759943      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:04.760360      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:05.761437      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:06.761850      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:07.762067      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:08.762846      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:09.763796      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:10.764173      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:11.764518      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:12.764818      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:13.765696      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:14.765980      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:15.766202      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:16.766354      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:17.769775      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:18.770273      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:19.770750      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:20.771460      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:21.771757      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:22.772400      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:23.772465      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:24.773449      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:25.773701      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:26.774764      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:27.775760      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:28.776712      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:29.777535      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:30.778010      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:31.778308      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:32.778748      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:33.779016      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:34.780072      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:35.780344      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:36.780632      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:37.780833      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:38.781070      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:39.781509      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:40.781671      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:41.781885      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:42.782124      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:46:42.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 01/10/24 16:46:42.976
  STEP: Destroying namespace "container-probe-2477" for this suite. @ 01/10/24 16:46:43.016
• [243.620 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:110
  STEP: Creating a kubernetes client @ 01/10/24 16:46:43.057
  Jan 10 16:46:43.057: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename kubelet-test @ 01/10/24 16:46:43.059
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:46:43.12
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:46:43.131
  E0110 16:46:43.782819      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:44.783860      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:45.784323      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:46.784872      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:46:47.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-1316" for this suite. @ 01/10/24 16:46:47.222
• [4.188 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]
test/e2e/apimachinery/webhook.go:314
  STEP: Creating a kubernetes client @ 01/10/24 16:46:47.247
  Jan 10 16:46:47.247: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename webhook @ 01/10/24 16:46:47.251
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:46:47.297
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:46:47.303
  STEP: Setting up server cert @ 01/10/24 16:46:47.374
  E0110 16:46:47.784931      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 01/10/24 16:46:48.43
  STEP: Deploying the webhook pod @ 01/10/24 16:46:48.45
  STEP: Wait for the deployment to be ready @ 01/10/24 16:46:48.485
  Jan 10 16:46:48.522: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0110 16:46:48.785115      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:49.785888      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 01/10/24 16:46:50.558
  STEP: Verifying the service has paired with the endpoint @ 01/10/24 16:46:50.588
  E0110 16:46:50.786727      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:46:51.589: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jan 10 16:46:51.597: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  E0110 16:46:51.786782      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:52.787232      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:53.788074      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:54.788625      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:55.789441      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:56.790140      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2314-crds.webhook.example.com via the AdmissionRegistration API @ 01/10/24 16:46:57.137
  Jan 10 16:46:57.209: INFO: Waiting for webhook configuration to be ready...
  STEP: Creating a custom resource while v1 is storage version @ 01/10/24 16:46:57.333
  E0110 16:46:57.791253      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:46:58.792459      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Patching Custom Resource Definition to set v2 as storage @ 01/10/24 16:46:59.481
  STEP: Patching the custom resource while v2 is storage version @ 01/10/24 16:46:59.5
  Jan 10 16:46:59.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0110 16:46:59.797873      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-6388" for this suite. @ 01/10/24 16:47:00.312
  STEP: Destroying namespace "webhook-markers-9522" for this suite. @ 01/10/24 16:47:00.328
• [13.093 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]
test/e2e/storage/subpath.go:92
  STEP: Creating a kubernetes client @ 01/10/24 16:47:00.342
  Jan 10 16:47:00.342: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename subpath @ 01/10/24 16:47:00.345
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:47:00.397
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:47:00.406
  STEP: Setting up data @ 01/10/24 16:47:00.416
  STEP: Creating pod pod-subpath-test-downwardapi-glfb @ 01/10/24 16:47:00.432
  STEP: Creating a pod to test atomic-volume-subpath @ 01/10/24 16:47:00.432
  E0110 16:47:00.798154      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:01.799053      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:02.799832      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:03.800605      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:04.801071      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:05.801422      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:06.802301      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:07.802755      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:08.803189      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:09.803284      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:10.803497      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:11.803716      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:12.804094      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:13.804587      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:14.805079      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:15.805792      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:16.806307      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:17.806793      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:18.807492      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:19.808460      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:20.809673      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:21.809971      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:22.810179      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:23.810708      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/10/24 16:47:24.654
  Jan 10 16:47:24.664: INFO: Trying to get logs from node env1-test-worker-1 pod pod-subpath-test-downwardapi-glfb container test-container-subpath-downwardapi-glfb: <nil>
  STEP: delete the pod @ 01/10/24 16:47:24.702
  STEP: Deleting pod pod-subpath-test-downwardapi-glfb @ 01/10/24 16:47:24.76
  Jan 10 16:47:24.760: INFO: Deleting pod "pod-subpath-test-downwardapi-glfb" in namespace "subpath-4107"
  Jan 10 16:47:24.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-4107" for this suite. @ 01/10/24 16:47:24.784
• [24.462 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS  E0110 16:47:24.811282      23 retrywatcher.go:130] "Watch failed" err="context canceled"
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:107
  STEP: Creating a kubernetes client @ 01/10/24 16:47:24.815
  Jan 10 16:47:24.816: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename container-probe @ 01/10/24 16:47:24.817
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:47:24.856
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:47:24.863
  E0110 16:47:25.811013      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:26.811402      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:27.811891      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:28.812095      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:29.813023      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:30.813631      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:31.813627      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:32.813821      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:33.814167      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:34.814926      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:35.814820      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:36.815954      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:37.815990      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:38.817081      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:39.818165      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:40.819466      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:41.819449      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:42.821837      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:43.820634      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:44.821130      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:45.821948      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:46.822645      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:47.822958      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:48.825103      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:49.825155      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:50.825421      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:51.826414      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:52.826514      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:53.827584      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:54.828520      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:55.828959      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:56.829835      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:57.830087      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:58.830593      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:47:59.831674      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:00.831627      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:01.831882      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:02.833050      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:03.833994      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:04.835161      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:05.835555      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:06.835958      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:07.836245      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:08.837237      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:09.837522      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:10.837812      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:11.838055      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:12.838244      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:13.839238      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:14.840123      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:15.841066      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:16.841269      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:17.841694      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:18.842788      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:19.843736      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:20.844827      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:21.845903      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:22.847111      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:23.847148      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:24.847831      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:48:24.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-2402" for this suite. @ 01/10/24 16:48:24.907
• [60.126 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:125
  STEP: Creating a kubernetes client @ 01/10/24 16:48:24.946
  Jan 10 16:48:24.946: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename secrets @ 01/10/24 16:48:24.948
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:48:24.991
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:48:24.999
  STEP: Creating secret with name secret-test-265cef39-1c14-495b-a616-1f34d0899dcc @ 01/10/24 16:48:25.006
  STEP: Creating a pod to test consume secrets @ 01/10/24 16:48:25.018
  E0110 16:48:25.848064      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:26.848961      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:27.849549      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:28.849811      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/10/24 16:48:29.079
  Jan 10 16:48:29.088: INFO: Trying to get logs from node env1-test-worker-1 pod pod-secrets-4e070346-f3f9-46d2-ab4e-aa71b16345a0 container secret-volume-test: <nil>
  STEP: delete the pod @ 01/10/24 16:48:29.113
  Jan 10 16:48:29.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7708" for this suite. @ 01/10/24 16:48:29.166
• [4.234 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:205
  STEP: Creating a kubernetes client @ 01/10/24 16:48:29.185
  Jan 10 16:48:29.186: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename secrets @ 01/10/24 16:48:29.188
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:48:29.246
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:48:29.254
  STEP: Creating secret with name s-test-opt-del-43b2f1be-2147-4a80-9d3a-3a1069e9f8f3 @ 01/10/24 16:48:29.274
  STEP: Creating secret with name s-test-opt-upd-cfa8b3e9-73a9-4ed0-a1c1-7c3da714c06d @ 01/10/24 16:48:29.288
  STEP: Creating the pod @ 01/10/24 16:48:29.303
  E0110 16:48:29.850433      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:30.850570      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:31.851671      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:32.852166      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting secret s-test-opt-del-43b2f1be-2147-4a80-9d3a-3a1069e9f8f3 @ 01/10/24 16:48:33.447
  STEP: Updating secret s-test-opt-upd-cfa8b3e9-73a9-4ed0-a1c1-7c3da714c06d @ 01/10/24 16:48:33.46
  STEP: Creating secret with name s-test-opt-create-0a5d4ebc-5dee-4e5e-95ca-a992eb63b414 @ 01/10/24 16:48:33.478
  STEP: waiting to observe update in volume @ 01/10/24 16:48:33.488
  E0110 16:48:33.853152      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:34.855483      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:35.856576      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:36.857136      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:37.858007      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:38.858744      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:39.859165      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:40.860024      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:41.860378      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:42.860591      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:43.860865      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:44.860947      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:45.861644      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:46.862863      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:47.863759      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:48.864074      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:49.865162      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:50.865426      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:51.865731      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:52.865948      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:53.866146      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:54.866659      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:55.866898      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:56.867555      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:57.868366      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:58.868883      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:48:59.869594      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:00.869777      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:01.870096      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:02.870908      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:03.871572      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:04.872699      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:05.872864      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:06.873138      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:07.873473      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:08.873714      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:09.874769      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:10.874950      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:11.880038      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:12.881041      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:13.881395      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:14.882446      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:15.882897      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:16.883483      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:17.883662      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:18.884719      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:19.884910      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:20.885770      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:21.885935      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:22.886221      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:23.886425      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:24.887361      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:25.888066      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:26.888706      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:27.888948      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:28.889191      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:29.889987      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:30.890531      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:31.890694      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:32.891732      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:33.892036      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:34.893183      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:35.893706      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:36.894773      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:37.895048      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:38.895447      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:39.896427      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:40.897183      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:41.897609      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:42.897740      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:43.898522      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:44.898177      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:45.898712      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:46.898823      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:47.899065      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:48.899169      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:49.899258      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:50.900238      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:51.900758      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:52.900932      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:53.901584      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:54.902007      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:55.902839      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:56.903877      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:57.904517      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:58.905248      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:49:59.906312      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:50:00.906895      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:50:01.907327      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:50:02.907612      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:50:03.908007      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:50:04.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-4374" for this suite. @ 01/10/24 16:50:04.688
• [95.537 seconds]
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]
test/e2e/scheduling/predicates.go:332
  STEP: Creating a kubernetes client @ 01/10/24 16:50:04.723
  Jan 10 16:50:04.723: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename sched-pred @ 01/10/24 16:50:04.725
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:50:04.766
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:50:04.778
  Jan 10 16:50:04.784: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jan 10 16:50:04.809: INFO: Waiting for terminating namespaces to be deleted...
  Jan 10 16:50:04.817: INFO: 
  Logging pods the apiserver thinks is on node env1-test-worker-0 before test
  Jan 10 16:50:04.839: INFO: filebeat-filebeat-q5bzw from filebeat started at 2024-01-10 01:14:23 +0000 UTC (1 container statuses recorded)
  Jan 10 16:50:04.839: INFO: 	Container filebeat ready: false, restart count 0
  Jan 10 16:50:04.839: INFO: kube-flannel-r8g5h from kube-system started at 2024-01-09 16:24:44 +0000 UTC (1 container statuses recorded)
  Jan 10 16:50:04.839: INFO: 	Container kube-flannel ready: true, restart count 0
  Jan 10 16:50:04.839: INFO: kube-proxy-445hs from kube-system started at 2024-01-10 09:13:13 +0000 UTC (1 container statuses recorded)
  Jan 10 16:50:04.839: INFO: 	Container kube-proxy ready: true, restart count 0
  Jan 10 16:50:04.839: INFO: metrics-server-6b7574f5b-jmbtm from kube-system started at 2024-01-09 16:32:13 +0000 UTC (1 container statuses recorded)
  Jan 10 16:50:04.839: INFO: 	Container metrics-server ready: true, restart count 0
  Jan 10 16:50:04.839: INFO: nginx-proxy-env1-test-worker-0 from kube-system started at 2024-01-09 16:31:48 +0000 UTC (1 container statuses recorded)
  Jan 10 16:50:04.839: INFO: 	Container nginx-proxy ready: true, restart count 0
  Jan 10 16:50:04.839: INFO: nodelocaldns-vkvkp from kube-system started at 2024-01-09 15:52:20 +0000 UTC (1 container statuses recorded)
  Jan 10 16:50:04.839: INFO: 	Container node-cache ready: true, restart count 0
  Jan 10 16:50:04.839: INFO: vsphere-csi-node-qkfth from kube-system started at 2024-01-10 09:21:38 +0000 UTC (3 container statuses recorded)
  Jan 10 16:50:04.839: INFO: 	Container liveness-probe ready: true, restart count 0
  Jan 10 16:50:04.839: INFO: 	Container node-driver-registrar ready: true, restart count 0
  Jan 10 16:50:04.839: INFO: 	Container vsphere-csi-node ready: true, restart count 0
  Jan 10 16:50:04.839: INFO: prometheus-kube-prometheus-operator-5f847644d6-gkll8 from prometheus started at 2024-01-10 03:09:01 +0000 UTC (1 container statuses recorded)
  Jan 10 16:50:04.839: INFO: 	Container kube-prometheus-stack ready: true, restart count 0
  Jan 10 16:50:04.839: INFO: prometheus-kube-state-metrics-f8b6b59f-g2nrl from prometheus started at 2024-01-10 03:09:01 +0000 UTC (1 container statuses recorded)
  Jan 10 16:50:04.839: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Jan 10 16:50:04.839: INFO: prometheus-prometheus-kube-prometheus-prometheus-0 from prometheus started at 2024-01-10 03:09:07 +0000 UTC (3 container statuses recorded)
  Jan 10 16:50:04.839: INFO: 	Container config-reloader ready: true, restart count 0
  Jan 10 16:50:04.839: INFO: 	Container prometheus ready: false, restart count 165
  Jan 10 16:50:04.839: INFO: 	Container thanos-sidecar ready: true, restart count 0
  Jan 10 16:50:04.839: INFO: prometheus-prometheus-node-exporter-4nknm from prometheus started at 2024-01-10 03:09:01 +0000 UTC (1 container statuses recorded)
  Jan 10 16:50:04.839: INFO: 	Container node-exporter ready: true, restart count 0
  Jan 10 16:50:04.839: INFO: thanos-query-6f697d54b8-sp4fg from prometheus started at 2024-01-10 13:06:38 +0000 UTC (1 container statuses recorded)
  Jan 10 16:50:04.839: INFO: 	Container query ready: true, restart count 0
  Jan 10 16:50:04.839: INFO: sonobuoy-systemd-logs-daemon-set-187fce9e0ddc499b-9m7s6 from sonobuoy started at 2024-01-10 15:26:38 +0000 UTC (2 container statuses recorded)
  Jan 10 16:50:04.839: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jan 10 16:50:04.839: INFO: 	Container systemd-logs ready: true, restart count 0
  Jan 10 16:50:04.839: INFO: traefik-ingress-g4tjs from traefik-ingress started at 2024-01-10 14:39:42 +0000 UTC (1 container statuses recorded)
  Jan 10 16:50:04.839: INFO: 	Container traefik-ingress ready: true, restart count 0
  Jan 10 16:50:04.839: INFO: velero-794b84894f-nwdwf from velero started at 2024-01-10 13:26:26 +0000 UTC (1 container statuses recorded)
  Jan 10 16:50:04.839: INFO: 	Container velero ready: true, restart count 0
  Jan 10 16:50:04.839: INFO: 
  Logging pods the apiserver thinks is on node env1-test-worker-1 before test
  Jan 10 16:50:04.866: INFO: filebeat-filebeat-dxph4 from filebeat started at 2024-01-10 15:44:08 +0000 UTC (1 container statuses recorded)
  Jan 10 16:50:04.866: INFO: 	Container filebeat ready: false, restart count 0
  Jan 10 16:50:04.866: INFO: kube-flannel-jxf5s from kube-system started at 2024-01-09 16:24:09 +0000 UTC (1 container statuses recorded)
  Jan 10 16:50:04.866: INFO: 	Container kube-flannel ready: true, restart count 0
  Jan 10 16:50:04.866: INFO: kube-proxy-78tcd from kube-system started at 2024-01-10 09:13:13 +0000 UTC (1 container statuses recorded)
  Jan 10 16:50:04.866: INFO: 	Container kube-proxy ready: true, restart count 0
  Jan 10 16:50:04.866: INFO: nginx-proxy-env1-test-worker-1 from kube-system started at 2024-01-09 16:38:31 +0000 UTC (1 container statuses recorded)
  Jan 10 16:50:04.866: INFO: 	Container nginx-proxy ready: true, restart count 0
  Jan 10 16:50:04.866: INFO: nodelocaldns-7qx4w from kube-system started at 2024-01-09 15:52:12 +0000 UTC (1 container statuses recorded)
  Jan 10 16:50:04.866: INFO: 	Container node-cache ready: true, restart count 0
  Jan 10 16:50:04.866: INFO: vsphere-csi-node-lr59t from kube-system started at 2024-01-10 09:21:38 +0000 UTC (3 container statuses recorded)
  Jan 10 16:50:04.866: INFO: 	Container liveness-probe ready: true, restart count 0
  Jan 10 16:50:04.866: INFO: 	Container node-driver-registrar ready: true, restart count 0
  Jan 10 16:50:04.866: INFO: 	Container vsphere-csi-node ready: true, restart count 0
  Jan 10 16:50:04.866: INFO: prometheus-prometheus-node-exporter-788fx from prometheus started at 2024-01-10 15:53:35 +0000 UTC (1 container statuses recorded)
  Jan 10 16:50:04.866: INFO: 	Container node-exporter ready: true, restart count 0
  Jan 10 16:50:04.866: INFO: pod-secrets-970be3b7-6ba0-4aca-9e6a-fc5180eeafe6 from secrets-4374 started at 2024-01-10 16:48:29 +0000 UTC (3 container statuses recorded)
  Jan 10 16:50:04.866: INFO: 	Container creates-volume-test ready: true, restart count 0
  Jan 10 16:50:04.866: INFO: 	Container dels-volume-test ready: true, restart count 0
  Jan 10 16:50:04.866: INFO: 	Container upds-volume-test ready: true, restart count 0
  Jan 10 16:50:04.866: INFO: sonobuoy from sonobuoy started at 2024-01-10 15:26:36 +0000 UTC (1 container statuses recorded)
  Jan 10 16:50:04.866: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jan 10 16:50:04.867: INFO: sonobuoy-e2e-job-b46f6697883e4f52 from sonobuoy started at 2024-01-10 15:26:37 +0000 UTC (2 container statuses recorded)
  Jan 10 16:50:04.867: INFO: 	Container e2e ready: true, restart count 0
  Jan 10 16:50:04.867: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jan 10 16:50:04.867: INFO: sonobuoy-systemd-logs-daemon-set-187fce9e0ddc499b-nntgv from sonobuoy started at 2024-01-10 15:26:38 +0000 UTC (2 container statuses recorded)
  Jan 10 16:50:04.867: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jan 10 16:50:04.867: INFO: 	Container systemd-logs ready: true, restart count 0
  Jan 10 16:50:04.867: INFO: traefik-ingress-lggr6 from traefik-ingress started at 2024-01-10 15:53:37 +0000 UTC (1 container statuses recorded)
  Jan 10 16:50:04.867: INFO: 	Container traefik-ingress ready: true, restart count 0
  E0110 16:50:04.908179      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the node has the label node env1-test-worker-0 @ 01/10/24 16:50:04.947
  STEP: verifying the node has the label node env1-test-worker-1 @ 01/10/24 16:50:05.001
  Jan 10 16:50:05.068: INFO: Pod filebeat-filebeat-dxph4 requesting resource cpu=200m on Node env1-test-worker-1
  Jan 10 16:50:05.068: INFO: Pod filebeat-filebeat-q5bzw requesting resource cpu=200m on Node env1-test-worker-0
  Jan 10 16:50:05.068: INFO: Pod kube-flannel-jxf5s requesting resource cpu=150m on Node env1-test-worker-1
  Jan 10 16:50:05.068: INFO: Pod kube-flannel-r8g5h requesting resource cpu=150m on Node env1-test-worker-0
  Jan 10 16:50:05.068: INFO: Pod kube-proxy-445hs requesting resource cpu=0m on Node env1-test-worker-0
  Jan 10 16:50:05.068: INFO: Pod kube-proxy-78tcd requesting resource cpu=0m on Node env1-test-worker-1
  Jan 10 16:50:05.068: INFO: Pod metrics-server-6b7574f5b-jmbtm requesting resource cpu=100m on Node env1-test-worker-0
  Jan 10 16:50:05.068: INFO: Pod nginx-proxy-env1-test-worker-0 requesting resource cpu=25m on Node env1-test-worker-0
  Jan 10 16:50:05.068: INFO: Pod nginx-proxy-env1-test-worker-1 requesting resource cpu=25m on Node env1-test-worker-1
  Jan 10 16:50:05.068: INFO: Pod nodelocaldns-7qx4w requesting resource cpu=100m on Node env1-test-worker-1
  Jan 10 16:50:05.069: INFO: Pod nodelocaldns-vkvkp requesting resource cpu=100m on Node env1-test-worker-0
  Jan 10 16:50:05.069: INFO: Pod vsphere-csi-node-lr59t requesting resource cpu=0m on Node env1-test-worker-1
  Jan 10 16:50:05.069: INFO: Pod vsphere-csi-node-qkfth requesting resource cpu=0m on Node env1-test-worker-0
  Jan 10 16:50:05.069: INFO: Pod prometheus-kube-prometheus-operator-5f847644d6-gkll8 requesting resource cpu=0m on Node env1-test-worker-0
  Jan 10 16:50:05.069: INFO: Pod prometheus-kube-state-metrics-f8b6b59f-g2nrl requesting resource cpu=0m on Node env1-test-worker-0
  Jan 10 16:50:05.069: INFO: Pod prometheus-prometheus-kube-prometheus-prometheus-0 requesting resource cpu=600m on Node env1-test-worker-0
  Jan 10 16:50:05.069: INFO: Pod prometheus-prometheus-node-exporter-4nknm requesting resource cpu=0m on Node env1-test-worker-0
  Jan 10 16:50:05.069: INFO: Pod prometheus-prometheus-node-exporter-788fx requesting resource cpu=0m on Node env1-test-worker-1
  Jan 10 16:50:05.069: INFO: Pod thanos-query-6f697d54b8-sp4fg requesting resource cpu=10m on Node env1-test-worker-0
  Jan 10 16:50:05.069: INFO: Pod pod-secrets-970be3b7-6ba0-4aca-9e6a-fc5180eeafe6 requesting resource cpu=0m on Node env1-test-worker-1
  Jan 10 16:50:05.069: INFO: Pod sonobuoy requesting resource cpu=0m on Node env1-test-worker-1
  Jan 10 16:50:05.069: INFO: Pod sonobuoy-e2e-job-b46f6697883e4f52 requesting resource cpu=0m on Node env1-test-worker-1
  Jan 10 16:50:05.069: INFO: Pod sonobuoy-systemd-logs-daemon-set-187fce9e0ddc499b-9m7s6 requesting resource cpu=0m on Node env1-test-worker-0
  Jan 10 16:50:05.069: INFO: Pod sonobuoy-systemd-logs-daemon-set-187fce9e0ddc499b-nntgv requesting resource cpu=0m on Node env1-test-worker-1
  Jan 10 16:50:05.069: INFO: Pod traefik-ingress-g4tjs requesting resource cpu=300m on Node env1-test-worker-0
  Jan 10 16:50:05.069: INFO: Pod traefik-ingress-lggr6 requesting resource cpu=300m on Node env1-test-worker-1
  Jan 10 16:50:05.069: INFO: Pod velero-794b84894f-nwdwf requesting resource cpu=500m on Node env1-test-worker-0
  STEP: Starting Pods to consume most of the cluster CPU. @ 01/10/24 16:50:05.069
  Jan 10 16:50:05.069: INFO: Creating a pod which consumes cpu=1410m on Node env1-test-worker-0
  Jan 10 16:50:05.099: INFO: Creating a pod which consumes cpu=2257m on Node env1-test-worker-1
  E0110 16:50:05.909074      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:50:06.909224      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating another pod that requires unavailable amount of CPU. @ 01/10/24 16:50:07.152
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-094e06cf-9f77-43b8-bc20-3aab2d751123.17a90a6ed961b393], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1353/filler-pod-094e06cf-9f77-43b8-bc20-3aab2d751123 to env1-test-worker-1] @ 01/10/24 16:50:07.171
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-094e06cf-9f77-43b8-bc20-3aab2d751123.17a90a6effbdeda8], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 01/10/24 16:50:07.172
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-094e06cf-9f77-43b8-bc20-3aab2d751123.17a90a6f00d6530a], Reason = [Created], Message = [Created container filler-pod-094e06cf-9f77-43b8-bc20-3aab2d751123] @ 01/10/24 16:50:07.173
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-094e06cf-9f77-43b8-bc20-3aab2d751123.17a90a6f0b0cdd49], Reason = [Started], Message = [Started container filler-pod-094e06cf-9f77-43b8-bc20-3aab2d751123] @ 01/10/24 16:50:07.174
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-b05c6e91-0f68-4a76-8e03-e19eca2d0493.17a90a6ed7ae6724], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1353/filler-pod-b05c6e91-0f68-4a76-8e03-e19eca2d0493 to env1-test-worker-0] @ 01/10/24 16:50:07.174
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-b05c6e91-0f68-4a76-8e03-e19eca2d0493.17a90a6efdc7aca1], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 01/10/24 16:50:07.174
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-b05c6e91-0f68-4a76-8e03-e19eca2d0493.17a90a6eff289f74], Reason = [Created], Message = [Created container filler-pod-b05c6e91-0f68-4a76-8e03-e19eca2d0493] @ 01/10/24 16:50:07.174
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-b05c6e91-0f68-4a76-8e03-e19eca2d0493.17a90a6f0b2d78f3], Reason = [Started], Message = [Started container filler-pod-b05c6e91-0f68-4a76-8e03-e19eca2d0493] @ 01/10/24 16:50:07.174
  STEP: Considering event: 
  Type = [Warning], Name = [additional-pod.17a90a6f52fb888f], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 Insufficient cpu, 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/5 nodes are available: 2 No preemption victims found for incoming pod, 3 Preemption is not helpful for scheduling..] @ 01/10/24 16:50:07.261
  E0110 16:50:07.909460      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label node off the node env1-test-worker-0 @ 01/10/24 16:50:08.202
  STEP: verifying the node doesn't have the label node @ 01/10/24 16:50:08.265
  STEP: removing the label node off the node env1-test-worker-1 @ 01/10/24 16:50:08.275
  STEP: verifying the node doesn't have the label node @ 01/10/24 16:50:08.313
  Jan 10 16:50:08.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-1353" for this suite. @ 01/10/24 16:50:08.342
• [3.646 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services should serve a basic endpoint from pods  [Conformance]
test/e2e/network/service.go:785
  STEP: Creating a kubernetes client @ 01/10/24 16:50:08.374
  Jan 10 16:50:08.374: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename services @ 01/10/24 16:50:08.376
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:50:08.42
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:50:08.428
  STEP: creating service endpoint-test2 in namespace services-1494 @ 01/10/24 16:50:08.463
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1494 to expose endpoints map[] @ 01/10/24 16:50:08.502
  Jan 10 16:50:08.521: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
  E0110 16:50:08.909616      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:50:09.548: INFO: successfully validated that service endpoint-test2 in namespace services-1494 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-1494 @ 01/10/24 16:50:09.548
  E0110 16:50:09.910142      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:50:10.910427      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1494 to expose endpoints map[pod1:[80]] @ 01/10/24 16:50:11.604
  Jan 10 16:50:11.644: INFO: successfully validated that service endpoint-test2 in namespace services-1494 exposes endpoints map[pod1:[80]]
  STEP: Checking if the Service forwards traffic to pod1 @ 01/10/24 16:50:11.644
  Jan 10 16:50:11.645: INFO: Creating new exec pod
  E0110 16:50:11.910973      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:50:12.911661      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:50:13.912026      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:50:14.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=services-1494 exec execpodtt7ft -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  E0110 16:50:14.912069      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:50:15.025: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Jan 10 16:50:15.025: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 10 16:50:15.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=services-1494 exec execpodtt7ft -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.41.214 80'
  Jan 10 16:50:15.314: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.41.214 80\nConnection to 10.233.41.214 80 port [tcp/http] succeeded!\n"
  Jan 10 16:50:15.314: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Creating pod pod2 in namespace services-1494 @ 01/10/24 16:50:15.314
  E0110 16:50:15.912276      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:50:16.912799      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1494 to expose endpoints map[pod1:[80] pod2:[80]] @ 01/10/24 16:50:17.392
  Jan 10 16:50:17.501: INFO: successfully validated that service endpoint-test2 in namespace services-1494 exposes endpoints map[pod1:[80] pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod1 and pod2 @ 01/10/24 16:50:17.501
  E0110 16:50:17.913597      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:50:18.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=services-1494 exec execpodtt7ft -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  E0110 16:50:18.914451      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:50:19.914933      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:50:20.909: INFO: rc: 1
  Jan 10 16:50:20.909: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=services-1494 exec execpodtt7ft -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80:
  Command stdout:

  stderr:
  + nc -v -t -w 2 endpoint-test2 80
  + echo hostName
  nc: connect to endpoint-test2 port 80 (tcp) timed out: Operation in progress
  command terminated with exit code 1

  error:
  exit status 1
  Retrying...
  E0110 16:50:20.915683      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:50:21.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=services-1494 exec execpodtt7ft -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  E0110 16:50:21.916547      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:50:22.264: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Jan 10 16:50:22.264: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 10 16:50:22.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=services-1494 exec execpodtt7ft -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.41.214 80'
  E0110 16:50:22.917194      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:50:23.917782      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:50:24.716: INFO: rc: 1
  Jan 10 16:50:24.716: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=services-1494 exec execpodtt7ft -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.41.214 80:
  Command stdout:

  stderr:
  + echo hostName
  + nc -v -t -w 2 10.233.41.214 80
  nc: connect to 10.233.41.214 port 80 (tcp) timed out: Operation in progress
  command terminated with exit code 1

  error:
  exit status 1
  Retrying...
  E0110 16:50:24.918408      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:50:25.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=services-1494 exec execpodtt7ft -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.41.214 80'
  E0110 16:50:25.918438      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:50:26.069: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.41.214 80\nConnection to 10.233.41.214 80 port [tcp/http] succeeded!\n"
  Jan 10 16:50:26.069: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-1494 @ 01/10/24 16:50:26.069
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1494 to expose endpoints map[pod2:[80]] @ 01/10/24 16:50:26.103
  E0110 16:50:26.919083      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:50:27.148: INFO: successfully validated that service endpoint-test2 in namespace services-1494 exposes endpoints map[pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod2 @ 01/10/24 16:50:27.148
  E0110 16:50:27.919324      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:50:28.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=services-1494 exec execpodtt7ft -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Jan 10 16:50:28.489: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Jan 10 16:50:28.489: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 10 16:50:28.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=services-1494 exec execpodtt7ft -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.41.214 80'
  Jan 10 16:50:28.865: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.41.214 80\nConnection to 10.233.41.214 80 port [tcp/http] succeeded!\n"
  Jan 10 16:50:28.865: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod2 in namespace services-1494 @ 01/10/24 16:50:28.866
  E0110 16:50:28.920148      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1494 to expose endpoints map[] @ 01/10/24 16:50:28.941
  Jan 10 16:50:28.990: INFO: successfully validated that service endpoint-test2 in namespace services-1494 exposes endpoints map[]
  Jan 10 16:50:28.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-1494" for this suite. @ 01/10/24 16:50:29.147
• [20.811 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
test/e2e/common/storage/projected_combined.go:44
  STEP: Creating a kubernetes client @ 01/10/24 16:50:29.193
  Jan 10 16:50:29.193: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename projected @ 01/10/24 16:50:29.199
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:50:29.255
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:50:29.263
  STEP: Creating configMap with name configmap-projected-all-test-volume-6ac6a0d3-3c93-4c86-82f4-1124994b30dd @ 01/10/24 16:50:29.27
  STEP: Creating secret with name secret-projected-all-test-volume-44c2d675-bdf7-442a-bd32-ba1dd26cfd46 @ 01/10/24 16:50:29.284
  STEP: Creating a pod to test Check all projections for projected volume plugin @ 01/10/24 16:50:29.296
  E0110 16:50:29.921084      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:50:30.921350      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:50:31.922498      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:50:32.922967      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/10/24 16:50:33.364
  Jan 10 16:50:33.373: INFO: Trying to get logs from node env1-test-worker-1 pod projected-volume-0c948755-9289-4405-a6b7-e74660784a47 container projected-all-volume-test: <nil>
  STEP: delete the pod @ 01/10/24 16:50:33.392
  Jan 10 16:50:33.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3669" for this suite. @ 01/10/24 16:50:33.442
• [4.267 seconds]
------------------------------
[sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:68
  STEP: Creating a kubernetes client @ 01/10/24 16:50:33.46
  Jan 10 16:50:33.460: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename secrets @ 01/10/24 16:50:33.462
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:50:33.518
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:50:33.525
  STEP: Creating secret with name secret-test-1c06cbf9-1a32-4f9a-bc21-a3fd58223d14 @ 01/10/24 16:50:33.531
  STEP: Creating a pod to test consume secrets @ 01/10/24 16:50:33.544
  E0110 16:50:33.924251      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:50:34.925194      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:50:35.926687      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:50:36.926903      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/10/24 16:50:37.61
  Jan 10 16:50:37.618: INFO: Trying to get logs from node env1-test-worker-1 pod pod-secrets-6b16aa1e-5344-4cc7-9054-87dec587bc68 container secret-volume-test: <nil>
  STEP: delete the pod @ 01/10/24 16:50:37.633
  Jan 10 16:50:37.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6842" for this suite. @ 01/10/24 16:50:37.698
• [4.254 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:208
  STEP: Creating a kubernetes client @ 01/10/24 16:50:37.715
  Jan 10 16:50:37.715: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename projected @ 01/10/24 16:50:37.717
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:50:37.763
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:50:37.77
  STEP: Creating a pod to test downward API volume plugin @ 01/10/24 16:50:37.779
  E0110 16:50:37.927301      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:50:38.930043      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:50:39.928879      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:50:40.929152      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/10/24 16:50:41.834
  Jan 10 16:50:41.842: INFO: Trying to get logs from node env1-test-worker-1 pod downwardapi-volume-bcc13caa-c2e1-4484-b588-743785417e6b container client-container: <nil>
  STEP: delete the pod @ 01/10/24 16:50:41.856
  Jan 10 16:50:41.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1405" for this suite. @ 01/10/24 16:50:41.922
  E0110 16:50:41.929085      23 retrywatcher.go:130] "Watch failed" err="context canceled"
• [4.246 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]
test/e2e/network/ingressclass.go:266
  STEP: Creating a kubernetes client @ 01/10/24 16:50:41.97
  Jan 10 16:50:41.971: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename ingressclass @ 01/10/24 16:50:41.973
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:50:42.07
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:50:42.079
  STEP: getting /apis @ 01/10/24 16:50:42.087
  STEP: getting /apis/networking.k8s.io @ 01/10/24 16:50:42.103
  STEP: getting /apis/networking.k8s.iov1 @ 01/10/24 16:50:42.106
  STEP: creating @ 01/10/24 16:50:42.11
  STEP: getting @ 01/10/24 16:50:42.143
  STEP: listing @ 01/10/24 16:50:42.151
  STEP: watching @ 01/10/24 16:50:42.159
  Jan 10 16:50:42.159: INFO: starting watch
  STEP: patching @ 01/10/24 16:50:42.162
  STEP: updating @ 01/10/24 16:50:42.173
  Jan 10 16:50:42.184: INFO: waiting for watch events with expected annotations
  Jan 10 16:50:42.185: INFO: saw patched and updated annotations
  STEP: deleting @ 01/10/24 16:50:42.185
  STEP: deleting a collection @ 01/10/24 16:50:42.228
  Jan 10 16:50:42.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingressclass-3089" for this suite. @ 01/10/24 16:50:42.282
• [0.348 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:69
  STEP: Creating a kubernetes client @ 01/10/24 16:50:42.323
  Jan 10 16:50:42.323: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename downward-api @ 01/10/24 16:50:42.326
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:50:42.383
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:50:42.389
  STEP: Creating a pod to test downward API volume plugin @ 01/10/24 16:50:42.423
  E0110 16:50:42.929558      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:50:43.930546      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:50:44.930790      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:50:45.931381      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/10/24 16:50:46.498
  Jan 10 16:50:46.508: INFO: Trying to get logs from node env1-test-worker-1 pod downwardapi-volume-ef8559e2-742a-4051-be37-7fc9da103a45 container client-container: <nil>
  STEP: delete the pod @ 01/10/24 16:50:46.525
  Jan 10 16:50:46.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5088" for this suite. @ 01/10/24 16:50:46.593
• [4.290 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]
test/e2e/apimachinery/webhook.go:300
  STEP: Creating a kubernetes client @ 01/10/24 16:50:46.613
  Jan 10 16:50:46.613: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename webhook @ 01/10/24 16:50:46.615
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:50:46.658
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:50:46.664
  STEP: Setting up server cert @ 01/10/24 16:50:46.719
  E0110 16:50:46.931960      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 01/10/24 16:50:47.634
  STEP: Deploying the webhook pod @ 01/10/24 16:50:47.658
  STEP: Wait for the deployment to be ready @ 01/10/24 16:50:47.683
  Jan 10 16:50:47.712: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0110 16:50:47.931995      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:50:48.935229      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 01/10/24 16:50:49.739
  STEP: Verifying the service has paired with the endpoint @ 01/10/24 16:50:49.781
  E0110 16:50:49.935504      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:50:50.781: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the crd webhook via the AdmissionRegistration API @ 01/10/24 16:50:50.791
  STEP: Creating a custom resource definition that should be denied by the webhook @ 01/10/24 16:50:50.828
  Jan 10 16:50:50.828: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  Jan 10 16:50:50.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0110 16:50:50.936227      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-1096" for this suite. @ 01/10/24 16:50:51.126
  STEP: Destroying namespace "webhook-markers-1849" for this suite. @ 01/10/24 16:50:51.149
• [4.580 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply a finalizer to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:398
  STEP: Creating a kubernetes client @ 01/10/24 16:50:51.203
  Jan 10 16:50:51.203: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename namespaces @ 01/10/24 16:50:51.206
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:50:51.281
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:50:51.29
  STEP: Creating namespace "e2e-ns-7d6dz" @ 01/10/24 16:50:51.306
  Jan 10 16:50:51.390: INFO: Namespace "e2e-ns-7d6dz-5453" has []v1.FinalizerName{"kubernetes"}
  STEP: Adding e2e finalizer to namespace "e2e-ns-7d6dz-5453" @ 01/10/24 16:50:51.391
  Jan 10 16:50:51.427: INFO: Namespace "e2e-ns-7d6dz-5453" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
  STEP: Removing e2e finalizer from namespace "e2e-ns-7d6dz-5453" @ 01/10/24 16:50:51.427
  Jan 10 16:50:51.463: INFO: Namespace "e2e-ns-7d6dz-5453" has []v1.FinalizerName{"kubernetes"}
  Jan 10 16:50:51.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-5823" for this suite. @ 01/10/24 16:50:51.482
  STEP: Destroying namespace "e2e-ns-7d6dz-5453" for this suite. @ 01/10/24 16:50:51.506
• [0.325 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]
test/e2e/apimachinery/watch.go:60
  STEP: Creating a kubernetes client @ 01/10/24 16:50:51.535
  Jan 10 16:50:51.535: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename watch @ 01/10/24 16:50:51.537
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:50:51.63
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:50:51.637
  STEP: creating a watch on configmaps with label A @ 01/10/24 16:50:51.646
  STEP: creating a watch on configmaps with label B @ 01/10/24 16:50:51.65
  STEP: creating a watch on configmaps with label A or B @ 01/10/24 16:50:51.657
  STEP: creating a configmap with label A and ensuring the correct watchers observe the notification @ 01/10/24 16:50:51.667
  Jan 10 16:50:51.685: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3509  5a99c201-5a26-4cdc-bc86-ee68c5c7df24 186797726 0 2024-01-10 16:50:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-10 16:50:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jan 10 16:50:51.685: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3509  5a99c201-5a26-4cdc-bc86-ee68c5c7df24 186797726 0 2024-01-10 16:50:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-10 16:50:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A and ensuring the correct watchers observe the notification @ 01/10/24 16:50:51.686
  Jan 10 16:50:51.727: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3509  5a99c201-5a26-4cdc-bc86-ee68c5c7df24 186797727 0 2024-01-10 16:50:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-10 16:50:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jan 10 16:50:51.728: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3509  5a99c201-5a26-4cdc-bc86-ee68c5c7df24 186797727 0 2024-01-10 16:50:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-10 16:50:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A again and ensuring the correct watchers observe the notification @ 01/10/24 16:50:51.729
  Jan 10 16:50:51.762: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3509  5a99c201-5a26-4cdc-bc86-ee68c5c7df24 186797729 0 2024-01-10 16:50:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-10 16:50:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jan 10 16:50:51.762: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3509  5a99c201-5a26-4cdc-bc86-ee68c5c7df24 186797729 0 2024-01-10 16:50:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-10 16:50:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap A and ensuring the correct watchers observe the notification @ 01/10/24 16:50:51.762
  Jan 10 16:50:51.790: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3509  5a99c201-5a26-4cdc-bc86-ee68c5c7df24 186797730 0 2024-01-10 16:50:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-10 16:50:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jan 10 16:50:51.791: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3509  5a99c201-5a26-4cdc-bc86-ee68c5c7df24 186797730 0 2024-01-10 16:50:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2024-01-10 16:50:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: creating a configmap with label B and ensuring the correct watchers observe the notification @ 01/10/24 16:50:51.792
  Jan 10 16:50:51.806: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3509  f3dfa012-5f11-41e8-ba1b-36a484ec1e35 186797731 0 2024-01-10 16:50:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2024-01-10 16:50:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jan 10 16:50:51.806: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3509  f3dfa012-5f11-41e8-ba1b-36a484ec1e35 186797731 0 2024-01-10 16:50:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2024-01-10 16:50:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  E0110 16:50:51.936721      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:50:52.937248      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:50:53.938152      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:50:54.939058      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:50:55.939652      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:50:56.939762      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:50:57.940296      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:50:58.940931      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:50:59.941251      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:51:00.941444      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting configmap B and ensuring the correct watchers observe the notification @ 01/10/24 16:51:01.806
  Jan 10 16:51:01.825: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3509  f3dfa012-5f11-41e8-ba1b-36a484ec1e35 186797800 0 2024-01-10 16:50:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2024-01-10 16:50:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jan 10 16:51:01.826: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3509  f3dfa012-5f11-41e8-ba1b-36a484ec1e35 186797800 0 2024-01-10 16:50:51 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2024-01-10 16:50:51 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  E0110 16:51:01.942347      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:51:02.942526      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:51:03.942988      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:51:04.943934      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:51:05.944243      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:51:06.944404      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:51:07.944560      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:51:08.944837      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:51:09.945728      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:51:10.945919      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:51:11.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-3509" for this suite. @ 01/10/24 16:51:11.845
• [20.330 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:197
  STEP: Creating a kubernetes client @ 01/10/24 16:51:11.871
  Jan 10 16:51:11.871: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename emptydir @ 01/10/24 16:51:11.873
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:51:11.925
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:51:11.934
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 01/10/24 16:51:11.944
  E0110 16:51:11.946067      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:51:12.946860      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:51:13.947006      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:51:14.947301      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:51:15.947724      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/10/24 16:51:16.006
  Jan 10 16:51:16.014: INFO: Trying to get logs from node env1-test-worker-1 pod pod-d323cd7a-ff1f-40f6-a059-b7bfca86a8b7 container test-container: <nil>
  STEP: delete the pod @ 01/10/24 16:51:16.033
  Jan 10 16:51:16.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-95" for this suite. @ 01/10/24 16:51:16.08
• [4.227 seconds]
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]
test/e2e/kubectl/kubectl.go:1480
  STEP: Creating a kubernetes client @ 01/10/24 16:51:16.098
  Jan 10 16:51:16.098: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename kubectl @ 01/10/24 16:51:16.1
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:51:16.14
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:51:16.146
  STEP: creating Agnhost RC @ 01/10/24 16:51:16.151
  Jan 10 16:51:16.151: INFO: namespace kubectl-9345
  Jan 10 16:51:16.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-9345 create -f -'
  E0110 16:51:16.948811      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:51:17.949640      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:51:18.950866      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:51:19.385: INFO: stderr: ""
  Jan 10 16:51:19.385: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 01/10/24 16:51:19.385
  E0110 16:51:19.951200      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:51:20.395: INFO: Selector matched 1 pods for map[app:agnhost]
  Jan 10 16:51:20.395: INFO: Found 0 / 1
  E0110 16:51:20.952199      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:51:21.398: INFO: Selector matched 1 pods for map[app:agnhost]
  Jan 10 16:51:21.399: INFO: Found 1 / 1
  Jan 10 16:51:21.399: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Jan 10 16:51:21.410: INFO: Selector matched 1 pods for map[app:agnhost]
  Jan 10 16:51:21.410: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jan 10 16:51:21.410: INFO: wait on agnhost-primary startup in kubectl-9345 
  Jan 10 16:51:21.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-9345 logs agnhost-primary-rv2l2 agnhost-primary'
  Jan 10 16:51:21.627: INFO: stderr: ""
  Jan 10 16:51:21.627: INFO: stdout: "Paused\n"
  STEP: exposing RC @ 01/10/24 16:51:21.628
  Jan 10 16:51:21.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-9345 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
  Jan 10 16:51:21.810: INFO: stderr: ""
  Jan 10 16:51:21.810: INFO: stdout: "service/rm2 exposed\n"
  Jan 10 16:51:21.832: INFO: Service rm2 in namespace kubectl-9345 found.
  E0110 16:51:21.953093      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:51:22.953428      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: exposing service @ 01/10/24 16:51:23.853
  Jan 10 16:51:23.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-9345 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
  E0110 16:51:23.954317      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:51:24.073: INFO: stderr: ""
  Jan 10 16:51:24.073: INFO: stdout: "service/rm3 exposed\n"
  Jan 10 16:51:24.092: INFO: Service rm3 in namespace kubectl-9345 found.
  E0110 16:51:24.954707      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:51:25.955684      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:51:26.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9345" for this suite. @ 01/10/24 16:51:26.126
• [10.045 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]
test/e2e/apimachinery/garbage_collector.go:817
  STEP: Creating a kubernetes client @ 01/10/24 16:51:26.145
  Jan 10 16:51:26.145: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename gc @ 01/10/24 16:51:26.148
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:51:26.202
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:51:26.209
  Jan 10 16:51:26.300: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"cb6d5ef2-1ff7-429a-ad6c-74135457532f", Controller:(*bool)(0xc0058e5e32), BlockOwnerDeletion:(*bool)(0xc0058e5e33)}}
  Jan 10 16:51:26.323: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"8b483ddf-b6d9-4c4d-914f-fcfcdc2407fc", Controller:(*bool)(0xc00470205a), BlockOwnerDeletion:(*bool)(0xc00470205b)}}
  Jan 10 16:51:26.342: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"d6d4ddc6-9c9a-4c30-a44b-03d98150916e", Controller:(*bool)(0xc00470227a), BlockOwnerDeletion:(*bool)(0xc00470227b)}}
  E0110 16:51:26.961785      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:51:27.961958      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:51:28.962480      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:51:29.963631      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:51:30.963955      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:51:31.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-4898" for this suite. @ 01/10/24 16:51:31.406
• [5.287 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:537
  STEP: Creating a kubernetes client @ 01/10/24 16:51:31.441
  Jan 10 16:51:31.441: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename pods @ 01/10/24 16:51:31.443
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:51:31.514
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:51:31.527
  Jan 10 16:51:31.538: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: creating the pod @ 01/10/24 16:51:31.54
  STEP: submitting the pod to kubernetes @ 01/10/24 16:51:31.54
  E0110 16:51:31.964743      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:51:32.965256      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:51:33.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-121" for this suite. @ 01/10/24 16:51:33.876
• [2.460 seconds]
------------------------------
S
------------------------------
[sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]
test/e2e/network/proxy.go:101
  STEP: Creating a kubernetes client @ 01/10/24 16:51:33.901
  Jan 10 16:51:33.901: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename proxy @ 01/10/24 16:51:33.903
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:51:33.955
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:51:33.962
  E0110 16:51:33.965619      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: starting an echo server on multiple ports @ 01/10/24 16:51:33.991
  STEP: creating replication controller proxy-service-5kcr7 in namespace proxy-1544 @ 01/10/24 16:51:33.992
  I0110 16:51:34.026423      23 runners.go:194] Created replication controller with name: proxy-service-5kcr7, namespace: proxy-1544, replica count: 1
  E0110 16:51:34.965641      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0110 16:51:35.077902      23 runners.go:194] proxy-service-5kcr7 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0110 16:51:35.966848      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0110 16:51:36.078809      23 runners.go:194] proxy-service-5kcr7 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jan 10 16:51:36.094: INFO: setup took 2.126467279s, starting test cases
  STEP: running 16 cases, 20 attempts per case, 320 total attempts @ 01/10/24 16:51:36.094
  Jan 10 16:51:36.137: INFO: (0) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:1080/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:1080/proxy/rewriteme">... (200; 42.508356ms)
  Jan 10 16:51:36.154: INFO: (0) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:162/proxy/: bar (200; 58.383503ms)
  Jan 10 16:51:36.162: INFO: (0) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:162/proxy/: bar (200; 66.37887ms)
  Jan 10 16:51:36.174: INFO: (0) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:160/proxy/: foo (200; 76.836379ms)
  Jan 10 16:51:36.174: INFO: (0) /api/v1/namespaces/proxy-1544/services/http:proxy-service-5kcr7:portname2/proxy/: bar (200; 77.326697ms)
  Jan 10 16:51:36.175: INFO: (0) /api/v1/namespaces/proxy-1544/services/http:proxy-service-5kcr7:portname1/proxy/: foo (200; 80.459768ms)
  Jan 10 16:51:36.176: INFO: (0) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr/proxy/rewriteme">test</a> (200; 80.120592ms)
  Jan 10 16:51:36.181: INFO: (0) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:160/proxy/: foo (200; 83.696632ms)
  Jan 10 16:51:36.182: INFO: (0) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:1080/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:1080/proxy/rewriteme">test<... (200; 84.232077ms)
  Jan 10 16:51:36.187: INFO: (0) /api/v1/namespaces/proxy-1544/services/proxy-service-5kcr7:portname1/proxy/: foo (200; 90.767652ms)
  Jan 10 16:51:36.187: INFO: (0) /api/v1/namespaces/proxy-1544/services/proxy-service-5kcr7:portname2/proxy/: bar (200; 89.377594ms)
  Jan 10 16:51:36.188: INFO: (0) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:460/proxy/: tls baz (200; 91.670646ms)
  Jan 10 16:51:36.188: INFO: (0) /api/v1/namespaces/proxy-1544/services/https:proxy-service-5kcr7:tlsportname1/proxy/: tls baz (200; 92.417076ms)
  Jan 10 16:51:36.190: INFO: (0) /api/v1/namespaces/proxy-1544/services/https:proxy-service-5kcr7:tlsportname2/proxy/: tls qux (200; 92.945734ms)
  Jan 10 16:51:36.191: INFO: (0) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:462/proxy/: tls qux (200; 93.800969ms)
  Jan 10 16:51:36.207: INFO: (0) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:443/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:443/proxy/tlsrewritem... (200; 110.131376ms)
  Jan 10 16:51:36.228: INFO: (1) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:460/proxy/: tls baz (200; 21.097503ms)
  Jan 10 16:51:36.232: INFO: (1) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:1080/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:1080/proxy/rewriteme">... (200; 21.052543ms)
  Jan 10 16:51:36.238: INFO: (1) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:462/proxy/: tls qux (200; 29.087833ms)
  Jan 10 16:51:36.238: INFO: (1) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:162/proxy/: bar (200; 27.445202ms)
  Jan 10 16:51:36.239: INFO: (1) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr/proxy/rewriteme">test</a> (200; 27.74177ms)
  Jan 10 16:51:36.239: INFO: (1) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:160/proxy/: foo (200; 31.21394ms)
  Jan 10 16:51:36.241: INFO: (1) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:1080/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:1080/proxy/rewriteme">test<... (200; 31.259357ms)
  Jan 10 16:51:36.242: INFO: (1) /api/v1/namespaces/proxy-1544/services/proxy-service-5kcr7:portname2/proxy/: bar (200; 32.991895ms)
  Jan 10 16:51:36.242: INFO: (1) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:443/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:443/proxy/tlsrewritem... (200; 33.579423ms)
  Jan 10 16:51:36.242: INFO: (1) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:160/proxy/: foo (200; 34.088144ms)
  Jan 10 16:51:36.244: INFO: (1) /api/v1/namespaces/proxy-1544/services/http:proxy-service-5kcr7:portname1/proxy/: foo (200; 34.688479ms)
  Jan 10 16:51:36.245: INFO: (1) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:162/proxy/: bar (200; 35.319379ms)
  Jan 10 16:51:36.245: INFO: (1) /api/v1/namespaces/proxy-1544/services/http:proxy-service-5kcr7:portname2/proxy/: bar (200; 36.416309ms)
  Jan 10 16:51:36.247: INFO: (1) /api/v1/namespaces/proxy-1544/services/proxy-service-5kcr7:portname1/proxy/: foo (200; 37.048521ms)
  Jan 10 16:51:36.248: INFO: (1) /api/v1/namespaces/proxy-1544/services/https:proxy-service-5kcr7:tlsportname1/proxy/: tls baz (200; 37.924159ms)
  Jan 10 16:51:36.252: INFO: (1) /api/v1/namespaces/proxy-1544/services/https:proxy-service-5kcr7:tlsportname2/proxy/: tls qux (200; 43.234262ms)
  Jan 10 16:51:36.276: INFO: (2) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:162/proxy/: bar (200; 22.118474ms)
  Jan 10 16:51:36.276: INFO: (2) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:460/proxy/: tls baz (200; 22.043109ms)
  Jan 10 16:51:36.277: INFO: (2) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr/proxy/rewriteme">test</a> (200; 22.678002ms)
  Jan 10 16:51:36.279: INFO: (2) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:160/proxy/: foo (200; 24.965647ms)
  Jan 10 16:51:36.284: INFO: (2) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:162/proxy/: bar (200; 29.505784ms)
  Jan 10 16:51:36.289: INFO: (2) /api/v1/namespaces/proxy-1544/services/https:proxy-service-5kcr7:tlsportname2/proxy/: tls qux (200; 35.549667ms)
  Jan 10 16:51:36.290: INFO: (2) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:462/proxy/: tls qux (200; 35.127036ms)
  Jan 10 16:51:36.291: INFO: (2) /api/v1/namespaces/proxy-1544/services/http:proxy-service-5kcr7:portname2/proxy/: bar (200; 37.043505ms)
  Jan 10 16:51:36.292: INFO: (2) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:443/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:443/proxy/tlsrewritem... (200; 37.860428ms)
  Jan 10 16:51:36.294: INFO: (2) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:1080/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:1080/proxy/rewriteme">... (200; 38.852308ms)
  Jan 10 16:51:36.294: INFO: (2) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:160/proxy/: foo (200; 40.460838ms)
  Jan 10 16:51:36.298: INFO: (2) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:1080/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:1080/proxy/rewriteme">test<... (200; 43.271583ms)
  Jan 10 16:51:36.304: INFO: (2) /api/v1/namespaces/proxy-1544/services/proxy-service-5kcr7:portname2/proxy/: bar (200; 49.220542ms)
  Jan 10 16:51:36.306: INFO: (2) /api/v1/namespaces/proxy-1544/services/http:proxy-service-5kcr7:portname1/proxy/: foo (200; 50.882463ms)
  Jan 10 16:51:36.307: INFO: (2) /api/v1/namespaces/proxy-1544/services/https:proxy-service-5kcr7:tlsportname1/proxy/: tls baz (200; 51.790083ms)
  Jan 10 16:51:36.307: INFO: (2) /api/v1/namespaces/proxy-1544/services/proxy-service-5kcr7:portname1/proxy/: foo (200; 52.141632ms)
  Jan 10 16:51:36.322: INFO: (3) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:1080/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:1080/proxy/rewriteme">test<... (200; 15.022586ms)
  Jan 10 16:51:36.325: INFO: (3) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr/proxy/rewriteme">test</a> (200; 17.947648ms)
  Jan 10 16:51:36.325: INFO: (3) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:160/proxy/: foo (200; 17.717704ms)
  Jan 10 16:51:36.326: INFO: (3) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:160/proxy/: foo (200; 19.301734ms)
  Jan 10 16:51:36.329: INFO: (3) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:443/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:443/proxy/tlsrewritem... (200; 22.119412ms)
  Jan 10 16:51:36.330: INFO: (3) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:1080/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:1080/proxy/rewriteme">... (200; 23.179659ms)
  Jan 10 16:51:36.331: INFO: (3) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:162/proxy/: bar (200; 23.721256ms)
  Jan 10 16:51:36.334: INFO: (3) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:460/proxy/: tls baz (200; 26.120001ms)
  Jan 10 16:51:36.339: INFO: (3) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:162/proxy/: bar (200; 32.058404ms)
  Jan 10 16:51:36.346: INFO: (3) /api/v1/namespaces/proxy-1544/services/http:proxy-service-5kcr7:portname2/proxy/: bar (200; 38.284206ms)
  Jan 10 16:51:36.350: INFO: (3) /api/v1/namespaces/proxy-1544/services/proxy-service-5kcr7:portname2/proxy/: bar (200; 42.741727ms)
  Jan 10 16:51:36.350: INFO: (3) /api/v1/namespaces/proxy-1544/services/https:proxy-service-5kcr7:tlsportname1/proxy/: tls baz (200; 43.208682ms)
  Jan 10 16:51:36.350: INFO: (3) /api/v1/namespaces/proxy-1544/services/http:proxy-service-5kcr7:portname1/proxy/: foo (200; 43.447254ms)
  Jan 10 16:51:36.353: INFO: (3) /api/v1/namespaces/proxy-1544/services/https:proxy-service-5kcr7:tlsportname2/proxy/: tls qux (200; 45.265606ms)
  Jan 10 16:51:36.353: INFO: (3) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:462/proxy/: tls qux (200; 45.875445ms)
  Jan 10 16:51:36.354: INFO: (3) /api/v1/namespaces/proxy-1544/services/proxy-service-5kcr7:portname1/proxy/: foo (200; 46.346966ms)
  Jan 10 16:51:36.380: INFO: (4) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:160/proxy/: foo (200; 25.390244ms)
  Jan 10 16:51:36.381: INFO: (4) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:162/proxy/: bar (200; 24.752476ms)
  Jan 10 16:51:36.381: INFO: (4) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:443/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:443/proxy/tlsrewritem... (200; 26.162487ms)
  Jan 10 16:51:36.381: INFO: (4) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr/proxy/rewriteme">test</a> (200; 24.389368ms)
  Jan 10 16:51:36.381: INFO: (4) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:1080/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:1080/proxy/rewriteme">... (200; 24.573677ms)
  Jan 10 16:51:36.381: INFO: (4) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:462/proxy/: tls qux (200; 26.139035ms)
  Jan 10 16:51:36.381: INFO: (4) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:460/proxy/: tls baz (200; 27.223045ms)
  Jan 10 16:51:36.381: INFO: (4) /api/v1/namespaces/proxy-1544/services/http:proxy-service-5kcr7:portname1/proxy/: foo (200; 25.509601ms)
  Jan 10 16:51:36.383: INFO: (4) /api/v1/namespaces/proxy-1544/services/http:proxy-service-5kcr7:portname2/proxy/: bar (200; 27.286068ms)
  Jan 10 16:51:36.383: INFO: (4) /api/v1/namespaces/proxy-1544/services/https:proxy-service-5kcr7:tlsportname1/proxy/: tls baz (200; 26.626603ms)
  Jan 10 16:51:36.384: INFO: (4) /api/v1/namespaces/proxy-1544/services/proxy-service-5kcr7:portname2/proxy/: bar (200; 28.442695ms)
  Jan 10 16:51:36.387: INFO: (4) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:1080/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:1080/proxy/rewriteme">test<... (200; 32.945147ms)
  Jan 10 16:51:36.388: INFO: (4) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:162/proxy/: bar (200; 32.247659ms)
  Jan 10 16:51:36.388: INFO: (4) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:160/proxy/: foo (200; 33.514953ms)
  Jan 10 16:51:36.389: INFO: (4) /api/v1/namespaces/proxy-1544/services/proxy-service-5kcr7:portname1/proxy/: foo (200; 32.936153ms)
  Jan 10 16:51:36.393: INFO: (4) /api/v1/namespaces/proxy-1544/services/https:proxy-service-5kcr7:tlsportname2/proxy/: tls qux (200; 37.208175ms)
  Jan 10 16:51:36.411: INFO: (5) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:160/proxy/: foo (200; 17.946792ms)
  Jan 10 16:51:36.411: INFO: (5) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:1080/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:1080/proxy/rewriteme">... (200; 17.851928ms)
  Jan 10 16:51:36.416: INFO: (5) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:162/proxy/: bar (200; 22.254424ms)
  Jan 10 16:51:36.416: INFO: (5) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:1080/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:1080/proxy/rewriteme">test<... (200; 23.087872ms)
  Jan 10 16:51:36.418: INFO: (5) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:460/proxy/: tls baz (200; 25.059615ms)
  Jan 10 16:51:36.419: INFO: (5) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:160/proxy/: foo (200; 25.129059ms)
  Jan 10 16:51:36.419: INFO: (5) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:162/proxy/: bar (200; 25.59117ms)
  Jan 10 16:51:36.420: INFO: (5) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:462/proxy/: tls qux (200; 26.373156ms)
  Jan 10 16:51:36.425: INFO: (5) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:443/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:443/proxy/tlsrewritem... (200; 30.882452ms)
  Jan 10 16:51:36.426: INFO: (5) /api/v1/namespaces/proxy-1544/services/proxy-service-5kcr7:portname2/proxy/: bar (200; 32.291055ms)
  Jan 10 16:51:36.428: INFO: (5) /api/v1/namespaces/proxy-1544/services/https:proxy-service-5kcr7:tlsportname1/proxy/: tls baz (200; 34.370067ms)
  Jan 10 16:51:36.429: INFO: (5) /api/v1/namespaces/proxy-1544/services/proxy-service-5kcr7:portname1/proxy/: foo (200; 35.183737ms)
  Jan 10 16:51:36.430: INFO: (5) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr/proxy/rewriteme">test</a> (200; 36.443212ms)
  Jan 10 16:51:36.431: INFO: (5) /api/v1/namespaces/proxy-1544/services/http:proxy-service-5kcr7:portname2/proxy/: bar (200; 37.28533ms)
  Jan 10 16:51:36.437: INFO: (5) /api/v1/namespaces/proxy-1544/services/https:proxy-service-5kcr7:tlsportname2/proxy/: tls qux (200; 43.138547ms)
  Jan 10 16:51:36.438: INFO: (5) /api/v1/namespaces/proxy-1544/services/http:proxy-service-5kcr7:portname1/proxy/: foo (200; 44.338493ms)
  Jan 10 16:51:36.458: INFO: (6) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr/proxy/rewriteme">test</a> (200; 19.880535ms)
  Jan 10 16:51:36.460: INFO: (6) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:1080/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:1080/proxy/rewriteme">... (200; 21.666009ms)
  Jan 10 16:51:36.462: INFO: (6) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:160/proxy/: foo (200; 23.4528ms)
  Jan 10 16:51:36.462: INFO: (6) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:162/proxy/: bar (200; 23.733404ms)
  Jan 10 16:51:36.466: INFO: (6) /api/v1/namespaces/proxy-1544/services/https:proxy-service-5kcr7:tlsportname1/proxy/: tls baz (200; 28.044137ms)
  Jan 10 16:51:36.467: INFO: (6) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:460/proxy/: tls baz (200; 27.765762ms)
  Jan 10 16:51:36.469: INFO: (6) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:160/proxy/: foo (200; 30.038454ms)
  Jan 10 16:51:36.476: INFO: (6) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:443/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:443/proxy/tlsrewritem... (200; 37.362296ms)
  Jan 10 16:51:36.477: INFO: (6) /api/v1/namespaces/proxy-1544/services/http:proxy-service-5kcr7:portname1/proxy/: foo (200; 38.22608ms)
  Jan 10 16:51:36.478: INFO: (6) /api/v1/namespaces/proxy-1544/services/proxy-service-5kcr7:portname1/proxy/: foo (200; 39.059752ms)
  Jan 10 16:51:36.480: INFO: (6) /api/v1/namespaces/proxy-1544/services/proxy-service-5kcr7:portname2/proxy/: bar (200; 41.021929ms)
  Jan 10 16:51:36.480: INFO: (6) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:1080/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:1080/proxy/rewriteme">test<... (200; 41.152336ms)
  Jan 10 16:51:36.481: INFO: (6) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:462/proxy/: tls qux (200; 41.918943ms)
  Jan 10 16:51:36.481: INFO: (6) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:162/proxy/: bar (200; 41.94563ms)
  Jan 10 16:51:36.482: INFO: (6) /api/v1/namespaces/proxy-1544/services/https:proxy-service-5kcr7:tlsportname2/proxy/: tls qux (200; 42.57512ms)
  Jan 10 16:51:36.483: INFO: (6) /api/v1/namespaces/proxy-1544/services/http:proxy-service-5kcr7:portname2/proxy/: bar (200; 43.700475ms)
  Jan 10 16:51:36.497: INFO: (7) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:443/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:443/proxy/tlsrewritem... (200; 13.635241ms)
  Jan 10 16:51:36.497: INFO: (7) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:460/proxy/: tls baz (200; 13.605505ms)
  Jan 10 16:51:36.509: INFO: (7) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:162/proxy/: bar (200; 25.65951ms)
  Jan 10 16:51:36.509: INFO: (7) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:1080/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:1080/proxy/rewriteme">test<... (200; 25.782685ms)
  Jan 10 16:51:36.509: INFO: (7) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:160/proxy/: foo (200; 25.823145ms)
  Jan 10 16:51:36.512: INFO: (7) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr/proxy/rewriteme">test</a> (200; 28.666684ms)
  Jan 10 16:51:36.518: INFO: (7) /api/v1/namespaces/proxy-1544/services/proxy-service-5kcr7:portname2/proxy/: bar (200; 35.172618ms)
  Jan 10 16:51:36.520: INFO: (7) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:1080/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:1080/proxy/rewriteme">... (200; 36.740838ms)
  Jan 10 16:51:36.521: INFO: (7) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:160/proxy/: foo (200; 37.853748ms)
  Jan 10 16:51:36.524: INFO: (7) /api/v1/namespaces/proxy-1544/services/https:proxy-service-5kcr7:tlsportname1/proxy/: tls baz (200; 40.742257ms)
  Jan 10 16:51:36.527: INFO: (7) /api/v1/namespaces/proxy-1544/services/proxy-service-5kcr7:portname1/proxy/: foo (200; 43.921243ms)
  Jan 10 16:51:36.528: INFO: (7) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:162/proxy/: bar (200; 44.275618ms)
  Jan 10 16:51:36.533: INFO: (7) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:462/proxy/: tls qux (200; 50.169107ms)
  Jan 10 16:51:36.534: INFO: (7) /api/v1/namespaces/proxy-1544/services/https:proxy-service-5kcr7:tlsportname2/proxy/: tls qux (200; 51.04279ms)
  Jan 10 16:51:36.534: INFO: (7) /api/v1/namespaces/proxy-1544/services/http:proxy-service-5kcr7:portname2/proxy/: bar (200; 51.052844ms)
  Jan 10 16:51:36.548: INFO: (7) /api/v1/namespaces/proxy-1544/services/http:proxy-service-5kcr7:portname1/proxy/: foo (200; 64.937011ms)
  Jan 10 16:51:36.564: INFO: (8) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:1080/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:1080/proxy/rewriteme">test<... (200; 15.397602ms)
  Jan 10 16:51:36.565: INFO: (8) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:160/proxy/: foo (200; 15.683772ms)
  Jan 10 16:51:36.572: INFO: (8) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:1080/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:1080/proxy/rewriteme">... (200; 22.32028ms)
  Jan 10 16:51:36.572: INFO: (8) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:443/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:443/proxy/tlsrewritem... (200; 22.954112ms)
  Jan 10 16:51:36.573: INFO: (8) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:462/proxy/: tls qux (200; 23.841967ms)
  Jan 10 16:51:36.581: INFO: (8) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:460/proxy/: tls baz (200; 31.944324ms)
  Jan 10 16:51:36.590: INFO: (8) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:162/proxy/: bar (200; 40.15192ms)
  Jan 10 16:51:36.590: INFO: (8) /api/v1/namespaces/proxy-1544/services/https:proxy-service-5kcr7:tlsportname1/proxy/: tls baz (200; 40.418958ms)
  Jan 10 16:51:36.602: INFO: (8) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr/proxy/rewriteme">test</a> (200; 52.06079ms)
  Jan 10 16:51:36.602: INFO: (8) /api/v1/namespaces/proxy-1544/services/proxy-service-5kcr7:portname2/proxy/: bar (200; 52.542024ms)
  Jan 10 16:51:36.602: INFO: (8) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:160/proxy/: foo (200; 52.694703ms)
  Jan 10 16:51:36.603: INFO: (8) /api/v1/namespaces/proxy-1544/services/proxy-service-5kcr7:portname1/proxy/: foo (200; 52.768573ms)
  Jan 10 16:51:36.608: INFO: (8) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:162/proxy/: bar (200; 59.183328ms)
  Jan 10 16:51:36.609: INFO: (8) /api/v1/namespaces/proxy-1544/services/https:proxy-service-5kcr7:tlsportname2/proxy/: tls qux (200; 59.048106ms)
  Jan 10 16:51:36.616: INFO: (8) /api/v1/namespaces/proxy-1544/services/http:proxy-service-5kcr7:portname1/proxy/: foo (200; 66.736134ms)
  Jan 10 16:51:36.618: INFO: (8) /api/v1/namespaces/proxy-1544/services/http:proxy-service-5kcr7:portname2/proxy/: bar (200; 68.252789ms)
  Jan 10 16:51:36.634: INFO: (9) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:443/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:443/proxy/tlsrewritem... (200; 15.811003ms)
  Jan 10 16:51:36.646: INFO: (9) /api/v1/namespaces/proxy-1544/services/https:proxy-service-5kcr7:tlsportname2/proxy/: tls qux (200; 27.233792ms)
  Jan 10 16:51:36.647: INFO: (9) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr/proxy/rewriteme">test</a> (200; 28.471252ms)
  Jan 10 16:51:36.648: INFO: (9) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:162/proxy/: bar (200; 28.910619ms)
  Jan 10 16:51:36.649: INFO: (9) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:1080/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:1080/proxy/rewriteme">test<... (200; 29.575122ms)
  Jan 10 16:51:36.650: INFO: (9) /api/v1/namespaces/proxy-1544/services/proxy-service-5kcr7:portname1/proxy/: foo (200; 32.221976ms)
  Jan 10 16:51:36.652: INFO: (9) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:160/proxy/: foo (200; 31.986047ms)
  Jan 10 16:51:36.652: INFO: (9) /api/v1/namespaces/proxy-1544/services/https:proxy-service-5kcr7:tlsportname1/proxy/: tls baz (200; 33.308444ms)
  Jan 10 16:51:36.652: INFO: (9) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:462/proxy/: tls qux (200; 32.410936ms)
  Jan 10 16:51:36.652: INFO: (9) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:162/proxy/: bar (200; 32.171594ms)
  Jan 10 16:51:36.660: INFO: (9) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:160/proxy/: foo (200; 40.10452ms)
  Jan 10 16:51:36.660: INFO: (9) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:460/proxy/: tls baz (200; 41.05744ms)
  Jan 10 16:51:36.660: INFO: (9) /api/v1/namespaces/proxy-1544/services/http:proxy-service-5kcr7:portname1/proxy/: foo (200; 42.193346ms)
  Jan 10 16:51:36.660: INFO: (9) /api/v1/namespaces/proxy-1544/services/proxy-service-5kcr7:portname2/proxy/: bar (200; 41.358242ms)
  Jan 10 16:51:36.660: INFO: (9) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:1080/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:1080/proxy/rewriteme">... (200; 41.736818ms)
  Jan 10 16:51:36.661: INFO: (9) /api/v1/namespaces/proxy-1544/services/http:proxy-service-5kcr7:portname2/proxy/: bar (200; 41.683601ms)
  Jan 10 16:51:36.682: INFO: (10) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:162/proxy/: bar (200; 20.329059ms)
  Jan 10 16:51:36.687: INFO: (10) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:160/proxy/: foo (200; 25.341489ms)
  Jan 10 16:51:36.691: INFO: (10) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:162/proxy/: bar (200; 29.312614ms)
  Jan 10 16:51:36.691: INFO: (10) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:1080/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:1080/proxy/rewriteme">... (200; 29.028361ms)
  Jan 10 16:51:36.697: INFO: (10) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:462/proxy/: tls qux (200; 34.639461ms)
  Jan 10 16:51:36.697: INFO: (10) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:160/proxy/: foo (200; 34.548485ms)
  Jan 10 16:51:36.697: INFO: (10) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:460/proxy/: tls baz (200; 34.947744ms)
  Jan 10 16:51:36.697: INFO: (10) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr/proxy/rewriteme">test</a> (200; 35.087603ms)
  Jan 10 16:51:36.705: INFO: (10) /api/v1/namespaces/proxy-1544/services/https:proxy-service-5kcr7:tlsportname1/proxy/: tls baz (200; 43.467062ms)
  Jan 10 16:51:36.706: INFO: (10) /api/v1/namespaces/proxy-1544/services/http:proxy-service-5kcr7:portname1/proxy/: foo (200; 44.850436ms)
  Jan 10 16:51:36.706: INFO: (10) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:1080/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:1080/proxy/rewriteme">test<... (200; 44.102574ms)
  Jan 10 16:51:36.706: INFO: (10) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:443/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:443/proxy/tlsrewritem... (200; 44.927074ms)
  Jan 10 16:51:36.706: INFO: (10) /api/v1/namespaces/proxy-1544/services/https:proxy-service-5kcr7:tlsportname2/proxy/: tls qux (200; 44.446243ms)
  Jan 10 16:51:36.710: INFO: (10) /api/v1/namespaces/proxy-1544/services/http:proxy-service-5kcr7:portname2/proxy/: bar (200; 48.094832ms)
  Jan 10 16:51:36.711: INFO: (10) /api/v1/namespaces/proxy-1544/services/proxy-service-5kcr7:portname1/proxy/: foo (200; 49.369869ms)
  Jan 10 16:51:36.716: INFO: (10) /api/v1/namespaces/proxy-1544/services/proxy-service-5kcr7:portname2/proxy/: bar (200; 53.887649ms)
  Jan 10 16:51:36.739: INFO: (11) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:162/proxy/: bar (200; 22.347482ms)
  Jan 10 16:51:36.747: INFO: (11) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr/proxy/rewriteme">test</a> (200; 30.285654ms)
  Jan 10 16:51:36.758: INFO: (11) /api/v1/namespaces/proxy-1544/services/proxy-service-5kcr7:portname1/proxy/: foo (200; 41.165417ms)
  Jan 10 16:51:36.761: INFO: (11) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:162/proxy/: bar (200; 43.387494ms)
  Jan 10 16:51:36.761: INFO: (11) /api/v1/namespaces/proxy-1544/services/http:proxy-service-5kcr7:portname2/proxy/: bar (200; 43.507964ms)
  Jan 10 16:51:36.762: INFO: (11) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:1080/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:1080/proxy/rewriteme">... (200; 44.534304ms)
  Jan 10 16:51:36.764: INFO: (11) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:160/proxy/: foo (200; 46.415016ms)
  Jan 10 16:51:36.765: INFO: (11) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:1080/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:1080/proxy/rewriteme">test<... (200; 47.48019ms)
  Jan 10 16:51:36.770: INFO: (11) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:460/proxy/: tls baz (200; 52.923758ms)
  Jan 10 16:51:36.770: INFO: (11) /api/v1/namespaces/proxy-1544/services/https:proxy-service-5kcr7:tlsportname2/proxy/: tls qux (200; 52.732876ms)
  Jan 10 16:51:36.774: INFO: (11) /api/v1/namespaces/proxy-1544/services/proxy-service-5kcr7:portname2/proxy/: bar (200; 56.917833ms)
  Jan 10 16:51:36.778: INFO: (11) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:462/proxy/: tls qux (200; 60.785762ms)
  Jan 10 16:51:36.780: INFO: (11) /api/v1/namespaces/proxy-1544/services/http:proxy-service-5kcr7:portname1/proxy/: foo (200; 63.032311ms)
  Jan 10 16:51:36.781: INFO: (11) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:160/proxy/: foo (200; 64.066136ms)
  Jan 10 16:51:36.782: INFO: (11) /api/v1/namespaces/proxy-1544/services/https:proxy-service-5kcr7:tlsportname1/proxy/: tls baz (200; 64.305715ms)
  Jan 10 16:51:36.782: INFO: (11) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:443/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:443/proxy/tlsrewritem... (200; 64.548599ms)
  Jan 10 16:51:36.803: INFO: (12) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:460/proxy/: tls baz (200; 19.122282ms)
  Jan 10 16:51:36.804: INFO: (12) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:443/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:443/proxy/tlsrewritem... (200; 19.512997ms)
  Jan 10 16:51:36.810: INFO: (12) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:160/proxy/: foo (200; 25.598309ms)
  Jan 10 16:51:36.814: INFO: (12) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:1080/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:1080/proxy/rewriteme">test<... (200; 29.450297ms)
  Jan 10 16:51:36.815: INFO: (12) /api/v1/namespaces/proxy-1544/services/http:proxy-service-5kcr7:portname1/proxy/: foo (200; 30.671319ms)
  Jan 10 16:51:36.816: INFO: (12) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:160/proxy/: foo (200; 31.091559ms)
  Jan 10 16:51:36.817: INFO: (12) /api/v1/namespaces/proxy-1544/services/https:proxy-service-5kcr7:tlsportname2/proxy/: tls qux (200; 32.200394ms)
  Jan 10 16:51:36.818: INFO: (12) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:162/proxy/: bar (200; 32.962999ms)
  Jan 10 16:51:36.821: INFO: (12) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:162/proxy/: bar (200; 35.669332ms)
  Jan 10 16:51:36.822: INFO: (12) /api/v1/namespaces/proxy-1544/services/http:proxy-service-5kcr7:portname2/proxy/: bar (200; 37.265629ms)
  Jan 10 16:51:36.824: INFO: (12) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:1080/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:1080/proxy/rewriteme">... (200; 38.51247ms)
  Jan 10 16:51:36.825: INFO: (12) /api/v1/namespaces/proxy-1544/services/proxy-service-5kcr7:portname2/proxy/: bar (200; 39.563832ms)
  Jan 10 16:51:36.825: INFO: (12) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:462/proxy/: tls qux (200; 39.9718ms)
  Jan 10 16:51:36.825: INFO: (12) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr/proxy/rewriteme">test</a> (200; 39.557592ms)
  Jan 10 16:51:36.835: INFO: (12) /api/v1/namespaces/proxy-1544/services/proxy-service-5kcr7:portname1/proxy/: foo (200; 50.565401ms)
  Jan 10 16:51:36.840: INFO: (12) /api/v1/namespaces/proxy-1544/services/https:proxy-service-5kcr7:tlsportname1/proxy/: tls baz (200; 55.033064ms)
  Jan 10 16:51:36.857: INFO: (13) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:162/proxy/: bar (200; 14.868259ms)
  Jan 10 16:51:36.863: INFO: (13) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:1080/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:1080/proxy/rewriteme">... (200; 18.872897ms)
  Jan 10 16:51:36.867: INFO: (13) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr/proxy/rewriteme">test</a> (200; 22.650867ms)
  Jan 10 16:51:36.867: INFO: (13) /api/v1/namespaces/proxy-1544/services/https:proxy-service-5kcr7:tlsportname1/proxy/: tls baz (200; 24.525691ms)
  Jan 10 16:51:36.876: INFO: (13) /api/v1/namespaces/proxy-1544/services/http:proxy-service-5kcr7:portname1/proxy/: foo (200; 34.822284ms)
  Jan 10 16:51:36.876: INFO: (13) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:460/proxy/: tls baz (200; 31.920023ms)
  Jan 10 16:51:36.881: INFO: (13) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:1080/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:1080/proxy/rewriteme">test<... (200; 35.392273ms)
  Jan 10 16:51:36.886: INFO: (13) /api/v1/namespaces/proxy-1544/services/proxy-service-5kcr7:portname2/proxy/: bar (200; 41.383933ms)
  Jan 10 16:51:36.886: INFO: (13) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:160/proxy/: foo (200; 41.632801ms)
  Jan 10 16:51:36.891: INFO: (13) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:443/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:443/proxy/tlsrewritem... (200; 46.475125ms)
  Jan 10 16:51:36.892: INFO: (13) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:462/proxy/: tls qux (200; 46.476638ms)
  Jan 10 16:51:36.892: INFO: (13) /api/v1/namespaces/proxy-1544/services/proxy-service-5kcr7:portname1/proxy/: foo (200; 49.238252ms)
  Jan 10 16:51:36.892: INFO: (13) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:160/proxy/: foo (200; 47.666117ms)
  Jan 10 16:51:36.892: INFO: (13) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:162/proxy/: bar (200; 47.57336ms)
  Jan 10 16:51:36.894: INFO: (13) /api/v1/namespaces/proxy-1544/services/https:proxy-service-5kcr7:tlsportname2/proxy/: tls qux (200; 48.507629ms)
  Jan 10 16:51:36.899: INFO: (13) /api/v1/namespaces/proxy-1544/services/http:proxy-service-5kcr7:portname2/proxy/: bar (200; 54.373948ms)
  Jan 10 16:51:36.920: INFO: (14) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:1080/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:1080/proxy/rewriteme">test<... (200; 20.291568ms)
  Jan 10 16:51:36.923: INFO: (14) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:443/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:443/proxy/tlsrewritem... (200; 22.951279ms)
  Jan 10 16:51:36.930: INFO: (14) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:160/proxy/: foo (200; 29.428336ms)
  Jan 10 16:51:36.939: INFO: (14) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:1080/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:1080/proxy/rewriteme">... (200; 38.162071ms)
  Jan 10 16:51:36.941: INFO: (14) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr/proxy/rewriteme">test</a> (200; 40.495018ms)
  Jan 10 16:51:36.942: INFO: (14) /api/v1/namespaces/proxy-1544/services/https:proxy-service-5kcr7:tlsportname2/proxy/: tls qux (200; 42.064442ms)
  Jan 10 16:51:36.944: INFO: (14) /api/v1/namespaces/proxy-1544/services/proxy-service-5kcr7:portname1/proxy/: foo (200; 43.905491ms)
  Jan 10 16:51:36.945: INFO: (14) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:460/proxy/: tls baz (200; 45.800955ms)
  Jan 10 16:51:36.946: INFO: (14) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:162/proxy/: bar (200; 45.775204ms)
  Jan 10 16:51:36.947: INFO: (14) /api/v1/namespaces/proxy-1544/services/http:proxy-service-5kcr7:portname1/proxy/: foo (200; 46.783253ms)
  Jan 10 16:51:36.950: INFO: (14) /api/v1/namespaces/proxy-1544/services/https:proxy-service-5kcr7:tlsportname1/proxy/: tls baz (200; 49.169418ms)
  Jan 10 16:51:36.951: INFO: (14) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:462/proxy/: tls qux (200; 50.750625ms)
  Jan 10 16:51:36.952: INFO: (14) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:160/proxy/: foo (200; 51.953522ms)
  E0110 16:51:36.967958      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:51:36.969: INFO: (14) /api/v1/namespaces/proxy-1544/services/http:proxy-service-5kcr7:portname2/proxy/: bar (200; 69.668134ms)
  Jan 10 16:51:36.969: INFO: (14) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:162/proxy/: bar (200; 68.487576ms)
  Jan 10 16:51:36.971: INFO: (14) /api/v1/namespaces/proxy-1544/services/proxy-service-5kcr7:portname2/proxy/: bar (200; 70.872998ms)
  Jan 10 16:51:36.983: INFO: (15) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:160/proxy/: foo (200; 11.891606ms)
  Jan 10 16:51:36.990: INFO: (15) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:1080/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:1080/proxy/rewriteme">test<... (200; 18.674736ms)
  Jan 10 16:51:36.993: INFO: (15) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:1080/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:1080/proxy/rewriteme">... (200; 21.524257ms)
  Jan 10 16:51:36.993: INFO: (15) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:443/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:443/proxy/tlsrewritem... (200; 21.914024ms)
  Jan 10 16:51:36.994: INFO: (15) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:462/proxy/: tls qux (200; 22.156682ms)
  Jan 10 16:51:36.996: INFO: (15) /api/v1/namespaces/proxy-1544/services/https:proxy-service-5kcr7:tlsportname2/proxy/: tls qux (200; 24.325123ms)
  Jan 10 16:51:36.996: INFO: (15) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:162/proxy/: bar (200; 24.227513ms)
  Jan 10 16:51:36.996: INFO: (15) /api/v1/namespaces/proxy-1544/services/proxy-service-5kcr7:portname2/proxy/: bar (200; 24.931097ms)
  Jan 10 16:51:37.000: INFO: (15) /api/v1/namespaces/proxy-1544/services/https:proxy-service-5kcr7:tlsportname1/proxy/: tls baz (200; 28.516027ms)
  Jan 10 16:51:37.001: INFO: (15) /api/v1/namespaces/proxy-1544/services/http:proxy-service-5kcr7:portname2/proxy/: bar (200; 29.78656ms)
  Jan 10 16:51:37.010: INFO: (15) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr/proxy/rewriteme">test</a> (200; 38.742427ms)
  Jan 10 16:51:37.012: INFO: (15) /api/v1/namespaces/proxy-1544/services/http:proxy-service-5kcr7:portname1/proxy/: foo (200; 41.498275ms)
  Jan 10 16:51:37.014: INFO: (15) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:460/proxy/: tls baz (200; 42.60501ms)
  Jan 10 16:51:37.015: INFO: (15) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:162/proxy/: bar (200; 43.015384ms)
  Jan 10 16:51:37.016: INFO: (15) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:160/proxy/: foo (200; 44.558629ms)
  Jan 10 16:51:37.016: INFO: (15) /api/v1/namespaces/proxy-1544/services/proxy-service-5kcr7:portname1/proxy/: foo (200; 44.446549ms)
  Jan 10 16:51:37.037: INFO: (16) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:462/proxy/: tls qux (200; 21.248968ms)
  Jan 10 16:51:37.038: INFO: (16) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:460/proxy/: tls baz (200; 21.567469ms)
  Jan 10 16:51:37.040: INFO: (16) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:1080/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:1080/proxy/rewriteme">... (200; 23.719827ms)
  Jan 10 16:51:37.040: INFO: (16) /api/v1/namespaces/proxy-1544/services/http:proxy-service-5kcr7:portname2/proxy/: bar (200; 24.001387ms)
  Jan 10 16:51:37.040: INFO: (16) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:443/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:443/proxy/tlsrewritem... (200; 23.806544ms)
  Jan 10 16:51:37.042: INFO: (16) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr/proxy/rewriteme">test</a> (200; 25.196809ms)
  Jan 10 16:51:37.043: INFO: (16) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:162/proxy/: bar (200; 26.159387ms)
  Jan 10 16:51:37.045: INFO: (16) /api/v1/namespaces/proxy-1544/services/https:proxy-service-5kcr7:tlsportname1/proxy/: tls baz (200; 28.832167ms)
  Jan 10 16:51:37.046: INFO: (16) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:160/proxy/: foo (200; 29.212151ms)
  Jan 10 16:51:37.049: INFO: (16) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:1080/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:1080/proxy/rewriteme">test<... (200; 32.376ms)
  Jan 10 16:51:37.053: INFO: (16) /api/v1/namespaces/proxy-1544/services/proxy-service-5kcr7:portname2/proxy/: bar (200; 36.456586ms)
  Jan 10 16:51:37.056: INFO: (16) /api/v1/namespaces/proxy-1544/services/https:proxy-service-5kcr7:tlsportname2/proxy/: tls qux (200; 39.935616ms)
  Jan 10 16:51:37.059: INFO: (16) /api/v1/namespaces/proxy-1544/services/proxy-service-5kcr7:portname1/proxy/: foo (200; 42.509886ms)
  Jan 10 16:51:37.059: INFO: (16) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:162/proxy/: bar (200; 42.615326ms)
  Jan 10 16:51:37.065: INFO: (16) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:160/proxy/: foo (200; 48.127262ms)
  Jan 10 16:51:37.065: INFO: (16) /api/v1/namespaces/proxy-1544/services/http:proxy-service-5kcr7:portname1/proxy/: foo (200; 49.388639ms)
  Jan 10 16:51:37.084: INFO: (17) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:162/proxy/: bar (200; 15.252996ms)
  Jan 10 16:51:37.084: INFO: (17) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:1080/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:1080/proxy/rewriteme">... (200; 16.028743ms)
  Jan 10 16:51:37.087: INFO: (17) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr/proxy/rewriteme">test</a> (200; 18.579594ms)
  Jan 10 16:51:37.088: INFO: (17) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:162/proxy/: bar (200; 19.64553ms)
  Jan 10 16:51:37.092: INFO: (17) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:1080/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:1080/proxy/rewriteme">test<... (200; 24.310259ms)
  Jan 10 16:51:37.095: INFO: (17) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:160/proxy/: foo (200; 25.620893ms)
  Jan 10 16:51:37.098: INFO: (17) /api/v1/namespaces/proxy-1544/services/https:proxy-service-5kcr7:tlsportname2/proxy/: tls qux (200; 29.733113ms)
  Jan 10 16:51:37.110: INFO: (17) /api/v1/namespaces/proxy-1544/services/proxy-service-5kcr7:portname1/proxy/: foo (200; 42.140416ms)
  Jan 10 16:51:37.111: INFO: (17) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:462/proxy/: tls qux (200; 41.892973ms)
  Jan 10 16:51:37.112: INFO: (17) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:160/proxy/: foo (200; 43.592106ms)
  Jan 10 16:51:37.116: INFO: (17) /api/v1/namespaces/proxy-1544/services/http:proxy-service-5kcr7:portname1/proxy/: foo (200; 48.981688ms)
  Jan 10 16:51:37.116: INFO: (17) /api/v1/namespaces/proxy-1544/services/http:proxy-service-5kcr7:portname2/proxy/: bar (200; 48.689294ms)
  Jan 10 16:51:37.116: INFO: (17) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:460/proxy/: tls baz (200; 47.597072ms)
  Jan 10 16:51:37.116: INFO: (17) /api/v1/namespaces/proxy-1544/services/https:proxy-service-5kcr7:tlsportname1/proxy/: tls baz (200; 48.22455ms)
  Jan 10 16:51:37.117: INFO: (17) /api/v1/namespaces/proxy-1544/services/proxy-service-5kcr7:portname2/proxy/: bar (200; 48.949576ms)
  Jan 10 16:51:37.117: INFO: (17) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:443/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:443/proxy/tlsrewritem... (200; 47.850974ms)
  Jan 10 16:51:37.145: INFO: (18) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:462/proxy/: tls qux (200; 27.167509ms)
  Jan 10 16:51:37.146: INFO: (18) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:443/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:443/proxy/tlsrewritem... (200; 29.207111ms)
  Jan 10 16:51:37.150: INFO: (18) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:162/proxy/: bar (200; 31.901332ms)
  Jan 10 16:51:37.152: INFO: (18) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:1080/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:1080/proxy/rewriteme">test<... (200; 33.714879ms)
  Jan 10 16:51:37.152: INFO: (18) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:160/proxy/: foo (200; 33.719662ms)
  Jan 10 16:51:37.154: INFO: (18) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:162/proxy/: bar (200; 35.589938ms)
  Jan 10 16:51:37.154: INFO: (18) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:460/proxy/: tls baz (200; 36.583304ms)
  Jan 10 16:51:37.154: INFO: (18) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr/proxy/rewriteme">test</a> (200; 35.368963ms)
  Jan 10 16:51:37.162: INFO: (18) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:1080/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:1080/proxy/rewriteme">... (200; 44.399532ms)
  Jan 10 16:51:37.162: INFO: (18) /api/v1/namespaces/proxy-1544/services/proxy-service-5kcr7:portname2/proxy/: bar (200; 44.223314ms)
  Jan 10 16:51:37.162: INFO: (18) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:160/proxy/: foo (200; 43.569883ms)
  Jan 10 16:51:37.172: INFO: (18) /api/v1/namespaces/proxy-1544/services/http:proxy-service-5kcr7:portname1/proxy/: foo (200; 54.006181ms)
  Jan 10 16:51:37.172: INFO: (18) /api/v1/namespaces/proxy-1544/services/https:proxy-service-5kcr7:tlsportname1/proxy/: tls baz (200; 54.158102ms)
  Jan 10 16:51:37.172: INFO: (18) /api/v1/namespaces/proxy-1544/services/https:proxy-service-5kcr7:tlsportname2/proxy/: tls qux (200; 54.782978ms)
  Jan 10 16:51:37.173: INFO: (18) /api/v1/namespaces/proxy-1544/services/proxy-service-5kcr7:portname1/proxy/: foo (200; 54.585213ms)
  Jan 10 16:51:37.173: INFO: (18) /api/v1/namespaces/proxy-1544/services/http:proxy-service-5kcr7:portname2/proxy/: bar (200; 54.306726ms)
  Jan 10 16:51:37.199: INFO: (19) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr/proxy/rewriteme">test</a> (200; 23.654555ms)
  Jan 10 16:51:37.204: INFO: (19) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:160/proxy/: foo (200; 28.989209ms)
  Jan 10 16:51:37.211: INFO: (19) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:443/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:443/proxy/tlsrewritem... (200; 35.794216ms)
  Jan 10 16:51:37.211: INFO: (19) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:460/proxy/: tls baz (200; 35.423176ms)
  Jan 10 16:51:37.212: INFO: (19) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:160/proxy/: foo (200; 36.655674ms)
  Jan 10 16:51:37.212: INFO: (19) /api/v1/namespaces/proxy-1544/pods/https:proxy-service-5kcr7-578vr:462/proxy/: tls qux (200; 36.604473ms)
  Jan 10 16:51:37.219: INFO: (19) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:162/proxy/: bar (200; 43.192842ms)
  Jan 10 16:51:37.220: INFO: (19) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:162/proxy/: bar (200; 44.04993ms)
  Jan 10 16:51:37.222: INFO: (19) /api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:1080/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/proxy-service-5kcr7-578vr:1080/proxy/rewriteme">test<... (200; 46.581229ms)
  Jan 10 16:51:37.222: INFO: (19) /api/v1/namespaces/proxy-1544/services/https:proxy-service-5kcr7:tlsportname2/proxy/: tls qux (200; 46.672376ms)
  Jan 10 16:51:37.223: INFO: (19) /api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:1080/proxy/: <a href="/api/v1/namespaces/proxy-1544/pods/http:proxy-service-5kcr7-578vr:1080/proxy/rewriteme">... (200; 47.22164ms)
  Jan 10 16:51:37.223: INFO: (19) /api/v1/namespaces/proxy-1544/services/proxy-service-5kcr7:portname2/proxy/: bar (200; 47.705617ms)
  Jan 10 16:51:37.227: INFO: (19) /api/v1/namespaces/proxy-1544/services/https:proxy-service-5kcr7:tlsportname1/proxy/: tls baz (200; 51.719313ms)
  Jan 10 16:51:37.227: INFO: (19) /api/v1/namespaces/proxy-1544/services/proxy-service-5kcr7:portname1/proxy/: foo (200; 51.717015ms)
  Jan 10 16:51:37.227: INFO: (19) /api/v1/namespaces/proxy-1544/services/http:proxy-service-5kcr7:portname1/proxy/: foo (200; 52.440617ms)
  Jan 10 16:51:37.234: INFO: (19) /api/v1/namespaces/proxy-1544/services/http:proxy-service-5kcr7:portname2/proxy/: bar (200; 59.344885ms)
  Jan 10 16:51:37.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController proxy-service-5kcr7 in namespace proxy-1544, will wait for the garbage collector to delete the pods @ 01/10/24 16:51:37.252
  Jan 10 16:51:37.328: INFO: Deleting ReplicationController proxy-service-5kcr7 took: 15.86842ms
  Jan 10 16:51:37.428: INFO: Terminating ReplicationController proxy-service-5kcr7 pods took: 100.937514ms
  E0110 16:51:37.968660      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:51:38.969254      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "proxy-1544" for this suite. @ 01/10/24 16:51:39.83
• [5.958 seconds]
------------------------------
SSS
------------------------------
[sig-network] Services should serve multiport endpoints from pods  [Conformance]
test/e2e/network/service.go:846
  STEP: Creating a kubernetes client @ 01/10/24 16:51:39.862
  Jan 10 16:51:39.863: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename services @ 01/10/24 16:51:39.866
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:51:39.958
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:51:39.964
  E0110 16:51:39.970222      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating service multi-endpoint-test in namespace services-528 @ 01/10/24 16:51:39.97
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-528 to expose endpoints map[] @ 01/10/24 16:51:40.013
  Jan 10 16:51:40.067: INFO: successfully validated that service multi-endpoint-test in namespace services-528 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-528 @ 01/10/24 16:51:40.067
  E0110 16:51:40.970600      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:51:41.971074      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-528 to expose endpoints map[pod1:[100]] @ 01/10/24 16:51:42.165
  Jan 10 16:51:42.190: INFO: successfully validated that service multi-endpoint-test in namespace services-528 exposes endpoints map[pod1:[100]]
  STEP: Creating pod pod2 in namespace services-528 @ 01/10/24 16:51:42.19
  E0110 16:51:42.971620      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:51:43.971881      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-528 to expose endpoints map[pod1:[100] pod2:[101]] @ 01/10/24 16:51:44.252
  Jan 10 16:51:44.318: INFO: successfully validated that service multi-endpoint-test in namespace services-528 exposes endpoints map[pod1:[100] pod2:[101]]
  STEP: Checking if the Service forwards traffic to pods @ 01/10/24 16:51:44.318
  Jan 10 16:51:44.318: INFO: Creating new exec pod
  E0110 16:51:44.972174      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:51:45.972747      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:51:46.972983      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:51:47.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=services-528 exec execpodpxcpr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
  Jan 10 16:51:47.772: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
  Jan 10 16:51:47.772: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 10 16:51:47.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=services-528 exec execpodpxcpr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.52.175 80'
  E0110 16:51:47.973485      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:51:48.144: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.52.175 80\nConnection to 10.233.52.175 80 port [tcp/http] succeeded!\n"
  Jan 10 16:51:48.145: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 10 16:51:48.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=services-528 exec execpodpxcpr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
  Jan 10 16:51:48.440: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
  Jan 10 16:51:48.440: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jan 10 16:51:48.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=services-528 exec execpodpxcpr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.52.175 81'
  Jan 10 16:51:48.776: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.52.175 81\nConnection to 10.233.52.175 81 port [tcp/*] succeeded!\n"
  Jan 10 16:51:48.776: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-528 @ 01/10/24 16:51:48.776
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-528 to expose endpoints map[pod2:[101]] @ 01/10/24 16:51:48.882
  E0110 16:51:48.982440      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:51:49.982888      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:51:50.077: INFO: successfully validated that service multi-endpoint-test in namespace services-528 exposes endpoints map[pod2:[101]]
  STEP: Deleting pod pod2 in namespace services-528 @ 01/10/24 16:51:50.077
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-528 to expose endpoints map[] @ 01/10/24 16:51:50.151
  Jan 10 16:51:50.199: INFO: successfully validated that service multi-endpoint-test in namespace services-528 exposes endpoints map[]
  Jan 10 16:51:50.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-528" for this suite. @ 01/10/24 16:51:50.283
• [10.461 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:88
  STEP: Creating a kubernetes client @ 01/10/24 16:51:50.333
  Jan 10 16:51:50.333: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename projected @ 01/10/24 16:51:50.336
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:51:50.44
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:51:50.453
  STEP: Creating projection with secret that has name projected-secret-test-map-7ebfb4f7-be28-41e4-b834-4f6c4b625548 @ 01/10/24 16:51:50.462
  STEP: Creating a pod to test consume secrets @ 01/10/24 16:51:50.48
  E0110 16:51:50.983702      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:51:51.984328      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:51:52.984724      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:51:53.985589      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/10/24 16:51:54.564
  Jan 10 16:51:54.577: INFO: Trying to get logs from node env1-test-worker-1 pod pod-projected-secrets-6be3076a-c6b8-4c07-8485-f379aed43cbc container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 01/10/24 16:51:54.605
  Jan 10 16:51:54.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1344" for this suite. @ 01/10/24 16:51:54.669
• [4.358 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment Deployment should have a working scale subresource [Conformance]
test/e2e/apps/deployment.go:150
  STEP: Creating a kubernetes client @ 01/10/24 16:51:54.695
  Jan 10 16:51:54.695: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename deployment @ 01/10/24 16:51:54.698
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:51:54.803
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:51:54.81
  Jan 10 16:51:54.815: INFO: Creating simple deployment test-new-deployment
  Jan 10 16:51:54.854: INFO: deployment "test-new-deployment" doesn't have the required revision set
  E0110 16:51:54.986434      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:51:55.986802      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: getting scale subresource @ 01/10/24 16:51:56.9
  STEP: updating a scale subresource @ 01/10/24 16:51:56.92
  STEP: verifying the deployment Spec.Replicas was modified @ 01/10/24 16:51:56.949
  STEP: Patch a scale subresource @ 01/10/24 16:51:56.964
  E0110 16:51:56.987123      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:51:57.010: INFO: Deployment "test-new-deployment":
  &Deployment{ObjectMeta:{test-new-deployment  deployment-1631  05b56efb-b3c1-4f64-bf82-2f5b2f473fe7 186798331 3 2024-01-10 16:51:54 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2024-01-10 16:51:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2024-01-10 16:51:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001e40798 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2024-01-10 16:51:55 +0000 UTC,LastTransitionTime:2024-01-10 16:51:55 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-67bd4bf6dc" has successfully progressed.,LastUpdateTime:2024-01-10 16:51:55 +0000 UTC,LastTransitionTime:2024-01-10 16:51:54 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jan 10 16:51:57.023: INFO: New ReplicaSet "test-new-deployment-67bd4bf6dc" of Deployment "test-new-deployment":
  &ReplicaSet{ObjectMeta:{test-new-deployment-67bd4bf6dc  deployment-1631  9d56802a-9ef2-4ed0-9d60-b96894cb8ec4 186798330 2 2024-01-10 16:51:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 05b56efb-b3c1-4f64-bf82-2f5b2f473fe7 0xc001e41787 0xc001e41788}] [] [{kube-controller-manager Update apps/v1 2024-01-10 16:51:55 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2024-01-10 16:51:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"05b56efb-b3c1-4f64-bf82-2f5b2f473fe7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001e41818 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jan 10 16:51:57.047: INFO: Pod "test-new-deployment-67bd4bf6dc-bfq8g" is available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-bfq8g test-new-deployment-67bd4bf6dc- deployment-1631  7ab5f5ed-aead-40cd-a79a-0432f1bdfcee 186798305 0 2024-01-10 16:51:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc 9d56802a-9ef2-4ed0-9d60-b96894cb8ec4 0xc00096d477 0xc00096d478}] [] [{kube-controller-manager Update v1 2024-01-10 16:51:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9d56802a-9ef2-4ed0-9d60-b96894cb8ec4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2024-01-10 16:51:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.68.210\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tgzr5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tgzr5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:51:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:51:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:51:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:51:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.61.1.201,PodIP:10.233.68.210,StartTime:2024-01-10 16:51:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2024-01-10 16:51:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://860747a4646ab08a74194774f6f38a6f4d1c46a4bda6cc667917610f0e054db4,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.68.210,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 10 16:51:57.051: INFO: Pod "test-new-deployment-67bd4bf6dc-llblt" is not available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-llblt test-new-deployment-67bd4bf6dc- deployment-1631  d3ad160b-b5e9-42f1-8274-8f2b77063a03 186798334 0 2024-01-10 16:51:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc 9d56802a-9ef2-4ed0-9d60-b96894cb8ec4 0xc00096d657 0xc00096d658}] [] [{kube-controller-manager Update v1 2024-01-10 16:51:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9d56802a-9ef2-4ed0-9d60-b96894cb8ec4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bmpq6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bmpq6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:env1-test-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2024-01-10 16:51:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jan 10 16:51:57.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-1631" for this suite. @ 01/10/24 16:51:57.086
• [2.415 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]
test/e2e/scheduling/preemption.go:130
  STEP: Creating a kubernetes client @ 01/10/24 16:51:57.114
  Jan 10 16:51:57.114: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename sched-preemption @ 01/10/24 16:51:57.116
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:51:57.22
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:51:57.228
  Jan 10 16:51:57.278: INFO: Waiting up to 1m0s for all nodes to be ready
  E0110 16:51:57.987174      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:51:58.987229      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:51:59.988436      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:00.989364      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:01.990380      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:02.991254      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:03.991324      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:04.991458      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:05.991878      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:06.992071      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:07.992324      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:08.992568      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:09.992719      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:10.993389      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:11.994763      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:12.995180      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:13.995364      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:14.995779      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:15.996438      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:16.996681      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:17.997095      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:18.997375      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:19.998559      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:20.999044      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:22.000132      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:23.001215      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:24.001480      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:25.002451      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:26.002981      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:27.003548      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:28.004591      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:29.005090      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:30.006157      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:31.007232      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:32.007393      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:33.007844      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:34.008620      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:35.009680      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:36.010898      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:37.011098      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:38.011595      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:39.012113      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:40.012880      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:41.013296      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:42.013621      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:43.013738      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:44.013938      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:45.014129      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:46.014931      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:47.015072      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:48.015655      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:49.015382      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:50.016043      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:51.016778      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:52.017253      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:53.017543      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:54.017937      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:55.018713      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:56.018916      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:57.019299      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:52:57.412: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 01/10/24 16:52:57.419
  Jan 10 16:52:57.482: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Jan 10 16:52:57.538: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Jan 10 16:52:57.684: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Jan 10 16:52:57.705: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 01/10/24 16:52:57.705
  E0110 16:52:58.020093      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:52:59.020589      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Run a high priority pod that has same requirements as that of lower priority pod @ 01/10/24 16:52:59.767
  E0110 16:53:00.020751      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:53:01.021278      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:53:02.021953      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:53:03.022873      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:53:04.023718      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:53:05.024057      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:53:05.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0110 16:53:06.024408      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "sched-preemption-8234" for this suite. @ 01/10/24 16:53:06.04
• [68.945 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:77
  STEP: Creating a kubernetes client @ 01/10/24 16:53:06.062
  Jan 10 16:53:06.062: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename sysctl @ 01/10/24 16:53:06.064
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:53:06.12
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:53:06.133
  STEP: Creating a pod with the kernel.shm_rmid_forced sysctl @ 01/10/24 16:53:06.14
  STEP: Watching for error events or started pod @ 01/10/24 16:53:06.159
  E0110 16:53:07.024565      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:53:08.024887      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for pod completion @ 01/10/24 16:53:08.17
  E0110 16:53:09.025097      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:53:10.026302      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Checking that the pod succeeded @ 01/10/24 16:53:10.205
  STEP: Getting logs from the pod @ 01/10/24 16:53:10.205
  STEP: Checking that the sysctl is actually updated @ 01/10/24 16:53:10.227
  Jan 10 16:53:10.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-6535" for this suite. @ 01/10/24 16:53:10.245
• [4.206 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:167
  STEP: Creating a kubernetes client @ 01/10/24 16:53:10.275
  Jan 10 16:53:10.275: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename emptydir @ 01/10/24 16:53:10.277
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:53:10.336
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:53:10.345
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 01/10/24 16:53:10.362
  E0110 16:53:11.026725      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:53:12.027419      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:53:13.028531      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:53:14.029010      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/10/24 16:53:14.421
  Jan 10 16:53:14.433: INFO: Trying to get logs from node env1-test-worker-1 pod pod-2af08980-cd26-4dd4-a9ae-7a07a42c3884 container test-container: <nil>
  STEP: delete the pod @ 01/10/24 16:53:14.462
  Jan 10 16:53:14.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5987" for this suite. @ 01/10/24 16:53:14.541
• [4.285 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:236
  STEP: Creating a kubernetes client @ 01/10/24 16:53:14.563
  Jan 10 16:53:14.564: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename crd-publish-openapi @ 01/10/24 16:53:14.567
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:53:14.627
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:53:14.638
  Jan 10 16:53:14.644: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  E0110 16:53:15.030078      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:53:16.030406      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:53:17.030723      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:53:18.030776      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:53:19.031502      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:53:20.031736      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:53:21.032155      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:53:22.032588      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 01/10/24 16:53:22.842
  Jan 10 16:53:22.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=crd-publish-openapi-105 --namespace=crd-publish-openapi-105 create -f -'
  E0110 16:53:23.033197      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:53:24.033712      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:53:25.034392      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:53:25.334: INFO: stderr: ""
  Jan 10 16:53:25.334: INFO: stdout: "e2e-test-crd-publish-openapi-4171-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Jan 10 16:53:25.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=crd-publish-openapi-105 --namespace=crd-publish-openapi-105 delete e2e-test-crd-publish-openapi-4171-crds test-cr'
  Jan 10 16:53:25.574: INFO: stderr: ""
  Jan 10 16:53:25.574: INFO: stdout: "e2e-test-crd-publish-openapi-4171-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  Jan 10 16:53:25.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=crd-publish-openapi-105 --namespace=crd-publish-openapi-105 apply -f -'
  E0110 16:53:26.035368      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:53:27.036033      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:53:27.748: INFO: stderr: ""
  Jan 10 16:53:27.748: INFO: stdout: "e2e-test-crd-publish-openapi-4171-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Jan 10 16:53:27.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=crd-publish-openapi-105 --namespace=crd-publish-openapi-105 delete e2e-test-crd-publish-openapi-4171-crds test-cr'
  Jan 10 16:53:27.943: INFO: stderr: ""
  Jan 10 16:53:27.943: INFO: stdout: "e2e-test-crd-publish-openapi-4171-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 01/10/24 16:53:27.943
  Jan 10 16:53:27.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=crd-publish-openapi-105 explain e2e-test-crd-publish-openapi-4171-crds'
  E0110 16:53:28.036943      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:53:29.037805      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:53:29.963: INFO: stderr: ""
  Jan 10 16:53:29.963: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-in-nested.example.com\nKIND:       e2e-test-crd-publish-openapi-4171-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties in nested field for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  E0110 16:53:30.038332      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:53:31.039034      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:53:32.039527      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:53:33.039946      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:53:33.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-105" for this suite. @ 01/10/24 16:53:33.886
• [19.342 seconds]
------------------------------
S
------------------------------
[sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]
test/e2e/storage/empty_dir_wrapper.go:67
  STEP: Creating a kubernetes client @ 01/10/24 16:53:33.906
  Jan 10 16:53:33.906: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename emptydir-wrapper @ 01/10/24 16:53:33.908
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:53:33.956
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:53:33.966
  E0110 16:53:34.040492      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:53:35.040608      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:53:36.040773      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:53:36.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Cleaning up the secret @ 01/10/24 16:53:36.064
  STEP: Cleaning up the configmap @ 01/10/24 16:53:36.079
  STEP: Cleaning up the pod @ 01/10/24 16:53:36.096
  STEP: Destroying namespace "emptydir-wrapper-3318" for this suite. @ 01/10/24 16:53:36.138
• [2.253 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:129
  STEP: Creating a kubernetes client @ 01/10/24 16:53:36.163
  Jan 10 16:53:36.163: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename runtimeclass @ 01/10/24 16:53:36.165
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:53:36.222
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:53:36.229
  E0110 16:53:37.040877      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:53:38.041948      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:53:38.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-7475" for this suite. @ 01/10/24 16:53:38.322
• [2.190 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]
test/e2e/network/endpointslicemirroring.go:55
  STEP: Creating a kubernetes client @ 01/10/24 16:53:38.358
  Jan 10 16:53:38.358: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename endpointslicemirroring @ 01/10/24 16:53:38.36
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:53:38.409
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:53:38.418
  STEP: mirroring a new custom Endpoint @ 01/10/24 16:53:38.463
  Jan 10 16:53:38.489: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
  E0110 16:53:39.042957      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:53:40.043029      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mirroring an update to a custom Endpoint @ 01/10/24 16:53:40.503
  STEP: mirroring deletion of a custom Endpoint @ 01/10/24 16:53:40.533
  Jan 10 16:53:40.574: INFO: Waiting for 0 EndpointSlices to exist, got 1
  E0110 16:53:41.044015      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:53:42.053730      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:53:42.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslicemirroring-8748" for this suite. @ 01/10/24 16:53:42.609
• [4.294 seconds]
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:97
  STEP: Creating a kubernetes client @ 01/10/24 16:53:42.654
  Jan 10 16:53:42.654: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename emptydir @ 01/10/24 16:53:42.656
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:53:42.712
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:53:42.719
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 01/10/24 16:53:42.725
  E0110 16:53:43.054517      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:53:44.054641      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:53:45.054797      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:53:46.055119      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/10/24 16:53:46.793
  Jan 10 16:53:46.805: INFO: Trying to get logs from node env1-test-worker-1 pod pod-38af309c-bdcf-43d3-8dae-fe6028e78d27 container test-container: <nil>
  STEP: delete the pod @ 01/10/24 16:53:46.839
  Jan 10 16:53:46.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4707" for this suite. @ 01/10/24 16:53:46.902
• [4.272 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:47
  STEP: Creating a kubernetes client @ 01/10/24 16:53:46.929
  Jan 10 16:53:46.929: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename secrets @ 01/10/24 16:53:46.932
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:53:46.983
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:53:46.993
  STEP: Creating secret with name secret-test-ef5cd4f5-39ea-4afd-b9d2-d2187288126b @ 01/10/24 16:53:47.004
  STEP: Creating a pod to test consume secrets @ 01/10/24 16:53:47.018
  E0110 16:53:47.055729      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:53:48.056819      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:53:49.057148      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:53:50.058131      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:53:51.058620      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/10/24 16:53:51.072
  Jan 10 16:53:51.082: INFO: Trying to get logs from node env1-test-worker-1 pod pod-secrets-2f29f415-0b0a-4c04-af0b-c64c0dfa5f84 container secret-volume-test: <nil>
  STEP: delete the pod @ 01/10/24 16:53:51.102
  Jan 10 16:53:51.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7395" for this suite. @ 01/10/24 16:53:51.16
• [4.250 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:87
  STEP: Creating a kubernetes client @ 01/10/24 16:53:51.183
  Jan 10 16:53:51.183: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename emptydir @ 01/10/24 16:53:51.186
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:53:51.259
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:53:51.275
  STEP: Creating a pod to test emptydir volume type on tmpfs @ 01/10/24 16:53:51.287
  E0110 16:53:52.059587      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:53:53.059684      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:53:54.060211      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:53:55.060222      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/10/24 16:53:55.365
  Jan 10 16:53:55.372: INFO: Trying to get logs from node env1-test-worker-1 pod pod-f5d85da4-be2d-418e-9fc4-a281fb436eb5 container test-container: <nil>
  STEP: delete the pod @ 01/10/24 16:53:55.388
  Jan 10 16:53:55.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-2418" for this suite. @ 01/10/24 16:53:55.436
• [4.270 seconds]
------------------------------
SSS
------------------------------
[sig-node] Pods should delete a collection of pods [Conformance]
test/e2e/common/node/pods.go:846
  STEP: Creating a kubernetes client @ 01/10/24 16:53:55.454
  Jan 10 16:53:55.454: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename pods @ 01/10/24 16:53:55.456
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:53:55.511
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:53:55.517
  STEP: Create set of pods @ 01/10/24 16:53:55.524
  Jan 10 16:53:55.543: INFO: created test-pod-1
  Jan 10 16:53:55.557: INFO: created test-pod-2
  Jan 10 16:53:55.579: INFO: created test-pod-3
  STEP: waiting for all 3 pods to be running @ 01/10/24 16:53:55.579
  E0110 16:53:56.060677      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:53:57.061374      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting for all pods to be deleted @ 01/10/24 16:53:57.688
  Jan 10 16:53:57.695: INFO: Pod quantity 3 is different from expected quantity 0
  E0110 16:53:58.062069      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:53:58.708: INFO: Pod quantity 3 is different from expected quantity 0
  E0110 16:53:59.062652      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:53:59.705: INFO: Pod quantity 1 is different from expected quantity 0
  E0110 16:54:00.062884      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:54:00.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-7523" for this suite. @ 01/10/24 16:54:00.725
• [5.288 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:269
  STEP: Creating a kubernetes client @ 01/10/24 16:54:00.746
  Jan 10 16:54:00.747: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename custom-resource-definition @ 01/10/24 16:54:00.75
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:54:00.797
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:54:00.803
  Jan 10 16:54:00.810: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  E0110 16:54:01.063980      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:54:02.065063      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:54:03.065783      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:54:04.066510      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:54:05.067339      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:54:06.067510      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:54:07.067959      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:54:08.068253      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:54:09.070003      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:54:09.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-2619" for this suite. @ 01/10/24 16:54:09.176
• [8.444 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for ExternalName services [Conformance]
test/e2e/network/dns.go:329
  STEP: Creating a kubernetes client @ 01/10/24 16:54:09.195
  Jan 10 16:54:09.195: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename dns @ 01/10/24 16:54:09.197
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:54:09.241
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:54:09.251
  STEP: Creating a test externalName service @ 01/10/24 16:54:09.259
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-608.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-608.svc.cluster.local; sleep 1; done
   @ 01/10/24 16:54:09.28
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-608.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-608.svc.cluster.local; sleep 1; done
   @ 01/10/24 16:54:09.28
  STEP: creating a pod to probe DNS @ 01/10/24 16:54:09.281
  STEP: submitting the pod to kubernetes @ 01/10/24 16:54:09.282
  E0110 16:54:10.070052      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:54:11.070172      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 01/10/24 16:54:11.368
  STEP: looking for the results for each expected name from probers @ 01/10/24 16:54:11.379
  Jan 10 16:54:11.394: INFO: File wheezy_udp@dns-test-service-3.dns-608.svc.cluster.local from pod  dns-608/dns-test-70c04cda-271f-4584-9582-0170737fb215 contains '' instead of 'foo.example.com.'
  Jan 10 16:54:11.411: INFO: Lookups using dns-608/dns-test-70c04cda-271f-4584-9582-0170737fb215 failed for: [wheezy_udp@dns-test-service-3.dns-608.svc.cluster.local]

  E0110 16:54:12.070526      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:54:13.071014      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:54:14.071685      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:54:15.072725      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:54:16.073561      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:54:16.431: INFO: DNS probes using dns-test-70c04cda-271f-4584-9582-0170737fb215 succeeded

  STEP: changing the externalName to bar.example.com @ 01/10/24 16:54:16.431
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-608.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-608.svc.cluster.local; sleep 1; done
   @ 01/10/24 16:54:16.458
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-608.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-608.svc.cluster.local; sleep 1; done
   @ 01/10/24 16:54:16.458
  STEP: creating a second pod to probe DNS @ 01/10/24 16:54:16.459
  STEP: submitting the pod to kubernetes @ 01/10/24 16:54:16.459
  E0110 16:54:17.073954      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:54:18.074230      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:54:19.074387      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:54:20.075376      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 01/10/24 16:54:20.516
  STEP: looking for the results for each expected name from probers @ 01/10/24 16:54:20.523
  Jan 10 16:54:20.535: INFO: File wheezy_udp@dns-test-service-3.dns-608.svc.cluster.local from pod  dns-608/dns-test-34381fa7-7858-4d58-8a88-ac581c8e2f48 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Jan 10 16:54:20.545: INFO: File jessie_udp@dns-test-service-3.dns-608.svc.cluster.local from pod  dns-608/dns-test-34381fa7-7858-4d58-8a88-ac581c8e2f48 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Jan 10 16:54:20.545: INFO: Lookups using dns-608/dns-test-34381fa7-7858-4d58-8a88-ac581c8e2f48 failed for: [wheezy_udp@dns-test-service-3.dns-608.svc.cluster.local jessie_udp@dns-test-service-3.dns-608.svc.cluster.local]

  E0110 16:54:21.076428      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:54:22.076964      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:54:23.077348      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:54:24.077790      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:54:25.077715      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:54:25.569: INFO: DNS probes using dns-test-34381fa7-7858-4d58-8a88-ac581c8e2f48 succeeded

  STEP: changing the service to type=ClusterIP @ 01/10/24 16:54:25.57
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-608.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-608.svc.cluster.local; sleep 1; done
   @ 01/10/24 16:54:25.604
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-608.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-608.svc.cluster.local; sleep 1; done
   @ 01/10/24 16:54:25.604
  STEP: creating a third pod to probe DNS @ 01/10/24 16:54:25.604
  STEP: submitting the pod to kubernetes @ 01/10/24 16:54:25.616
  E0110 16:54:26.078721      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:54:27.079123      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:54:28.079922      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:54:29.079947      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 01/10/24 16:54:29.709
  STEP: looking for the results for each expected name from probers @ 01/10/24 16:54:29.716
  Jan 10 16:54:29.738: INFO: DNS probes using dns-test-280e9c34-c613-4a5b-adbf-d83912a031a1 succeeded

  Jan 10 16:54:29.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 01/10/24 16:54:29.752
  STEP: deleting the pod @ 01/10/24 16:54:29.785
  STEP: deleting the pod @ 01/10/24 16:54:29.84
  STEP: deleting the test externalName service @ 01/10/24 16:54:29.883
  STEP: Destroying namespace "dns-608" for this suite. @ 01/10/24 16:54:29.976
• [20.810 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
test/e2e/apimachinery/garbage_collector.go:638
  STEP: Creating a kubernetes client @ 01/10/24 16:54:30.016
  Jan 10 16:54:30.016: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename gc @ 01/10/24 16:54:30.019
  E0110 16:54:30.080560      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:54:30.088
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:54:30.095
  STEP: create the rc @ 01/10/24 16:54:30.125
  W0110 16:54:30.147368      23 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0110 16:54:31.080881      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:54:32.081687      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:54:33.085550      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:54:34.085692      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:54:35.085975      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:54:36.086270      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 01/10/24 16:54:36.159
  STEP: wait for the rc to be deleted @ 01/10/24 16:54:36.174
  E0110 16:54:37.086412      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:54:37.237: INFO: 80 pods remaining
  Jan 10 16:54:37.237: INFO: 80 pods has nil DeletionTimestamp
  Jan 10 16:54:37.237: INFO: 
  E0110 16:54:38.087678      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:54:38.201: INFO: 73 pods remaining
  Jan 10 16:54:38.201: INFO: 71 pods has nil DeletionTimestamp
  Jan 10 16:54:38.202: INFO: 
  E0110 16:54:39.088569      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:54:39.202: INFO: 60 pods remaining
  Jan 10 16:54:39.202: INFO: 60 pods has nil DeletionTimestamp
  Jan 10 16:54:39.202: INFO: 
  E0110 16:54:40.089287      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:54:40.201: INFO: 40 pods remaining
  Jan 10 16:54:40.201: INFO: 40 pods has nil DeletionTimestamp
  Jan 10 16:54:40.201: INFO: 
  E0110 16:54:41.093646      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:54:41.228: INFO: 32 pods remaining
  Jan 10 16:54:41.228: INFO: 32 pods has nil DeletionTimestamp
  Jan 10 16:54:41.228: INFO: 
  E0110 16:54:42.093647      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:54:42.188: INFO: 20 pods remaining
  Jan 10 16:54:42.188: INFO: 20 pods has nil DeletionTimestamp
  Jan 10 16:54:42.188: INFO: 
  E0110 16:54:43.094058      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 01/10/24 16:54:43.192
  Jan 10 16:54:43.483: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jan 10 16:54:43.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-9383" for this suite. @ 01/10/24 16:54:43.494
• [13.493 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/secrets_volume.go:386
  STEP: Creating a kubernetes client @ 01/10/24 16:54:43.51
  Jan 10 16:54:43.510: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename secrets @ 01/10/24 16:54:43.511
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:54:43.554
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:54:43.56
  Jan 10 16:54:43.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-4417" for this suite. @ 01/10/24 16:54:43.68
• [0.182 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]
test/e2e/apimachinery/resource_quota.go:232
  STEP: Creating a kubernetes client @ 01/10/24 16:54:43.696
  Jan 10 16:54:43.696: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename resourcequota @ 01/10/24 16:54:43.698
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:54:43.724
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:54:43.728
  STEP: Counting existing ResourceQuota @ 01/10/24 16:54:43.731
  E0110 16:54:44.094480      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:54:45.095383      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:54:46.095546      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:54:47.096060      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:54:48.096571      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 01/10/24 16:54:48.74
  STEP: Ensuring resource quota status is calculated @ 01/10/24 16:54:48.754
  E0110 16:54:49.097671      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:54:50.098706      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Pod that fits quota @ 01/10/24 16:54:50.769
  STEP: Ensuring ResourceQuota status captures the pod usage @ 01/10/24 16:54:50.819
  E0110 16:54:51.099939      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:54:52.100712      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Not allowing a pod to be created that exceeds remaining quota @ 01/10/24 16:54:52.831
  STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) @ 01/10/24 16:54:52.837
  STEP: Ensuring a pod cannot update its resource requirements @ 01/10/24 16:54:52.843
  STEP: Ensuring attempts to update pod resource requirements did not change quota usage @ 01/10/24 16:54:52.855
  E0110 16:54:53.101291      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:54:54.101669      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 01/10/24 16:54:54.863
  STEP: Ensuring resource quota status released the pod usage @ 01/10/24 16:54:54.916
  E0110 16:54:55.102497      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:54:56.102945      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:54:56.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-1174" for this suite. @ 01/10/24 16:54:56.961
• [13.292 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:167
  STEP: Creating a kubernetes client @ 01/10/24 16:54:56.989
  Jan 10 16:54:56.989: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename downward-api @ 01/10/24 16:54:56.991
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:54:57.043
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:54:57.05
  STEP: Creating a pod to test downward api env vars @ 01/10/24 16:54:57.076
  E0110 16:54:57.103461      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:54:58.104034      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:54:59.105766      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:00.106952      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:01.107154      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/10/24 16:55:01.146
  Jan 10 16:55:01.156: INFO: Trying to get logs from node env1-test-worker-1 pod downward-api-07db7ca1-059e-445e-823b-9470c20cefee container dapi-container: <nil>
  STEP: delete the pod @ 01/10/24 16:55:01.187
  Jan 10 16:55:01.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2740" for this suite. @ 01/10/24 16:55:01.255
• [4.286 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:236
  STEP: Creating a kubernetes client @ 01/10/24 16:55:01.299
  Jan 10 16:55:01.300: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename downward-api @ 01/10/24 16:55:01.304
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:55:01.366
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:55:01.375
  STEP: Creating a pod to test downward API volume plugin @ 01/10/24 16:55:01.381
  E0110 16:55:02.107488      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:03.107906      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:04.108678      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:05.108721      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/10/24 16:55:05.438
  Jan 10 16:55:05.447: INFO: Trying to get logs from node env1-test-worker-1 pod downwardapi-volume-18ababcd-aa56-419f-9879-35204f30757f container client-container: <nil>
  STEP: delete the pod @ 01/10/24 16:55:05.465
  Jan 10 16:55:05.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-737" for this suite. @ 01/10/24 16:55:05.529
• [4.289 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]
test/e2e/kubectl/kubectl.go:1574
  STEP: Creating a kubernetes client @ 01/10/24 16:55:05.589
  Jan 10 16:55:05.589: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename kubectl @ 01/10/24 16:55:05.591
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:55:05.662
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:55:05.67
  STEP: creating the pod @ 01/10/24 16:55:05.679
  Jan 10 16:55:05.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-3732 create -f -'
  E0110 16:55:06.109184      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:07.109793      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:08.110538      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:55:08.818: INFO: stderr: ""
  Jan 10 16:55:08.818: INFO: stdout: "pod/pause created\n"
  E0110 16:55:09.111271      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:10.111463      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: adding the label testing-label with value testing-label-value to a pod @ 01/10/24 16:55:10.854
  Jan 10 16:55:10.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-3732 label pods pause testing-label=testing-label-value'
  Jan 10 16:55:11.089: INFO: stderr: ""
  Jan 10 16:55:11.089: INFO: stdout: "pod/pause labeled\n"
  STEP: verifying the pod has the label testing-label with the value testing-label-value @ 01/10/24 16:55:11.089
  Jan 10 16:55:11.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-3732 get pod pause -L testing-label'
  E0110 16:55:11.112625      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:55:11.279: INFO: stderr: ""
  Jan 10 16:55:11.279: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
  STEP: removing the label testing-label of a pod @ 01/10/24 16:55:11.279
  Jan 10 16:55:11.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-3732 label pods pause testing-label-'
  Jan 10 16:55:11.497: INFO: stderr: ""
  Jan 10 16:55:11.497: INFO: stdout: "pod/pause unlabeled\n"
  STEP: verifying the pod doesn't have the label testing-label @ 01/10/24 16:55:11.497
  Jan 10 16:55:11.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-3732 get pod pause -L testing-label'
  Jan 10 16:55:11.706: INFO: stderr: ""
  Jan 10 16:55:11.706: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
  STEP: using delete to clean up resources @ 01/10/24 16:55:11.707
  Jan 10 16:55:11.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-3732 delete --grace-period=0 --force -f -'
  Jan 10 16:55:11.940: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jan 10 16:55:11.940: INFO: stdout: "pod \"pause\" force deleted\n"
  Jan 10 16:55:11.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-3732 get rc,svc -l name=pause --no-headers'
  E0110 16:55:12.113426      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:55:12.171: INFO: stderr: "No resources found in kubectl-3732 namespace.\n"
  Jan 10 16:55:12.171: INFO: stdout: ""
  Jan 10 16:55:12.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-3732 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Jan 10 16:55:12.357: INFO: stderr: ""
  Jan 10 16:55:12.357: INFO: stdout: ""
  Jan 10 16:55:12.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3732" for this suite. @ 01/10/24 16:55:12.391
• [6.825 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]
test/e2e/auth/certificates.go:200
  STEP: Creating a kubernetes client @ 01/10/24 16:55:12.425
  Jan 10 16:55:12.426: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename certificates @ 01/10/24 16:55:12.427
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:55:12.464
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:55:12.471
  E0110 16:55:13.113627      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:14.113795      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: getting /apis @ 01/10/24 16:55:15.045
  STEP: getting /apis/certificates.k8s.io @ 01/10/24 16:55:15.089
  STEP: getting /apis/certificates.k8s.io/v1 @ 01/10/24 16:55:15.093
  STEP: creating @ 01/10/24 16:55:15.096
  E0110 16:55:15.114642      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: getting @ 01/10/24 16:55:15.14
  STEP: listing @ 01/10/24 16:55:15.149
  STEP: watching @ 01/10/24 16:55:15.161
  Jan 10 16:55:15.161: INFO: starting watch
  STEP: patching @ 01/10/24 16:55:15.163
  STEP: updating @ 01/10/24 16:55:15.176
  Jan 10 16:55:15.188: INFO: waiting for watch events with expected annotations
  Jan 10 16:55:15.188: INFO: saw patched and updated annotations
  STEP: getting /approval @ 01/10/24 16:55:15.189
  STEP: patching /approval @ 01/10/24 16:55:15.2
  STEP: updating /approval @ 01/10/24 16:55:15.216
  STEP: getting /status @ 01/10/24 16:55:15.232
  STEP: patching /status @ 01/10/24 16:55:15.239
  STEP: updating /status @ 01/10/24 16:55:15.258
  STEP: deleting @ 01/10/24 16:55:15.275
  STEP: deleting a collection @ 01/10/24 16:55:15.311
  Jan 10 16:55:15.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "certificates-7561" for this suite. @ 01/10/24 16:55:15.366
• [2.961 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
test/e2e/instrumentation/events.go:98
  STEP: Creating a kubernetes client @ 01/10/24 16:55:15.393
  Jan 10 16:55:15.394: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename events @ 01/10/24 16:55:15.397
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:55:15.45
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:55:15.458
  STEP: creating a test event @ 01/10/24 16:55:15.464
  STEP: listing events in all namespaces @ 01/10/24 16:55:15.499
  STEP: listing events in test namespace @ 01/10/24 16:55:15.508
  STEP: listing events with field selection filtering on source @ 01/10/24 16:55:15.517
  STEP: listing events with field selection filtering on reportingController @ 01/10/24 16:55:15.526
  STEP: getting the test event @ 01/10/24 16:55:15.543
  STEP: patching the test event @ 01/10/24 16:55:15.551
  STEP: getting the test event @ 01/10/24 16:55:15.574
  STEP: updating the test event @ 01/10/24 16:55:15.591
  STEP: getting the test event @ 01/10/24 16:55:15.61
  STEP: deleting the test event @ 01/10/24 16:55:15.619
  STEP: listing events in all namespaces @ 01/10/24 16:55:15.637
  STEP: listing events in test namespace @ 01/10/24 16:55:15.648
  Jan 10 16:55:15.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-3328" for this suite. @ 01/10/24 16:55:15.669
• [0.291 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:334
  STEP: Creating a kubernetes client @ 01/10/24 16:55:15.688
  Jan 10 16:55:15.688: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename init-container @ 01/10/24 16:55:15.69
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:55:15.726
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:55:15.733
  STEP: creating the pod @ 01/10/24 16:55:15.74
  Jan 10 16:55:15.741: INFO: PodSpec: initContainers in spec.initContainers
  E0110 16:55:16.114918      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:17.116393      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:18.116647      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:19.117213      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:20.118339      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:21.118770      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:22.119544      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:23.120497      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:24.120913      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:25.120893      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:26.121580      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:27.121756      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:28.122321      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:29.122980      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:30.123849      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:31.124675      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:32.125039      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:33.125916      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:34.125758      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:35.126810      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:36.127695      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:37.128303      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:38.128471      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:39.129107      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:40.129605      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:41.129967      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:42.130286      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:43.131191      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:44.131491      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:45.131583      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:46.132127      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:47.132537      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:48.132790      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:49.132881      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:50.133770      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:51.134659      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:52.134955      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:53.134719      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:54.135152      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:55.135357      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:56.135657      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:57.135926      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:58.136405      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:55:59.137578      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:55:59.403: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-f9b8c2de-1f2d-44f9-b546-12e5a4274a29", GenerateName:"", Namespace:"init-container-5813", SelfLink:"", UID:"62eb93aa-c89b-4bd1-bd7f-b17d37c3eb9c", ResourceVersion:"186801551", Generation:0, CreationTimestamp:time.Date(2024, time.January, 10, 16, 55, 15, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"741081915"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2024, time.January, 10, 16, 55, 15, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00a72a1e0), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2024, time.January, 10, 16, 55, 59, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00a72a210), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-24g4n", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc009bc8c60), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-24g4n", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-24g4n", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-24g4n", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc009bcc5d8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"env1-test-worker-1", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00a712000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc009bcc650)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc009bcc670)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc009bcc678), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc009bcc67c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc009bd0ac0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2024, time.January, 10, 16, 55, 15, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2024, time.January, 10, 16, 55, 15, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2024, time.January, 10, 16, 55, 15, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2024, time.January, 10, 16, 55, 15, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.61.1.201", PodIP:"10.233.68.27", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.233.68.27"}}, StartTime:time.Date(2024, time.January, 10, 16, 55, 15, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00a7120e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00a712150)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://1f0f424a20e606c02cb70183c96af2c39b65ba3fa954123aaacc8750d676fe4e", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc009bc8ce0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc009bc8cc0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc009bcc6ff), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil), Resize:""}}
  Jan 10 16:55:59.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-5813" for this suite. @ 01/10/24 16:55:59.421
• [43.750 seconds]
------------------------------
SSSS
------------------------------
[sig-network] Services should test the lifecycle of an Endpoint [Conformance]
test/e2e/network/service.go:3138
  STEP: Creating a kubernetes client @ 01/10/24 16:55:59.438
  Jan 10 16:55:59.438: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename services @ 01/10/24 16:55:59.44
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:55:59.476
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:55:59.482
  STEP: creating an Endpoint @ 01/10/24 16:55:59.501
  STEP: waiting for available Endpoint @ 01/10/24 16:55:59.514
  STEP: listing all Endpoints @ 01/10/24 16:55:59.518
  STEP: updating the Endpoint @ 01/10/24 16:55:59.529
  STEP: fetching the Endpoint @ 01/10/24 16:55:59.546
  STEP: patching the Endpoint @ 01/10/24 16:55:59.557
  STEP: fetching the Endpoint @ 01/10/24 16:55:59.593
  STEP: deleting the Endpoint by Collection @ 01/10/24 16:55:59.6
  STEP: waiting for Endpoint deletion @ 01/10/24 16:55:59.621
  STEP: fetching the Endpoint @ 01/10/24 16:55:59.624
  Jan 10 16:55:59.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-7345" for this suite. @ 01/10/24 16:55:59.653
• [0.227 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]
test/e2e/kubectl/kubectl.go:830
  STEP: Creating a kubernetes client @ 01/10/24 16:55:59.666
  Jan 10 16:55:59.666: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename kubectl @ 01/10/24 16:55:59.668
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:55:59.708
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:55:59.713
  STEP: validating api versions @ 01/10/24 16:55:59.721
  Jan 10 16:55:59.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=kubectl-6286 api-versions'
  Jan 10 16:55:59.927: INFO: stderr: ""
  Jan 10 16:55:59.927: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncns.vmware.com/v1alpha1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nmygroup.example.com/v1\nmygroup.example.com/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\ntraefik.containo.us/v1alpha1\nv1\nvelero.io/v1\n"
  Jan 10 16:55:59.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6286" for this suite. @ 01/10/24 16:55:59.943
• [0.296 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:276
  STEP: Creating a kubernetes client @ 01/10/24 16:55:59.965
  Jan 10 16:55:59.965: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename crd-publish-openapi @ 01/10/24 16:55:59.967
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:56:00.011
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:56:00.02
  STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation @ 01/10/24 16:56:00.029
  Jan 10 16:56:00.030: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  E0110 16:56:00.138329      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:56:01.138413      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:56:02.138909      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:56:03.139848      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:56:04.140483      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:56:05.140695      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:56:06.141767      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:56:07.142767      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:56:08.143540      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:56:09.143698      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:56:09.194: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  E0110 16:56:10.144314      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:56:11.145498      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:56:12.146018      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:56:13.146478      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:56:14.146439      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:56:15.147498      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:56:16.148486      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:56:17.149585      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:56:18.150683      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:56:19.151187      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:56:20.152292      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:56:21.153050      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:56:22.153727      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:56:23.155023      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:56:24.156243      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:56:25.157553      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:56:26.157838      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:56:27.158922      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:56:28.160222      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:56:29.160850      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:56:29.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-5917" for this suite. @ 01/10/24 16:56:29.705
• [29.757 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]
test/e2e/apps/daemon_set.go:166
  STEP: Creating a kubernetes client @ 01/10/24 16:56:29.721
  Jan 10 16:56:29.721: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename daemonsets @ 01/10/24 16:56:29.723
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:56:29.762
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:56:29.768
  STEP: Creating simple DaemonSet "daemon-set" @ 01/10/24 16:56:29.825
  STEP: Check that daemon pods launch on every node of the cluster. @ 01/10/24 16:56:29.84
  Jan 10 16:56:29.856: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 16:56:29.857: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 16:56:29.857: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 16:56:29.866: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 10 16:56:29.866: INFO: Node env1-test-worker-0 is running 0 daemon pod, expected 1
  E0110 16:56:30.161705      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:56:30.880: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 16:56:30.880: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 16:56:30.880: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 16:56:30.894: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jan 10 16:56:30.894: INFO: Node env1-test-worker-1 is running 0 daemon pod, expected 1
  E0110 16:56:31.162601      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:56:31.879: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 16:56:31.879: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 16:56:31.879: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 16:56:31.888: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jan 10 16:56:31.888: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Stop a daemon pod, check that the daemon pod is revived. @ 01/10/24 16:56:31.893
  Jan 10 16:56:31.944: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 16:56:31.944: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 16:56:31.944: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 16:56:31.959: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jan 10 16:56:31.960: INFO: Node env1-test-worker-0 is running 0 daemon pod, expected 1
  E0110 16:56:32.163204      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:56:32.981: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 16:56:32.981: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 16:56:32.981: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 16:56:32.993: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jan 10 16:56:32.994: INFO: Node env1-test-worker-0 is running 0 daemon pod, expected 1
  E0110 16:56:33.164128      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:56:33.985: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 16:56:33.985: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 16:56:33.986: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 16:56:33.995: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jan 10 16:56:33.995: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 01/10/24 16:56:34.008
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4348, will wait for the garbage collector to delete the pods @ 01/10/24 16:56:34.008
  Jan 10 16:56:34.087: INFO: Deleting DaemonSet.extensions daemon-set took: 19.595718ms
  E0110 16:56:34.165067      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:56:34.188: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.206872ms
  E0110 16:56:35.164700      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:56:36.005: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 10 16:56:36.005: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jan 10 16:56:36.016: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"186801812"},"items":null}

  Jan 10 16:56:36.024: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"186801812"},"items":null}

  Jan 10 16:56:36.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-4348" for this suite. @ 01/10/24 16:56:36.081
• [6.378 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:117
  STEP: Creating a kubernetes client @ 01/10/24 16:56:36.1
  Jan 10 16:56:36.100: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename field-validation @ 01/10/24 16:56:36.102
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:56:36.148
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:56:36.155
  STEP: apply creating a deployment @ 01/10/24 16:56:36.164
  E0110 16:56:36.164972      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:56:36.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-9427" for this suite. @ 01/10/24 16:56:36.225
• [0.149 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
test/e2e/network/endpointslice.go:207
  STEP: Creating a kubernetes client @ 01/10/24 16:56:36.258
  Jan 10 16:56:36.258: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename endpointslice @ 01/10/24 16:56:36.26
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:56:36.296
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:56:36.304
  E0110 16:56:37.165533      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:56:38.165808      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:56:39.165788      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:56:40.169558      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:56:41.171918      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing a single matching pod @ 01/10/24 16:56:41.69
  E0110 16:56:42.173206      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:56:43.173684      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:56:44.173867      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:56:45.173965      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:56:46.174246      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing matching pods with named port @ 01/10/24 16:56:46.716
  E0110 16:56:47.174336      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:56:48.174478      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:56:49.175082      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:56:50.175323      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:56:51.175378      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating empty Endpoints and EndpointSlices for no matching Pods @ 01/10/24 16:56:51.734
  E0110 16:56:52.176491      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:56:53.176933      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:56:54.177550      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:56:55.178584      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:56:56.179440      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: recreating EndpointSlices after they've been deleted @ 01/10/24 16:56:56.751
  Jan 10 16:56:56.833: INFO: EndpointSlice for Service endpointslice-8961/example-named-port not found
  E0110 16:56:57.180087      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:56:58.181252      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:56:59.181799      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:00.182008      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:01.182603      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:02.183469      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:03.184123      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:04.184947      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:05.185648      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:06.185824      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:57:06.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-8961" for this suite. @ 01/10/24 16:57:06.874
• [30.642 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:52
  STEP: Creating a kubernetes client @ 01/10/24 16:57:06.904
  Jan 10 16:57:06.904: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename kubelet-test @ 01/10/24 16:57:06.906
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:57:06.958
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:57:06.965
  E0110 16:57:07.186949      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:08.187757      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 16:57:09.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-982" for this suite. @ 01/10/24 16:57:09.089
• [2.217 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:93
  STEP: Creating a kubernetes client @ 01/10/24 16:57:09.125
  Jan 10 16:57:09.125: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename configmap @ 01/10/24 16:57:09.127
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:57:09.166
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:57:09.174
  STEP: Creating configMap configmap-9507/configmap-test-f44f5f71-8543-4889-b63e-4118543f20db @ 01/10/24 16:57:09.181
  E0110 16:57:09.187989      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a pod to test consume configMaps @ 01/10/24 16:57:09.194
  E0110 16:57:10.188388      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:11.188979      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:12.189091      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:13.189134      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/10/24 16:57:13.265
  Jan 10 16:57:13.278: INFO: Trying to get logs from node env1-test-worker-1 pod pod-configmaps-6444c5c7-e4c1-4ae0-ae22-95e04bb996c6 container env-test: <nil>
  STEP: delete the pod @ 01/10/24 16:57:13.307
  Jan 10 16:57:13.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9507" for this suite. @ 01/10/24 16:57:13.373
• [4.277 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota should apply changes to a resourcequota status [Conformance]
test/e2e/apimachinery/resource_quota.go:1013
  STEP: Creating a kubernetes client @ 01/10/24 16:57:13.402
  Jan 10 16:57:13.403: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename resourcequota @ 01/10/24 16:57:13.406
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 16:57:13.454
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 16:57:13.467
  STEP: Creating resourceQuota "e2e-rq-status-bstq8" @ 01/10/24 16:57:13.496
  Jan 10 16:57:13.548: INFO: Resource quota "e2e-rq-status-bstq8" reports spec: hard cpu limit of 500m
  Jan 10 16:57:13.548: INFO: Resource quota "e2e-rq-status-bstq8" reports spec: hard memory limit of 500Mi
  STEP: Updating resourceQuota "e2e-rq-status-bstq8" /status @ 01/10/24 16:57:13.548
  STEP: Confirm /status for "e2e-rq-status-bstq8" resourceQuota via watch @ 01/10/24 16:57:13.597
  Jan 10 16:57:13.603: INFO: observed resourceQuota "e2e-rq-status-bstq8" in namespace "resourcequota-9209" with hard status: v1.ResourceList(nil)
  Jan 10 16:57:13.603: INFO: Found resourceQuota "e2e-rq-status-bstq8" in namespace "resourcequota-9209" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Jan 10 16:57:13.603: INFO: ResourceQuota "e2e-rq-status-bstq8" /status was updated
  STEP: Patching hard spec values for cpu & memory @ 01/10/24 16:57:13.613
  Jan 10 16:57:13.632: INFO: Resource quota "e2e-rq-status-bstq8" reports spec: hard cpu limit of 1
  Jan 10 16:57:13.632: INFO: Resource quota "e2e-rq-status-bstq8" reports spec: hard memory limit of 1Gi
  STEP: Patching "e2e-rq-status-bstq8" /status @ 01/10/24 16:57:13.632
  STEP: Confirm /status for "e2e-rq-status-bstq8" resourceQuota via watch @ 01/10/24 16:57:13.66
  Jan 10 16:57:13.664: INFO: observed resourceQuota "e2e-rq-status-bstq8" in namespace "resourcequota-9209" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Jan 10 16:57:13.664: INFO: Found resourceQuota "e2e-rq-status-bstq8" in namespace "resourcequota-9209" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
  Jan 10 16:57:13.664: INFO: ResourceQuota "e2e-rq-status-bstq8" /status was patched
  STEP: Get "e2e-rq-status-bstq8" /status @ 01/10/24 16:57:13.664
  Jan 10 16:57:13.673: INFO: Resourcequota "e2e-rq-status-bstq8" reports status: hard cpu of 1
  Jan 10 16:57:13.673: INFO: Resourcequota "e2e-rq-status-bstq8" reports status: hard memory of 1Gi
  STEP: Repatching "e2e-rq-status-bstq8" /status before checking Spec is unchanged @ 01/10/24 16:57:13.681
  Jan 10 16:57:13.698: INFO: Resourcequota "e2e-rq-status-bstq8" reports status: hard cpu of 2
  Jan 10 16:57:13.698: INFO: Resourcequota "e2e-rq-status-bstq8" reports status: hard memory of 2Gi
  Jan 10 16:57:13.702: INFO: Found resourceQuota "e2e-rq-status-bstq8" in namespace "resourcequota-9209" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
  E0110 16:57:14.189972      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:15.190443      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:16.191021      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:17.191622      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:18.191778      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:19.192788      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:20.192960      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:21.193916      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:22.194788      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:23.195522      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:24.195568      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:25.196676      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:26.197437      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:27.198253      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:28.198938      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:29.200021      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:30.200233      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:31.201055      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:32.201676      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:33.202284      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:34.203360      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:35.204309      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:36.204872      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:37.205412      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:38.205627      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:39.205874      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:40.206040      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:41.207029      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:42.208203      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:43.208868      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:44.209021      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:45.209776      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:46.209944      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:47.210122      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:48.210373      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:49.210892      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:50.211309      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:51.211977      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:52.212137      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:53.212861      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:54.213719      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:55.213906      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:56.214869      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:57.214964      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:58.215951      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:57:59.216191      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:00.220535      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:01.221022      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:02.221734      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:03.221752      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:04.222346      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:05.222855      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:06.222959      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:07.223307      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:08.223523      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:09.223974      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:10.224199      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:11.224913      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:12.225460      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:13.226031      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:14.226853      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:15.227058      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:16.227329      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:17.227751      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:18.228463      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:19.228813      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:20.229296      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:21.230036      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:22.230255      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:23.230747      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:24.231322      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:25.231399      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:26.231747      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:27.232074      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:28.232599      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:29.232843      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:30.233039      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:31.233538      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:32.234388      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:33.235041      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:34.235986      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:35.236417      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:36.236866      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:37.237545      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:38.238013      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:39.239147      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:40.239520      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:41.240395      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:42.240879      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:43.241901      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:44.242568      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:45.243222      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:46.243662      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:47.243978      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:48.244468      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:49.245518      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:50.245822      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:51.246258      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:52.247553      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:53.247893      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:54.248486      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:55.248322      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:56.248694      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:57.249233      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:58.249503      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:58:59.249599      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:00.249940      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:01.250492      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:02.250445      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:03.251641      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:04.252612      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:05.253004      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:06.253243      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:07.253356      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:08.253565      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:09.254479      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:10.254971      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:11.255235      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:12.255691      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:13.255611      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:14.255895      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:15.256743      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:16.257103      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:17.257598      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:18.257612      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:19.258859      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:20.259649      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:21.260181      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:22.260337      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:23.261467      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:24.262527      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:25.263386      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:26.263812      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:27.264042      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:28.264550      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:29.264678      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:30.265136      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:31.265693      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:32.266191      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:33.266357      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:34.267464      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:35.267638      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:36.268851      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:37.269521      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:38.269645      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:39.269935      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:40.270211      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:41.271193      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:42.271539      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:43.271794      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:44.273967      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:45.274431      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:46.275062      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:47.275617      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:48.275773      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:49.276559      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:50.276888      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:51.277835      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:52.277503      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:53.278426      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:54.278570      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:55.278870      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:56.279888      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:57.280787      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:58.281650      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 16:59:59.281908      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:00.282006      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:01.282747      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:02.283074      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:03.283288      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:04.284176      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:05.285023      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:06.285487      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:07.286011      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:08.286406      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 17:00:08.722: INFO: ResourceQuota "e2e-rq-status-bstq8" Spec was unchanged and /status reset
  Jan 10 17:00:08.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9209" for this suite. @ 01/10/24 17:00:08.733
• [175.345 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:402
  STEP: Creating a kubernetes client @ 01/10/24 17:00:08.752
  Jan 10 17:00:08.752: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename webhook @ 01/10/24 17:00:08.755
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 17:00:08.858
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 17:00:08.867
  STEP: Setting up server cert @ 01/10/24 17:00:08.915
  E0110 17:00:09.286961      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 01/10/24 17:00:09.995
  STEP: Deploying the webhook pod @ 01/10/24 17:00:10.011
  STEP: Wait for the deployment to be ready @ 01/10/24 17:00:10.035
  Jan 10 17:00:10.051: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0110 17:00:10.288088      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:11.288939      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 01/10/24 17:00:12.076
  STEP: Verifying the service has paired with the endpoint @ 01/10/24 17:00:12.098
  E0110 17:00:12.290960      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 17:00:13.098: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a validating webhook configuration @ 01/10/24 17:00:13.108
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 01/10/24 17:00:13.136
  STEP: Updating a validating webhook configuration's rules to not include the create operation @ 01/10/24 17:00:13.151
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 01/10/24 17:00:13.168
  STEP: Patching a validating webhook configuration's rules to include the create operation @ 01/10/24 17:00:13.186
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 01/10/24 17:00:13.199
  Jan 10 17:00:13.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0110 17:00:13.291932      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-4566" for this suite. @ 01/10/24 17:00:13.357
  STEP: Destroying namespace "webhook-markers-3459" for this suite. @ 01/10/24 17:00:13.376
• [4.651 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]
test/e2e/apps/cronjob.go:161
  STEP: Creating a kubernetes client @ 01/10/24 17:00:13.404
  Jan 10 17:00:13.404: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename cronjob @ 01/10/24 17:00:13.409
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 17:00:13.451
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 17:00:13.466
  STEP: Creating a ReplaceConcurrent cronjob @ 01/10/24 17:00:13.479
  STEP: Ensuring a job is scheduled @ 01/10/24 17:00:13.509
  E0110 17:00:14.292369      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:15.292463      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:16.292665      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:17.293147      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:18.293382      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:19.293590      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:20.293875      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:21.294747      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:22.295761      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:23.296186      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:24.296351      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:25.297155      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:26.297364      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:27.297573      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:28.297849      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:29.298323      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:30.299322      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:31.299719      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:32.299845      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:33.300110      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:34.300393      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:35.301208      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:36.301841      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:37.302686      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:38.303693      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:39.304590      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:40.304845      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:41.305210      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:42.305394      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:43.305562      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:44.305855      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:45.305998      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:46.306129      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:47.307019      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:48.307112      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:49.307375      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:50.308106      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:51.308640      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:52.308954      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:53.309708      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:54.310333      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:55.311115      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:56.311334      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:57.311952      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:58.311988      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:00:59.312470      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:00.313144      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:01.313753      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring exactly one is scheduled @ 01/10/24 17:01:01.518
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 01/10/24 17:01:01.531
  STEP: Ensuring the job is replaced with a new one @ 01/10/24 17:01:01.542
  E0110 17:01:02.314072      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:03.314504      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:04.314524      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:05.315020      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:06.316123      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:07.316704      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:08.317150      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:09.317570      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:10.318448      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:11.318930      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:12.319128      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:13.319381      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:14.319580      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:15.319754      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:16.319947      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:17.320126      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:18.320337      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:19.320671      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:20.321666      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:21.322020      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:22.322267      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:23.322505      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:24.324534      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:25.324679      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:26.325264      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:27.325662      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:28.325800      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:29.326274      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:30.326999      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:31.327360      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:32.327893      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:33.328423      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:34.328739      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:35.329066      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:36.329683      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:37.329803      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:38.330155      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:39.330737      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:40.330999      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:41.331489      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:42.332279      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:43.332459      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:44.332780      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:45.333842      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:46.334997      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:47.336018      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:48.336292      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:49.337359      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:50.338047      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:51.338279      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:52.338532      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:53.339163      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:54.339670      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:55.339830      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:56.340002      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:57.340921      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:58.341652      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:01:59.342106      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:02:00.342511      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:02:01.343030      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing cronjob @ 01/10/24 17:02:01.55
  Jan 10 17:02:01.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-2391" for this suite. @ 01/10/24 17:02:01.585
• [108.220 seconds]
------------------------------
SSS
------------------------------
[sig-network] Ingress API should support creating Ingress API operations [Conformance]
test/e2e/network/ingress.go:556
  STEP: Creating a kubernetes client @ 01/10/24 17:02:01.625
  Jan 10 17:02:01.625: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename ingress @ 01/10/24 17:02:01.627
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 17:02:01.687
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 17:02:01.694
  STEP: getting /apis @ 01/10/24 17:02:01.706
  STEP: getting /apis/networking.k8s.io @ 01/10/24 17:02:01.715
  STEP: getting /apis/networking.k8s.iov1 @ 01/10/24 17:02:01.718
  STEP: creating @ 01/10/24 17:02:01.721
  STEP: getting @ 01/10/24 17:02:01.754
  STEP: listing @ 01/10/24 17:02:01.764
  STEP: watching @ 01/10/24 17:02:01.772
  Jan 10 17:02:01.772: INFO: starting watch
  STEP: cluster-wide listing @ 01/10/24 17:02:01.775
  STEP: cluster-wide watching @ 01/10/24 17:02:01.782
  Jan 10 17:02:01.782: INFO: starting watch
  STEP: patching @ 01/10/24 17:02:01.785
  STEP: updating @ 01/10/24 17:02:01.804
  Jan 10 17:02:01.821: INFO: waiting for watch events with expected annotations
  Jan 10 17:02:01.821: INFO: saw patched and updated annotations
  STEP: patching /status @ 01/10/24 17:02:01.822
  STEP: updating /status @ 01/10/24 17:02:01.832
  STEP: get /status @ 01/10/24 17:02:01.854
  STEP: deleting @ 01/10/24 17:02:01.862
  STEP: deleting a collection @ 01/10/24 17:02:01.9
  Jan 10 17:02:01.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingress-5051" for this suite. @ 01/10/24 17:02:01.954
• [0.344 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support CSIVolumeSource in Pod API [Conformance]
test/e2e/storage/csi_inline.go:131
  STEP: Creating a kubernetes client @ 01/10/24 17:02:01.972
  Jan 10 17:02:01.972: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename csiinlinevolumes @ 01/10/24 17:02:01.973
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 17:02:02.001
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 17:02:02.009
  STEP: creating @ 01/10/24 17:02:02.014
  STEP: getting @ 01/10/24 17:02:02.043
  STEP: listing in namespace @ 01/10/24 17:02:02.058
  STEP: patching @ 01/10/24 17:02:02.074
  STEP: deleting @ 01/10/24 17:02:02.124
  Jan 10 17:02:02.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-2127" for this suite. @ 01/10/24 17:02:02.172
• [0.225 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:85
  STEP: Creating a kubernetes client @ 01/10/24 17:02:02.197
  Jan 10 17:02:02.198: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename downward-api @ 01/10/24 17:02:02.2
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 17:02:02.243
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 17:02:02.25
  STEP: Creating a pod to test downward API volume plugin @ 01/10/24 17:02:02.257
  E0110 17:02:02.343776      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:02:03.344116      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:02:04.344933      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:02:05.344975      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/10/24 17:02:06.318
  Jan 10 17:02:06.326: INFO: Trying to get logs from node env1-test-worker-1 pod downwardapi-volume-5d1b5da3-3925-44ae-b421-eb61190e5f86 container client-container: <nil>
  E0110 17:02:06.345631      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod @ 01/10/24 17:02:06.376
  Jan 10 17:02:06.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8251" for this suite. @ 01/10/24 17:02:06.423
• [4.241 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
test/e2e/apps/job.go:370
  STEP: Creating a kubernetes client @ 01/10/24 17:02:06.444
  Jan 10 17:02:06.444: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename job @ 01/10/24 17:02:06.446
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 17:02:06.482
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 17:02:06.487
  STEP: Creating Indexed job @ 01/10/24 17:02:06.492
  STEP: Ensuring job reaches completions @ 01/10/24 17:02:06.506
  E0110 17:02:07.346784      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:02:08.347333      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:02:09.349689      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:02:10.349855      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:02:11.350020      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:02:12.350850      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:02:13.351002      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:02:14.351230      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:02:15.351678      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:02:16.352134      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring pods with index for job exist @ 01/10/24 17:02:16.519
  Jan 10 17:02:16.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-360" for this suite. @ 01/10/24 17:02:16.547
• [10.121 seconds]
------------------------------
[sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]
test/e2e/network/service.go:1455
  STEP: Creating a kubernetes client @ 01/10/24 17:02:16.566
  Jan 10 17:02:16.566: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename services @ 01/10/24 17:02:16.569
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 17:02:16.626
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 17:02:16.634
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-1702 @ 01/10/24 17:02:16.646
  STEP: changing the ExternalName service to type=NodePort @ 01/10/24 17:02:16.663
  STEP: creating replication controller externalname-service in namespace services-1702 @ 01/10/24 17:02:16.722
  I0110 17:02:16.756501      23 runners.go:194] Created replication controller with name: externalname-service, namespace: services-1702, replica count: 2
  E0110 17:02:17.353037      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:02:18.353428      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:02:19.353728      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0110 17:02:19.811759      23 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jan 10 17:02:19.811: INFO: Creating new exec pod
  E0110 17:02:20.354538      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:02:21.354866      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:02:22.356081      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 17:02:22.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=services-1702 exec execpodttthh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Jan 10 17:02:23.317: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Jan 10 17:02:23.317: INFO: stdout: "externalname-service-k5rr8"
  Jan 10 17:02:23.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=services-1702 exec execpodttthh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.13.69 80'
  E0110 17:02:23.356586      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 17:02:23.668: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.13.69 80\nConnection to 10.233.13.69 80 port [tcp/http] succeeded!\n"
  Jan 10 17:02:23.668: INFO: stdout: ""
  E0110 17:02:24.356924      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 17:02:24.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=services-1702 exec execpodttthh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.13.69 80'
  Jan 10 17:02:24.984: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.13.69 80\nConnection to 10.233.13.69 80 port [tcp/http] succeeded!\n"
  Jan 10 17:02:24.985: INFO: stdout: "externalname-service-k5rr8"
  Jan 10 17:02:24.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=services-1702 exec execpodttthh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.61.1.200 31255'
  Jan 10 17:02:25.301: INFO: stderr: "+ nc -v -t -w 2 10.61.1.200 31255\n+ echo hostName\nConnection to 10.61.1.200 31255 port [tcp/*] succeeded!\n"
  Jan 10 17:02:25.301: INFO: stdout: ""
  E0110 17:02:25.357449      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 17:02:26.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=services-1702 exec execpodttthh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.61.1.200 31255'
  E0110 17:02:26.358366      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 17:02:26.607: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.61.1.200 31255\nConnection to 10.61.1.200 31255 port [tcp/*] succeeded!\n"
  Jan 10 17:02:26.607: INFO: stdout: ""
  Jan 10 17:02:27.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=services-1702 exec execpodttthh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.61.1.200 31255'
  E0110 17:02:27.358476      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 17:02:27.630: INFO: stderr: "+ + nc -vecho -t -w hostName 2 10.61.1.200\n 31255\nConnection to 10.61.1.200 31255 port [tcp/*] succeeded!\n"
  Jan 10 17:02:27.630: INFO: stdout: "externalname-service-k5rr8"
  Jan 10 17:02:27.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2435010822 --namespace=services-1702 exec execpodttthh -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.61.1.201 31255'
  Jan 10 17:02:27.943: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.61.1.201 31255\nConnection to 10.61.1.201 31255 port [tcp/*] succeeded!\n"
  Jan 10 17:02:27.943: INFO: stdout: "externalname-service-k5rr8"
  Jan 10 17:02:27.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jan 10 17:02:27.958: INFO: Cleaning up the ExternalName to NodePort test service
  STEP: Destroying namespace "services-1702" for this suite. @ 01/10/24 17:02:28.045
• [11.506 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]
test/e2e/apps/daemon_set.go:294
  STEP: Creating a kubernetes client @ 01/10/24 17:02:28.074
  Jan 10 17:02:28.074: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename daemonsets @ 01/10/24 17:02:28.076
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 17:02:28.121
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 17:02:28.131
  STEP: Creating a simple DaemonSet "daemon-set" @ 01/10/24 17:02:28.187
  STEP: Check that daemon pods launch on every node of the cluster. @ 01/10/24 17:02:28.208
  Jan 10 17:02:28.231: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 17:02:28.231: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 17:02:28.231: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 17:02:28.245: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 10 17:02:28.245: INFO: Node env1-test-worker-0 is running 0 daemon pod, expected 1
  E0110 17:02:28.360574      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 17:02:29.259: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 17:02:29.259: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 17:02:29.259: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 17:02:29.268: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 10 17:02:29.268: INFO: Node env1-test-worker-0 is running 0 daemon pod, expected 1
  E0110 17:02:29.360951      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 17:02:30.264: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 17:02:30.264: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 17:02:30.264: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 17:02:30.275: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jan 10 17:02:30.276: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. @ 01/10/24 17:02:30.284
  Jan 10 17:02:30.346: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 17:02:30.347: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 17:02:30.347: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0110 17:02:30.361566      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 17:02:30.369: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jan 10 17:02:30.370: INFO: Node env1-test-worker-1 is running 0 daemon pod, expected 1
  E0110 17:02:31.361746      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 17:02:31.393: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 17:02:31.393: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 17:02:31.394: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 17:02:31.409: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jan 10 17:02:31.409: INFO: Node env1-test-worker-1 is running 0 daemon pod, expected 1
  E0110 17:02:32.362808      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 17:02:32.386: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 17:02:32.386: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 17:02:32.386: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 17:02:32.400: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jan 10 17:02:32.400: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Wait for the failed daemon pod to be completely deleted. @ 01/10/24 17:02:32.401
  STEP: Deleting DaemonSet "daemon-set" @ 01/10/24 17:02:32.419
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7398, will wait for the garbage collector to delete the pods @ 01/10/24 17:02:32.419
  Jan 10 17:02:32.530: INFO: Deleting DaemonSet.extensions daemon-set took: 50.420716ms
  Jan 10 17:02:32.730: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.489487ms
  E0110 17:02:33.364211      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 17:02:33.749: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 10 17:02:33.749: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jan 10 17:02:33.758: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"186803781"},"items":null}

  Jan 10 17:02:33.767: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"186803781"},"items":null}

  Jan 10 17:02:33.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-7398" for this suite. @ 01/10/24 17:02:33.834
• [5.783 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Hostname [Conformance]
test/e2e/network/dns.go:244
  STEP: Creating a kubernetes client @ 01/10/24 17:02:33.86
  Jan 10 17:02:33.860: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename dns @ 01/10/24 17:02:33.861
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 17:02:33.941
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 17:02:33.95
  STEP: Creating a test headless service @ 01/10/24 17:02:33.957
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4217.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-4217.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
   @ 01/10/24 17:02:33.978
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4217.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-4217.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
   @ 01/10/24 17:02:33.978
  STEP: creating a pod to probe DNS @ 01/10/24 17:02:33.978
  STEP: submitting the pod to kubernetes @ 01/10/24 17:02:33.978
  E0110 17:02:34.364360      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:02:35.365059      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 01/10/24 17:02:36.038
  STEP: looking for the results for each expected name from probers @ 01/10/24 17:02:36.051
  Jan 10 17:02:36.102: INFO: DNS probes using dns-4217/dns-test-da4a8693-ac7b-49e7-95c8-20ef63b36a1f succeeded

  Jan 10 17:02:36.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 01/10/24 17:02:36.116
  STEP: deleting the test headless service @ 01/10/24 17:02:36.149
  STEP: Destroying namespace "dns-4217" for this suite. @ 01/10/24 17:02:36.252
• [2.411 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/configmap_volume.go:504
  STEP: Creating a kubernetes client @ 01/10/24 17:02:36.273
  Jan 10 17:02:36.273: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename configmap @ 01/10/24 17:02:36.275
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 17:02:36.314
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 17:02:36.323
  E0110 17:02:36.365928      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 17:02:36.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8053" for this suite. @ 01/10/24 17:02:36.472
• [0.212 seconds]
------------------------------
SS
------------------------------
[sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:399
  STEP: Creating a kubernetes client @ 01/10/24 17:02:36.487
  Jan 10 17:02:36.488: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename pods @ 01/10/24 17:02:36.491
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 17:02:36.523
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 17:02:36.53
  STEP: creating the pod @ 01/10/24 17:02:36.535
  STEP: submitting the pod to kubernetes @ 01/10/24 17:02:36.535
  W0110 17:02:36.557689      23 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  E0110 17:02:37.366715      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:02:38.367155      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod is in kubernetes @ 01/10/24 17:02:38.605
  STEP: updating the pod @ 01/10/24 17:02:38.622
  Jan 10 17:02:39.167: INFO: Successfully updated pod "pod-update-activedeadlineseconds-90c66fd2-2f0c-44a4-95b7-cc162dddee30"
  E0110 17:02:39.368552      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:02:40.368721      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:02:41.368781      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:02:42.369120      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 17:02:43.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-6836" for this suite. @ 01/10/24 17:02:43.228
• [6.759 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
test/e2e/scheduling/predicates.go:705
  STEP: Creating a kubernetes client @ 01/10/24 17:02:43.259
  Jan 10 17:02:43.259: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename sched-pred @ 01/10/24 17:02:43.26
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 17:02:43.297
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 17:02:43.301
  Jan 10 17:02:43.307: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jan 10 17:02:43.325: INFO: Waiting for terminating namespaces to be deleted...
  Jan 10 17:02:43.334: INFO: 
  Logging pods the apiserver thinks is on node env1-test-worker-0 before test
  Jan 10 17:02:43.358: INFO: filebeat-filebeat-q5bzw from filebeat started at 2024-01-10 01:14:23 +0000 UTC (1 container statuses recorded)
  Jan 10 17:02:43.358: INFO: 	Container filebeat ready: false, restart count 0
  Jan 10 17:02:43.358: INFO: kube-flannel-r8g5h from kube-system started at 2024-01-09 16:24:44 +0000 UTC (1 container statuses recorded)
  Jan 10 17:02:43.358: INFO: 	Container kube-flannel ready: true, restart count 0
  Jan 10 17:02:43.358: INFO: kube-proxy-445hs from kube-system started at 2024-01-10 09:13:13 +0000 UTC (1 container statuses recorded)
  Jan 10 17:02:43.358: INFO: 	Container kube-proxy ready: true, restart count 0
  Jan 10 17:02:43.358: INFO: metrics-server-6b7574f5b-jmbtm from kube-system started at 2024-01-09 16:32:13 +0000 UTC (1 container statuses recorded)
  Jan 10 17:02:43.358: INFO: 	Container metrics-server ready: true, restart count 0
  Jan 10 17:02:43.358: INFO: nginx-proxy-env1-test-worker-0 from kube-system started at 2024-01-09 16:31:48 +0000 UTC (1 container statuses recorded)
  Jan 10 17:02:43.358: INFO: 	Container nginx-proxy ready: true, restart count 0
  Jan 10 17:02:43.359: INFO: nodelocaldns-vkvkp from kube-system started at 2024-01-09 15:52:20 +0000 UTC (1 container statuses recorded)
  Jan 10 17:02:43.359: INFO: 	Container node-cache ready: true, restart count 0
  Jan 10 17:02:43.359: INFO: vsphere-csi-node-qkfth from kube-system started at 2024-01-10 09:21:38 +0000 UTC (3 container statuses recorded)
  Jan 10 17:02:43.359: INFO: 	Container liveness-probe ready: true, restart count 0
  Jan 10 17:02:43.359: INFO: 	Container node-driver-registrar ready: true, restart count 0
  Jan 10 17:02:43.359: INFO: 	Container vsphere-csi-node ready: true, restart count 0
  Jan 10 17:02:43.359: INFO: prometheus-kube-prometheus-operator-5f847644d6-gkll8 from prometheus started at 2024-01-10 03:09:01 +0000 UTC (1 container statuses recorded)
  Jan 10 17:02:43.359: INFO: 	Container kube-prometheus-stack ready: true, restart count 0
  Jan 10 17:02:43.359: INFO: prometheus-kube-state-metrics-f8b6b59f-g2nrl from prometheus started at 2024-01-10 03:09:01 +0000 UTC (1 container statuses recorded)
  Jan 10 17:02:43.359: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Jan 10 17:02:43.359: INFO: prometheus-prometheus-kube-prometheus-prometheus-0 from prometheus started at 2024-01-10 03:09:07 +0000 UTC (3 container statuses recorded)
  Jan 10 17:02:43.359: INFO: 	Container config-reloader ready: true, restart count 0
  Jan 10 17:02:43.359: INFO: 	Container prometheus ready: false, restart count 167
  Jan 10 17:02:43.359: INFO: 	Container thanos-sidecar ready: true, restart count 0
  Jan 10 17:02:43.359: INFO: prometheus-prometheus-node-exporter-4nknm from prometheus started at 2024-01-10 03:09:01 +0000 UTC (1 container statuses recorded)
  Jan 10 17:02:43.359: INFO: 	Container node-exporter ready: true, restart count 0
  Jan 10 17:02:43.359: INFO: thanos-query-6f697d54b8-sp4fg from prometheus started at 2024-01-10 13:06:38 +0000 UTC (1 container statuses recorded)
  Jan 10 17:02:43.359: INFO: 	Container query ready: true, restart count 0
  Jan 10 17:02:43.359: INFO: sonobuoy-systemd-logs-daemon-set-187fce9e0ddc499b-9m7s6 from sonobuoy started at 2024-01-10 15:26:38 +0000 UTC (2 container statuses recorded)
  Jan 10 17:02:43.359: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jan 10 17:02:43.359: INFO: 	Container systemd-logs ready: true, restart count 0
  Jan 10 17:02:43.359: INFO: traefik-ingress-g4tjs from traefik-ingress started at 2024-01-10 14:39:42 +0000 UTC (1 container statuses recorded)
  Jan 10 17:02:43.359: INFO: 	Container traefik-ingress ready: true, restart count 0
  Jan 10 17:02:43.359: INFO: velero-794b84894f-nwdwf from velero started at 2024-01-10 13:26:26 +0000 UTC (1 container statuses recorded)
  Jan 10 17:02:43.359: INFO: 	Container velero ready: true, restart count 0
  Jan 10 17:02:43.359: INFO: 
  Logging pods the apiserver thinks is on node env1-test-worker-1 before test
  E0110 17:02:43.369565      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 17:02:43.383: INFO: pod-csi-inline-volumes from csiinlinevolumes-2127 started at 2024-01-10 17:02:02 +0000 UTC (1 container statuses recorded)
  Jan 10 17:02:43.383: INFO: 	Container pod-csi-inline-volumes ready: false, restart count 0
  Jan 10 17:02:43.383: INFO: filebeat-filebeat-dxph4 from filebeat started at 2024-01-10 15:44:08 +0000 UTC (1 container statuses recorded)
  Jan 10 17:02:43.383: INFO: 	Container filebeat ready: false, restart count 0
  Jan 10 17:02:43.383: INFO: kube-flannel-jxf5s from kube-system started at 2024-01-09 16:24:09 +0000 UTC (1 container statuses recorded)
  Jan 10 17:02:43.383: INFO: 	Container kube-flannel ready: true, restart count 0
  Jan 10 17:02:43.383: INFO: kube-proxy-78tcd from kube-system started at 2024-01-10 09:13:13 +0000 UTC (1 container statuses recorded)
  Jan 10 17:02:43.383: INFO: 	Container kube-proxy ready: true, restart count 0
  Jan 10 17:02:43.383: INFO: nginx-proxy-env1-test-worker-1 from kube-system started at 2024-01-09 16:38:31 +0000 UTC (1 container statuses recorded)
  Jan 10 17:02:43.383: INFO: 	Container nginx-proxy ready: true, restart count 0
  Jan 10 17:02:43.383: INFO: nodelocaldns-7qx4w from kube-system started at 2024-01-09 15:52:12 +0000 UTC (1 container statuses recorded)
  Jan 10 17:02:43.383: INFO: 	Container node-cache ready: true, restart count 0
  Jan 10 17:02:43.383: INFO: vsphere-csi-node-lr59t from kube-system started at 2024-01-10 09:21:38 +0000 UTC (3 container statuses recorded)
  Jan 10 17:02:43.383: INFO: 	Container liveness-probe ready: true, restart count 0
  Jan 10 17:02:43.383: INFO: 	Container node-driver-registrar ready: true, restart count 0
  Jan 10 17:02:43.383: INFO: 	Container vsphere-csi-node ready: true, restart count 0
  Jan 10 17:02:43.383: INFO: pod-update-activedeadlineseconds-90c66fd2-2f0c-44a4-95b7-cc162dddee30 from pods-6836 started at 2024-01-10 17:02:36 +0000 UTC (1 container statuses recorded)
  Jan 10 17:02:43.383: INFO: 	Container pause ready: false, restart count 0
  Jan 10 17:02:43.383: INFO: prometheus-prometheus-node-exporter-788fx from prometheus started at 2024-01-10 15:53:35 +0000 UTC (1 container statuses recorded)
  Jan 10 17:02:43.383: INFO: 	Container node-exporter ready: true, restart count 0
  Jan 10 17:02:43.383: INFO: sonobuoy from sonobuoy started at 2024-01-10 15:26:36 +0000 UTC (1 container statuses recorded)
  Jan 10 17:02:43.383: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jan 10 17:02:43.383: INFO: sonobuoy-e2e-job-b46f6697883e4f52 from sonobuoy started at 2024-01-10 15:26:37 +0000 UTC (2 container statuses recorded)
  Jan 10 17:02:43.383: INFO: 	Container e2e ready: true, restart count 0
  Jan 10 17:02:43.383: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jan 10 17:02:43.383: INFO: sonobuoy-systemd-logs-daemon-set-187fce9e0ddc499b-nntgv from sonobuoy started at 2024-01-10 15:26:38 +0000 UTC (2 container statuses recorded)
  Jan 10 17:02:43.384: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jan 10 17:02:43.384: INFO: 	Container systemd-logs ready: true, restart count 0
  Jan 10 17:02:43.384: INFO: traefik-ingress-lggr6 from traefik-ingress started at 2024-01-10 15:53:37 +0000 UTC (1 container statuses recorded)
  Jan 10 17:02:43.384: INFO: 	Container traefik-ingress ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 01/10/24 17:02:43.384
  E0110 17:02:44.370445      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:02:45.370791      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 01/10/24 17:02:45.43
  STEP: Trying to apply a random label on the found node. @ 01/10/24 17:02:45.481
  STEP: verifying the node has the label kubernetes.io/e2e-7a7a48a5-f781-4a12-8297-81a86ee85c01 95 @ 01/10/24 17:02:45.53
  STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled @ 01/10/24 17:02:45.562
  E0110 17:02:46.371864      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:02:47.372368      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:02:48.372694      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:02:49.373061      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.61.1.201 on the node which pod4 resides and expect not scheduled @ 01/10/24 17:02:49.623
  E0110 17:02:50.373484      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:02:51.374334      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:02:52.374827      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:02:53.375489      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:02:54.375760      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:02:55.375850      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:02:56.376293      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:02:57.376977      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:02:58.377089      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:02:59.377975      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:00.378097      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:01.378478      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:02.378665      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:03.379116      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:04.379430      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:05.380417      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:06.381762      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:07.381685      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:08.382026      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:09.382878      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:10.383847      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:11.384420      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:12.384543      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:13.384772      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:14.385210      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:15.385828      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:16.386270      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:17.386681      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:18.387170      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:19.388167      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:20.388508      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:21.388730      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:22.389074      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:23.389712      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:24.389965      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:25.390555      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:26.390804      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:27.391196      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:28.391557      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:29.391973      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:30.392350      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:31.392866      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:32.393607      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:33.393796      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:34.394751      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:35.395450      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:36.395425      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:37.395670      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:38.396625      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:39.397379      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:40.397411      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:41.397618      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:42.397832      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:43.398072      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:44.398752      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:45.399063      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:46.399890      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:47.400359      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:48.400744      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:49.401156      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:50.402139      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:51.402735      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:52.402947      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:53.403486      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:54.404253      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:55.405007      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:56.405192      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:57.405446      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:58.406373      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:03:59.407325      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:00.407329      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:01.407770      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:02.408321      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:03.408725      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:04.409802      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:05.410337      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:06.411100      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:07.411553      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:08.412560      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:09.413651      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:10.414340      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:11.414853      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:12.415410      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:13.415697      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:14.416795      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:15.416952      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:16.418188      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:17.418850      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:18.419029      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:19.419441      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:20.419480      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:21.420105      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:22.420266      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:23.420905      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:24.421524      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:25.422048      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:26.422177      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:27.422426      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:28.422629      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:29.423065      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:30.424265      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:31.424099      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:32.424941      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:33.425202      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:34.426181      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:35.426751      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:36.427231      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:37.427532      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:38.428526      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:39.429407      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:40.429677      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:41.429839      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:42.430521      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:43.430596      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:44.431549      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:45.432142      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:46.432805      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:47.433901      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:48.434800      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:49.434880      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:50.435224      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:51.435522      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:52.436195      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:53.436371      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:54.437205      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:55.437660      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:56.438650      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:57.439662      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:58.440416      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:04:59.441633      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:00.442445      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:01.442897      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:02.443552      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:03.443697      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:04.444534      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:05.444687      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:06.445461      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:07.445599      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:08.446128      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:09.446400      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:10.446716      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:11.447717      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:12.448034      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:13.448196      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:14.449090      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:15.449069      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:16.449713      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:17.449919      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:18.450441      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:19.450938      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:20.451472      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:21.451445      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:22.451726      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:23.452169      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:24.452080      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:25.452485      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:26.452778      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:27.453005      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:28.453601      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:29.454292      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:30.454798      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:31.455713      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:32.456432      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:33.457045      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:34.457777      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:35.457921      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:36.458144      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:37.459280      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:38.459812      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:39.460393      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:40.460987      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:41.461205      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:42.461497      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:43.461666      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:44.461790      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:45.462013      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:46.462431      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:47.462641      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:48.462893      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:49.463389      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:50.463668      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:51.463844      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:52.464358      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:53.464510      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:54.465567      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:55.465829      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:56.466342      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:57.466611      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:58.467331      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:05:59.467326      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:00.467997      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:01.468527      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:02.469230      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:03.469034      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:04.470286      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:05.470653      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:06.471324      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:07.472348      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:08.472967      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:09.473519      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:10.473616      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:11.474569      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:12.475214      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:13.476347      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:14.477580      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:15.477711      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:16.477960      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:17.478964      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:18.479140      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:19.479218      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:20.479439      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:21.480351      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:22.481136      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:23.481448      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:24.482613      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:25.483635      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:26.483841      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:27.484905      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:28.485161      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:29.485949      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:30.486477      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:31.487818      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:32.487886      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:33.488816      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:34.489018      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:35.489949      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:36.490616      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:37.491851      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:38.492511      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:39.493619      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:40.493834      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:41.494075      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:42.495096      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:43.496078      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:44.497089      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:45.497331      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:46.497694      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:47.498678      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:48.499442      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:49.499757      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:50.500435      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:51.501163      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:52.501403      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:53.501659      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:54.502641      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:55.503019      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:56.503668      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:57.504314      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:58.504516      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:06:59.505142      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:00.505920      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:01.506267      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:02.506109      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:03.507136      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:04.508427      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:05.508914      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:06.509338      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:07.509432      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:08.510297      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:09.511530      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:10.512137      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:11.513576      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:12.514188      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:13.515024      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:14.515091      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:15.515742      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:16.516389      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:17.517246      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:18.517725      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:19.518950      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:20.520328      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:21.521496      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:22.521552      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:23.521796      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:24.522773      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:25.523517      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:26.523846      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:27.524789      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:28.525408      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:29.525942      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:30.526270      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:31.527429      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:32.527816      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:33.528142      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:34.528397      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:35.528712      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:36.529242      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:37.530182      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:38.531849      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:39.531991      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:40.532355      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:41.533120      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:42.533190      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:43.534347      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:44.535255      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:45.535966      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:46.536842      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:47.537389      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:48.538177      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:49.539121      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label kubernetes.io/e2e-7a7a48a5-f781-4a12-8297-81a86ee85c01 off the node env1-test-worker-1 @ 01/10/24 17:07:49.649
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-7a7a48a5-f781-4a12-8297-81a86ee85c01 @ 01/10/24 17:07:49.71
  Jan 10 17:07:49.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-1651" for this suite. @ 01/10/24 17:07:49.747
• [306.509 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version should find the server version [Conformance]
test/e2e/apimachinery/server_version.go:40
  STEP: Creating a kubernetes client @ 01/10/24 17:07:49.774
  Jan 10 17:07:49.775: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename server-version @ 01/10/24 17:07:49.777
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 17:07:49.823
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 17:07:49.832
  STEP: Request ServerVersion @ 01/10/24 17:07:49.844
  STEP: Confirm major version @ 01/10/24 17:07:49.848
  Jan 10 17:07:49.848: INFO: Major version: 1
  STEP: Confirm minor version @ 01/10/24 17:07:49.848
  Jan 10 17:07:49.848: INFO: cleanMinorVersion: 27
  Jan 10 17:07:49.848: INFO: Minor version: 27
  Jan 10 17:07:49.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "server-version-8037" for this suite. @ 01/10/24 17:07:49.863
• [0.109 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]
test/e2e/apimachinery/webhook.go:198
  STEP: Creating a kubernetes client @ 01/10/24 17:07:49.889
  Jan 10 17:07:49.889: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename webhook @ 01/10/24 17:07:49.892
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 17:07:49.948
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 17:07:49.957
  STEP: Setting up server cert @ 01/10/24 17:07:50.031
  E0110 17:07:50.539752      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:51.540736      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 01/10/24 17:07:51.743
  STEP: Deploying the webhook pod @ 01/10/24 17:07:51.763
  STEP: Wait for the deployment to be ready @ 01/10/24 17:07:51.796
  Jan 10 17:07:51.817: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0110 17:07:52.541127      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:53.541449      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 01/10/24 17:07:53.861
  STEP: Verifying the service has paired with the endpoint @ 01/10/24 17:07:53.898
  E0110 17:07:54.541715      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 17:07:54.900: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 01/10/24 17:07:54.91
  STEP: create a pod that should be denied by the webhook @ 01/10/24 17:07:54.96
  STEP: create a pod that causes the webhook to hang @ 01/10/24 17:07:54.986
  E0110 17:07:55.542813      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:56.543412      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:57.544311      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:58.545935      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:07:59.546067      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:00.546441      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:01.546541      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:02.547461      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:03.547609      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:04.548747      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create a configmap that should be denied by the webhook @ 01/10/24 17:08:05.001
  STEP: create a configmap that should be admitted by the webhook @ 01/10/24 17:08:05.022
  STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook @ 01/10/24 17:08:05.043
  STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook @ 01/10/24 17:08:05.062
  STEP: create a namespace that bypass the webhook @ 01/10/24 17:08:05.073
  STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace @ 01/10/24 17:08:05.126
  Jan 10 17:08:05.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2016" for this suite. @ 01/10/24 17:08:05.329
  STEP: Destroying namespace "webhook-markers-7151" for this suite. @ 01/10/24 17:08:05.362
  STEP: Destroying namespace "exempted-namespace-6816" for this suite. @ 01/10/24 17:08:05.386
• [15.527 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
test/e2e/apimachinery/resource_quota.go:76
  STEP: Creating a kubernetes client @ 01/10/24 17:08:05.418
  Jan 10 17:08:05.418: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename resourcequota @ 01/10/24 17:08:05.421
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 17:08:05.467
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 17:08:05.472
  STEP: Counting existing ResourceQuota @ 01/10/24 17:08:05.48
  E0110 17:08:05.548810      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:06.549741      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:07.550785      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:08.551345      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:09.551697      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 01/10/24 17:08:10.488
  STEP: Ensuring resource quota status is calculated @ 01/10/24 17:08:10.518
  E0110 17:08:10.552664      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:11.553390      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 17:08:12.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2200" for this suite. @ 01/10/24 17:08:12.543
  E0110 17:08:12.553885      23 retrywatcher.go:130] "Watch failed" err="context canceled"
• [7.146 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]
test/e2e/apps/disruption.go:87
  STEP: Creating a kubernetes client @ 01/10/24 17:08:12.564
  Jan 10 17:08:12.564: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename disruption @ 01/10/24 17:08:12.566
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 17:08:12.617
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 17:08:12.625
  STEP: Creating a kubernetes client @ 01/10/24 17:08:12.632
  Jan 10 17:08:12.632: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename disruption-2 @ 01/10/24 17:08:12.634
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 17:08:12.669
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 17:08:12.681
  STEP: Waiting for the pdb to be processed @ 01/10/24 17:08:12.701
  E0110 17:08:13.555082      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:14.556056      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be processed @ 01/10/24 17:08:14.743
  STEP: Waiting for the pdb to be processed @ 01/10/24 17:08:14.769
  STEP: listing a collection of PDBs across all namespaces @ 01/10/24 17:08:14.785
  STEP: listing a collection of PDBs in namespace disruption-9118 @ 01/10/24 17:08:14.796
  STEP: deleting a collection of PDBs @ 01/10/24 17:08:14.803
  STEP: Waiting for the PDB collection to be deleted @ 01/10/24 17:08:14.838
  Jan 10 17:08:14.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jan 10 17:08:14.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2-7339" for this suite. @ 01/10/24 17:08:14.867
  STEP: Destroying namespace "disruption-9118" for this suite. @ 01/10/24 17:08:14.886
• [2.336 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
test/e2e/apps/cronjob.go:125
  STEP: Creating a kubernetes client @ 01/10/24 17:08:14.907
  Jan 10 17:08:14.907: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename cronjob @ 01/10/24 17:08:14.909
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 17:08:14.968
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 17:08:14.991
  STEP: Creating a ForbidConcurrent cronjob @ 01/10/24 17:08:14.998
  STEP: Ensuring a job is scheduled @ 01/10/24 17:08:15.012
  E0110 17:08:15.556818      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:16.557245      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:17.557601      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:18.557913      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:19.558067      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:20.558367      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:21.558543      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:22.558944      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:23.559610      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:24.560175      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:25.560177      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:26.560430      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:27.560648      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:28.560713      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:29.560987      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:30.561176      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:31.563763      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:32.562305      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:33.563273      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:34.564217      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:35.564732      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:36.565437      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:37.566532      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:38.567213      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:39.568268      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:40.568838      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:41.569737      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:42.569733      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:43.570787      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:44.571627      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:45.572453      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:46.572754      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:47.573059      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:48.573665      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:49.574627      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:50.574965      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:51.575217      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:52.575950      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:53.576893      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:54.577951      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:55.578871      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:56.579540      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:57.579601      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:58.579706      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:08:59.580609      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:00.580954      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring exactly one is scheduled @ 01/10/24 17:09:01.026
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 01/10/24 17:09:01.037
  STEP: Ensuring no more jobs are scheduled @ 01/10/24 17:09:01.044
  E0110 17:09:01.581719      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:02.582290      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:03.583230      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:04.584171      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:05.585207      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:06.585778      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:07.586408      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:08.587244      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:09.587710      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:10.588124      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:11.589022      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:12.589553      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:13.590523      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:14.591727      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:15.592659      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:16.593502      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:17.594238      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:18.594838      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:19.595391      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:20.595894      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:21.596883      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:22.597463      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:23.598178      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:24.599122      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:25.600164      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:26.601227      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:27.601435      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:28.601651      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:29.601742      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:30.601946      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:31.603001      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:32.603309      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:33.603443      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:34.604269      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:35.604950      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:36.605450      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:37.605567      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:38.606673      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:39.607281      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:40.607887      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:41.608004      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:42.608531      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:43.609375      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:44.610057      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:45.610351      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:46.610501      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:47.610784      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:48.613598      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:49.614364      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:50.614672      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:51.614753      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:52.615056      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:53.615321      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:54.616074      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:55.617342      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:56.618022      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:57.618976      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:58.619211      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:09:59.619969      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:00.620140      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:01.620292      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:02.620568      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:03.621112      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:04.622009      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:05.623253      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:06.624058      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:07.624154      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:08.625875      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:09.626590      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:10.626905      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:11.627268      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:12.627259      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:13.628438      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:14.629519      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:15.630056      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:16.630473      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:17.631536      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:18.631695      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:19.632589      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:20.633038      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:21.634103      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:22.635046      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:23.635304      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:24.636214      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:25.636805      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:26.637341      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:27.637743      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:28.639015      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:29.639164      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:30.639666      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:31.640715      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:32.642016      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:33.642808      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:34.643562      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:35.644741      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:36.645242      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:37.645666      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:38.646724      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:39.647784      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:40.648113      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:41.648242      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:42.648742      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:43.648905      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:44.649950      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:45.650265      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:46.650475      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:47.651497      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:48.651734      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:49.652553      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:50.653203      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:51.653487      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:52.654243      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:53.654658      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:54.655556      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:55.656517      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:56.657025      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:57.657059      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:58.657476      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:10:59.658201      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:00.659131      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:01.659310      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:02.659712      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:03.659980      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:04.660240      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:05.660434      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:06.661296      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:07.661651      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:08.661774      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:09.662396      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:10.662550      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:11.662769      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:12.662965      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:13.663357      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:14.664060      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:15.664929      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:16.665849      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:17.667047      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:18.667356      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:19.667871      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:20.668192      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:21.668400      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:22.668710      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:23.668861      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:24.669120      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:25.670218      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:26.670843      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:27.671297      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:28.671858      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:29.672583      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:30.672727      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:31.672954      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:32.673122      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:33.673395      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:34.674068      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:35.674934      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:36.675329      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:37.675793      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:38.675937      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:39.676543      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:40.676713      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:41.676983      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:42.677158      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:43.678237      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:44.679137      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:45.679623      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:46.679863      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:47.680829      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:48.681009      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:49.681248      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:50.681701      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:51.682648      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:52.683192      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:53.683477      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:54.683750      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:55.684255      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:56.684888      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:57.685161      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:58.685342      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:11:59.686250      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:00.686497      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:01.686864      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:02.687084      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:03.688296      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:04.688395      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:05.688652      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:06.689248      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:07.689533      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:08.690673      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:09.690740      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:10.690948      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:11.691368      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:12.691556      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:13.691757      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:14.691858      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:15.692947      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:16.694026      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:17.693939      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:18.694940      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:19.695866      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:20.696212      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:21.697202      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:22.697719      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:23.698851      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:24.700017      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:25.700250      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:26.700557      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:27.700984      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:28.701110      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:29.701983      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:30.702615      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:31.703557      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:32.703843      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:33.704273      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:34.705121      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:35.705274      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:36.705726      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:37.705710      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:38.706258      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:39.706518      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:40.706986      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:41.707561      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:42.707625      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:43.707697      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:44.707954      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:45.708086      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:46.708318      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:47.708483      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:48.708681      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:49.708757      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:50.708936      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:51.709172      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:52.709474      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:53.709549      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:54.709769      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:55.710036      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:56.710571      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:57.710747      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:58.711314      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:12:59.711538      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:00.712110      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:01.713342      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:02.713816      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:03.714913      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:04.715927      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:05.716015      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:06.716625      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:07.716914      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:08.717426      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:09.717548      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:10.717680      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:11.717987      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:12.718493      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:13.718834      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:14.719882      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:15.720203      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:16.720323      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:17.721062      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:18.721667      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:19.722775      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:20.723226      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:21.723668      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:22.723898      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:23.724169      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:24.724267      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:25.724430      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:26.724950      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:27.725209      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:28.725554      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:29.725639      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:30.725773      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:31.726280      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:32.726390      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:33.727009      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:34.727861      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:35.728429      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:36.729011      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:37.729500      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:38.729783      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:39.729972      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:40.730547      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:41.730607      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:42.730828      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:43.731958      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:44.733103      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:45.733226      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:46.733775      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:47.733866      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:48.734405      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:49.735112      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:50.735331      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:51.735509      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:52.735950      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:53.736370      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:54.737291      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:55.737589      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:56.738012      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:57.739082      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:58.739612      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:13:59.740918      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:14:00.741178      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing cronjob @ 01/10/24 17:14:01.067
  Jan 10 17:14:01.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-8071" for this suite. @ 01/10/24 17:14:01.117
• [346.241 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]
test/e2e/apps/daemon_set.go:432
  STEP: Creating a kubernetes client @ 01/10/24 17:14:01.155
  Jan 10 17:14:01.155: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename daemonsets @ 01/10/24 17:14:01.158
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 17:14:01.237
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 17:14:01.257
  Jan 10 17:14:01.438: INFO: Create a RollingUpdate DaemonSet
  Jan 10 17:14:01.456: INFO: Check that daemon pods launch on every node of the cluster
  Jan 10 17:14:01.483: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 17:14:01.483: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 17:14:01.483: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 17:14:01.499: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 10 17:14:01.499: INFO: Node env1-test-worker-0 is running 0 daemon pod, expected 1
  E0110 17:14:01.742288      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 17:14:02.509: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 17:14:02.509: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 17:14:02.509: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 17:14:02.515: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 10 17:14:02.515: INFO: Node env1-test-worker-0 is running 0 daemon pod, expected 1
  E0110 17:14:02.743379      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 17:14:03.511: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 17:14:03.511: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 17:14:03.512: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 17:14:03.519: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jan 10 17:14:03.519: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  Jan 10 17:14:03.519: INFO: Update the DaemonSet to trigger a rollout
  Jan 10 17:14:03.547: INFO: Updating DaemonSet daemon-set
  E0110 17:14:03.745031      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 17:14:04.642: INFO: Roll back the DaemonSet before rollout is complete
  Jan 10 17:14:04.688: INFO: Updating DaemonSet daemon-set
  Jan 10 17:14:04.689: INFO: Make sure DaemonSet rollback is complete
  Jan 10 17:14:04.701: INFO: Wrong image for pod: daemon-set-qskxt. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
  Jan 10 17:14:04.701: INFO: Pod daemon-set-qskxt is not available
  Jan 10 17:14:04.723: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 17:14:04.723: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 17:14:04.723: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0110 17:14:04.745666      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:14:05.746508      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 17:14:05.753: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 17:14:05.753: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 17:14:05.753: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0110 17:14:06.747427      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 17:14:06.755: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 17:14:06.755: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 17:14:06.755: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0110 17:14:07.747662      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 17:14:07.751: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 17:14:07.751: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 17:14:07.751: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0110 17:14:08.747732      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 17:14:08.752: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 17:14:08.752: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 17:14:08.753: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 17:14:09.737: INFO: Pod daemon-set-npm4j is not available
  E0110 17:14:09.748491      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 17:14:09.754: INFO: DaemonSet pods can't tolerate node env1-test-master-0 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 17:14:09.754: INFO: DaemonSet pods can't tolerate node env1-test-master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  Jan 10 17:14:09.755: INFO: DaemonSet pods can't tolerate node env1-test-master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  [FAILED] in [It] - test/e2e/apps/daemon_set.go:500 @ 01/10/24 17:14:09.771
  STEP: Deleting DaemonSet "daemon-set" @ 01/10/24 17:14:09.78
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5684, will wait for the garbage collector to delete the pods @ 01/10/24 17:14:09.78
  Jan 10 17:14:09.861: INFO: Deleting DaemonSet.extensions daemon-set took: 18.455037ms
  Jan 10 17:14:09.962: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.744537ms
  E0110 17:14:10.748595      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:14:11.749917      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 17:14:12.171: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jan 10 17:14:12.171: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jan 10 17:14:12.179: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"186806964"},"items":null}

  Jan 10 17:14:12.191: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"186806964"},"items":null}

  Jan 10 17:14:12.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: dump namespace information after failure @ 01/10/24 17:14:12.249
  STEP: Collecting events from namespace "daemonsets-5684". @ 01/10/24 17:14:12.249
  STEP: Found 25 events. @ 01/10/24 17:14:12.261
  Jan 10 17:14:12.261: INFO: At 0001-01-01 00:00:00 +0000 UTC - event for daemon-set-npm4j: { } Scheduled: Successfully assigned daemonsets-5684/daemon-set-npm4j to env1-test-worker-0
  Jan 10 17:14:12.261: INFO: At 0001-01-01 00:00:00 +0000 UTC - event for daemon-set-qd4z5: { } Scheduled: Successfully assigned daemonsets-5684/daemon-set-qd4z5 to env1-test-worker-1
  Jan 10 17:14:12.261: INFO: At 0001-01-01 00:00:00 +0000 UTC - event for daemon-set-qskxt: { } Scheduled: Successfully assigned daemonsets-5684/daemon-set-qskxt to env1-test-worker-0
  Jan 10 17:14:12.262: INFO: At 0001-01-01 00:00:00 +0000 UTC - event for daemon-set-tc95b: { } Scheduled: Successfully assigned daemonsets-5684/daemon-set-tc95b to env1-test-worker-0
  Jan 10 17:14:12.262: INFO: At 2024-01-10 17:14:01 +0000 UTC - event for daemon-set: {daemonset-controller } SuccessfulCreate: Created pod: daemon-set-qd4z5
  Jan 10 17:14:12.262: INFO: At 2024-01-10 17:14:01 +0000 UTC - event for daemon-set: {daemonset-controller } SuccessfulCreate: Created pod: daemon-set-tc95b
  Jan 10 17:14:12.262: INFO: At 2024-01-10 17:14:02 +0000 UTC - event for daemon-set-qd4z5: {kubelet env1-test-worker-1} Pulled: Container image "registry.k8s.io/e2e-test-images/httpd:2.4.38-4" already present on machine
  Jan 10 17:14:12.262: INFO: At 2024-01-10 17:14:02 +0000 UTC - event for daemon-set-qd4z5: {kubelet env1-test-worker-1} Created: Created container app
  Jan 10 17:14:12.262: INFO: At 2024-01-10 17:14:02 +0000 UTC - event for daemon-set-qd4z5: {kubelet env1-test-worker-1} Started: Started container app
  Jan 10 17:14:12.262: INFO: At 2024-01-10 17:14:02 +0000 UTC - event for daemon-set-tc95b: {kubelet env1-test-worker-0} Started: Started container app
  Jan 10 17:14:12.262: INFO: At 2024-01-10 17:14:02 +0000 UTC - event for daemon-set-tc95b: {kubelet env1-test-worker-0} Created: Created container app
  Jan 10 17:14:12.262: INFO: At 2024-01-10 17:14:02 +0000 UTC - event for daemon-set-tc95b: {kubelet env1-test-worker-0} Pulled: Container image "registry.k8s.io/e2e-test-images/httpd:2.4.38-4" already present on machine
  Jan 10 17:14:12.262: INFO: At 2024-01-10 17:14:03 +0000 UTC - event for daemon-set: {daemonset-controller } SuccessfulDelete: Deleted pod: daemon-set-tc95b
  Jan 10 17:14:12.263: INFO: At 2024-01-10 17:14:03 +0000 UTC - event for daemon-set-tc95b: {kubelet env1-test-worker-0} Killing: Stopping container app
  Jan 10 17:14:12.263: INFO: At 2024-01-10 17:14:04 +0000 UTC - event for daemon-set: {daemonset-controller } SuccessfulCreate: Created pod: daemon-set-qskxt
  Jan 10 17:14:12.263: INFO: At 2024-01-10 17:14:04 +0000 UTC - event for daemon-set: {daemonset-controller } SuccessfulDelete: Deleted pod: daemon-set-qskxt
  Jan 10 17:14:12.263: INFO: At 2024-01-10 17:14:05 +0000 UTC - event for daemon-set-qskxt: {kubelet env1-test-worker-0} Pulling: Pulling image "foo:non-existent"
  Jan 10 17:14:12.263: INFO: At 2024-01-10 17:14:07 +0000 UTC - event for daemon-set-qskxt: {kubelet env1-test-worker-0} Failed: Error: ErrImagePull
  Jan 10 17:14:12.263: INFO: At 2024-01-10 17:14:07 +0000 UTC - event for daemon-set-qskxt: {kubelet env1-test-worker-0} Failed: Failed to pull image "foo:non-existent": rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/foo:non-existent": failed to resolve reference "docker.io/library/foo:non-existent": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed
  Jan 10 17:14:12.263: INFO: At 2024-01-10 17:14:09 +0000 UTC - event for daemon-set: {daemonset-controller } SuccessfulCreate: Created pod: daemon-set-npm4j
  Jan 10 17:14:12.263: INFO: At 2024-01-10 17:14:09 +0000 UTC - event for daemon-set-npm4j: {kubelet env1-test-worker-0} Created: Created container app
  Jan 10 17:14:12.263: INFO: At 2024-01-10 17:14:09 +0000 UTC - event for daemon-set-npm4j: {kubelet env1-test-worker-0} Pulled: Container image "registry.k8s.io/e2e-test-images/httpd:2.4.38-4" already present on machine
  Jan 10 17:14:12.263: INFO: At 2024-01-10 17:14:09 +0000 UTC - event for daemon-set-qd4z5: {kubelet env1-test-worker-1} Killing: Stopping container app
  Jan 10 17:14:12.263: INFO: At 2024-01-10 17:14:10 +0000 UTC - event for daemon-set-npm4j: {kubelet env1-test-worker-0} Killing: Stopping container app
  Jan 10 17:14:12.263: INFO: At 2024-01-10 17:14:10 +0000 UTC - event for daemon-set-npm4j: {kubelet env1-test-worker-0} Started: Started container app
  Jan 10 17:14:12.274: INFO: POD  NODE  PHASE  GRACE  CONDITIONS
  Jan 10 17:14:12.274: INFO: 
  Jan 10 17:14:12.290: INFO: 
  Logging node info for node env1-test-master-0
  Jan 10 17:14:12.303: INFO: Node Info: &Node{ObjectMeta:{env1-test-master-0    2a4e2194-9b3b-46a4-b94b-6ce08bd70294 186806959 0 2022-03-30 13:31:56 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:vsphere-vm.cpu-2.mem-7gb.os-ubuntu beta.kubernetes.io/os:linux kubernetes.io/arch:amd64 kubernetes.io/hostname:env1-test-master-0 kubernetes.io/os:linux node-role.kubernetes.io/control-plane: node.kubernetes.io/exclude-from-external-load-balancers: node.kubernetes.io/instance-type:vsphere-vm.cpu-2.mem-7gb.os-ubuntu] map[alpha.kubernetes.io/provided-node-ip:10.61.1.197 csi.volume.kubernetes.io/nodeid:{"csi.vsphere.vmware.com":"42086e21-1067-e398-4609-6a5784f3d5d6"} flannel.alpha.coreos.com/backend-data:{"VNI":4096,"VtepMAC":"26:d1:5d:16:e7:71"} flannel.alpha.coreos.com/backend-type:vxlan flannel.alpha.coreos.com/kube-subnet-manager:true flannel.alpha.coreos.com/public-ip:10.61.1.197 kubeadm.alpha.kubernetes.io/cri-socket:unix:///var/run/containerd/containerd.sock node.alpha.kubernetes.io/ttl:0 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [] [{flanneld Update v1 2022-03-30 13:35:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:flannel.alpha.coreos.com/backend-type":{},"f:flannel.alpha.coreos.com/kube-subnet-manager":{},"f:flannel.alpha.coreos.com/public-ip":{}}},"f:status":{"f:conditions":{"k:{\"type\":\"NetworkUnavailable\"}":{".":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}}}} } {vsphere-cloud-controller-manager Update v1 2022-03-30 13:39:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{"f:beta.kubernetes.io/instance-type":{},"f:node.kubernetes.io/instance-type":{}}},"f:spec":{"f:providerID":{}}} } {vsphere-cloud-controller-manager Update v1 2022-03-30 13:39:30 +0000 UTC FieldsV1 {"f:status":{"f:addresses":{"k:{\"type\":\"ExternalIP\"}":{".":{},"f:address":{},"f:type":{}}}}} status} {kubeadm Update v1 2023-02-02 11:01:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:kubeadm.alpha.kubernetes.io/cri-socket":{}},"f:labels":{"f:node-role.kubernetes.io/control-plane":{},"f:node.kubernetes.io/exclude-from-external-load-balancers":{}}}} } {kube-controller-manager Update v1 2024-01-09 15:53:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}}},"f:spec":{"f:podCIDR":{},"f:podCIDRs":{".":{},"v:\"10.233.64.0/24\"":{}},"f:taints":{}}} } {flanneld Update v1 2024-01-09 16:25:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:flannel.alpha.coreos.com/backend-data":{}}},"f:status":{"f:conditions":{"k:{\"type\":\"NetworkUnavailable\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{}}}}} status} {kubelet Update v1 2024-01-10 17:14:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:csi.volume.kubernetes.io/nodeid":{}}},"f:status":{"f:allocatable":{"f:cpu":{},"f:ephemeral-storage":{},"f:memory":{}},"f:capacity":{"f:memory":{}},"f:conditions":{"k:{\"type\":\"DiskPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"MemoryPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"PIDPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"Ready\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}}},"f:images":{},"f:nodeInfo":{"f:bootID":{},"f:containerRuntimeVersion":{},"f:kubeProxyVersion":{},"f:kubeletVersion":{}}}} status}]},Spec:NodeSpec{PodCIDR:10.233.64.0/24,DoNotUseExternalID:,ProviderID:vsphere://42086e21-1067-e398-4609-6a5784f3d5d6,Unschedulable:false,Taints:[]Taint{Taint{Key:node-role.kubernetes.io/control-plane,Value:,Effect:NoSchedule,TimeAdded:<nil>,},},ConfigSource:nil,PodCIDRs:[10.233.64.0/24],},Status:NodeStatus{Capacity:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{62254768128 0} {<nil>}  BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8248193024 0} {<nil>} 8054876Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{56029291223 0} {<nil>} 56029291223 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8143335424 0} {<nil>} 7952476Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2024-01-09 16:25:58 +0000 UTC,LastTransitionTime:2024-01-09 16:25:58 +0000 UTC,Reason:FlannelIsUp,Message:Flannel is running on this node,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2024-01-10 17:14:11 +0000 UTC,LastTransitionTime:2024-01-09 15:50:24 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2024-01-10 17:14:11 +0000 UTC,LastTransitionTime:2024-01-09 15:50:24 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2024-01-10 17:14:11 +0000 UTC,LastTransitionTime:2024-01-09 15:50:24 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2024-01-10 17:14:11 +0000 UTC,LastTransitionTime:2024-01-09 15:53:19 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:Hostname,Address:env1-test-master-0,},NodeAddress{Type:InternalIP,Address:10.61.1.197,},NodeAddress{Type:ExternalIP,Address:10.61.1.197,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:4284f3e04f9c47a4b59e54ce697ed9a6,SystemUUID:42086e21-1067-e398-4609-6a5784f3d5d6,BootID:24165872-ba8d-4771-a127-74ef58934caf,KernelVersion:5.4.0-67-generic,OSImage:Ubuntu 20.04.2 LTS,ContainerRuntimeVersion:containerd://1.7.5,KubeletVersion:v1.27.5,KubeProxyVersion:v1.27.5,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[docker.elastic.co/beats/filebeat-oss@sha256:43ffa23888153b62ee8cb0e38a17484cf28d9f048dae39ff13bd814ff8b31ef4 docker.elastic.co/beats/filebeat-oss:7.12.1],SizeBytes:162242645,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:0b8b2b7b43b03f7db26e7e4b99be402495b77b496f5a5b0425b3c226bc1e9cbd docker.io/sonobuoy/systemd-logs:v0.4],SizeBytes:127795048,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[gcr.io/cloud-provider-vsphere/csi/release/driver@sha256:c25d9d24a22d6d787dbbe7fe1cc0734113da252f5e1e3dc86429230aa9047fb3 gcr.io/cloud-provider-vsphere/csi/release/driver:v2.4.1],SizeBytes:108688290,},ContainerImage{Names:[gcr.io/cloud-provider-vsphere/csi/release/driver@sha256:ce9f031aaf5b053ca565ee02bb13b0e0a05e9fbb97103ed68d5acaaff7910cfe gcr.io/cloud-provider-vsphere/csi/release/driver:v2.7.0],SizeBytes:103859482,},ContainerImage{Names:[gcr.io/cloud-provider-vsphere/csi/release/driver@sha256:15b57e33b108b86390994112a5b8df7590c3c40be4d27e4142e12a47792a1a28 gcr.io/cloud-provider-vsphere/csi/release/driver:v2.5.2],SizeBytes:100433136,},ContainerImage{Names:[gcr.io/cloud-provider-vsphere/csi/release/driver@sha256:06cbc2350a65c0080fc320d14e4630af576342f0f8febfbf12b5877c6b63fc29 gcr.io/cloud-provider-vsphere/csi/release/driver:v2.5.1],SizeBytes:100143334,},ContainerImage{Names:[gcr.io/cloud-provider-vsphere/csi/release/driver@sha256:645a61f7a3ff314fe2dd274bbdfd9faba79fa1b011078453a2a20a6fc36b569f gcr.io/cloud-provider-vsphere/csi/release/driver:v2.6.1],SizeBytes:99607874,},ContainerImage{Names:[gcr.io/cloud-provider-vsphere/csi/release/driver@sha256:f67ff4a250ea73abee9b5a899a58587755cd07d7a80332d2c5c190f227ad6dfe gcr.io/cloud-provider-vsphere/csi/release/driver:v3.0.1],SizeBytes:93529994,},ContainerImage{Names:[k8s.gcr.io/dns/k8s-dns-node-cache:1.21.1],SizeBytes:42449267,},ContainerImage{Names:[k8s.gcr.io/kube-proxy:v1.24.6],SizeBytes:39519168,},ContainerImage{Names:[k8s.gcr.io/kube-proxy:v1.23.0],SizeBytes:39273076,},ContainerImage{Names:[k8s.gcr.io/kube-proxy:v1.22.6],SizeBytes:35949036,},ContainerImage{Names:[k8s.gcr.io/kube-proxy:v1.22.5],SizeBytes:35948128,},ContainerImage{Names:[registry.k8s.io/kube-apiserver:v1.26.5],SizeBytes:35516748,},ContainerImage{Names:[gcr.io/cloud-provider-vsphere/csi/release/syncer@sha256:146720d0686cdafc5955e700a54db0df20d5f877440d8e244439e20ad0cf54fe gcr.io/cloud-provider-vsphere/csi/release/syncer:v2.7.0],SizeBytes:34513556,},ContainerImage{Names:[k8s.gcr.io/kube-apiserver:v1.25.6],SizeBytes:34253490,},ContainerImage{Names:[k8s.gcr.io/kube-apiserver:v1.24.6],SizeBytes:33811854,},ContainerImage{Names:[registry.k8s.io/kube-apiserver:v1.27.5],SizeBytes:33386504,},ContainerImage{Names:[k8s.gcr.io/kube-apiserver:v1.23.0],SizeBytes:32593833,},ContainerImage{Names:[registry.k8s.io/kube-controller-manager:v1.26.5],SizeBytes:32367033,},ContainerImage{Names:[gcr.io/cloud-provider-vsphere/csi/release/syncer@sha256:8f87c99e9d1ee4d0a9562e4f5db673f8ac44420f0dabea9f5c6ddffbde1b471e gcr.io/cloud-provider-vsphere/csi/release/syncer:v2.5.1],SizeBytes:31908445,},ContainerImage{Names:[gcr.io/cloud-provider-vsphere/csi/release/syncer@sha256:6e42ffb4e0fb0e6df651e5a35652ff754d04d1608b89f1c531808b07ccd71623 gcr.io/cloud-provider-vsphere/csi/release/syncer:v3.0.1],SizeBytes:31716123,},ContainerImage{Names:[gcr.io/cloud-provider-vsphere/csi/release/syncer@sha256:b10f343081b4b0bf62ae2e6491162797d3003b146970082689ab3e0b454c53d2 gcr.io/cloud-provider-vsphere/csi/release/syncer:v2.5.2],SizeBytes:31360861,},ContainerImage{Names:[k8s.gcr.io/kube-controller-manager:v1.25.6],SizeBytes:31265286,},ContainerImage{Names:[k8s.gcr.io/kube-apiserver:v1.22.6],SizeBytes:31263796,},ContainerImage{Names:[k8s.gcr.io/kube-apiserver:v1.22.5],SizeBytes:31262066,},ContainerImage{Names:[k8s.gcr.io/kube-controller-manager:v1.24.6],SizeBytes:31043902,},ContainerImage{Names:[registry.k8s.io/kube-controller-manager:v1.27.5],SizeBytes:30978722,},ContainerImage{Names:[gcr.io/cloud-provider-vsphere/csi/release/syncer@sha256:5c6757f836ee104bba886f0adce1690de66a6c75c1d566c6d5a03b081029a980 gcr.io/cloud-provider-vsphere/csi/release/syncer:v2.4.1],SizeBytes:30725347,},ContainerImage{Names:[registry.k8s.io/dns/k8s-dns-node-cache:1.22.20],SizeBytes:30467856,},ContainerImage{Names:[gcr.io/cloud-provider-vsphere/csi/release/syncer@sha256:08b29dde4bc78b51beadf352f508b2d717b29f8f9f0be80033d132f5b01d682a gcr.io/cloud-provider-vsphere/csi/release/syncer:v2.6.1],SizeBytes:30452923,},ContainerImage{Names:[k8s.gcr.io/kube-controller-manager:v1.23.0],SizeBytes:30155587,},ContainerImage{Names:[k8s.gcr.io/kube-controller-manager:v1.22.6],SizeBytes:29801896,},ContainerImage{Names:[k8s.gcr.io/kube-controller-manager:v1.22.5],SizeBytes:29801594,},ContainerImage{Names:[registry.k8s.io/dns/k8s-dns-node-cache:1.22.18],SizeBytes:29747704,},ContainerImage{Names:[k8s.gcr.io/metrics-server/metrics-server:v0.6.2 registry.k8s.io/metrics-server/metrics-server:v0.6.2],SizeBytes:28135299,},ContainerImage{Names:[k8s.gcr.io/metrics-server/metrics-server:v0.6.1],SizeBytes:28058350,},ContainerImage{Names:[registry.k8s.io/sig-storage/csi-provisioner@sha256:e468dddcd275163a042ab297b2d8c2aca50d5e148d2d22f3b6ba119e2f31fa79 registry.k8s.io/sig-storage/csi-provisioner:v3.4.0],SizeBytes:27427836,},ContainerImage{Names:[docker-registry.k8s.cblabkra3.int/flannel/flannel:v0.22.0],SizeBytes:26855532,},ContainerImage{Names:[k8s.gcr.io/metrics-server/metrics-server:v0.5.2],SizeBytes:26023008,},ContainerImage{Names:[registry.k8s.io/sig-storage/csi-resizer@sha256:3a7bdf5d105783d05d0962fa06ca53032b01694556e633f27366201c2881e01d registry.k8s.io/sig-storage/csi-resizer:v1.7.0],SizeBytes:25809460,},ContainerImage{Names:[k8s.gcr.io/metrics-server/metrics-server:v0.5.0],SizeBytes:25804692,},ContainerImage{Names:[registry.k8s.io/sig-storage/csi-attacher@sha256:34cf9b32736c6624fc9787fb149ea6e0fbeb45415707ac2f6440ac960f1116e6 registry.k8s.io/sig-storage/csi-attacher:v4.2.0],SizeBytes:25508181,},ContainerImage{Names:[k8s.gcr.io/sig-storage/csi-provisioner@sha256:4ad5fcdbe7e9147b541a863d74e4d1d519bf435ecda4c7bde5abe237a43f7029 k8s.gcr.io/sig-storage/csi-provisioner:v3.2.1],SizeBytes:25288158,},ContainerImage{Names:[docker-registry.k8s.cblabkra3.int/flannel/flannel:v0.21.4],SizeBytes:24252120,},ContainerImage{Names:[registry.k8s.io/kube-proxy:v1.27.5],SizeBytes:23898800,},ContainerImage{Names:[k8s.gcr.io/sig-storage/csi-resizer@sha256:8f7520bd957e7151fda9886eb5090739439811aeec5ddffb50ad7c8191548d97 k8s.gcr.io/sig-storage/csi-resizer:v1.5.0],SizeBytes:23898753,},ContainerImage{Names:[k8s.gcr.io/sig-storage/csi-attacher@sha256:dd245051317e957423bc3e2aecddf56c745bd6714920f0dc108e505f5afb3472 k8s.gcr.io/sig-storage/csi-attacher:v3.5.0],SizeBytes:23607548,},ContainerImage{Names:[docker-registry.k8s.cblabkra3.int/sonobuoy/sonobuoy@sha256:17e9b30f011869f93fefa012dc06e0def7971dc86aeb8f5fea590e3a5f836e64 docker-registry.k8s.cblabkra3.int/sonobuoy/sonobuoy:v0.57.1],SizeBytes:23415498,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
  Jan 10 17:14:12.305: INFO: 
  Logging kubelet events for node env1-test-master-0
  Jan 10 17:14:12.311: INFO: 
  Logging pods the kubelet thinks is on node env1-test-master-0
  Jan 10 17:14:12.376: INFO: nodelocaldns-llks8 started at 2024-01-09 15:52:16 +0000 UTC (0+1 container statuses recorded)
  Jan 10 17:14:12.376: INFO: 	Container node-cache ready: true, restart count 0
  Jan 10 17:14:12.376: INFO: dns-autoscaler-7f7b458498-ncsvl started at 2024-01-09 16:07:56 +0000 UTC (0+1 container statuses recorded)
  Jan 10 17:14:12.376: INFO: 	Container autoscaler ready: true, restart count 0
  Jan 10 17:14:12.376: INFO: kube-flannel-pxzh2 started at 2024-01-09 16:25:53 +0000 UTC (2+1 container statuses recorded)
  Jan 10 17:14:12.377: INFO: 	Init container install-cni-plugin ready: true, restart count 0
  Jan 10 17:14:12.377: INFO: 	Init container install-cni ready: true, restart count 0
  Jan 10 17:14:12.377: INFO: 	Container kube-flannel ready: true, restart count 0
  Jan 10 17:14:12.377: INFO: vsphere-csi-node-9wghb started at 2024-01-10 09:21:38 +0000 UTC (0+3 container statuses recorded)
  Jan 10 17:14:12.377: INFO: 	Container liveness-probe ready: true, restart count 0
  Jan 10 17:14:12.378: INFO: 	Container node-driver-registrar ready: true, restart count 0
  Jan 10 17:14:12.378: INFO: 	Container vsphere-csi-node ready: true, restart count 0
  Jan 10 17:14:12.378: INFO: kube-scheduler-env1-test-master-0 started at 2024-01-05 12:00:45 +0000 UTC (0+1 container statuses recorded)
  Jan 10 17:14:12.378: INFO: 	Container kube-scheduler ready: true, restart count 0
  Jan 10 17:14:12.378: INFO: sonobuoy-systemd-logs-daemon-set-187fce9e0ddc499b-ltck5 started at 2024-01-10 15:26:37 +0000 UTC (0+2 container statuses recorded)
  Jan 10 17:14:12.378: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jan 10 17:14:12.379: INFO: 	Container systemd-logs ready: true, restart count 0
  Jan 10 17:14:12.379: INFO: filebeat-filebeat-4p44c started at 2024-01-10 01:14:23 +0000 UTC (0+1 container statuses recorded)
  Jan 10 17:14:12.379: INFO: 	Container filebeat ready: false, restart count 0
  Jan 10 17:14:12.379: INFO: kube-proxy-tvjwn started at 2024-01-10 09:13:13 +0000 UTC (0+1 container statuses recorded)
  Jan 10 17:14:12.379: INFO: 	Container kube-proxy ready: true, restart count 0
  Jan 10 17:14:12.379: INFO: prometheus-prometheus-node-exporter-mktvs started at 2024-01-10 03:09:01 +0000 UTC (0+1 container statuses recorded)
  Jan 10 17:14:12.380: INFO: 	Container node-exporter ready: true, restart count 0
  Jan 10 17:14:12.380: INFO: kube-apiserver-env1-test-master-0 started at 2023-06-23 11:51:27 +0000 UTC (0+1 container statuses recorded)
  Jan 10 17:14:12.380: INFO: 	Container kube-apiserver ready: true, restart count 0
  Jan 10 17:14:12.380: INFO: kube-controller-manager-env1-test-master-0 started at 2024-01-05 12:00:45 +0000 UTC (0+1 container statuses recorded)
  Jan 10 17:14:12.380: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Jan 10 17:14:12.381: INFO: vsphere-csi-controller-c6bb68754-hphk6 started at 2024-01-10 09:21:35 +0000 UTC (0+6 container statuses recorded)
  Jan 10 17:14:12.381: INFO: 	Container csi-attacher ready: true, restart count 0
  Jan 10 17:14:12.381: INFO: 	Container csi-provisioner ready: true, restart count 0
  Jan 10 17:14:12.381: INFO: 	Container csi-resizer ready: true, restart count 0
  Jan 10 17:14:12.381: INFO: 	Container liveness-probe ready: true, restart count 0
  Jan 10 17:14:12.381: INFO: 	Container vsphere-csi-controller ready: true, restart count 0
  Jan 10 17:14:12.382: INFO: 	Container vsphere-syncer ready: true, restart count 0
  Jan 10 17:14:12.382: INFO: vsphere-cloud-controller-manager-gbzjw started at 2024-01-10 14:20:23 +0000 UTC (0+1 container statuses recorded)
  Jan 10 17:14:12.382: INFO: 	Container vsphere-cloud-controller-manager ready: true, restart count 0
  Jan 10 17:14:12.647: INFO: 
  Latency metrics for node env1-test-master-0
  Jan 10 17:14:12.647: INFO: 
  Logging node info for node env1-test-master-1
  Jan 10 17:14:12.657: INFO: Node Info: &Node{ObjectMeta:{env1-test-master-1    ba61c7fd-b873-4d72-b3bf-82d6d1eee496 186806905 0 2022-03-30 13:32:42 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:vsphere-vm.cpu-2.mem-7gb.os-ubuntu beta.kubernetes.io/os:linux kubernetes.io/arch:amd64 kubernetes.io/hostname:env1-test-master-1 kubernetes.io/os:linux node-role.kubernetes.io/control-plane: node.kubernetes.io/exclude-from-external-load-balancers: node.kubernetes.io/instance-type:vsphere-vm.cpu-2.mem-7gb.os-ubuntu] map[alpha.kubernetes.io/provided-node-ip:10.61.1.198 csi.volume.kubernetes.io/nodeid:{"csi.vsphere.vmware.com":"4208aac2-afbe-7b19-a557-8c1427113976"} flannel.alpha.coreos.com/backend-data:{"VNI":4096,"VtepMAC":"1a:b9:a5:7d:c8:86"} flannel.alpha.coreos.com/backend-type:vxlan flannel.alpha.coreos.com/kube-subnet-manager:true flannel.alpha.coreos.com/public-ip:10.61.1.198 kubeadm.alpha.kubernetes.io/cri-socket:unix:///var/run/containerd/containerd.sock node.alpha.kubernetes.io/ttl:0 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [] [{ancient-changes Update v1 2022-03-30 13:35:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:flannel.alpha.coreos.com/backend-type":{},"f:flannel.alpha.coreos.com/kube-subnet-manager":{},"f:flannel.alpha.coreos.com/public-ip":{}}},"f:status":{"f:capacity":{"f:ephemeral-storage":{}},"f:conditions":{"k:{\"type\":\"NetworkUnavailable\"}":{".":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}}}} } {vsphere-cloud-controller-manager Update v1 2022-03-30 13:39:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{"f:beta.kubernetes.io/instance-type":{},"f:node.kubernetes.io/instance-type":{}}},"f:spec":{"f:providerID":{}}} } {vsphere-cloud-controller-manager Update v1 2022-03-30 13:39:30 +0000 UTC FieldsV1 {"f:status":{"f:addresses":{"k:{\"type\":\"ExternalIP\"}":{".":{},"f:address":{},"f:type":{}}}}} status} {kubeadm Update v1 2023-02-02 11:16:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:kubeadm.alpha.kubernetes.io/cri-socket":{}},"f:labels":{"f:node-role.kubernetes.io/control-plane":{},"f:node.kubernetes.io/exclude-from-external-load-balancers":{}}}} } {kube-controller-manager Update v1 2024-01-09 16:07:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}}},"f:spec":{"f:podCIDR":{},"f:podCIDRs":{".":{},"v:\"10.233.65.0/24\"":{}},"f:taints":{}}} } {flanneld Update v1 2024-01-09 16:23:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:flannel.alpha.coreos.com/backend-data":{}}},"f:status":{"f:conditions":{"k:{\"type\":\"NetworkUnavailable\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{}}}}} status} {kubelet Update v1 2024-01-10 17:14:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:csi.volume.kubernetes.io/nodeid":{}}},"f:status":{"f:allocatable":{"f:cpu":{},"f:ephemeral-storage":{},"f:memory":{}},"f:capacity":{"f:memory":{}},"f:conditions":{"k:{\"type\":\"DiskPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"MemoryPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"PIDPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"Ready\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}}},"f:images":{},"f:nodeInfo":{"f:bootID":{},"f:containerRuntimeVersion":{},"f:kubeProxyVersion":{},"f:kubeletVersion":{}}}} status}]},Spec:NodeSpec{PodCIDR:10.233.65.0/24,DoNotUseExternalID:,ProviderID:vsphere://4208aac2-afbe-7b19-a557-8c1427113976,Unschedulable:false,Taints:[]Taint{Taint{Key:node-role.kubernetes.io/control-plane,Value:,Effect:NoSchedule,TimeAdded:<nil>,},},ConfigSource:nil,PodCIDRs:[10.233.65.0/24],},Status:NodeStatus{Capacity:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{62254768128 0} {<nil>}  BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8248201216 0} {<nil>} 8054884Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{56029291223 0} {<nil>} 56029291223 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8143343616 0} {<nil>} 7952484Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2024-01-09 16:23:38 +0000 UTC,LastTransitionTime:2024-01-09 16:23:38 +0000 UTC,Reason:FlannelIsUp,Message:Flannel is running on this node,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2024-01-10 17:14:07 +0000 UTC,LastTransitionTime:2023-06-23 12:02:05 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2024-01-10 17:14:07 +0000 UTC,LastTransitionTime:2023-06-23 12:02:05 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2024-01-10 17:14:07 +0000 UTC,LastTransitionTime:2023-06-23 12:02:05 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2024-01-10 17:14:07 +0000 UTC,LastTransitionTime:2024-01-09 16:07:42 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:Hostname,Address:env1-test-master-1,},NodeAddress{Type:InternalIP,Address:10.61.1.198,},NodeAddress{Type:ExternalIP,Address:10.61.1.198,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:4284f3e04f9c47a4b59e54ce697ed9a6,SystemUUID:4208aac2-afbe-7b19-a557-8c1427113976,BootID:96f9741b-eed3-46f6-8aa3-cf48ab1de0ba,KernelVersion:5.4.0-67-generic,OSImage:Ubuntu 20.04.2 LTS,ContainerRuntimeVersion:containerd://1.7.5,KubeletVersion:v1.27.5,KubeProxyVersion:v1.27.5,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[docker.elastic.co/beats/filebeat-oss@sha256:43ffa23888153b62ee8cb0e38a17484cf28d9f048dae39ff13bd814ff8b31ef4 docker.elastic.co/beats/filebeat-oss:7.12.1],SizeBytes:162242645,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:0b8b2b7b43b03f7db26e7e4b99be402495b77b496f5a5b0425b3c226bc1e9cbd docker.io/sonobuoy/systemd-logs:v0.4],SizeBytes:127795048,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[gcr.io/cloud-provider-vsphere/csi/release/driver@sha256:c25d9d24a22d6d787dbbe7fe1cc0734113da252f5e1e3dc86429230aa9047fb3 gcr.io/cloud-provider-vsphere/csi/release/driver:v2.4.1],SizeBytes:108688290,},ContainerImage{Names:[gcr.io/cloud-provider-vsphere/csi/release/driver@sha256:ce9f031aaf5b053ca565ee02bb13b0e0a05e9fbb97103ed68d5acaaff7910cfe gcr.io/cloud-provider-vsphere/csi/release/driver:v2.7.0],SizeBytes:103859482,},ContainerImage{Names:[gcr.io/cloud-provider-vsphere/csi/release/driver@sha256:15b57e33b108b86390994112a5b8df7590c3c40be4d27e4142e12a47792a1a28 gcr.io/cloud-provider-vsphere/csi/release/driver:v2.5.2],SizeBytes:100433136,},ContainerImage{Names:[gcr.io/cloud-provider-vsphere/csi/release/driver@sha256:06cbc2350a65c0080fc320d14e4630af576342f0f8febfbf12b5877c6b63fc29 gcr.io/cloud-provider-vsphere/csi/release/driver:v2.5.1],SizeBytes:100143334,},ContainerImage{Names:[gcr.io/cloud-provider-vsphere/csi/release/driver@sha256:645a61f7a3ff314fe2dd274bbdfd9faba79fa1b011078453a2a20a6fc36b569f gcr.io/cloud-provider-vsphere/csi/release/driver:v2.6.1],SizeBytes:99607874,},ContainerImage{Names:[gcr.io/cloud-provider-vsphere/csi/release/driver@sha256:f67ff4a250ea73abee9b5a899a58587755cd07d7a80332d2c5c190f227ad6dfe gcr.io/cloud-provider-vsphere/csi/release/driver:v3.0.1],SizeBytes:93529994,},ContainerImage{Names:[k8s.gcr.io/dns/k8s-dns-node-cache:1.21.1],SizeBytes:42449267,},ContainerImage{Names:[k8s.gcr.io/kube-proxy:v1.24.6],SizeBytes:39519168,},ContainerImage{Names:[k8s.gcr.io/kube-proxy:v1.23.0],SizeBytes:39273076,},ContainerImage{Names:[k8s.gcr.io/kube-proxy:v1.22.6],SizeBytes:35949036,},ContainerImage{Names:[k8s.gcr.io/kube-proxy:v1.22.5],SizeBytes:35948128,},ContainerImage{Names:[registry.k8s.io/kube-apiserver:v1.26.5],SizeBytes:35516748,},ContainerImage{Names:[gcr.io/cloud-provider-vsphere/csi/release/syncer@sha256:146720d0686cdafc5955e700a54db0df20d5f877440d8e244439e20ad0cf54fe gcr.io/cloud-provider-vsphere/csi/release/syncer:v2.7.0],SizeBytes:34513556,},ContainerImage{Names:[k8s.gcr.io/kube-apiserver:v1.25.6],SizeBytes:34253490,},ContainerImage{Names:[k8s.gcr.io/kube-apiserver:v1.24.6],SizeBytes:33811854,},ContainerImage{Names:[registry.k8s.io/kube-apiserver:v1.27.5],SizeBytes:33386504,},ContainerImage{Names:[k8s.gcr.io/kube-apiserver:v1.23.0],SizeBytes:32593833,},ContainerImage{Names:[registry.k8s.io/kube-controller-manager:v1.26.5],SizeBytes:32367033,},ContainerImage{Names:[gcr.io/cloud-provider-vsphere/csi/release/syncer@sha256:8f87c99e9d1ee4d0a9562e4f5db673f8ac44420f0dabea9f5c6ddffbde1b471e gcr.io/cloud-provider-vsphere/csi/release/syncer:v2.5.1],SizeBytes:31908445,},ContainerImage{Names:[gcr.io/cloud-provider-vsphere/csi/release/syncer@sha256:6e42ffb4e0fb0e6df651e5a35652ff754d04d1608b89f1c531808b07ccd71623 gcr.io/cloud-provider-vsphere/csi/release/syncer:v3.0.1],SizeBytes:31716123,},ContainerImage{Names:[gcr.io/cloud-provider-vsphere/csi/release/syncer@sha256:b10f343081b4b0bf62ae2e6491162797d3003b146970082689ab3e0b454c53d2 gcr.io/cloud-provider-vsphere/csi/release/syncer:v2.5.2],SizeBytes:31360861,},ContainerImage{Names:[k8s.gcr.io/kube-controller-manager:v1.25.6],SizeBytes:31265286,},ContainerImage{Names:[k8s.gcr.io/kube-apiserver:v1.22.6],SizeBytes:31263796,},ContainerImage{Names:[k8s.gcr.io/kube-apiserver:v1.22.5],SizeBytes:31262066,},ContainerImage{Names:[k8s.gcr.io/kube-controller-manager:v1.24.6],SizeBytes:31043902,},ContainerImage{Names:[registry.k8s.io/kube-controller-manager:v1.27.5],SizeBytes:30978722,},ContainerImage{Names:[gcr.io/cloud-provider-vsphere/csi/release/syncer@sha256:5c6757f836ee104bba886f0adce1690de66a6c75c1d566c6d5a03b081029a980 gcr.io/cloud-provider-vsphere/csi/release/syncer:v2.4.1],SizeBytes:30725347,},ContainerImage{Names:[registry.k8s.io/dns/k8s-dns-node-cache:1.22.20],SizeBytes:30467856,},ContainerImage{Names:[gcr.io/cloud-provider-vsphere/csi/release/syncer@sha256:08b29dde4bc78b51beadf352f508b2d717b29f8f9f0be80033d132f5b01d682a gcr.io/cloud-provider-vsphere/csi/release/syncer:v2.6.1],SizeBytes:30452923,},ContainerImage{Names:[k8s.gcr.io/kube-controller-manager:v1.23.0],SizeBytes:30155587,},ContainerImage{Names:[k8s.gcr.io/kube-controller-manager:v1.22.6],SizeBytes:29801896,},ContainerImage{Names:[k8s.gcr.io/kube-controller-manager:v1.22.5],SizeBytes:29801594,},ContainerImage{Names:[registry.k8s.io/dns/k8s-dns-node-cache:1.22.18],SizeBytes:29747704,},ContainerImage{Names:[k8s.gcr.io/metrics-server/metrics-server:v0.6.2 registry.k8s.io/metrics-server/metrics-server:v0.6.2],SizeBytes:28135299,},ContainerImage{Names:[k8s.gcr.io/metrics-server/metrics-server:v0.6.1],SizeBytes:28058350,},ContainerImage{Names:[registry.k8s.io/sig-storage/csi-provisioner@sha256:e468dddcd275163a042ab297b2d8c2aca50d5e148d2d22f3b6ba119e2f31fa79 registry.k8s.io/sig-storage/csi-provisioner:v3.4.0],SizeBytes:27427836,},ContainerImage{Names:[docker-registry.k8s.cblabkra3.int/flannel/flannel:v0.22.0],SizeBytes:26855532,},ContainerImage{Names:[k8s.gcr.io/metrics-server/metrics-server:v0.5.2],SizeBytes:26023008,},ContainerImage{Names:[registry.k8s.io/sig-storage/csi-resizer@sha256:3a7bdf5d105783d05d0962fa06ca53032b01694556e633f27366201c2881e01d registry.k8s.io/sig-storage/csi-resizer:v1.7.0],SizeBytes:25809460,},ContainerImage{Names:[k8s.gcr.io/metrics-server/metrics-server:v0.5.0],SizeBytes:25804692,},ContainerImage{Names:[registry.k8s.io/sig-storage/csi-attacher@sha256:34cf9b32736c6624fc9787fb149ea6e0fbeb45415707ac2f6440ac960f1116e6 registry.k8s.io/sig-storage/csi-attacher:v4.2.0],SizeBytes:25508181,},ContainerImage{Names:[k8s.gcr.io/sig-storage/csi-provisioner@sha256:4ad5fcdbe7e9147b541a863d74e4d1d519bf435ecda4c7bde5abe237a43f7029 k8s.gcr.io/sig-storage/csi-provisioner:v3.2.1],SizeBytes:25288158,},ContainerImage{Names:[docker-registry.k8s.cblabkra3.int/flannel/flannel:v0.21.4],SizeBytes:24252120,},ContainerImage{Names:[registry.k8s.io/kube-proxy:v1.27.5],SizeBytes:23898800,},ContainerImage{Names:[k8s.gcr.io/sig-storage/csi-resizer@sha256:8f7520bd957e7151fda9886eb5090739439811aeec5ddffb50ad7c8191548d97 k8s.gcr.io/sig-storage/csi-resizer:v1.5.0],SizeBytes:23898753,},ContainerImage{Names:[k8s.gcr.io/sig-storage/csi-attacher@sha256:dd245051317e957423bc3e2aecddf56c745bd6714920f0dc108e505f5afb3472 k8s.gcr.io/sig-storage/csi-attacher:v3.5.0],SizeBytes:23607548,},ContainerImage{Names:[docker-registry.k8s.cblabkra3.int/sonobuoy/sonobuoy@sha256:17e9b30f011869f93fefa012dc06e0def7971dc86aeb8f5fea590e3a5f836e64 docker-registry.k8s.cblabkra3.int/sonobuoy/sonobuoy:v0.57.1],SizeBytes:23415498,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
  Jan 10 17:14:12.658: INFO: 
  Logging kubelet events for node env1-test-master-1
  Jan 10 17:14:12.668: INFO: 
  Logging pods the kubelet thinks is on node env1-test-master-1
  Jan 10 17:14:12.738: INFO: nodelocaldns-tfcx4 started at 2024-01-09 15:52:14 +0000 UTC (0+1 container statuses recorded)
  Jan 10 17:14:12.738: INFO: 	Container node-cache ready: true, restart count 0
  Jan 10 17:14:12.738: INFO: kube-flannel-82gmg started at 2024-01-09 16:23:34 +0000 UTC (2+1 container statuses recorded)
  Jan 10 17:14:12.738: INFO: 	Init container install-cni-plugin ready: true, restart count 0
  Jan 10 17:14:12.738: INFO: 	Init container install-cni ready: true, restart count 0
  Jan 10 17:14:12.738: INFO: 	Container kube-flannel ready: true, restart count 0
  Jan 10 17:14:12.738: INFO: coredns-5c469774b8-7zlbc started at 2024-01-09 16:40:07 +0000 UTC (0+1 container statuses recorded)
  Jan 10 17:14:12.738: INFO: 	Container coredns ready: true, restart count 0
  Jan 10 17:14:12.738: INFO: kube-proxy-rz54s started at 2024-01-10 09:13:14 +0000 UTC (0+1 container statuses recorded)
  Jan 10 17:14:12.738: INFO: 	Container kube-proxy ready: true, restart count 0
  Jan 10 17:14:12.739: INFO: kube-controller-manager-env1-test-master-1 started at 2024-01-09 16:04:35 +0000 UTC (0+1 container statuses recorded)
  Jan 10 17:14:12.739: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Jan 10 17:14:12.739: INFO: kube-scheduler-env1-test-master-1 started at 2024-01-05 12:00:44 +0000 UTC (0+1 container statuses recorded)
  Jan 10 17:14:12.739: INFO: 	Container kube-scheduler ready: true, restart count 0
  Jan 10 17:14:12.739: INFO: prometheus-prometheus-node-exporter-5xkxs started at 2024-01-10 03:09:01 +0000 UTC (0+1 container statuses recorded)
  Jan 10 17:14:12.739: INFO: 	Container node-exporter ready: true, restart count 0
  Jan 10 17:14:12.739: INFO: vsphere-csi-node-lv2rb started at 2024-01-10 09:21:38 +0000 UTC (0+3 container statuses recorded)
  Jan 10 17:14:12.739: INFO: 	Container liveness-probe ready: true, restart count 0
  Jan 10 17:14:12.739: INFO: 	Container node-driver-registrar ready: true, restart count 0
  Jan 10 17:14:12.739: INFO: 	Container vsphere-csi-node ready: true, restart count 0
  Jan 10 17:14:12.739: INFO: vsphere-cloud-controller-manager-dsshs started at 2024-01-10 14:14:31 +0000 UTC (0+1 container statuses recorded)
  Jan 10 17:14:12.739: INFO: 	Container vsphere-cloud-controller-manager ready: true, restart count 0
  Jan 10 17:14:12.739: INFO: kube-apiserver-env1-test-master-1 started at 2024-01-09 16:06:17 +0000 UTC (0+1 container statuses recorded)
  Jan 10 17:14:12.739: INFO: 	Container kube-apiserver ready: true, restart count 0
  Jan 10 17:14:12.739: INFO: vsphere-csi-controller-c6bb68754-nkdpd started at 2024-01-10 09:21:38 +0000 UTC (0+6 container statuses recorded)
  Jan 10 17:14:12.739: INFO: 	Container csi-attacher ready: true, restart count 0
  Jan 10 17:14:12.739: INFO: 	Container csi-provisioner ready: true, restart count 0
  Jan 10 17:14:12.739: INFO: 	Container csi-resizer ready: true, restart count 0
  Jan 10 17:14:12.739: INFO: 	Container liveness-probe ready: true, restart count 0
  Jan 10 17:14:12.740: INFO: 	Container vsphere-csi-controller ready: true, restart count 0
  Jan 10 17:14:12.740: INFO: 	Container vsphere-syncer ready: true, restart count 0
  Jan 10 17:14:12.740: INFO: sonobuoy-systemd-logs-daemon-set-187fce9e0ddc499b-z9w98 started at 2024-01-10 15:26:37 +0000 UTC (0+2 container statuses recorded)
  Jan 10 17:14:12.740: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jan 10 17:14:12.740: INFO: 	Container systemd-logs ready: true, restart count 0
  Jan 10 17:14:12.740: INFO: filebeat-filebeat-pkhs9 started at 2024-01-10 01:14:23 +0000 UTC (0+1 container statuses recorded)
  Jan 10 17:14:12.741: INFO: 	Container filebeat ready: false, restart count 0
  E0110 17:14:12.750181      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 17:14:12.999: INFO: 
  Latency metrics for node env1-test-master-1
  Jan 10 17:14:13.001: INFO: 
  Logging node info for node env1-test-master-2
  Jan 10 17:14:13.015: INFO: Node Info: &Node{ObjectMeta:{env1-test-master-2    b97afb2f-cac7-49a8-a7bc-f0e2ff86a097 186806964 0 2022-03-30 13:33:09 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:vsphere-vm.cpu-2.mem-7gb.os-ubuntu beta.kubernetes.io/os:linux kubernetes.io/arch:amd64 kubernetes.io/hostname:env1-test-master-2 kubernetes.io/os:linux node-role.kubernetes.io/control-plane: node.kubernetes.io/exclude-from-external-load-balancers: node.kubernetes.io/instance-type:vsphere-vm.cpu-2.mem-7gb.os-ubuntu] map[alpha.kubernetes.io/provided-node-ip:10.61.1.199 csi.volume.kubernetes.io/nodeid:{"csi.vsphere.vmware.com":"42084a3e-e63f-6cfa-cf2a-f425c1df3648"} flannel.alpha.coreos.com/backend-data:{"VNI":4096,"VtepMAC":"36:df:bf:27:dd:de"} flannel.alpha.coreos.com/backend-type:vxlan flannel.alpha.coreos.com/kube-subnet-manager:true flannel.alpha.coreos.com/public-ip:10.61.1.199 kubeadm.alpha.kubernetes.io/cri-socket:unix:///var/run/containerd/containerd.sock node.alpha.kubernetes.io/ttl:0 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [] [{ancient-changes Update v1 2022-03-30 13:35:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:flannel.alpha.coreos.com/backend-type":{},"f:flannel.alpha.coreos.com/kube-subnet-manager":{},"f:flannel.alpha.coreos.com/public-ip":{}}},"f:status":{"f:capacity":{"f:ephemeral-storage":{}},"f:conditions":{"k:{\"type\":\"NetworkUnavailable\"}":{".":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}}}} } {vsphere-cloud-controller-manager Update v1 2022-03-30 13:39:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{"f:beta.kubernetes.io/instance-type":{},"f:node.kubernetes.io/instance-type":{}}},"f:spec":{"f:providerID":{}}} } {vsphere-cloud-controller-manager Update v1 2022-03-30 13:39:30 +0000 UTC FieldsV1 {"f:status":{"f:addresses":{"k:{\"type\":\"ExternalIP\"}":{".":{},"f:address":{},"f:type":{}}}}} status} {kubeadm Update v1 2023-02-02 11:27:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:kubeadm.alpha.kubernetes.io/cri-socket":{}},"f:labels":{"f:node-role.kubernetes.io/control-plane":{},"f:node.kubernetes.io/exclude-from-external-load-balancers":{}}}} } {kube-controller-manager Update v1 2024-01-09 16:22:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}}},"f:spec":{"f:podCIDR":{},"f:podCIDRs":{".":{},"v:\"10.233.66.0/24\"":{}},"f:taints":{}}} } {flanneld Update v1 2024-01-09 16:25:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:flannel.alpha.coreos.com/backend-data":{}}},"f:status":{"f:conditions":{"k:{\"type\":\"NetworkUnavailable\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{}}}}} status} {kubelet Update v1 2024-01-10 17:14:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:csi.volume.kubernetes.io/nodeid":{}}},"f:status":{"f:allocatable":{"f:cpu":{},"f:ephemeral-storage":{},"f:memory":{}},"f:capacity":{"f:memory":{}},"f:conditions":{"k:{\"type\":\"DiskPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"MemoryPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"PIDPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"Ready\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}}},"f:images":{},"f:nodeInfo":{"f:bootID":{},"f:containerRuntimeVersion":{},"f:kubeProxyVersion":{},"f:kubeletVersion":{}}}} status}]},Spec:NodeSpec{PodCIDR:10.233.66.0/24,DoNotUseExternalID:,ProviderID:vsphere://42084a3e-e63f-6cfa-cf2a-f425c1df3648,Unschedulable:false,Taints:[]Taint{Taint{Key:node-role.kubernetes.io/control-plane,Value:,Effect:NoSchedule,TimeAdded:<nil>,},},ConfigSource:nil,PodCIDRs:[10.233.66.0/24],},Status:NodeStatus{Capacity:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{62254768128 0} {<nil>}  BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8248193024 0} {<nil>} 8054876Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{56029291223 0} {<nil>} 56029291223 DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8143335424 0} {<nil>} 7952476Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2024-01-09 16:25:23 +0000 UTC,LastTransitionTime:2024-01-09 16:25:23 +0000 UTC,Reason:FlannelIsUp,Message:Flannel is running on this node,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2024-01-10 17:14:12 +0000 UTC,LastTransitionTime:2024-01-09 16:20:55 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2024-01-10 17:14:12 +0000 UTC,LastTransitionTime:2024-01-09 16:20:55 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2024-01-10 17:14:12 +0000 UTC,LastTransitionTime:2024-01-09 16:20:55 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2024-01-10 17:14:12 +0000 UTC,LastTransitionTime:2024-01-09 16:22:38 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:Hostname,Address:env1-test-master-2,},NodeAddress{Type:InternalIP,Address:10.61.1.199,},NodeAddress{Type:ExternalIP,Address:10.61.1.199,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:4284f3e04f9c47a4b59e54ce697ed9a6,SystemUUID:42084a3e-e63f-6cfa-cf2a-f425c1df3648,BootID:b9801ffe-03a0-44cb-a885-ff510d6b157a,KernelVersion:5.4.0-67-generic,OSImage:Ubuntu 20.04.2 LTS,ContainerRuntimeVersion:containerd://1.7.5,KubeletVersion:v1.27.5,KubeProxyVersion:v1.27.5,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[docker.elastic.co/beats/filebeat-oss@sha256:43ffa23888153b62ee8cb0e38a17484cf28d9f048dae39ff13bd814ff8b31ef4 docker.elastic.co/beats/filebeat-oss:7.12.1],SizeBytes:162242645,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:0b8b2b7b43b03f7db26e7e4b99be402495b77b496f5a5b0425b3c226bc1e9cbd docker.io/sonobuoy/systemd-logs:v0.4],SizeBytes:127795048,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[gcr.io/cloud-provider-vsphere/csi/release/driver@sha256:c25d9d24a22d6d787dbbe7fe1cc0734113da252f5e1e3dc86429230aa9047fb3 gcr.io/cloud-provider-vsphere/csi/release/driver:v2.4.1],SizeBytes:108688290,},ContainerImage{Names:[gcr.io/cloud-provider-vsphere/csi/release/driver@sha256:ce9f031aaf5b053ca565ee02bb13b0e0a05e9fbb97103ed68d5acaaff7910cfe gcr.io/cloud-provider-vsphere/csi/release/driver:v2.7.0],SizeBytes:103859482,},ContainerImage{Names:[gcr.io/cloud-provider-vsphere/csi/release/driver@sha256:15b57e33b108b86390994112a5b8df7590c3c40be4d27e4142e12a47792a1a28 gcr.io/cloud-provider-vsphere/csi/release/driver:v2.5.2],SizeBytes:100433136,},ContainerImage{Names:[gcr.io/cloud-provider-vsphere/csi/release/driver@sha256:06cbc2350a65c0080fc320d14e4630af576342f0f8febfbf12b5877c6b63fc29 gcr.io/cloud-provider-vsphere/csi/release/driver:v2.5.1],SizeBytes:100143334,},ContainerImage{Names:[gcr.io/cloud-provider-vsphere/csi/release/driver@sha256:645a61f7a3ff314fe2dd274bbdfd9faba79fa1b011078453a2a20a6fc36b569f gcr.io/cloud-provider-vsphere/csi/release/driver:v2.6.1],SizeBytes:99607874,},ContainerImage{Names:[gcr.io/cloud-provider-vsphere/csi/release/driver@sha256:f67ff4a250ea73abee9b5a899a58587755cd07d7a80332d2c5c190f227ad6dfe gcr.io/cloud-provider-vsphere/csi/release/driver:v3.0.1],SizeBytes:93529994,},ContainerImage{Names:[k8s.gcr.io/dns/k8s-dns-node-cache:1.21.1],SizeBytes:42449267,},ContainerImage{Names:[k8s.gcr.io/kube-proxy:v1.24.6],SizeBytes:39519168,},ContainerImage{Names:[k8s.gcr.io/kube-proxy:v1.23.0],SizeBytes:39273076,},ContainerImage{Names:[k8s.gcr.io/kube-proxy:v1.22.6],SizeBytes:35949036,},ContainerImage{Names:[k8s.gcr.io/kube-proxy:v1.22.5],SizeBytes:35948128,},ContainerImage{Names:[registry.k8s.io/kube-apiserver:v1.26.5],SizeBytes:35516748,},ContainerImage{Names:[gcr.io/cloud-provider-vsphere/csi/release/syncer@sha256:146720d0686cdafc5955e700a54db0df20d5f877440d8e244439e20ad0cf54fe gcr.io/cloud-provider-vsphere/csi/release/syncer:v2.7.0],SizeBytes:34513556,},ContainerImage{Names:[k8s.gcr.io/kube-apiserver:v1.25.6],SizeBytes:34253490,},ContainerImage{Names:[k8s.gcr.io/kube-apiserver:v1.24.6],SizeBytes:33811854,},ContainerImage{Names:[registry.k8s.io/kube-apiserver:v1.27.5],SizeBytes:33386504,},ContainerImage{Names:[k8s.gcr.io/kube-apiserver:v1.23.0],SizeBytes:32593833,},ContainerImage{Names:[registry.k8s.io/kube-controller-manager:v1.26.5],SizeBytes:32367033,},ContainerImage{Names:[gcr.io/cloud-provider-vsphere/csi/release/syncer@sha256:8f87c99e9d1ee4d0a9562e4f5db673f8ac44420f0dabea9f5c6ddffbde1b471e gcr.io/cloud-provider-vsphere/csi/release/syncer:v2.5.1],SizeBytes:31908445,},ContainerImage{Names:[gcr.io/cloud-provider-vsphere/csi/release/syncer@sha256:6e42ffb4e0fb0e6df651e5a35652ff754d04d1608b89f1c531808b07ccd71623 gcr.io/cloud-provider-vsphere/csi/release/syncer:v3.0.1],SizeBytes:31716123,},ContainerImage{Names:[gcr.io/cloud-provider-vsphere/csi/release/syncer@sha256:b10f343081b4b0bf62ae2e6491162797d3003b146970082689ab3e0b454c53d2 gcr.io/cloud-provider-vsphere/csi/release/syncer:v2.5.2],SizeBytes:31360861,},ContainerImage{Names:[k8s.gcr.io/kube-controller-manager:v1.25.6],SizeBytes:31265286,},ContainerImage{Names:[k8s.gcr.io/kube-apiserver:v1.22.6],SizeBytes:31263796,},ContainerImage{Names:[k8s.gcr.io/kube-apiserver:v1.22.5],SizeBytes:31262066,},ContainerImage{Names:[k8s.gcr.io/kube-controller-manager:v1.24.6],SizeBytes:31043902,},ContainerImage{Names:[registry.k8s.io/kube-controller-manager:v1.27.5],SizeBytes:30978722,},ContainerImage{Names:[gcr.io/cloud-provider-vsphere/csi/release/syncer@sha256:5c6757f836ee104bba886f0adce1690de66a6c75c1d566c6d5a03b081029a980 gcr.io/cloud-provider-vsphere/csi/release/syncer:v2.4.1],SizeBytes:30725347,},ContainerImage{Names:[registry.k8s.io/dns/k8s-dns-node-cache:1.22.20],SizeBytes:30467856,},ContainerImage{Names:[gcr.io/cloud-provider-vsphere/csi/release/syncer@sha256:08b29dde4bc78b51beadf352f508b2d717b29f8f9f0be80033d132f5b01d682a gcr.io/cloud-provider-vsphere/csi/release/syncer:v2.6.1],SizeBytes:30452923,},ContainerImage{Names:[k8s.gcr.io/kube-controller-manager:v1.23.0],SizeBytes:30155587,},ContainerImage{Names:[k8s.gcr.io/kube-controller-manager:v1.22.6],SizeBytes:29801896,},ContainerImage{Names:[k8s.gcr.io/kube-controller-manager:v1.22.5],SizeBytes:29801594,},ContainerImage{Names:[registry.k8s.io/dns/k8s-dns-node-cache:1.22.18],SizeBytes:29747704,},ContainerImage{Names:[k8s.gcr.io/metrics-server/metrics-server:v0.6.2 registry.k8s.io/metrics-server/metrics-server:v0.6.2],SizeBytes:28135299,},ContainerImage{Names:[k8s.gcr.io/metrics-server/metrics-server:v0.6.1],SizeBytes:28058350,},ContainerImage{Names:[registry.k8s.io/sig-storage/csi-provisioner@sha256:e468dddcd275163a042ab297b2d8c2aca50d5e148d2d22f3b6ba119e2f31fa79 registry.k8s.io/sig-storage/csi-provisioner:v3.4.0],SizeBytes:27427836,},ContainerImage{Names:[docker-registry.k8s.cblabkra3.int/flannel/flannel:v0.22.0],SizeBytes:26855532,},ContainerImage{Names:[k8s.gcr.io/metrics-server/metrics-server:v0.5.2],SizeBytes:26023008,},ContainerImage{Names:[registry.k8s.io/sig-storage/csi-resizer@sha256:3a7bdf5d105783d05d0962fa06ca53032b01694556e633f27366201c2881e01d registry.k8s.io/sig-storage/csi-resizer:v1.7.0],SizeBytes:25809460,},ContainerImage{Names:[k8s.gcr.io/metrics-server/metrics-server:v0.5.0],SizeBytes:25804692,},ContainerImage{Names:[registry.k8s.io/sig-storage/csi-attacher@sha256:34cf9b32736c6624fc9787fb149ea6e0fbeb45415707ac2f6440ac960f1116e6 registry.k8s.io/sig-storage/csi-attacher:v4.2.0],SizeBytes:25508181,},ContainerImage{Names:[k8s.gcr.io/sig-storage/csi-provisioner@sha256:4ad5fcdbe7e9147b541a863d74e4d1d519bf435ecda4c7bde5abe237a43f7029 k8s.gcr.io/sig-storage/csi-provisioner:v3.2.1],SizeBytes:25288158,},ContainerImage{Names:[docker-registry.k8s.cblabkra3.int/flannel/flannel:v0.21.4],SizeBytes:24252120,},ContainerImage{Names:[registry.k8s.io/kube-proxy:v1.27.5],SizeBytes:23898800,},ContainerImage{Names:[k8s.gcr.io/sig-storage/csi-resizer@sha256:8f7520bd957e7151fda9886eb5090739439811aeec5ddffb50ad7c8191548d97 k8s.gcr.io/sig-storage/csi-resizer:v1.5.0],SizeBytes:23898753,},ContainerImage{Names:[k8s.gcr.io/sig-storage/csi-attacher@sha256:dd245051317e957423bc3e2aecddf56c745bd6714920f0dc108e505f5afb3472 k8s.gcr.io/sig-storage/csi-attacher:v3.5.0],SizeBytes:23607548,},ContainerImage{Names:[docker-registry.k8s.cblabkra3.int/sonobuoy/sonobuoy@sha256:17e9b30f011869f93fefa012dc06e0def7971dc86aeb8f5fea590e3a5f836e64 docker-registry.k8s.cblabkra3.int/sonobuoy/sonobuoy:v0.57.1],SizeBytes:23415498,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
  Jan 10 17:14:13.016: INFO: 
  Logging kubelet events for node env1-test-master-2
  Jan 10 17:14:13.028: INFO: 
  Logging pods the kubelet thinks is on node env1-test-master-2
  Jan 10 17:14:13.091: INFO: nodelocaldns-r2ps5 started at 2024-01-09 15:52:18 +0000 UTC (0+1 container statuses recorded)
  Jan 10 17:14:13.092: INFO: 	Container node-cache ready: true, restart count 0
  Jan 10 17:14:13.092: INFO: kube-flannel-px8zb started at 2024-01-09 16:25:18 +0000 UTC (2+1 container statuses recorded)
  Jan 10 17:14:13.092: INFO: 	Init container install-cni-plugin ready: true, restart count 0
  Jan 10 17:14:13.093: INFO: 	Init container install-cni ready: true, restart count 0
  Jan 10 17:14:13.093: INFO: 	Container kube-flannel ready: true, restart count 0
  Jan 10 17:14:13.094: INFO: kube-scheduler-env1-test-master-2 started at 2024-01-05 12:00:44 +0000 UTC (0+1 container statuses recorded)
  Jan 10 17:14:13.095: INFO: 	Container kube-scheduler ready: true, restart count 0
  Jan 10 17:14:13.095: INFO: coredns-5c469774b8-zs8vg started at 2024-01-09 16:39:59 +0000 UTC (0+1 container statuses recorded)
  Jan 10 17:14:13.095: INFO: 	Container coredns ready: true, restart count 0
  Jan 10 17:14:13.096: INFO: sonobuoy-systemd-logs-daemon-set-187fce9e0ddc499b-ffghp started at 2024-01-10 15:26:37 +0000 UTC (0+2 container statuses recorded)
  Jan 10 17:14:13.096: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jan 10 17:14:13.096: INFO: 	Container systemd-logs ready: true, restart count 0
  Jan 10 17:14:13.096: INFO: vsphere-csi-controller-c6bb68754-2g2cf started at 2024-01-10 09:21:38 +0000 UTC (0+6 container statuses recorded)
  Jan 10 17:14:13.097: INFO: 	Container csi-attacher ready: true, restart count 0
  Jan 10 17:14:13.097: INFO: 	Container csi-provisioner ready: true, restart count 0
  Jan 10 17:14:13.097: INFO: 	Container csi-resizer ready: true, restart count 0
  Jan 10 17:14:13.097: INFO: 	Container liveness-probe ready: true, restart count 0
  Jan 10 17:14:13.098: INFO: 	Container vsphere-csi-controller ready: true, restart count 0
  Jan 10 17:14:13.098: INFO: 	Container vsphere-syncer ready: true, restart count 0
  Jan 10 17:14:13.098: INFO: prometheus-prometheus-node-exporter-7hn4c started at 2024-01-10 03:09:01 +0000 UTC (0+1 container statuses recorded)
  Jan 10 17:14:13.098: INFO: 	Container node-exporter ready: true, restart count 0
  Jan 10 17:14:13.099: INFO: vsphere-csi-node-4vdxp started at 2024-01-10 09:21:38 +0000 UTC (0+3 container statuses recorded)
  Jan 10 17:14:13.099: INFO: 	Container liveness-probe ready: true, restart count 0
  Jan 10 17:14:13.100: INFO: 	Container node-driver-registrar ready: true, restart count 0
  Jan 10 17:14:13.100: INFO: 	Container vsphere-csi-node ready: true, restart count 0
  Jan 10 17:14:13.100: INFO: vsphere-cloud-controller-manager-582lt started at 2024-01-10 14:15:21 +0000 UTC (0+1 container statuses recorded)
  Jan 10 17:14:13.101: INFO: 	Container vsphere-cloud-controller-manager ready: true, restart count 0
  Jan 10 17:14:13.101: INFO: kube-apiserver-env1-test-master-2 started at 2023-06-23 12:16:08 +0000 UTC (0+1 container statuses recorded)
  Jan 10 17:14:13.101: INFO: 	Container kube-apiserver ready: true, restart count 0
  Jan 10 17:14:13.102: INFO: kube-controller-manager-env1-test-master-2 started at 2024-01-05 12:00:44 +0000 UTC (0+1 container statuses recorded)
  Jan 10 17:14:13.102: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Jan 10 17:14:13.103: INFO: kube-proxy-jrkp7 started at 2024-01-10 09:13:13 +0000 UTC (0+1 container statuses recorded)
  Jan 10 17:14:13.104: INFO: 	Container kube-proxy ready: true, restart count 0
  Jan 10 17:14:13.104: INFO: filebeat-filebeat-kbss7 started at 2024-01-10 01:14:23 +0000 UTC (0+1 container statuses recorded)
  Jan 10 17:14:13.105: INFO: 	Container filebeat ready: false, restart count 0
  Jan 10 17:14:13.409: INFO: 
  Latency metrics for node env1-test-master-2
  Jan 10 17:14:13.409: INFO: 
  Logging node info for node env1-test-worker-0
  Jan 10 17:14:13.420: INFO: Node Info: &Node{ObjectMeta:{env1-test-worker-0    549d4876-2917-4b69-b30e-a268a2810df9 186806892 0 2022-03-30 13:34:33 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:vsphere-vm.cpu-4.mem-7gb.os-ubuntu beta.kubernetes.io/os:linux kubernetes.io/arch:amd64 kubernetes.io/hostname:env1-test-worker-0 kubernetes.io/os:linux node.kubernetes.io/instance-type:vsphere-vm.cpu-4.mem-7gb.os-ubuntu] map[alpha.kubernetes.io/provided-node-ip:10.61.1.200 csi.volume.kubernetes.io/nodeid:{"csi.vsphere.vmware.com":"4208fb3e-b545-b1df-e33a-dc94c2e60b2d"} flannel.alpha.coreos.com/backend-data:{"VNI":4096,"VtepMAC":"26:91:8f:7f:c4:41"} flannel.alpha.coreos.com/backend-type:vxlan flannel.alpha.coreos.com/kube-subnet-manager:true flannel.alpha.coreos.com/public-ip:10.61.1.200 kubeadm.alpha.kubernetes.io/cri-socket:/var/run/containerd/containerd.sock node.alpha.kubernetes.io/ttl:0 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [] [{ancient-changes Update v1 2022-03-30 13:35:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:flannel.alpha.coreos.com/backend-type":{},"f:flannel.alpha.coreos.com/kube-subnet-manager":{},"f:flannel.alpha.coreos.com/public-ip":{},"f:kubeadm.alpha.kubernetes.io/cri-socket":{}}},"f:status":{"f:conditions":{"k:{\"type\":\"NetworkUnavailable\"}":{".":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}}}} } {vsphere-cloud-controller-manager Update v1 2022-03-30 13:39:29 +0000 UTC FieldsV1 {"f:status":{"f:addresses":{"k:{\"type\":\"ExternalIP\"}":{".":{},"f:address":{},"f:type":{}}}}} status} {vsphere-cloud-controller-manager Update v1 2022-06-21 22:00:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{"f:beta.kubernetes.io/instance-type":{},"f:node.kubernetes.io/instance-type":{}}},"f:spec":{"f:providerID":{}}} } {Go-http-client Update v1 2022-10-06 14:02:11 +0000 UTC FieldsV1 {"f:status":{"f:capacity":{"f:ephemeral-storage":{},"f:memory":{}},"f:nodeInfo":{"f:osImage":{}}}} status} {flanneld Update v1 2024-01-09 16:24:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:flannel.alpha.coreos.com/backend-data":{}}},"f:status":{"f:conditions":{"k:{\"type\":\"NetworkUnavailable\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{}}}}} status} {kube-controller-manager Update v1 2024-01-09 16:31:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}}},"f:spec":{"f:podCIDR":{},"f:podCIDRs":{".":{},"v:\"10.233.67.0/24\"":{}}}} } {kube-controller-manager Update v1 2024-01-10 03:09:08 +0000 UTC FieldsV1 {"f:status":{"f:volumesAttached":{}}} status} {e2e.test Update v1 2024-01-10 15:52:05 +0000 UTC FieldsV1 {"f:status":{"f:capacity":{"f:example.com/fakecpu":{}}}} status} {kubelet Update v1 2024-01-10 17:14:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:csi.volume.kubernetes.io/nodeid":{}}},"f:status":{"f:allocatable":{"f:cpu":{},"f:ephemeral-storage":{},"f:example.com/fakecpu":{},"f:memory":{}},"f:conditions":{"k:{\"type\":\"DiskPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"MemoryPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"PIDPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"Ready\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}}},"f:images":{},"f:nodeInfo":{"f:bootID":{},"f:containerRuntimeVersion":{},"f:kernelVersion":{},"f:kubeProxyVersion":{},"f:kubeletVersion":{}},"f:volumesInUse":{}}} status}]},Spec:NodeSpec{PodCIDR:10.233.67.0/24,DoNotUseExternalID:,ProviderID:vsphere://4208fb3e-b545-b1df-e33a-dc94c2e60b2d,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.233.67.0/24],},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{207929917440 0} {<nil>}  BinarySI},example.com/fakecpu: {{1 3} {<nil>} 1k DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8243097600 0} {<nil>} 8049900Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{187136925387 0} {<nil>} 187136925387 DecimalSI},example.com/fakecpu: {{1 3} {<nil>} 1k DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8138240000 0} {<nil>} 7947500Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2024-01-09 16:24:48 +0000 UTC,LastTransitionTime:2024-01-09 16:24:48 +0000 UTC,Reason:FlannelIsUp,Message:Flannel is running on this node,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2024-01-10 17:14:05 +0000 UTC,LastTransitionTime:2023-06-19 15:27:16 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2024-01-10 17:14:05 +0000 UTC,LastTransitionTime:2023-06-19 15:27:16 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2024-01-10 17:14:05 +0000 UTC,LastTransitionTime:2023-06-19 15:27:16 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2024-01-10 17:14:05 +0000 UTC,LastTransitionTime:2024-01-09 16:31:47 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:Hostname,Address:env1-test-worker-0,},NodeAddress{Type:InternalIP,Address:10.61.1.200,},NodeAddress{Type:ExternalIP,Address:10.61.1.200,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:4284f3e04f9c47a4b59e54ce697ed9a6,SystemUUID:4208fb3e-b545-b1df-e33a-dc94c2e60b2d,BootID:7417b31f-8760-402f-bdd7-bc07c9447b0d,KernelVersion:5.4.0-126-generic,OSImage:Ubuntu 20.04.5 LTS,ContainerRuntimeVersion:containerd://1.7.5,KubeletVersion:v1.27.5,KubeProxyVersion:v1.27.5,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[docker.elastic.co/beats/filebeat-oss@sha256:43ffa23888153b62ee8cb0e38a17484cf28d9f048dae39ff13bd814ff8b31ef4 docker.elastic.co/beats/filebeat-oss:7.12.1],SizeBytes:162242645,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:0b8b2b7b43b03f7db26e7e4b99be402495b77b496f5a5b0425b3c226bc1e9cbd docker.io/sonobuoy/systemd-logs:v0.4],SizeBytes:127795048,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/jessie-dnsutils@sha256:11e6a66017ba4e4b938c1612b7a54a3befcefd354796c04e1dba76873a13518e registry.k8s.io/e2e-test-images/jessie-dnsutils@sha256:11e6a66017ba4e4b938c1612b7a54a3befcefd354796c04e1dba76873a13518e k8s.gcr.io/e2e-test-images/jessie-dnsutils:1.5 registry.k8s.io/e2e-test-images/jessie-dnsutils:1.5],SizeBytes:112030526,},ContainerImage{Names:[registry.k8s.io/e2e-test-images/jessie-dnsutils@sha256:24aaf2626d6b27864c29de2097e8bbb840b3a414271bf7c8995e431e47d8408e registry.k8s.io/e2e-test-images/jessie-dnsutils:1.7],SizeBytes:112030336,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/jessie-dnsutils@sha256:702a992280fb7c3303e84a5801acbb4c9c7fcf48cffe0e9c8be3f0c60f74cf89 k8s.gcr.io/e2e-test-images/jessie-dnsutils:1.4],SizeBytes:112029652,},ContainerImage{Names:[gcr.io/cloud-provider-vsphere/csi/release/driver@sha256:c25d9d24a22d6d787dbbe7fe1cc0734113da252f5e1e3dc86429230aa9047fb3 gcr.io/cloud-provider-vsphere/csi/release/driver:v2.4.1],SizeBytes:108688290,},ContainerImage{Names:[gcr.io/cloud-provider-vsphere/csi/release/driver@sha256:ce9f031aaf5b053ca565ee02bb13b0e0a05e9fbb97103ed68d5acaaff7910cfe gcr.io/cloud-provider-vsphere/csi/release/driver:v2.7.0],SizeBytes:103859482,},ContainerImage{Names:[registry.k8s.io/etcd@sha256:dd75ec974b0a2a6f6bb47001ba09207976e625db898d1b16735528c009cb171c registry.k8s.io/etcd:3.5.6-0],SizeBytes:102542580,},ContainerImage{Names:[gcr.io/cloud-provider-vsphere/csi/release/driver@sha256:15b57e33b108b86390994112a5b8df7590c3c40be4d27e4142e12a47792a1a28 gcr.io/cloud-provider-vsphere/csi/release/driver:v2.5.2],SizeBytes:100433136,},ContainerImage{Names:[gcr.io/cloud-provider-vsphere/csi/release/driver@sha256:06cbc2350a65c0080fc320d14e4630af576342f0f8febfbf12b5877c6b63fc29 gcr.io/cloud-provider-vsphere/csi/release/driver:v2.5.1],SizeBytes:100143334,},ContainerImage{Names:[gcr.io/cloud-provider-vsphere/csi/release/driver@sha256:645a61f7a3ff314fe2dd274bbdfd9faba79fa1b011078453a2a20a6fc36b569f gcr.io/cloud-provider-vsphere/csi/release/driver:v2.6.1],SizeBytes:99607874,},ContainerImage{Names:[gcr.io/cloud-provider-vsphere/csi/release/driver@sha256:f67ff4a250ea73abee9b5a899a58587755cd07d7a80332d2c5c190f227ad6dfe gcr.io/cloud-provider-vsphere/csi/release/driver:v3.0.1],SizeBytes:93529994,},ContainerImage{Names:[quay.io/prometheus/prometheus@sha256:0f0b7feb6f02620df7d493ad7437b6ee95b6d16d8d18799f3607124e501444b1 quay.io/prometheus/prometheus:v2.44.0],SizeBytes:92967276,},ContainerImage{Names:[quay.io/prometheus/prometheus@sha256:2028b133eee0854854a6ddfff37a8a2e74e0520873164b602abbb04ad62ff572 quay.io/prometheus/prometheus:v2.40.5],SizeBytes:90097452,},ContainerImage{Names:[k8s.gcr.io/etcd@sha256:4ad90a11b55313b182afc186b9876c8e891531b8db4c9bf1541953021618d0e2 k8s.gcr.io/etcd:3.4.13-0],SizeBytes:86742272,},ContainerImage{Names:[quay.io/prometheus/prometheus@sha256:aa1687dd552ed98df598cc0fed2effbc62a0f05236bc2253c65520ddd4f2afce quay.io/prometheus/prometheus:v2.38.0],SizeBytes:86458158,},ContainerImage{Names:[quay.io/prometheus/prometheus@sha256:2acfab1966f0dbecc6afbead13eca7f47062cfe8726bb9db25e39e0c0b88e9c3 quay.io/prometheus/prometheus:v2.35.0],SizeBytes:82872637,},ContainerImage{Names:[registry.k8s.io/conformance@sha256:df500faee5ee9dc0732c336dad1faebc5baf627ff0103484fc59cf080b359eb5 registry.k8s.io/conformance:v1.25.6],SizeBytes:78945069,},ContainerImage{Names:[k8s.gcr.io/conformance@sha256:7c461fc67c880511cbc0d2ca8b1008825e8e5b945e898848edf4c1335ccc73aa k8s.gcr.io/conformance:v1.23.0],SizeBytes:72965035,},ContainerImage{Names:[quay.io/prometheus/prometheus@sha256:5c030438c1e4c86bdc7428f24ee1ad18476eefdfa8a7f76a8ccc9b74f1970d81 quay.io/prometheus/prometheus:v2.28.1],SizeBytes:72780839,},ContainerImage{Names:[docker-registry.k8s.cblabkra3.int/library/nginx:1.21.4],SizeBytes:56718949,},ContainerImage{Names:[docker-registry.k8s.cblabkra3.int/velero/velero@sha256:fabf2d40f640019aed794f477013ec03f2a4b91e3f5aa80f9becdd8d040c5c6b docker-registry.k8s.cblabkra3.int/velero/velero:v1.11.1],SizeBytes:52416175,},ContainerImage{Names:[docker-registry.k8s.cblabkra3.int/velero/velero@sha256:80fc407e950931bf594d97aee2f4de3b1f41e596fba0af590e1530347554ec1c docker-registry.k8s.cblabkra3.int/velero/velero:v1.11.0],SizeBytes:52061680,},ContainerImage{Names:[registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e registry.k8s.io/e2e-test-images/agnhost:2.43],SizeBytes:51706353,},ContainerImage{Names:[registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146 registry.k8s.io/e2e-test-images/agnhost:2.40],SizeBytes:51155161,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:7e8bdd271312fd25fc5ff5a8f04727be84044eb3d7d8d03611972a6752e2e11e k8s.gcr.io/e2e-test-images/agnhost:2.39],SizeBytes:51105200,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1 k8s.gcr.io/e2e-test-images/agnhost:2.32],SizeBytes:50002177,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/nautilus@sha256:99c0d6f1ad24a1aa1905d9c6534d193f268f7b23f9add2ae6bb41f31094bdd5c registry.k8s.io/e2e-test-images/nautilus@sha256:99c0d6f1ad24a1aa1905d9c6534d193f268f7b23f9add2ae6bb41f31094bdd5c k8s.gcr.io/e2e-test-images/nautilus:1.5 registry.k8s.io/e2e-test-images/nautilus:1.5],SizeBytes:49642095,},ContainerImage{Names:[registry.k8s.io/e2e-test-images/nautilus@sha256:80ba6c8c44f9623f06e868a1aa66026c8ec438ad814f9ec95e9333b415fe3550 registry.k8s.io/e2e-test-images/nautilus:1.7],SizeBytes:49641698,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:5b3a9f1c71c09c00649d8374224642ff7029ce91a721ec9132e6ed45fa73fd43 k8s.gcr.io/e2e-test-images/agnhost:2.33],SizeBytes:49628485,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/nautilus@sha256:1f36a24cfb5e0c3f725d7565a867c2384282fcbeccc77b07b423c9da95763a9a k8s.gcr.io/e2e-test-images/nautilus:1.4],SizeBytes:49230179,},ContainerImage{Names:[docker.io/bitnami/kubectl@sha256:84d6c24d6e7cc7a0741a1a1e019d6e344e879caf76eea368516f0b14c9abcd70 docker.io/bitnami/kubectl:1.22],SizeBytes:49170066,},ContainerImage{Names:[docker-registry.k8s.cblabkra3.int/velero/velero@sha256:8d784580931c77892458be8a3b4046ca0c751039c11ea1218d522e9d03cb92cb docker-registry.k8s.cblabkra3.int/velero/velero:v1.8.1],SizeBytes:45262026,},ContainerImage{Names:[docker-registry.k8s.cblabkra3.int/velero/velero@sha256:6be50b0730c97c7ae651e64aa21a0a844e997f11a07716aa3d6a4b005f0e51f7 docker-registry.k8s.cblabkra3.int/velero/velero:v1.8.0],SizeBytes:45254572,},ContainerImage{Names:[k8s.gcr.io/dns/k8s-dns-node-cache:1.21.1],SizeBytes:42449267,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/httpd@sha256:716d2f68314c5c4ddd5ecdb45183fcb4ed8019015982c1321571f863989b70b0 k8s.gcr.io/e2e-test-images/httpd:2.4.39-1],SizeBytes:41902332,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/httpd@sha256:20f25f275d46aa728f7615a1ccc19c78b2ed89435bf943a44b339f70f45508e6 registry.k8s.io/e2e-test-images/httpd@sha256:20f25f275d46aa728f7615a1ccc19c78b2ed89435bf943a44b339f70f45508e6 k8s.gcr.io/e2e-test-images/httpd:2.4.39-2 registry.k8s.io/e2e-test-images/httpd:2.4.39-2],SizeBytes:41902010,},ContainerImage{Names:[registry.k8s.io/e2e-test-images/httpd@sha256:3fe7acf013d1264ffded116b80a73dc129a449b0fccdb8d21af8279f2233f36e registry.k8s.io/e2e-test-images/httpd:2.4.39-4],SizeBytes:41901587,},ContainerImage{Names:[docker.io/library/traefik@sha256:1489caffaedb09f2a9e90f85074e2330ca186dee3c151d3ab849ca74185508a1 docker.io/library/traefik:v2.10.1],SizeBytes:40997444,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/httpd@sha256:b913fa234cc3473cfe16e937d106b455a7609f927f59031c81aca791e2689b50 k8s.gcr.io/e2e-test-images/httpd:2.4.38-1],SizeBytes:40765006,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3 registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3 k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 registry.k8s.io/e2e-test-images/httpd:2.4.38-2],SizeBytes:40764680,},ContainerImage{Names:[registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22 registry.k8s.io/e2e-test-images/httpd:2.4.38-4],SizeBytes:40764257,},ContainerImage{Names:[quay.io/thanos/thanos@sha256:e7d337d6ac2aea3f0f9314ec9830291789e16e2b480b9d353be02d05ce7f2a7e quay.io/thanos/thanos:v0.31.0],SizeBytes:39989732,},ContainerImage{Names:[k8s.gcr.io/kube-proxy:v1.24.6],SizeBytes:39519168,},ContainerImage{Names:[k8s.gcr.io/kube-proxy:v1.23.0],SizeBytes:39273076,},ContainerImage{Names:[docker-registry.k8s.cblabkra3.int/bitnami/thanos@sha256:f054901b5155a97ba81032c24f27de440c1a8b765c55b4e579b74e8397a0bf05 docker-registry.k8s.cblabkra3.int/bitnami/thanos:0.31.0-scratch-r5],SizeBytes:38794053,},ContainerImage{Names:[quay.io/thanos/thanos@sha256:4766a6caef0d834280fed2d8d059e922bc8781e054ca11f62de058222669d9dd quay.io/thanos/thanos:v0.29.0],SizeBytes:38140921,},ContainerImage{Names:[quay.io/thanos/thanos@sha256:12f1b46277902803e1f0a43fdd20aa8910f9d658ea4a3a86a0e7739996356bff quay.io/thanos/thanos:v0.28.0],SizeBytes:36419126,},ContainerImage{Names:[k8s.gcr.io/kube-proxy:v1.22.6],SizeBytes:35949036,},},VolumesInUse:[kubernetes.io/csi/csi.vsphere.vmware.com^8bc4d027-66d6-4864-b1fb-6829708205f7],VolumesAttached:[]AttachedVolume{AttachedVolume{Name:kubernetes.io/csi/csi.vsphere.vmware.com^8bc4d027-66d6-4864-b1fb-6829708205f7,DevicePath:,},},Config:nil,},}
  Jan 10 17:14:13.422: INFO: 
  Logging kubelet events for node env1-test-worker-0
  Jan 10 17:14:13.431: INFO: 
  Logging pods the kubelet thinks is on node env1-test-worker-0
  Jan 10 17:14:13.494: INFO: velero-794b84894f-nwdwf started at 2024-01-10 13:26:26 +0000 UTC (1+1 container statuses recorded)
  Jan 10 17:14:13.494: INFO: 	Init container velero-plugin-for-aws ready: true, restart count 0
  Jan 10 17:14:13.494: INFO: 	Container velero ready: true, restart count 0
  Jan 10 17:14:13.494: INFO: metrics-server-6b7574f5b-jmbtm started at 2024-01-09 16:32:13 +0000 UTC (0+1 container statuses recorded)
  Jan 10 17:14:13.494: INFO: 	Container metrics-server ready: true, restart count 0
  Jan 10 17:14:13.494: INFO: sonobuoy-systemd-logs-daemon-set-187fce9e0ddc499b-9m7s6 started at 2024-01-10 15:26:38 +0000 UTC (0+2 container statuses recorded)
  Jan 10 17:14:13.494: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jan 10 17:14:13.494: INFO: 	Container systemd-logs ready: true, restart count 0
  Jan 10 17:14:13.494: INFO: prometheus-kube-state-metrics-f8b6b59f-g2nrl started at 2024-01-10 03:09:01 +0000 UTC (0+1 container statuses recorded)
  Jan 10 17:14:13.494: INFO: 	Container kube-state-metrics ready: true, restart count 0
  Jan 10 17:14:13.494: INFO: kube-flannel-r8g5h started at 2024-01-09 16:24:44 +0000 UTC (2+1 container statuses recorded)
  Jan 10 17:14:13.495: INFO: 	Init container install-cni-plugin ready: true, restart count 0
  Jan 10 17:14:13.495: INFO: 	Init container install-cni ready: true, restart count 0
  Jan 10 17:14:13.495: INFO: 	Container kube-flannel ready: true, restart count 0
  Jan 10 17:14:13.495: INFO: nodelocaldns-vkvkp started at 2024-01-09 15:52:20 +0000 UTC (0+1 container statuses recorded)
  Jan 10 17:14:13.495: INFO: 	Container node-cache ready: true, restart count 0
  Jan 10 17:14:13.495: INFO: prometheus-prometheus-node-exporter-4nknm started at 2024-01-10 03:09:01 +0000 UTC (0+1 container statuses recorded)
  Jan 10 17:14:13.495: INFO: 	Container node-exporter ready: true, restart count 0
  Jan 10 17:14:13.495: INFO: nginx-proxy-env1-test-worker-0 started at 2024-01-09 16:31:48 +0000 UTC (0+1 container statuses recorded)
  Jan 10 17:14:13.495: INFO: 	Container nginx-proxy ready: true, restart count 0
  Jan 10 17:14:13.495: INFO: prometheus-kube-prometheus-operator-5f847644d6-gkll8 started at 2024-01-10 03:09:01 +0000 UTC (0+1 container statuses recorded)
  Jan 10 17:14:13.495: INFO: 	Container kube-prometheus-stack ready: true, restart count 0
  Jan 10 17:14:13.495: INFO: kube-proxy-445hs started at 2024-01-10 09:13:13 +0000 UTC (0+1 container statuses recorded)
  Jan 10 17:14:13.495: INFO: 	Container kube-proxy ready: true, restart count 0
  Jan 10 17:14:13.495: INFO: thanos-query-6f697d54b8-sp4fg started at 2024-01-10 13:06:38 +0000 UTC (0+1 container statuses recorded)
  Jan 10 17:14:13.495: INFO: 	Container query ready: true, restart count 0
  Jan 10 17:14:13.495: INFO: traefik-ingress-g4tjs started at 2024-01-10 14:39:42 +0000 UTC (0+1 container statuses recorded)
  Jan 10 17:14:13.495: INFO: 	Container traefik-ingress ready: true, restart count 0
  Jan 10 17:14:13.495: INFO: filebeat-filebeat-q5bzw started at 2024-01-10 01:14:23 +0000 UTC (0+1 container statuses recorded)
  Jan 10 17:14:13.495: INFO: 	Container filebeat ready: false, restart count 0
  Jan 10 17:14:13.495: INFO: prometheus-prometheus-kube-prometheus-prometheus-0 started at 2024-01-10 03:09:07 +0000 UTC (1+3 container statuses recorded)
  Jan 10 17:14:13.495: INFO: 	Init container init-config-reloader ready: true, restart count 0
  Jan 10 17:14:13.495: INFO: 	Container config-reloader ready: true, restart count 0
  Jan 10 17:14:13.495: INFO: 	Container prometheus ready: false, restart count 169
  Jan 10 17:14:13.495: INFO: 	Container thanos-sidecar ready: true, restart count 0
  Jan 10 17:14:13.495: INFO: vsphere-csi-node-qkfth started at 2024-01-10 09:21:38 +0000 UTC (0+3 container statuses recorded)
  Jan 10 17:14:13.495: INFO: 	Container liveness-probe ready: true, restart count 0
  Jan 10 17:14:13.495: INFO: 	Container node-driver-registrar ready: true, restart count 0
  Jan 10 17:14:13.495: INFO: 	Container vsphere-csi-node ready: true, restart count 0
  E0110 17:14:13.750265      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 17:14:13.775: INFO: 
  Latency metrics for node env1-test-worker-0
  Jan 10 17:14:13.775: INFO: 
  Logging node info for node env1-test-worker-1
  Jan 10 17:14:13.785: INFO: Node Info: &Node{ObjectMeta:{env1-test-worker-1    61ccc843-df2c-4f64-a151-3986badee4f7 186806967 0 2022-03-30 13:34:33 +0000 UTC <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/instance-type:vsphere-vm.cpu-4.mem-7gb.os-ubuntu beta.kubernetes.io/os:linux kubernetes.io/arch:amd64 kubernetes.io/hostname:env1-test-worker-1 kubernetes.io/os:linux node.kubernetes.io/instance-type:vsphere-vm.cpu-4.mem-7gb.os-ubuntu] map[alpha.kubernetes.io/provided-node-ip:10.61.1.201 csi.volume.kubernetes.io/nodeid:{"csi.vsphere.vmware.com":"420879b9-629c-c735-5c6b-dca09d264331"} flannel.alpha.coreos.com/backend-data:{"VNI":4096,"VtepMAC":"62:46:5c:d6:aa:4d"} flannel.alpha.coreos.com/backend-type:vxlan flannel.alpha.coreos.com/kube-subnet-manager:true flannel.alpha.coreos.com/public-ip:10.61.1.201 kubeadm.alpha.kubernetes.io/cri-socket:/var/run/containerd/containerd.sock node.alpha.kubernetes.io/ttl:0 volumes.kubernetes.io/controller-managed-attach-detach:true] [] [] [{ancient-changes Update v1 2022-03-30 13:35:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:flannel.alpha.coreos.com/backend-type":{},"f:flannel.alpha.coreos.com/kube-subnet-manager":{},"f:flannel.alpha.coreos.com/public-ip":{},"f:kubeadm.alpha.kubernetes.io/cri-socket":{}}},"f:status":{"f:conditions":{"k:{\"type\":\"NetworkUnavailable\"}":{".":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}}}} } {vsphere-cloud-controller-manager Update v1 2022-03-30 13:39:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{"f:beta.kubernetes.io/instance-type":{},"f:node.kubernetes.io/instance-type":{}}},"f:spec":{"f:providerID":{}}} } {vsphere-cloud-controller-manager Update v1 2022-03-30 13:39:30 +0000 UTC FieldsV1 {"f:status":{"f:addresses":{"k:{\"type\":\"ExternalIP\"}":{".":{},"f:address":{},"f:type":{}}}}} status} {Go-http-client Update v1 2022-10-06 14:05:47 +0000 UTC FieldsV1 {"f:status":{"f:capacity":{"f:ephemeral-storage":{}},"f:nodeInfo":{"f:osImage":{}}}} status} {flanneld Update v1 2024-01-09 16:24:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:flannel.alpha.coreos.com/backend-data":{}}},"f:status":{"f:conditions":{"k:{\"type\":\"NetworkUnavailable\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{}}}}} status} {kube-controller-manager Update v1 2024-01-09 16:38:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}}},"f:spec":{"f:podCIDR":{},"f:podCIDRs":{".":{},"v:\"10.233.68.0/24\"":{}}}} } {e2e.test Update v1 2024-01-10 15:52:05 +0000 UTC FieldsV1 {"f:status":{"f:allocatable":{"f:example.com/fakecpu":{}},"f:capacity":{"f:example.com/fakecpu":{}}}} status} {kubelet Update v1 2024-01-10 17:14:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:csi.volume.kubernetes.io/nodeid":{}}},"f:status":{"f:allocatable":{"f:cpu":{},"f:ephemeral-storage":{},"f:memory":{}},"f:capacity":{"f:memory":{}},"f:conditions":{"k:{\"type\":\"DiskPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"MemoryPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"PIDPressure\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}},"k:{\"type\":\"Ready\"}":{"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{}}},"f:images":{},"f:nodeInfo":{"f:bootID":{},"f:containerRuntimeVersion":{},"f:kernelVersion":{},"f:kubeProxyVersion":{},"f:kubeletVersion":{}}}} status}]},Spec:NodeSpec{PodCIDR:10.233.68.0/24,DoNotUseExternalID:,ProviderID:vsphere://420879b9-629c-c735-5c6b-dca09d264331,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[10.233.68.0/24],},Status:NodeStatus{Capacity:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{207929917440 0} {<nil>}  BinarySI},example.com/fakecpu: {{1 3} {<nil>} 1k DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8243105792 0} {<nil>} 8049908Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{4 0} {<nil>} 4 DecimalSI},ephemeral-storage: {{187136925387 0} {<nil>} 187136925387 DecimalSI},example.com/fakecpu: {{1 3} {<nil>} 1k DecimalSI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8138248192 0} {<nil>} 7947508Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:NetworkUnavailable,Status:False,LastHeartbeatTime:2024-01-09 16:24:13 +0000 UTC,LastTransitionTime:2024-01-09 16:24:13 +0000 UTC,Reason:FlannelIsUp,Message:Flannel is running on this node,},NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2024-01-10 17:14:13 +0000 UTC,LastTransitionTime:2023-04-19 08:56:44 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2024-01-10 17:14:13 +0000 UTC,LastTransitionTime:2023-04-19 08:56:44 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2024-01-10 17:14:13 +0000 UTC,LastTransitionTime:2023-04-19 08:56:44 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2024-01-10 17:14:13 +0000 UTC,LastTransitionTime:2024-01-09 16:38:31 +0000 UTC,Reason:KubeletReady,Message:kubelet is posting ready status. AppArmor enabled,},},Addresses:[]NodeAddress{NodeAddress{Type:Hostname,Address:env1-test-worker-1,},NodeAddress{Type:InternalIP,Address:10.61.1.201,},NodeAddress{Type:ExternalIP,Address:10.61.1.201,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:4284f3e04f9c47a4b59e54ce697ed9a6,SystemUUID:420879b9-629c-c735-5c6b-dca09d264331,BootID:2f5d8ede-cbed-4a5e-b0f4-ef6c171f84c4,KernelVersion:5.4.0-126-generic,OSImage:Ubuntu 20.04.5 LTS,ContainerRuntimeVersion:containerd://1.7.5,KubeletVersion:v1.27.5,KubeProxyVersion:v1.27.5,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[docker.elastic.co/beats/filebeat-oss@sha256:43ffa23888153b62ee8cb0e38a17484cf28d9f048dae39ff13bd814ff8b31ef4 docker.elastic.co/beats/filebeat-oss:7.12.1],SizeBytes:162242645,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:0b8b2b7b43b03f7db26e7e4b99be402495b77b496f5a5b0425b3c226bc1e9cbd docker.io/sonobuoy/systemd-logs:v0.4],SizeBytes:127795048,},ContainerImage{Names:[docker.io/sonobuoy/systemd-logs@sha256:fadad24a66ddd544987c38811108a73d1a306dd3b5e3f090b207786f2825ffde docker.io/sonobuoy/systemd-logs:v0.3],SizeBytes:121664960,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/jessie-dnsutils@sha256:11e6a66017ba4e4b938c1612b7a54a3befcefd354796c04e1dba76873a13518e k8s.gcr.io/e2e-test-images/jessie-dnsutils:1.5],SizeBytes:112030526,},ContainerImage{Names:[registry.k8s.io/e2e-test-images/jessie-dnsutils@sha256:24aaf2626d6b27864c29de2097e8bbb840b3a414271bf7c8995e431e47d8408e registry.k8s.io/e2e-test-images/jessie-dnsutils:1.7],SizeBytes:112030336,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/jessie-dnsutils@sha256:702a992280fb7c3303e84a5801acbb4c9c7fcf48cffe0e9c8be3f0c60f74cf89 k8s.gcr.io/e2e-test-images/jessie-dnsutils:1.4],SizeBytes:112029652,},ContainerImage{Names:[gcr.io/cloud-provider-vsphere/csi/release/driver@sha256:c25d9d24a22d6d787dbbe7fe1cc0734113da252f5e1e3dc86429230aa9047fb3 gcr.io/cloud-provider-vsphere/csi/release/driver:v2.4.1],SizeBytes:108688290,},ContainerImage{Names:[gcr.io/cloud-provider-vsphere/csi/release/driver@sha256:ce9f031aaf5b053ca565ee02bb13b0e0a05e9fbb97103ed68d5acaaff7910cfe gcr.io/cloud-provider-vsphere/csi/release/driver:v2.7.0],SizeBytes:103859482,},ContainerImage{Names:[registry.k8s.io/etcd@sha256:dd75ec974b0a2a6f6bb47001ba09207976e625db898d1b16735528c009cb171c registry.k8s.io/etcd:3.5.6-0],SizeBytes:102542580,},ContainerImage{Names:[k8s.gcr.io/etcd@sha256:13f53ed1d91e2e11aac476ee9a0269fdda6cc4874eba903efd40daf50c55eee5 k8s.gcr.io/etcd:3.5.3-0],SizeBytes:102143581,},ContainerImage{Names:[registry.k8s.io/etcd@sha256:51eae8381dcb1078289fa7b4f3df2630cdc18d09fb56f8e56b41c40e191d6c83 registry.k8s.io/etcd:3.5.7-0],SizeBytes:101639218,},ContainerImage{Names:[gcr.io/cloud-provider-vsphere/csi/release/driver@sha256:15b57e33b108b86390994112a5b8df7590c3c40be4d27e4142e12a47792a1a28 gcr.io/cloud-provider-vsphere/csi/release/driver:v2.5.2],SizeBytes:100433136,},ContainerImage{Names:[gcr.io/cloud-provider-vsphere/csi/release/driver@sha256:06cbc2350a65c0080fc320d14e4630af576342f0f8febfbf12b5877c6b63fc29 gcr.io/cloud-provider-vsphere/csi/release/driver:v2.5.1],SizeBytes:100143334,},ContainerImage{Names:[gcr.io/cloud-provider-vsphere/csi/release/driver@sha256:645a61f7a3ff314fe2dd274bbdfd9faba79fa1b011078453a2a20a6fc36b569f gcr.io/cloud-provider-vsphere/csi/release/driver:v2.6.1],SizeBytes:99607874,},ContainerImage{Names:[k8s.gcr.io/etcd@sha256:64b9ea357325d5db9f8a723dcf503b5a449177b17ac87d69481e126bb724c263 k8s.gcr.io/etcd:3.5.1-0],SizeBytes:98888614,},ContainerImage{Names:[gcr.io/cloud-provider-vsphere/csi/release/driver@sha256:f67ff4a250ea73abee9b5a899a58587755cd07d7a80332d2c5c190f227ad6dfe gcr.io/cloud-provider-vsphere/csi/release/driver:v3.0.1],SizeBytes:93529994,},ContainerImage{Names:[quay.io/prometheus/prometheus@sha256:0f0b7feb6f02620df7d493ad7437b6ee95b6d16d8d18799f3607124e501444b1 quay.io/prometheus/prometheus:v2.44.0],SizeBytes:92967276,},ContainerImage{Names:[quay.io/prometheus/prometheus@sha256:2028b133eee0854854a6ddfff37a8a2e74e0520873164b602abbb04ad62ff572 quay.io/prometheus/prometheus:v2.40.5],SizeBytes:90097452,},ContainerImage{Names:[k8s.gcr.io/etcd@sha256:4ad90a11b55313b182afc186b9876c8e891531b8db4c9bf1541953021618d0e2 k8s.gcr.io/etcd:3.4.13-0],SizeBytes:86742272,},ContainerImage{Names:[quay.io/prometheus/prometheus@sha256:aa1687dd552ed98df598cc0fed2effbc62a0f05236bc2253c65520ddd4f2afce quay.io/prometheus/prometheus:v2.38.0],SizeBytes:86458158,},ContainerImage{Names:[quay.io/prometheus/prometheus@sha256:2acfab1966f0dbecc6afbead13eca7f47062cfe8726bb9db25e39e0c0b88e9c3 quay.io/prometheus/prometheus:v2.35.0],SizeBytes:82872637,},ContainerImage{Names:[registry.k8s.io/conformance@sha256:1213400eac79f7c3b4445360e3ae4d1d329fefe664ea7230df56cdb87894c282 registry.k8s.io/conformance:v1.26.5],SizeBytes:81824431,},ContainerImage{Names:[docker.io/bitnami/kubectl@sha256:6208c0c337c81b3d508e3428195ccb80b17b234be824bf7aa7bd22c8255a75de docker.io/bitnami/kubectl:1.23],SizeBytes:81366699,},ContainerImage{Names:[docker.io/bitnami/kubectl@sha256:f1d7cc293f48a6c6a7ffbe8e92b612b051c076b81ee2fa63395a5409b4773601 docker.io/bitnami/kubectl:1.27],SizeBytes:80599850,},ContainerImage{Names:[docker.io/bitnami/kubectl@sha256:f07b53fae6ae87dd4a1933b546e9309b7b97dab0ee8b82a338914c9b9fad8b1c docker.io/bitnami/kubectl:1.24],SizeBytes:80529708,},ContainerImage{Names:[docker.io/bitnami/kubectl@sha256:6cfec5c1eadb3a8fef532aa1806c6ce5d759d9fc178af4cbff5a1f6d53774f6f docker.io/bitnami/kubectl:1.26],SizeBytes:80252189,},ContainerImage{Names:[docker.io/bitnami/kubectl@sha256:8a546704e7842cc0ed235d841c83d9b2bd580c40adcdf1a9fbbc58f7e9ec47f0 docker.io/bitnami/kubectl:1.25],SizeBytes:79672887,},ContainerImage{Names:[k8s.gcr.io/conformance@sha256:df500faee5ee9dc0732c336dad1faebc5baf627ff0103484fc59cf080b359eb5 k8s.gcr.io/conformance:v1.25.6],SizeBytes:78945069,},ContainerImage{Names:[k8s.gcr.io/conformance@sha256:36011a62e49a4de557b1c1de5c58d486cc7727dc7adbb65fbf81e078e60b866d registry.k8s.io/conformance@sha256:36011a62e49a4de557b1c1de5c58d486cc7727dc7adbb65fbf81e078e60b866d k8s.gcr.io/conformance:v1.27.1 registry.k8s.io/conformance:v1.27.1],SizeBytes:78238328,},ContainerImage{Names:[k8s.gcr.io/conformance@sha256:ae82f4d235f63d651b6f8c217ab015a5f87ce029e505b8b82cbc38bddbe4c2b1 k8s.gcr.io/conformance:v1.24.6],SizeBytes:76516050,},ContainerImage{Names:[k8s.gcr.io/conformance@sha256:7c461fc67c880511cbc0d2ca8b1008825e8e5b945e898848edf4c1335ccc73aa k8s.gcr.io/conformance:v1.23.0],SizeBytes:72965035,},ContainerImage{Names:[quay.io/prometheus/prometheus@sha256:5c030438c1e4c86bdc7428f24ee1ad18476eefdfa8a7f76a8ccc9b74f1970d81 quay.io/prometheus/prometheus:v2.28.1],SizeBytes:72780839,},ContainerImage{Names:[k8s.gcr.io/conformance@sha256:82f1c79299013c839fb635f0de8f77fcfc1ff37be48021e93e3dd2a0f7300fc3 k8s.gcr.io/conformance:v1.22.6],SizeBytes:71688588,},ContainerImage{Names:[k8s.gcr.io/conformance@sha256:e4560c4033e475d5d7f702ac210d87ea72aa5601f32e2e90f9c899aac2af8772 k8s.gcr.io/conformance:v1.22.5],SizeBytes:71688444,},ContainerImage{Names:[docker-registry.k8s.cblabkra3.int/library/nginx:1.21.4],SizeBytes:56718949,},ContainerImage{Names:[docker-registry.k8s.cblabkra3.int/velero/velero@sha256:fabf2d40f640019aed794f477013ec03f2a4b91e3f5aa80f9becdd8d040c5c6b docker-registry.k8s.cblabkra3.int/velero/velero:v1.11.1],SizeBytes:52416175,},ContainerImage{Names:[docker-registry.k8s.cblabkra3.int/velero/velero@sha256:80fc407e950931bf594d97aee2f4de3b1f41e596fba0af590e1530347554ec1c docker-registry.k8s.cblabkra3.int/velero/velero:v1.11.0],SizeBytes:52061680,},ContainerImage{Names:[registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e registry.k8s.io/e2e-test-images/agnhost:2.43],SizeBytes:51706353,},ContainerImage{Names:[registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146 registry.k8s.io/e2e-test-images/agnhost:2.40],SizeBytes:51155161,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:7e8bdd271312fd25fc5ff5a8f04727be84044eb3d7d8d03611972a6752e2e11e k8s.gcr.io/e2e-test-images/agnhost:2.39],SizeBytes:51105200,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:758db666ac7028534dba72e7e9bb1e57bb81b8196f976f7a5cc351ef8b3529e1 k8s.gcr.io/e2e-test-images/agnhost:2.32],SizeBytes:50002177,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/nautilus@sha256:99c0d6f1ad24a1aa1905d9c6534d193f268f7b23f9add2ae6bb41f31094bdd5c registry.k8s.io/e2e-test-images/nautilus@sha256:99c0d6f1ad24a1aa1905d9c6534d193f268f7b23f9add2ae6bb41f31094bdd5c k8s.gcr.io/e2e-test-images/nautilus:1.5 registry.k8s.io/e2e-test-images/nautilus:1.5],SizeBytes:49642095,},ContainerImage{Names:[registry.k8s.io/e2e-test-images/nautilus@sha256:80ba6c8c44f9623f06e868a1aa66026c8ec438ad814f9ec95e9333b415fe3550 registry.k8s.io/e2e-test-images/nautilus:1.7],SizeBytes:49641698,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/agnhost@sha256:5b3a9f1c71c09c00649d8374224642ff7029ce91a721ec9132e6ed45fa73fd43 k8s.gcr.io/e2e-test-images/agnhost:2.33],SizeBytes:49628485,},ContainerImage{Names:[k8s.gcr.io/e2e-test-images/nautilus@sha256:1f36a24cfb5e0c3f725d7565a867c2384282fcbeccc77b07b423c9da95763a9a k8s.gcr.io/e2e-test-images/nautilus:1.4],SizeBytes:49230179,},ContainerImage{Names:[docker.io/bitnami/kubectl@sha256:a843a5b8242da8b6a0e5947a28045e9160a9f047a4eac76c32e18b9581f2a12a docker.io/bitnami/kubectl:1.22],SizeBytes:49163183,},ContainerImage{Names:[docker-registry.k8s.cblabkra3.int/velero/velero@sha256:e69a8b1a0787f3c5e81301d6315b094a4dc60a378257762ba1c867abd083b31b docker-registry.k8s.cblabkra3.int/velero/velero:v1.9.5],SizeBytes:47200396,},ContainerImage{Names:[docker-registry.k8s.cblabkra3.int/velero/velero@sha256:8d784580931c77892458be8a3b4046ca0c751039c11ea1218d522e9d03cb92cb docker-registry.k8s.cblabkra3.int/velero/velero:v1.8.1],SizeBytes:45262026,},ContainerImage{Names:[docker-registry.k8s.cblabkra3.int/velero/velero@sha256:6be50b0730c97c7ae651e64aa21a0a844e997f11a07716aa3d6a4b005f0e51f7 docker-registry.k8s.cblabkra3.int/velero/velero:v1.8.0],SizeBytes:45254572,},ContainerImage{Names:[k8s.gcr.io/dns/k8s-dns-node-cache:1.21.1],SizeBytes:42449267,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
  Jan 10 17:14:13.787: INFO: 
  Logging kubelet events for node env1-test-worker-1
  Jan 10 17:14:13.796: INFO: 
  Logging pods the kubelet thinks is on node env1-test-worker-1
  Jan 10 17:14:13.852: INFO: traefik-ingress-lggr6 started at 2024-01-10 15:53:37 +0000 UTC (0+1 container statuses recorded)
  Jan 10 17:14:13.852: INFO: 	Container traefik-ingress ready: true, restart count 0
  Jan 10 17:14:13.852: INFO: sonobuoy started at 2024-01-10 15:26:36 +0000 UTC (0+1 container statuses recorded)
  Jan 10 17:14:13.852: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jan 10 17:14:13.852: INFO: sonobuoy-e2e-job-b46f6697883e4f52 started at 2024-01-10 15:26:37 +0000 UTC (0+2 container statuses recorded)
  Jan 10 17:14:13.852: INFO: 	Container e2e ready: true, restart count 0
  Jan 10 17:14:13.852: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jan 10 17:14:13.852: INFO: nginx-proxy-env1-test-worker-1 started at 2024-01-09 16:38:31 +0000 UTC (0+1 container statuses recorded)
  Jan 10 17:14:13.852: INFO: 	Container nginx-proxy ready: true, restart count 0
  Jan 10 17:14:13.852: INFO: kube-proxy-78tcd started at 2024-01-10 09:13:13 +0000 UTC (0+1 container statuses recorded)
  Jan 10 17:14:13.852: INFO: 	Container kube-proxy ready: true, restart count 0
  Jan 10 17:14:13.852: INFO: nodelocaldns-7qx4w started at 2024-01-09 15:52:12 +0000 UTC (0+1 container statuses recorded)
  Jan 10 17:14:13.852: INFO: 	Container node-cache ready: true, restart count 0
  Jan 10 17:14:13.852: INFO: kube-flannel-jxf5s started at 2024-01-09 16:24:09 +0000 UTC (2+1 container statuses recorded)
  Jan 10 17:14:13.853: INFO: 	Init container install-cni-plugin ready: true, restart count 0
  Jan 10 17:14:13.853: INFO: 	Init container install-cni ready: true, restart count 0
  Jan 10 17:14:13.853: INFO: 	Container kube-flannel ready: true, restart count 0
  Jan 10 17:14:13.853: INFO: sonobuoy-systemd-logs-daemon-set-187fce9e0ddc499b-nntgv started at 2024-01-10 15:26:38 +0000 UTC (0+2 container statuses recorded)
  Jan 10 17:14:13.853: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jan 10 17:14:13.853: INFO: 	Container systemd-logs ready: true, restart count 0
  Jan 10 17:14:13.853: INFO: prometheus-prometheus-node-exporter-788fx started at 2024-01-10 15:53:35 +0000 UTC (0+1 container statuses recorded)
  Jan 10 17:14:13.853: INFO: 	Container node-exporter ready: true, restart count 0
  Jan 10 17:14:13.853: INFO: vsphere-csi-node-lr59t started at 2024-01-10 09:21:38 +0000 UTC (0+3 container statuses recorded)
  Jan 10 17:14:13.853: INFO: 	Container liveness-probe ready: true, restart count 0
  Jan 10 17:14:13.853: INFO: 	Container node-driver-registrar ready: true, restart count 0
  Jan 10 17:14:13.853: INFO: 	Container vsphere-csi-node ready: true, restart count 0
  Jan 10 17:14:13.853: INFO: filebeat-filebeat-dxph4 started at 2024-01-10 15:44:08 +0000 UTC (0+1 container statuses recorded)
  Jan 10 17:14:13.853: INFO: 	Container filebeat ready: false, restart count 0
  Jan 10 17:14:14.100: INFO: 
  Latency metrics for node env1-test-worker-1
  STEP: Destroying namespace "daemonsets-5684" for this suite. @ 01/10/24 17:14:14.101
• [FAILED] [12.967 seconds]
[sig-apps] Daemon set [Serial] [It] should rollback without unnecessary restarts [Conformance]
test/e2e/apps/daemon_set.go:432

  [FAILED] unexpected pod daemon-set-tc95b be restarted
  In [It] at: test/e2e/apps/daemon_set.go:500 @ 01/10/24 17:14:09.771
------------------------------
SSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:177
  STEP: Creating a kubernetes client @ 01/10/24 17:14:14.123
  Jan 10 17:14:14.123: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename init-container @ 01/10/24 17:14:14.125
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 17:14:14.173
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 17:14:14.182
  STEP: creating the pod @ 01/10/24 17:14:14.19
  Jan 10 17:14:14.190: INFO: PodSpec: initContainers in spec.initContainers
  E0110 17:14:14.753470      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:14:15.754196      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:14:16.755055      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:14:17.755567      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:14:18.755820      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 17:14:18.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-46" for this suite. @ 01/10/24 17:14:19.01
• [4.919 seconds]
------------------------------
[sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]
test/e2e/auth/service_accounts.go:740
  STEP: Creating a kubernetes client @ 01/10/24 17:14:19.043
  Jan 10 17:14:19.043: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename svcaccounts @ 01/10/24 17:14:19.047
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 17:14:19.09
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 17:14:19.103
  Jan 10 17:14:19.120: INFO: Got root ca configmap in namespace "svcaccounts-7725"
  Jan 10 17:14:19.144: INFO: Deleted root ca configmap in namespace "svcaccounts-7725"
  STEP: waiting for a new root ca configmap created @ 01/10/24 17:14:19.646
  Jan 10 17:14:19.661: INFO: Recreated root ca configmap in namespace "svcaccounts-7725"
  Jan 10 17:14:19.682: INFO: Updated root ca configmap in namespace "svcaccounts-7725"
  E0110 17:14:19.755788      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting for the root ca configmap reconciled @ 01/10/24 17:14:20.183
  Jan 10 17:14:20.193: INFO: Reconciled root ca configmap in namespace "svcaccounts-7725"
  Jan 10 17:14:20.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-7725" for this suite. @ 01/10/24 17:14:20.214
• [1.195 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency should not be very high  [Conformance]
test/e2e/network/service_latency.go:59
  STEP: Creating a kubernetes client @ 01/10/24 17:14:20.248
  Jan 10 17:14:20.248: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename svc-latency @ 01/10/24 17:14:20.251
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 17:14:20.317
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 17:14:20.332
  Jan 10 17:14:20.348: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: creating replication controller svc-latency-rc in namespace svc-latency-9103 @ 01/10/24 17:14:20.35
  I0110 17:14:20.371087      23 runners.go:194] Created replication controller with name: svc-latency-rc, namespace: svc-latency-9103, replica count: 1
  E0110 17:14:20.756673      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0110 17:14:21.422702      23 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0110 17:14:21.757025      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0110 17:14:22.423006      23 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jan 10 17:14:22.556: INFO: Created: latency-svc-7mrxl
  Jan 10 17:14:22.588: INFO: Got endpoints: latency-svc-7mrxl [64.745411ms]
  Jan 10 17:14:22.654: INFO: Created: latency-svc-9c9bd
  Jan 10 17:14:22.670: INFO: Got endpoints: latency-svc-9c9bd [81.876313ms]
  Jan 10 17:14:22.681: INFO: Created: latency-svc-czrsl
  Jan 10 17:14:22.702: INFO: Got endpoints: latency-svc-czrsl [113.51594ms]
  Jan 10 17:14:22.710: INFO: Created: latency-svc-pf28k
  Jan 10 17:14:22.725: INFO: Created: latency-svc-9xnsp
  Jan 10 17:14:22.734: INFO: Got endpoints: latency-svc-pf28k [145.44325ms]
  Jan 10 17:14:22.757: INFO: Got endpoints: latency-svc-9xnsp [168.195189ms]
  E0110 17:14:22.758258      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 17:14:22.782: INFO: Created: latency-svc-9tsx7
  Jan 10 17:14:22.793: INFO: Got endpoints: latency-svc-9tsx7 [203.466952ms]
  Jan 10 17:14:22.804: INFO: Created: latency-svc-6rn6p
  Jan 10 17:14:22.817: INFO: Got endpoints: latency-svc-6rn6p [228.230903ms]
  Jan 10 17:14:22.833: INFO: Created: latency-svc-wt6rt
  Jan 10 17:14:22.852: INFO: Got endpoints: latency-svc-wt6rt [262.055062ms]
  Jan 10 17:14:22.878: INFO: Created: latency-svc-zrttd
  Jan 10 17:14:22.907: INFO: Got endpoints: latency-svc-zrttd [317.329466ms]
  Jan 10 17:14:22.914: INFO: Created: latency-svc-4w2j7
  Jan 10 17:14:22.942: INFO: Got endpoints: latency-svc-4w2j7 [352.375786ms]
  Jan 10 17:14:22.972: INFO: Created: latency-svc-p85s4
  Jan 10 17:14:22.999: INFO: Got endpoints: latency-svc-p85s4 [409.487918ms]
  Jan 10 17:14:23.005: INFO: Created: latency-svc-l58vw
  Jan 10 17:14:23.015: INFO: Got endpoints: latency-svc-l58vw [424.921284ms]
  Jan 10 17:14:23.064: INFO: Created: latency-svc-24r2x
  Jan 10 17:14:23.091: INFO: Got endpoints: latency-svc-24r2x [500.72982ms]
  Jan 10 17:14:23.148: INFO: Created: latency-svc-m285r
  Jan 10 17:14:23.153: INFO: Created: latency-svc-jg5vg
  Jan 10 17:14:23.188: INFO: Got endpoints: latency-svc-m285r [597.466178ms]
  Jan 10 17:14:23.204: INFO: Got endpoints: latency-svc-jg5vg [613.42446ms]
  Jan 10 17:14:23.232: INFO: Created: latency-svc-c6s6j
  Jan 10 17:14:23.262: INFO: Got endpoints: latency-svc-c6s6j [671.540385ms]
  Jan 10 17:14:23.263: INFO: Created: latency-svc-vswm7
  Jan 10 17:14:23.273: INFO: Got endpoints: latency-svc-vswm7 [602.83185ms]
  Jan 10 17:14:23.540: INFO: Created: latency-svc-fxqvz
  Jan 10 17:14:23.547: INFO: Created: latency-svc-nt4pb
  Jan 10 17:14:23.549: INFO: Created: latency-svc-bvn24
  Jan 10 17:14:23.550: INFO: Created: latency-svc-bf52c
  Jan 10 17:14:23.552: INFO: Created: latency-svc-9l7jb
  Jan 10 17:14:23.552: INFO: Created: latency-svc-h7fnt
  Jan 10 17:14:23.552: INFO: Created: latency-svc-qt7t4
  Jan 10 17:14:23.557: INFO: Created: latency-svc-4n2pb
  Jan 10 17:14:23.565: INFO: Created: latency-svc-n9r4f
  Jan 10 17:14:23.566: INFO: Got endpoints: latency-svc-n9r4f [772.848958ms]
  Jan 10 17:14:23.567: INFO: Created: latency-svc-djf95
  Jan 10 17:14:23.568: INFO: Created: latency-svc-b4p5l
  Jan 10 17:14:23.569: INFO: Created: latency-svc-j7p4w
  Jan 10 17:14:23.569: INFO: Created: latency-svc-jvz4j
  Jan 10 17:14:23.570: INFO: Created: latency-svc-hdcml
  Jan 10 17:14:23.570: INFO: Created: latency-svc-rkk44
  Jan 10 17:14:23.582: INFO: Got endpoints: latency-svc-djf95 [880.202678ms]
  Jan 10 17:14:23.624: INFO: Got endpoints: latency-svc-b4p5l [715.976549ms]
  Jan 10 17:14:23.626: INFO: Got endpoints: latency-svc-fxqvz [364.42637ms]
  Jan 10 17:14:23.626: INFO: Got endpoints: latency-svc-rkk44 [611.530783ms]
  Jan 10 17:14:23.652: INFO: Got endpoints: latency-svc-bf52c [798.843756ms]
  Jan 10 17:14:23.662: INFO: Created: latency-svc-mc67k
  Jan 10 17:14:23.675: INFO: Got endpoints: latency-svc-h7fnt [402.22135ms]
  Jan 10 17:14:23.676: INFO: Got endpoints: latency-svc-nt4pb [917.838593ms]
  Jan 10 17:14:23.682: INFO: Got endpoints: latency-svc-9l7jb [682.931745ms]
  Jan 10 17:14:23.682: INFO: Got endpoints: latency-svc-bvn24 [494.699622ms]
  Jan 10 17:14:23.709: INFO: Created: latency-svc-hnfg9
  Jan 10 17:14:23.710: INFO: Got endpoints: latency-svc-4n2pb [505.951771ms]
  Jan 10 17:14:23.710: INFO: Got endpoints: latency-svc-qt7t4 [975.502875ms]
  Jan 10 17:14:23.724: INFO: Got endpoints: latency-svc-j7p4w [632.004445ms]
  Jan 10 17:14:23.737: INFO: Created: latency-svc-x4q8g
  Jan 10 17:14:23.745: INFO: Got endpoints: latency-svc-jvz4j [801.044861ms]
  Jan 10 17:14:23.745: INFO: Got endpoints: latency-svc-hdcml [927.854314ms]
  E0110 17:14:23.760221      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 17:14:23.761: INFO: Created: latency-svc-4txgq
  Jan 10 17:14:23.769: INFO: Got endpoints: latency-svc-mc67k [203.600817ms]
  Jan 10 17:14:23.769: INFO: Got endpoints: latency-svc-hnfg9 [186.999889ms]
  Jan 10 17:14:23.770: INFO: Got endpoints: latency-svc-x4q8g [145.629407ms]
  Jan 10 17:14:23.782: INFO: Created: latency-svc-cnjn9
  Jan 10 17:14:23.789: INFO: Got endpoints: latency-svc-4txgq [162.041109ms]
  Jan 10 17:14:23.792: INFO: Created: latency-svc-dldtz
  Jan 10 17:14:23.799: INFO: Got endpoints: latency-svc-cnjn9 [172.708456ms]
  Jan 10 17:14:23.827: INFO: Got endpoints: latency-svc-dldtz [175.157911ms]
  Jan 10 17:14:23.830: INFO: Created: latency-svc-5lsnm
  Jan 10 17:14:23.835: INFO: Got endpoints: latency-svc-5lsnm [158.400489ms]
  Jan 10 17:14:23.841: INFO: Created: latency-svc-hs4ds
  Jan 10 17:14:23.849: INFO: Got endpoints: latency-svc-hs4ds [166.783506ms]
  Jan 10 17:14:23.870: INFO: Created: latency-svc-6q5tm
  Jan 10 17:14:23.883: INFO: Got endpoints: latency-svc-6q5tm [207.350439ms]
  Jan 10 17:14:23.900: INFO: Created: latency-svc-2m5ps
  Jan 10 17:14:23.908: INFO: Got endpoints: latency-svc-2m5ps [225.894695ms]
  Jan 10 17:14:23.931: INFO: Created: latency-svc-2bbx5
  Jan 10 17:14:23.949: INFO: Got endpoints: latency-svc-2bbx5 [239.215519ms]
  Jan 10 17:14:23.960: INFO: Created: latency-svc-msgfm
  Jan 10 17:14:23.974: INFO: Got endpoints: latency-svc-msgfm [263.217981ms]
  Jan 10 17:14:23.983: INFO: Created: latency-svc-m9pgg
  Jan 10 17:14:24.001: INFO: Got endpoints: latency-svc-m9pgg [277.755586ms]
  Jan 10 17:14:24.006: INFO: Created: latency-svc-n45dc
  Jan 10 17:14:24.019: INFO: Got endpoints: latency-svc-n45dc [274.321813ms]
  Jan 10 17:14:24.029: INFO: Created: latency-svc-fjkws
  Jan 10 17:14:24.038: INFO: Got endpoints: latency-svc-fjkws [292.32405ms]
  Jan 10 17:14:24.051: INFO: Created: latency-svc-5j7mp
  Jan 10 17:14:24.071: INFO: Created: latency-svc-krlwl
  Jan 10 17:14:24.102: INFO: Created: latency-svc-x5zjb
  Jan 10 17:14:24.104: INFO: Got endpoints: latency-svc-5j7mp [334.03251ms]
  Jan 10 17:14:24.109: INFO: Got endpoints: latency-svc-krlwl [338.780082ms]
  Jan 10 17:14:24.128: INFO: Got endpoints: latency-svc-x5zjb [358.355698ms]
  Jan 10 17:14:24.142: INFO: Created: latency-svc-dft7s
  Jan 10 17:14:24.156: INFO: Created: latency-svc-htct8
  Jan 10 17:14:24.158: INFO: Got endpoints: latency-svc-dft7s [368.345291ms]
  Jan 10 17:14:24.179: INFO: Got endpoints: latency-svc-htct8 [379.077034ms]
  Jan 10 17:14:24.202: INFO: Created: latency-svc-mx6ph
  Jan 10 17:14:24.203: INFO: Got endpoints: latency-svc-mx6ph [375.383275ms]
  Jan 10 17:14:24.219: INFO: Created: latency-svc-kfvq9
  Jan 10 17:14:24.252: INFO: Got endpoints: latency-svc-kfvq9 [417.054811ms]
  Jan 10 17:14:24.277: INFO: Created: latency-svc-4sjj8
  Jan 10 17:14:24.278: INFO: Got endpoints: latency-svc-4sjj8 [427.841015ms]
  Jan 10 17:14:24.277: INFO: Created: latency-svc-9nrxg
  Jan 10 17:14:24.290: INFO: Got endpoints: latency-svc-9nrxg [406.86647ms]
  Jan 10 17:14:24.326: INFO: Created: latency-svc-wdv6p
  Jan 10 17:14:24.350: INFO: Got endpoints: latency-svc-wdv6p [441.440232ms]
  Jan 10 17:14:24.360: INFO: Created: latency-svc-5sgnk
  Jan 10 17:14:24.379: INFO: Got endpoints: latency-svc-5sgnk [429.637144ms]
  Jan 10 17:14:24.392: INFO: Created: latency-svc-8g2zv
  Jan 10 17:14:24.401: INFO: Got endpoints: latency-svc-8g2zv [426.292397ms]
  Jan 10 17:14:24.430: INFO: Created: latency-svc-mg5t7
  Jan 10 17:14:24.437: INFO: Got endpoints: latency-svc-mg5t7 [435.681041ms]
  Jan 10 17:14:24.451: INFO: Created: latency-svc-5rqz8
  Jan 10 17:14:24.464: INFO: Got endpoints: latency-svc-5rqz8 [444.653878ms]
  Jan 10 17:14:24.498: INFO: Created: latency-svc-w5gh8
  Jan 10 17:14:24.514: INFO: Got endpoints: latency-svc-w5gh8 [475.460874ms]
  Jan 10 17:14:24.527: INFO: Created: latency-svc-p6qh7
  Jan 10 17:14:24.540: INFO: Got endpoints: latency-svc-p6qh7 [430.610753ms]
  Jan 10 17:14:24.553: INFO: Created: latency-svc-n78cz
  Jan 10 17:14:24.570: INFO: Got endpoints: latency-svc-n78cz [465.69536ms]
  Jan 10 17:14:24.578: INFO: Created: latency-svc-l282d
  Jan 10 17:14:24.618: INFO: Got endpoints: latency-svc-l282d [489.864641ms]
  Jan 10 17:14:24.622: INFO: Created: latency-svc-x4zkm
  Jan 10 17:14:24.645: INFO: Got endpoints: latency-svc-x4zkm [487.620665ms]
  Jan 10 17:14:24.649: INFO: Created: latency-svc-8wmbv
  Jan 10 17:14:24.688: INFO: Got endpoints: latency-svc-8wmbv [508.748482ms]
  Jan 10 17:14:24.689: INFO: Created: latency-svc-2ft56
  Jan 10 17:14:24.700: INFO: Got endpoints: latency-svc-2ft56 [496.609192ms]
  Jan 10 17:14:24.722: INFO: Created: latency-svc-ffcd5
  Jan 10 17:14:24.732: INFO: Created: latency-svc-mr2f7
  Jan 10 17:14:24.738: INFO: Got endpoints: latency-svc-ffcd5 [485.475111ms]
  Jan 10 17:14:24.753: INFO: Created: latency-svc-xrllf
  E0110 17:14:24.760527      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 17:14:24.768: INFO: Got endpoints: latency-svc-mr2f7 [489.787589ms]
  Jan 10 17:14:24.770: INFO: Created: latency-svc-97542
  Jan 10 17:14:24.775: INFO: Got endpoints: latency-svc-xrllf [485.058277ms]
  Jan 10 17:14:24.796: INFO: Got endpoints: latency-svc-97542 [446.084473ms]
  Jan 10 17:14:24.805: INFO: Created: latency-svc-hfdxw
  Jan 10 17:14:24.843: INFO: Got endpoints: latency-svc-hfdxw [464.21634ms]
  Jan 10 17:14:24.986: INFO: Created: latency-svc-bhpvg
  Jan 10 17:14:24.987: INFO: Created: latency-svc-449d5
  Jan 10 17:14:24.987: INFO: Created: latency-svc-fs49r
  Jan 10 17:14:24.987: INFO: Created: latency-svc-vmqlz
  Jan 10 17:14:24.988: INFO: Created: latency-svc-8vzp4
  Jan 10 17:14:24.998: INFO: Created: latency-svc-9wg62
  Jan 10 17:14:24.999: INFO: Created: latency-svc-4v2ls
  Jan 10 17:14:25.000: INFO: Created: latency-svc-qx6vx
  Jan 10 17:14:25.000: INFO: Created: latency-svc-67b4s
  Jan 10 17:14:25.001: INFO: Created: latency-svc-x5h88
  Jan 10 17:14:25.001: INFO: Got endpoints: latency-svc-x5h88 [157.647645ms]
  Jan 10 17:14:25.003: INFO: Created: latency-svc-vlkf2
  Jan 10 17:14:25.003: INFO: Created: latency-svc-gp4lc
  Jan 10 17:14:25.004: INFO: Created: latency-svc-ffblw
  Jan 10 17:14:25.004: INFO: Created: latency-svc-tpftz
  Jan 10 17:14:25.004: INFO: Created: latency-svc-nfk24
  Jan 10 17:14:25.020: INFO: Got endpoints: latency-svc-qx6vx [331.604262ms]
  Jan 10 17:14:25.039: INFO: Got endpoints: latency-svc-9wg62 [638.162909ms]
  Jan 10 17:14:25.045: INFO: Got endpoints: latency-svc-4v2ls [276.877824ms]
  Jan 10 17:14:25.048: INFO: Created: latency-svc-n4fxn
  Jan 10 17:14:25.057: INFO: Created: latency-svc-jk692
  Jan 10 17:14:25.072: INFO: Created: latency-svc-v76md
  Jan 10 17:14:25.080: INFO: Created: latency-svc-fgbs9
  Jan 10 17:14:25.090: INFO: Got endpoints: latency-svc-bhpvg [625.123155ms]
  Jan 10 17:14:25.108: INFO: Created: latency-svc-5szj4
  Jan 10 17:14:25.138: INFO: Got endpoints: latency-svc-449d5 [362.904586ms]
  Jan 10 17:14:25.160: INFO: Created: latency-svc-2nl4n
  Jan 10 17:14:25.189: INFO: Got endpoints: latency-svc-67b4s [619.024112ms]
  Jan 10 17:14:25.225: INFO: Created: latency-svc-m4sqj
  Jan 10 17:14:25.244: INFO: Got endpoints: latency-svc-nfk24 [729.800128ms]
  Jan 10 17:14:25.284: INFO: Created: latency-svc-vhmhr
  Jan 10 17:14:25.300: INFO: Got endpoints: latency-svc-gp4lc [862.248768ms]
  Jan 10 17:14:25.316: INFO: Created: latency-svc-hn2dd
  Jan 10 17:14:25.344: INFO: Got endpoints: latency-svc-8vzp4 [804.094172ms]
  Jan 10 17:14:25.373: INFO: Created: latency-svc-v6kj6
  Jan 10 17:14:25.393: INFO: Got endpoints: latency-svc-vmqlz [746.897011ms]
  Jan 10 17:14:25.415: INFO: Created: latency-svc-gjvxf
  Jan 10 17:14:25.441: INFO: Got endpoints: latency-svc-fs49r [702.132916ms]
  Jan 10 17:14:25.468: INFO: Created: latency-svc-qnk58
  Jan 10 17:14:25.493: INFO: Got endpoints: latency-svc-ffblw [875.21919ms]
  Jan 10 17:14:25.523: INFO: Created: latency-svc-vmngd
  Jan 10 17:14:25.539: INFO: Got endpoints: latency-svc-tpftz [838.379146ms]
  Jan 10 17:14:25.565: INFO: Created: latency-svc-jflzz
  Jan 10 17:14:25.597: INFO: Got endpoints: latency-svc-vlkf2 [801.185132ms]
  Jan 10 17:14:25.616: INFO: Created: latency-svc-4cvcz
  Jan 10 17:14:25.660: INFO: Got endpoints: latency-svc-n4fxn [659.698163ms]
  Jan 10 17:14:25.688: INFO: Created: latency-svc-gwk2r
  Jan 10 17:14:25.696: INFO: Got endpoints: latency-svc-jk692 [675.663181ms]
  Jan 10 17:14:25.720: INFO: Created: latency-svc-bzlsl
  Jan 10 17:14:25.749: INFO: Got endpoints: latency-svc-v76md [709.972315ms]
  E0110 17:14:25.760989      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 17:14:25.770: INFO: Created: latency-svc-g6jms
  Jan 10 17:14:25.791: INFO: Got endpoints: latency-svc-fgbs9 [745.010387ms]
  Jan 10 17:14:25.816: INFO: Created: latency-svc-f8qng
  Jan 10 17:14:25.838: INFO: Got endpoints: latency-svc-5szj4 [747.927958ms]
  Jan 10 17:14:25.862: INFO: Created: latency-svc-zkwrz
  Jan 10 17:14:25.890: INFO: Got endpoints: latency-svc-2nl4n [751.518484ms]
  Jan 10 17:14:25.912: INFO: Created: latency-svc-tv7dg
  Jan 10 17:14:25.957: INFO: Got endpoints: latency-svc-m4sqj [768.102431ms]
  Jan 10 17:14:25.983: INFO: Created: latency-svc-rwpgk
  Jan 10 17:14:25.993: INFO: Got endpoints: latency-svc-vhmhr [749.578333ms]
  Jan 10 17:14:26.024: INFO: Created: latency-svc-qbcs7
  Jan 10 17:14:26.040: INFO: Got endpoints: latency-svc-hn2dd [739.378325ms]
  Jan 10 17:14:26.058: INFO: Created: latency-svc-sm6k6
  Jan 10 17:14:26.085: INFO: Got endpoints: latency-svc-v6kj6 [741.346327ms]
  Jan 10 17:14:26.108: INFO: Created: latency-svc-nnn6v
  Jan 10 17:14:26.139: INFO: Got endpoints: latency-svc-gjvxf [746.098428ms]
  Jan 10 17:14:26.161: INFO: Created: latency-svc-g7k9c
  Jan 10 17:14:26.193: INFO: Got endpoints: latency-svc-qnk58 [751.704672ms]
  Jan 10 17:14:26.209: INFO: Created: latency-svc-gvhqj
  Jan 10 17:14:26.238: INFO: Got endpoints: latency-svc-vmngd [744.217908ms]
  Jan 10 17:14:26.260: INFO: Created: latency-svc-hvjnf
  Jan 10 17:14:26.289: INFO: Got endpoints: latency-svc-jflzz [750.604705ms]
  Jan 10 17:14:26.320: INFO: Created: latency-svc-6vhms
  Jan 10 17:14:26.340: INFO: Got endpoints: latency-svc-4cvcz [742.454071ms]
  Jan 10 17:14:26.363: INFO: Created: latency-svc-d2ndh
  Jan 10 17:14:26.391: INFO: Got endpoints: latency-svc-gwk2r [730.555968ms]
  Jan 10 17:14:26.416: INFO: Created: latency-svc-8chqs
  Jan 10 17:14:26.447: INFO: Got endpoints: latency-svc-bzlsl [750.769183ms]
  Jan 10 17:14:26.465: INFO: Created: latency-svc-gjps6
  Jan 10 17:14:26.505: INFO: Got endpoints: latency-svc-g6jms [755.314225ms]
  Jan 10 17:14:26.528: INFO: Created: latency-svc-x2h2m
  Jan 10 17:14:26.542: INFO: Got endpoints: latency-svc-f8qng [751.04216ms]
  Jan 10 17:14:26.558: INFO: Created: latency-svc-68whb
  Jan 10 17:14:26.588: INFO: Got endpoints: latency-svc-zkwrz [750.206277ms]
  Jan 10 17:14:26.605: INFO: Created: latency-svc-7t2kf
  Jan 10 17:14:26.637: INFO: Got endpoints: latency-svc-tv7dg [746.701587ms]
  Jan 10 17:14:26.652: INFO: Created: latency-svc-h98hg
  Jan 10 17:14:26.690: INFO: Got endpoints: latency-svc-rwpgk [732.335818ms]
  Jan 10 17:14:26.706: INFO: Created: latency-svc-n65qg
  Jan 10 17:14:26.742: INFO: Got endpoints: latency-svc-qbcs7 [748.131294ms]
  Jan 10 17:14:26.758: INFO: Created: latency-svc-79fhv
  E0110 17:14:26.761490      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 17:14:26.791: INFO: Got endpoints: latency-svc-sm6k6 [751.196313ms]
  Jan 10 17:14:26.809: INFO: Created: latency-svc-29ss5
  Jan 10 17:14:26.840: INFO: Got endpoints: latency-svc-nnn6v [754.341613ms]
  Jan 10 17:14:26.862: INFO: Created: latency-svc-c8gfs
  Jan 10 17:14:26.892: INFO: Got endpoints: latency-svc-g7k9c [753.473277ms]
  Jan 10 17:14:26.911: INFO: Created: latency-svc-bv7jz
  Jan 10 17:14:26.938: INFO: Got endpoints: latency-svc-gvhqj [744.854405ms]
  Jan 10 17:14:26.956: INFO: Created: latency-svc-psxnk
  Jan 10 17:14:26.991: INFO: Got endpoints: latency-svc-hvjnf [753.295689ms]
  Jan 10 17:14:27.012: INFO: Created: latency-svc-46gpl
  Jan 10 17:14:27.037: INFO: Got endpoints: latency-svc-6vhms [747.160897ms]
  Jan 10 17:14:27.061: INFO: Created: latency-svc-nnm7m
  Jan 10 17:14:27.093: INFO: Got endpoints: latency-svc-d2ndh [753.283997ms]
  Jan 10 17:14:27.110: INFO: Created: latency-svc-kc44z
  Jan 10 17:14:27.151: INFO: Got endpoints: latency-svc-8chqs [759.448724ms]
  Jan 10 17:14:27.170: INFO: Created: latency-svc-fhql2
  Jan 10 17:14:27.187: INFO: Got endpoints: latency-svc-gjps6 [740.085861ms]
  Jan 10 17:14:27.212: INFO: Created: latency-svc-rgtm4
  Jan 10 17:14:27.241: INFO: Got endpoints: latency-svc-x2h2m [735.884145ms]
  Jan 10 17:14:27.270: INFO: Created: latency-svc-nq82f
  Jan 10 17:14:27.303: INFO: Got endpoints: latency-svc-68whb [760.617778ms]
  Jan 10 17:14:27.330: INFO: Created: latency-svc-vp65x
  Jan 10 17:14:27.340: INFO: Got endpoints: latency-svc-7t2kf [751.508018ms]
  Jan 10 17:14:27.361: INFO: Created: latency-svc-vng4g
  Jan 10 17:14:27.390: INFO: Got endpoints: latency-svc-h98hg [752.746811ms]
  Jan 10 17:14:27.407: INFO: Created: latency-svc-c6hpg
  Jan 10 17:14:27.437: INFO: Got endpoints: latency-svc-n65qg [746.895463ms]
  Jan 10 17:14:27.454: INFO: Created: latency-svc-nh5qp
  Jan 10 17:14:27.488: INFO: Got endpoints: latency-svc-79fhv [746.249578ms]
  Jan 10 17:14:27.510: INFO: Created: latency-svc-g2spn
  Jan 10 17:14:27.541: INFO: Got endpoints: latency-svc-29ss5 [749.326946ms]
  Jan 10 17:14:27.558: INFO: Created: latency-svc-jzm5x
  Jan 10 17:14:27.589: INFO: Got endpoints: latency-svc-c8gfs [747.440367ms]
  Jan 10 17:14:27.603: INFO: Created: latency-svc-nn762
  Jan 10 17:14:27.638: INFO: Got endpoints: latency-svc-bv7jz [744.740688ms]
  Jan 10 17:14:27.657: INFO: Created: latency-svc-kj6rj
  Jan 10 17:14:27.687: INFO: Got endpoints: latency-svc-psxnk [748.503326ms]
  Jan 10 17:14:27.712: INFO: Created: latency-svc-7h8j8
  Jan 10 17:14:27.738: INFO: Got endpoints: latency-svc-46gpl [746.624725ms]
  Jan 10 17:14:27.760: INFO: Created: latency-svc-m7vbf
  E0110 17:14:27.762351      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 17:14:27.789: INFO: Got endpoints: latency-svc-nnm7m [751.736581ms]
  Jan 10 17:14:27.808: INFO: Created: latency-svc-r7n88
  Jan 10 17:14:27.837: INFO: Got endpoints: latency-svc-kc44z [743.200995ms]
  Jan 10 17:14:27.855: INFO: Created: latency-svc-2mn4l
  Jan 10 17:14:27.887: INFO: Got endpoints: latency-svc-fhql2 [735.78833ms]
  Jan 10 17:14:27.912: INFO: Created: latency-svc-kc22l
  Jan 10 17:14:27.937: INFO: Got endpoints: latency-svc-rgtm4 [749.356299ms]
  Jan 10 17:14:27.968: INFO: Created: latency-svc-qp4gm
  Jan 10 17:14:27.986: INFO: Got endpoints: latency-svc-nq82f [745.38068ms]
  Jan 10 17:14:28.007: INFO: Created: latency-svc-zmfsp
  Jan 10 17:14:28.043: INFO: Got endpoints: latency-svc-vp65x [740.233829ms]
  Jan 10 17:14:28.062: INFO: Created: latency-svc-l6rz5
  Jan 10 17:14:28.086: INFO: Got endpoints: latency-svc-vng4g [745.233524ms]
  Jan 10 17:14:28.105: INFO: Created: latency-svc-k82b4
  Jan 10 17:14:28.140: INFO: Got endpoints: latency-svc-c6hpg [750.03326ms]
  Jan 10 17:14:28.157: INFO: Created: latency-svc-bgbxz
  Jan 10 17:14:28.188: INFO: Got endpoints: latency-svc-nh5qp [751.333178ms]
  Jan 10 17:14:28.212: INFO: Created: latency-svc-x2dh9
  Jan 10 17:14:28.236: INFO: Got endpoints: latency-svc-g2spn [748.129524ms]
  Jan 10 17:14:28.256: INFO: Created: latency-svc-m5vcx
  Jan 10 17:14:28.288: INFO: Got endpoints: latency-svc-jzm5x [747.346421ms]
  Jan 10 17:14:28.307: INFO: Created: latency-svc-pdv9j
  Jan 10 17:14:28.341: INFO: Got endpoints: latency-svc-nn762 [751.662578ms]
  Jan 10 17:14:28.361: INFO: Created: latency-svc-5pq7r
  Jan 10 17:14:28.388: INFO: Got endpoints: latency-svc-kj6rj [748.777628ms]
  Jan 10 17:14:28.409: INFO: Created: latency-svc-txjpk
  Jan 10 17:14:28.447: INFO: Got endpoints: latency-svc-7h8j8 [759.295789ms]
  Jan 10 17:14:28.465: INFO: Created: latency-svc-vmmk2
  Jan 10 17:14:28.499: INFO: Got endpoints: latency-svc-m7vbf [761.259389ms]
  Jan 10 17:14:28.540: INFO: Created: latency-svc-2t8nc
  Jan 10 17:14:28.541: INFO: Got endpoints: latency-svc-r7n88 [752.276271ms]
  Jan 10 17:14:28.587: INFO: Created: latency-svc-jkmbk
  Jan 10 17:14:28.597: INFO: Got endpoints: latency-svc-2mn4l [759.986557ms]
  Jan 10 17:14:28.619: INFO: Created: latency-svc-fd6mt
  Jan 10 17:14:28.642: INFO: Got endpoints: latency-svc-kc22l [754.362001ms]
  Jan 10 17:14:28.661: INFO: Created: latency-svc-hstrd
  Jan 10 17:14:28.687: INFO: Got endpoints: latency-svc-qp4gm [750.201551ms]
  Jan 10 17:14:28.731: INFO: Created: latency-svc-8dngr
  Jan 10 17:14:28.738: INFO: Got endpoints: latency-svc-zmfsp [751.45911ms]
  Jan 10 17:14:28.757: INFO: Created: latency-svc-z7j29
  E0110 17:14:28.763333      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 17:14:28.805: INFO: Got endpoints: latency-svc-l6rz5 [761.300263ms]
  Jan 10 17:14:28.837: INFO: Created: latency-svc-flg7p
  Jan 10 17:14:28.843: INFO: Got endpoints: latency-svc-k82b4 [756.842177ms]
  Jan 10 17:14:28.886: INFO: Created: latency-svc-jgzjd
  Jan 10 17:14:28.896: INFO: Got endpoints: latency-svc-bgbxz [756.073319ms]
  Jan 10 17:14:28.976: INFO: Got endpoints: latency-svc-x2dh9 [787.883137ms]
  Jan 10 17:14:28.980: INFO: Created: latency-svc-mnddz
  Jan 10 17:14:29.035: INFO: Got endpoints: latency-svc-m5vcx [798.478203ms]
  Jan 10 17:14:29.058: INFO: Created: latency-svc-hwvj4
  Jan 10 17:14:29.080: INFO: Got endpoints: latency-svc-pdv9j [791.920979ms]
  Jan 10 17:14:29.130: INFO: Got endpoints: latency-svc-5pq7r [788.237045ms]
  Jan 10 17:14:29.147: INFO: Created: latency-svc-bt7tv
  Jan 10 17:14:29.170: INFO: Created: latency-svc-kg4cm
  Jan 10 17:14:29.178: INFO: Got endpoints: latency-svc-txjpk [790.436925ms]
  Jan 10 17:14:29.212: INFO: Created: latency-svc-p2dvg
  Jan 10 17:14:29.216: INFO: Got endpoints: latency-svc-vmmk2 [768.717015ms]
  Jan 10 17:14:29.229: INFO: Created: latency-svc-m6hfx
  Jan 10 17:14:29.275: INFO: Got endpoints: latency-svc-2t8nc [775.183556ms]
  Jan 10 17:14:29.277: INFO: Created: latency-svc-gtmc6
  Jan 10 17:14:29.306: INFO: Got endpoints: latency-svc-jkmbk [764.2586ms]
  Jan 10 17:14:29.320: INFO: Created: latency-svc-87vn6
  Jan 10 17:14:29.362: INFO: Got endpoints: latency-svc-fd6mt [764.362279ms]
  Jan 10 17:14:29.383: INFO: Created: latency-svc-g7nxk
  Jan 10 17:14:29.412: INFO: Got endpoints: latency-svc-hstrd [769.967754ms]
  Jan 10 17:14:29.428: INFO: Created: latency-svc-4dplh
  Jan 10 17:14:29.440: INFO: Got endpoints: latency-svc-8dngr [752.347909ms]
  Jan 10 17:14:29.447: INFO: Created: latency-svc-2nbft
  Jan 10 17:14:29.469: INFO: Created: latency-svc-9mtwp
  Jan 10 17:14:29.489: INFO: Got endpoints: latency-svc-z7j29 [751.24763ms]
  Jan 10 17:14:29.517: INFO: Created: latency-svc-jplmv
  Jan 10 17:14:29.562: INFO: Got endpoints: latency-svc-flg7p [755.423514ms]
  Jan 10 17:14:29.596: INFO: Got endpoints: latency-svc-jgzjd [752.851083ms]
  Jan 10 17:14:29.606: INFO: Created: latency-svc-pctzw
  Jan 10 17:14:29.635: INFO: Created: latency-svc-6tp2v
  Jan 10 17:14:29.659: INFO: Got endpoints: latency-svc-mnddz [762.758597ms]
  Jan 10 17:14:29.687: INFO: Created: latency-svc-2pnbg
  Jan 10 17:14:29.690: INFO: Got endpoints: latency-svc-hwvj4 [713.551994ms]
  Jan 10 17:14:29.713: INFO: Created: latency-svc-khljb
  Jan 10 17:14:29.742: INFO: Got endpoints: latency-svc-bt7tv [706.214456ms]
  E0110 17:14:29.763991      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 17:14:29.766: INFO: Created: latency-svc-8s2vs
  Jan 10 17:14:29.790: INFO: Got endpoints: latency-svc-kg4cm [709.498064ms]
  Jan 10 17:14:29.804: INFO: Created: latency-svc-47kpk
  Jan 10 17:14:29.837: INFO: Got endpoints: latency-svc-p2dvg [705.996903ms]
  Jan 10 17:14:29.865: INFO: Created: latency-svc-d5996
  Jan 10 17:14:29.888: INFO: Got endpoints: latency-svc-m6hfx [710.372582ms]
  Jan 10 17:14:29.910: INFO: Created: latency-svc-hj5jh
  Jan 10 17:14:29.945: INFO: Got endpoints: latency-svc-gtmc6 [729.431293ms]
  Jan 10 17:14:29.972: INFO: Created: latency-svc-j6qhn
  Jan 10 17:14:29.985: INFO: Got endpoints: latency-svc-87vn6 [709.941773ms]
  Jan 10 17:14:30.013: INFO: Created: latency-svc-n59n2
  Jan 10 17:14:30.038: INFO: Got endpoints: latency-svc-g7nxk [731.856205ms]
  Jan 10 17:14:30.058: INFO: Created: latency-svc-tn5fn
  Jan 10 17:14:30.086: INFO: Got endpoints: latency-svc-4dplh [723.40818ms]
  Jan 10 17:14:30.106: INFO: Created: latency-svc-ss7v5
  Jan 10 17:14:30.136: INFO: Got endpoints: latency-svc-2nbft [723.192452ms]
  Jan 10 17:14:30.148: INFO: Created: latency-svc-9bj5k
  Jan 10 17:14:30.189: INFO: Got endpoints: latency-svc-9mtwp [749.102746ms]
  Jan 10 17:14:30.208: INFO: Created: latency-svc-5jgjp
  Jan 10 17:14:30.237: INFO: Got endpoints: latency-svc-jplmv [747.788289ms]
  Jan 10 17:14:30.255: INFO: Created: latency-svc-wcrb9
  Jan 10 17:14:30.293: INFO: Got endpoints: latency-svc-pctzw [731.850704ms]
  Jan 10 17:14:30.311: INFO: Created: latency-svc-lst8v
  Jan 10 17:14:30.338: INFO: Got endpoints: latency-svc-6tp2v [742.714037ms]
  Jan 10 17:14:30.360: INFO: Created: latency-svc-g8554
  Jan 10 17:14:30.388: INFO: Got endpoints: latency-svc-2pnbg [729.120468ms]
  Jan 10 17:14:30.408: INFO: Created: latency-svc-n8lrz
  Jan 10 17:14:30.441: INFO: Got endpoints: latency-svc-khljb [751.589727ms]
  Jan 10 17:14:30.468: INFO: Created: latency-svc-p8bsw
  Jan 10 17:14:30.491: INFO: Got endpoints: latency-svc-8s2vs [749.077592ms]
  Jan 10 17:14:30.509: INFO: Created: latency-svc-sg45h
  Jan 10 17:14:30.539: INFO: Got endpoints: latency-svc-47kpk [748.890648ms]
  Jan 10 17:14:30.592: INFO: Got endpoints: latency-svc-d5996 [754.337295ms]
  Jan 10 17:14:30.636: INFO: Got endpoints: latency-svc-hj5jh [747.918353ms]
  Jan 10 17:14:30.697: INFO: Got endpoints: latency-svc-j6qhn [752.148743ms]
  Jan 10 17:14:30.743: INFO: Got endpoints: latency-svc-n59n2 [757.988146ms]
  E0110 17:14:30.764598      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 17:14:30.787: INFO: Got endpoints: latency-svc-tn5fn [749.640849ms]
  Jan 10 17:14:30.842: INFO: Got endpoints: latency-svc-ss7v5 [755.03946ms]
  Jan 10 17:14:30.890: INFO: Got endpoints: latency-svc-9bj5k [753.709913ms]
  Jan 10 17:14:30.942: INFO: Got endpoints: latency-svc-5jgjp [752.851332ms]
  Jan 10 17:14:30.988: INFO: Got endpoints: latency-svc-wcrb9 [751.107326ms]
  Jan 10 17:14:31.038: INFO: Got endpoints: latency-svc-lst8v [744.978606ms]
  Jan 10 17:14:31.094: INFO: Got endpoints: latency-svc-g8554 [755.830792ms]
  Jan 10 17:14:31.142: INFO: Got endpoints: latency-svc-n8lrz [753.526576ms]
  Jan 10 17:14:31.191: INFO: Got endpoints: latency-svc-p8bsw [749.348799ms]
  Jan 10 17:14:31.240: INFO: Got endpoints: latency-svc-sg45h [749.033033ms]
  Jan 10 17:14:31.241: INFO: Latencies: [81.876313ms 113.51594ms 145.44325ms 145.629407ms 157.647645ms 158.400489ms 162.041109ms 166.783506ms 168.195189ms 172.708456ms 175.157911ms 186.999889ms 203.466952ms 203.600817ms 207.350439ms 225.894695ms 228.230903ms 239.215519ms 262.055062ms 263.217981ms 274.321813ms 276.877824ms 277.755586ms 292.32405ms 317.329466ms 331.604262ms 334.03251ms 338.780082ms 352.375786ms 358.355698ms 362.904586ms 364.42637ms 368.345291ms 375.383275ms 379.077034ms 402.22135ms 406.86647ms 409.487918ms 417.054811ms 424.921284ms 426.292397ms 427.841015ms 429.637144ms 430.610753ms 435.681041ms 441.440232ms 444.653878ms 446.084473ms 464.21634ms 465.69536ms 475.460874ms 485.058277ms 485.475111ms 487.620665ms 489.787589ms 489.864641ms 494.699622ms 496.609192ms 500.72982ms 505.951771ms 508.748482ms 597.466178ms 602.83185ms 611.530783ms 613.42446ms 619.024112ms 625.123155ms 632.004445ms 638.162909ms 659.698163ms 671.540385ms 675.663181ms 682.931745ms 702.132916ms 705.996903ms 706.214456ms 709.498064ms 709.941773ms 709.972315ms 710.372582ms 713.551994ms 715.976549ms 723.192452ms 723.40818ms 729.120468ms 729.431293ms 729.800128ms 730.555968ms 731.850704ms 731.856205ms 732.335818ms 735.78833ms 735.884145ms 739.378325ms 740.085861ms 740.233829ms 741.346327ms 742.454071ms 742.714037ms 743.200995ms 744.217908ms 744.740688ms 744.854405ms 744.978606ms 745.010387ms 745.233524ms 745.38068ms 746.098428ms 746.249578ms 746.624725ms 746.701587ms 746.895463ms 746.897011ms 747.160897ms 747.346421ms 747.440367ms 747.788289ms 747.918353ms 747.927958ms 748.129524ms 748.131294ms 748.503326ms 748.777628ms 748.890648ms 749.033033ms 749.077592ms 749.102746ms 749.326946ms 749.348799ms 749.356299ms 749.578333ms 749.640849ms 750.03326ms 750.201551ms 750.206277ms 750.604705ms 750.769183ms 751.04216ms 751.107326ms 751.196313ms 751.24763ms 751.333178ms 751.45911ms 751.508018ms 751.518484ms 751.589727ms 751.662578ms 751.704672ms 751.736581ms 752.148743ms 752.276271ms 752.347909ms 752.746811ms 752.851083ms 752.851332ms 753.283997ms 753.295689ms 753.473277ms 753.526576ms 753.709913ms 754.337295ms 754.341613ms 754.362001ms 755.03946ms 755.314225ms 755.423514ms 755.830792ms 756.073319ms 756.842177ms 757.988146ms 759.295789ms 759.448724ms 759.986557ms 760.617778ms 761.259389ms 761.300263ms 762.758597ms 764.2586ms 764.362279ms 768.102431ms 768.717015ms 769.967754ms 772.848958ms 775.183556ms 787.883137ms 788.237045ms 790.436925ms 791.920979ms 798.478203ms 798.843756ms 801.044861ms 801.185132ms 804.094172ms 838.379146ms 862.248768ms 875.21919ms 880.202678ms 917.838593ms 927.854314ms 975.502875ms]
  Jan 10 17:14:31.241: INFO: 50 %ile: 744.217908ms
  Jan 10 17:14:31.241: INFO: 90 %ile: 768.717015ms
  Jan 10 17:14:31.241: INFO: 99 %ile: 927.854314ms
  Jan 10 17:14:31.241: INFO: Total sample count: 200
  Jan 10 17:14:31.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svc-latency-9103" for this suite. @ 01/10/24 17:14:31.268
• [11.041 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:497
  STEP: Creating a kubernetes client @ 01/10/24 17:14:31.293
  Jan 10 17:14:31.293: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename webhook @ 01/10/24 17:14:31.295
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 17:14:31.334
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 17:14:31.341
  STEP: Setting up server cert @ 01/10/24 17:14:31.397
  E0110 17:14:31.765118      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 01/10/24 17:14:32.497
  STEP: Deploying the webhook pod @ 01/10/24 17:14:32.526
  STEP: Wait for the deployment to be ready @ 01/10/24 17:14:32.564
  Jan 10 17:14:32.614: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0110 17:14:32.765726      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:14:33.766349      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 01/10/24 17:14:34.64
  STEP: Verifying the service has paired with the endpoint @ 01/10/24 17:14:34.677
  E0110 17:14:34.767188      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 17:14:35.677: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a mutating webhook configuration @ 01/10/24 17:14:35.685
  STEP: Updating a mutating webhook configuration's rules to not include the create operation @ 01/10/24 17:14:35.732
  STEP: Creating a configMap that should not be mutated @ 01/10/24 17:14:35.747
  E0110 17:14:35.767649      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Patching a mutating webhook configuration's rules to include the create operation @ 01/10/24 17:14:35.777
  STEP: Creating a configMap that should be mutated @ 01/10/24 17:14:35.8
  Jan 10 17:14:35.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1654" for this suite. @ 01/10/24 17:14:36.076
  STEP: Destroying namespace "webhook-markers-1475" for this suite. @ 01/10/24 17:14:36.106
• [4.851 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:127
  STEP: Creating a kubernetes client @ 01/10/24 17:14:36.151
  Jan 10 17:14:36.152: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename emptydir @ 01/10/24 17:14:36.154
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 17:14:36.217
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 17:14:36.227
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 01/10/24 17:14:36.236
  E0110 17:14:36.768141      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:14:37.768774      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:14:38.768863      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:14:39.769167      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/10/24 17:14:40.314
  Jan 10 17:14:40.326: INFO: Trying to get logs from node env1-test-worker-1 pod pod-f300d60a-6dbd-47dc-bcb4-39041f12e8a7 container test-container: <nil>
  STEP: delete the pod @ 01/10/24 17:14:40.364
  Jan 10 17:14:40.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-2105" for this suite. @ 01/10/24 17:14:40.426
• [4.292 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:184
  STEP: Creating a kubernetes client @ 01/10/24 17:14:40.448
  Jan 10 17:14:40.448: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename kubelet-test @ 01/10/24 17:14:40.45
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 17:14:40.512
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 17:14:40.52
  E0110 17:14:40.770175      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:14:41.770719      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 17:14:42.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-7536" for this suite. @ 01/10/24 17:14:42.705
• [2.283 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:58
  STEP: Creating a kubernetes client @ 01/10/24 17:14:42.735
  Jan 10 17:14:42.736: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename custom-resource-definition @ 01/10/24 17:14:42.739
  E0110 17:14:42.770977      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 17:14:42.788
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 17:14:42.802
  Jan 10 17:14:42.819: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  E0110 17:14:43.771052      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:14:44.771209      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:14:45.771826      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:14:46.772038      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:14:47.772083      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:14:48.772424      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 17:14:48.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-2430" for this suite. @ 01/10/24 17:14:49.049
• [6.427 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]
test/e2e/apimachinery/watch.go:142
  STEP: Creating a kubernetes client @ 01/10/24 17:14:49.165
  Jan 10 17:14:49.165: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename watch @ 01/10/24 17:14:49.168
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 17:14:49.236
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 17:14:49.249
  STEP: creating a new configmap @ 01/10/24 17:14:49.269
  STEP: modifying the configmap once @ 01/10/24 17:14:49.277
  STEP: modifying the configmap a second time @ 01/10/24 17:14:49.306
  STEP: deleting the configmap @ 01/10/24 17:14:49.325
  STEP: creating a watch on configmaps from the resource version returned by the first update @ 01/10/24 17:14:49.341
  STEP: Expecting to observe notifications for all changes to the configmap after the first update @ 01/10/24 17:14:49.343
  Jan 10 17:14:49.344: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8722  5b125638-f2fb-42ed-b722-7cb658cd41cf 186808851 0 2024-01-10 17:14:49 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2024-01-10 17:14:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jan 10 17:14:49.345: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8722  5b125638-f2fb-42ed-b722-7cb658cd41cf 186808855 0 2024-01-10 17:14:49 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2024-01-10 17:14:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jan 10 17:14:49.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-8722" for this suite. @ 01/10/24 17:14:49.362
• [0.213 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:41
  STEP: Creating a kubernetes client @ 01/10/24 17:14:49.382
  Jan 10 17:14:49.382: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename containers @ 01/10/24 17:14:49.383
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 17:14:49.498
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 17:14:49.505
  E0110 17:14:49.772642      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:14:50.773204      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 17:14:51.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-7089" for this suite. @ 01/10/24 17:14:51.667
• [2.303 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:445
  STEP: Creating a kubernetes client @ 01/10/24 17:14:51.685
  Jan 10 17:14:51.685: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename pods @ 01/10/24 17:14:51.687
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 17:14:51.732
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 17:14:51.739
  E0110 17:14:51.773252      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:14:52.773674      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:14:53.774818      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:14:54.775008      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:14:55.775348      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:14:56.775642      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:14:57.775687      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 01/10/24 17:14:57.95
  Jan 10 17:14:57.963: INFO: Trying to get logs from node env1-test-worker-1 pod client-envvars-20277ec6-c0bd-407c-b4f7-0c50b5aa63c0 container env3cont: <nil>
  STEP: delete the pod @ 01/10/24 17:14:57.985
  Jan 10 17:14:58.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3309" for this suite. @ 01/10/24 17:14:58.121
• [6.458 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should delete a collection of events [Conformance]
test/e2e/instrumentation/events.go:207
  STEP: Creating a kubernetes client @ 01/10/24 17:14:58.144
  Jan 10 17:14:58.144: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename events @ 01/10/24 17:14:58.146
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 17:14:58.186
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 17:14:58.194
  STEP: Create set of events @ 01/10/24 17:14:58.206
  STEP: get a list of Events with a label in the current namespace @ 01/10/24 17:14:58.242
  STEP: delete a list of events @ 01/10/24 17:14:58.253
  Jan 10 17:14:58.254: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 01/10/24 17:14:58.308
  Jan 10 17:14:58.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-4418" for this suite. @ 01/10/24 17:14:58.337
• [0.220 seconds]
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:458
  STEP: Creating a kubernetes client @ 01/10/24 17:14:58.367
  Jan 10 17:14:58.367: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename init-container @ 01/10/24 17:14:58.37
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 17:14:58.434
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 17:14:58.441
  STEP: creating the pod @ 01/10/24 17:14:58.451
  Jan 10 17:14:58.451: INFO: PodSpec: initContainers in spec.initContainers
  E0110 17:14:58.776767      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:14:59.777015      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:15:00.777210      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:15:01.778179      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 17:15:02.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-7047" for this suite. @ 01/10/24 17:15:02.237
• [3.894 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:104
  STEP: Creating a kubernetes client @ 01/10/24 17:15:02.262
  Jan 10 17:15:02.262: INFO: >>> kubeConfig: /tmp/kubeconfig-2435010822
  STEP: Building a namespace api object, basename runtimeclass @ 01/10/24 17:15:02.264
  STEP: Waiting for a default service account to be provisioned in namespace @ 01/10/24 17:15:02.301
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 01/10/24 17:15:02.307
  E0110 17:15:02.779215      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0110 17:15:03.779627      23 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jan 10 17:15:04.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-9021" for this suite. @ 01/10/24 17:15:04.398
• [2.224 seconds]
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
  Jan 10 17:15:04.489: INFO: Running AfterSuite actions on node 1
  Jan 10 17:15:04.489: INFO: Skipping dumping logs from cluster
[SynchronizedAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:152
[ReportAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:593
[ReportAfterSuite] PASSED [0.154 seconds]
------------------------------

Summarizing 1 Failure:
  [FAIL] [sig-apps] Daemon set [Serial] [It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:500

Ran 378 of 7207 Specs in 6504.648 seconds
FAIL! -- 377 Passed | 1 Failed | 0 Pending | 6829 Skipped
--- FAIL: TestE2E (6505.27s)
FAIL

Ginkgo ran 1 suite in 1h48m25.477096227s

Test Suite Failed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.9.1[0m

