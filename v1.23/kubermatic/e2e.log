I0531 13:07:50.248254      19 e2e.go:132] Starting e2e run "77d1639e-6015-4707-8ed5-d2f0bab378c6" on Ginkgo node 1
{"msg":"Test Suite starting","total":346,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1654002470 - Will randomize all specs
Will run 346 of 7044 specs

May 31 13:07:53.922: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
May 31 13:07:53.923: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
May 31 13:07:53.954: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
May 31 13:07:53.995: INFO: 16 / 16 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
May 31 13:07:53.995: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
May 31 13:07:53.995: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
May 31 13:07:54.014: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'aws-node-termination-handler' (0 seconds elapsed)
May 31 13:07:54.014: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'canal' (0 seconds elapsed)
May 31 13:07:54.014: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
May 31 13:07:54.014: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'node-local-dns' (0 seconds elapsed)
May 31 13:07:54.014: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'user-ssh-keys-agent' (0 seconds elapsed)
May 31 13:07:54.014: INFO: e2e test version: v1.23.6
May 31 13:07:54.017: INFO: kube-apiserver version: v1.23.6
May 31 13:07:54.017: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
May 31 13:07:54.025: INFO: Cluster IP family: ipv4
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:07:54.026: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename emptydir
W0531 13:07:54.067210      19 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
May 31 13:07:54.067: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
May 31 13:07:54.085: INFO: No PSP annotation exists on dry run pod; assuming PodSecurityPolicy is disabled
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 31 13:07:54.124: INFO: Waiting up to 5m0s for pod "pod-2e849cee-b166-4bb4-854e-f1a4d5cfad57" in namespace "emptydir-5709" to be "Succeeded or Failed"
May 31 13:07:54.143: INFO: Pod "pod-2e849cee-b166-4bb4-854e-f1a4d5cfad57": Phase="Pending", Reason="", readiness=false. Elapsed: 18.415401ms
E0531 13:07:54.512167      19 progress.go:119] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
May 31 13:07:56.155: INFO: Pod "pod-2e849cee-b166-4bb4-854e-f1a4d5cfad57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030909262s
May 31 13:07:58.168: INFO: Pod "pod-2e849cee-b166-4bb4-854e-f1a4d5cfad57": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043707293s
May 31 13:08:00.177: INFO: Pod "pod-2e849cee-b166-4bb4-854e-f1a4d5cfad57": Phase="Pending", Reason="", readiness=false. Elapsed: 6.052901443s
May 31 13:08:02.191: INFO: Pod "pod-2e849cee-b166-4bb4-854e-f1a4d5cfad57": Phase="Pending", Reason="", readiness=false. Elapsed: 8.06609212s
May 31 13:08:04.203: INFO: Pod "pod-2e849cee-b166-4bb4-854e-f1a4d5cfad57": Phase="Pending", Reason="", readiness=false. Elapsed: 10.078267806s
May 31 13:08:06.215: INFO: Pod "pod-2e849cee-b166-4bb4-854e-f1a4d5cfad57": Phase="Pending", Reason="", readiness=false. Elapsed: 12.090140225s
May 31 13:08:08.227: INFO: Pod "pod-2e849cee-b166-4bb4-854e-f1a4d5cfad57": Phase="Pending", Reason="", readiness=false. Elapsed: 14.102645176s
May 31 13:08:10.238: INFO: Pod "pod-2e849cee-b166-4bb4-854e-f1a4d5cfad57": Phase="Pending", Reason="", readiness=false. Elapsed: 16.113376112s
May 31 13:08:12.252: INFO: Pod "pod-2e849cee-b166-4bb4-854e-f1a4d5cfad57": Phase="Pending", Reason="", readiness=false. Elapsed: 18.127761289s
May 31 13:08:14.261: INFO: Pod "pod-2e849cee-b166-4bb4-854e-f1a4d5cfad57": Phase="Pending", Reason="", readiness=false. Elapsed: 20.136689469s
May 31 13:08:16.274: INFO: Pod "pod-2e849cee-b166-4bb4-854e-f1a4d5cfad57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.149398243s
STEP: Saw pod success
May 31 13:08:16.274: INFO: Pod "pod-2e849cee-b166-4bb4-854e-f1a4d5cfad57" satisfied condition "Succeeded or Failed"
May 31 13:08:16.282: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod pod-2e849cee-b166-4bb4-854e-f1a4d5cfad57 container test-container: <nil>
STEP: delete the pod
May 31 13:08:16.367: INFO: Waiting for pod pod-2e849cee-b166-4bb4-854e-f1a4d5cfad57 to disappear
May 31 13:08:16.375: INFO: Pod pod-2e849cee-b166-4bb4-854e-f1a4d5cfad57 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:08:16.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5709" for this suite.

• [SLOW TEST:22.381 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":1,"skipped":10,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:08:16.425: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
May 31 13:08:16.507: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 31 13:08:16.508: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 31 13:08:16.521: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 31 13:08:16.521: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 31 13:08:16.545: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 31 13:08:16.545: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 31 13:08:16.603: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 31 13:08:16.603: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 0 and labels map[test-deployment-static:true]
May 31 13:08:17.991: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 1 and labels map[test-deployment-static:true]
May 31 13:08:17.991: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 1 and labels map[test-deployment-static:true]
May 31 13:08:27.870: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
May 31 13:08:27.896: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
May 31 13:08:27.903: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 0
May 31 13:08:27.903: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 0
May 31 13:08:27.903: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 0
May 31 13:08:27.903: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 0
May 31 13:08:27.904: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 0
May 31 13:08:27.906: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 0
May 31 13:08:27.906: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 0
May 31 13:08:27.906: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 0
May 31 13:08:27.906: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 1
May 31 13:08:27.906: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 1
May 31 13:08:27.906: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 2
May 31 13:08:27.906: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 2
May 31 13:08:27.907: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 2
May 31 13:08:27.907: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 2
May 31 13:08:27.918: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 2
May 31 13:08:27.918: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 2
May 31 13:08:27.959: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 2
May 31 13:08:27.959: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 2
May 31 13:08:27.976: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 1
May 31 13:08:27.976: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 1
May 31 13:08:28.017: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 1
May 31 13:08:28.017: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 1
May 31 13:08:30.092: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 2
May 31 13:08:30.092: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 2
May 31 13:08:30.135: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 1
STEP: listing Deployments
May 31 13:08:30.145: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
May 31 13:08:30.180: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
May 31 13:08:30.320: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 31 13:08:30.321: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 31 13:08:30.321: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 31 13:08:30.321: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 31 13:08:30.321: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
May 31 13:08:32.884: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
May 31 13:08:45.114: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
May 31 13:08:45.173: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
May 31 13:08:45.198: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
May 31 13:08:53.977: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
May 31 13:08:54.071: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 1
May 31 13:08:54.071: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 1
May 31 13:08:54.071: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 1
May 31 13:08:54.071: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 1
May 31 13:08:54.071: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 1
May 31 13:08:54.072: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 2
May 31 13:08:54.072: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 3
May 31 13:08:54.072: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 2
May 31 13:08:54.072: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 2
May 31 13:08:54.072: INFO: observed Deployment test-deployment in namespace deployment-3077 with ReadyReplicas 3
STEP: deleting the Deployment
May 31 13:08:54.096: INFO: observed event type MODIFIED
May 31 13:08:54.096: INFO: observed event type MODIFIED
May 31 13:08:54.097: INFO: observed event type MODIFIED
May 31 13:08:54.097: INFO: observed event type MODIFIED
May 31 13:08:54.097: INFO: observed event type MODIFIED
May 31 13:08:54.097: INFO: observed event type MODIFIED
May 31 13:08:54.097: INFO: observed event type MODIFIED
May 31 13:08:54.097: INFO: observed event type MODIFIED
May 31 13:08:54.097: INFO: observed event type MODIFIED
May 31 13:08:54.098: INFO: observed event type MODIFIED
May 31 13:08:54.098: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
May 31 13:08:54.109: INFO: Log out all the ReplicaSets if there is no deployment created
May 31 13:08:54.123: INFO: ReplicaSet "test-deployment-5ddd8b47d8":
&ReplicaSet{ObjectMeta:{test-deployment-5ddd8b47d8  deployment-3077  a31fb39e-a3e8-4706-b338-2ec2bfccd57d 4354 4 2022-05-31 13:08:27 +0000 UTC <nil> <nil> map[pod-template-hash:5ddd8b47d8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment fb496391-1372-41a6-bf87-2486fcb83eb9 0xc0031a2c17 0xc0031a2c18}] []  [{kube-controller-manager Update apps/v1 2022-05-31 13:08:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb496391-1372-41a6-bf87-2486fcb83eb9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-31 13:08:53 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 5ddd8b47d8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:5ddd8b47d8 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.6 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0031a2ca0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

May 31 13:08:54.131: INFO: pod: "test-deployment-5ddd8b47d8-889jg":
&Pod{ObjectMeta:{test-deployment-5ddd8b47d8-889jg test-deployment-5ddd8b47d8- deployment-3077  c818ad4b-7e36-4368-ba22-2f4b0fe94117 4350 0 2022-05-31 13:08:30 +0000 UTC 2022-05-31 13:08:54 +0000 UTC 0xc0031a3120 map[pod-template-hash:5ddd8b47d8 test-deployment-static:true] map[cni.projectcalico.org/containerID:2fda97f6e64703693be0102be7e59f9f14d85e3e9f41c61f33f57c986d23168f cni.projectcalico.org/podIP:172.25.2.5/32 cni.projectcalico.org/podIPs:172.25.2.5/32] [{apps/v1 ReplicaSet test-deployment-5ddd8b47d8 a31fb39e-a3e8-4706-b338-2ec2bfccd57d 0xc0031a3177 0xc0031a3178}] []  [{calico Update v1 2022-05-31 13:08:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-05-31 13:08:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a31fb39e-a3e8-4706-b338-2ec2bfccd57d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-31 13:08:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.5\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xkchz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/pause:3.6,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xkchz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-160.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 13:08:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 13:08:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 13:08:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 13:08:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.160,PodIP:172.25.2.5,StartTime:2022-05-31 13:08:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-31 13:08:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/pause:3.6,ImageID:k8s.gcr.io/pause@sha256:3d380ca8864549e74af4b29c10f9cb0956236dfb01c40ca076fb6c37253234db,ContainerID:containerd://a680fe3ac8646159dc0fd534e314416b50c050682673d2936bc52e517254b25d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.5,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

May 31 13:08:54.132: INFO: ReplicaSet "test-deployment-6cdc5bc678":
&ReplicaSet{ObjectMeta:{test-deployment-6cdc5bc678  deployment-3077  9c0057e8-be47-423a-ac8e-40efdc35c0d3 4183 3 2022-05-31 13:08:16 +0000 UTC <nil> <nil> map[pod-template-hash:6cdc5bc678 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment fb496391-1372-41a6-bf87-2486fcb83eb9 0xc0031a2d07 0xc0031a2d08}] []  [{kube-controller-manager Update apps/v1 2022-05-31 13:08:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb496391-1372-41a6-bf87-2486fcb83eb9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-31 13:08:30 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6cdc5bc678,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6cdc5bc678 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0031a2d90 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

May 31 13:08:54.140: INFO: ReplicaSet "test-deployment-854fdc678":
&ReplicaSet{ObjectMeta:{test-deployment-854fdc678  deployment-3077  a1d7013f-a586-4e18-8332-c04ab95431be 4346 2 2022-05-31 13:08:30 +0000 UTC <nil> <nil> map[pod-template-hash:854fdc678 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment fb496391-1372-41a6-bf87-2486fcb83eb9 0xc0031a2df7 0xc0031a2df8}] []  [{kube-controller-manager Update apps/v1 2022-05-31 13:08:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb496391-1372-41a6-bf87-2486fcb83eb9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-31 13:08:45 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 854fdc678,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:854fdc678 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0031a2e80 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

May 31 13:08:54.150: INFO: pod: "test-deployment-854fdc678-klmc6":
&Pod{ObjectMeta:{test-deployment-854fdc678-klmc6 test-deployment-854fdc678- deployment-3077  b2a859be-4bcb-445c-88cb-359ae3ebbaa2 4282 0 2022-05-31 13:08:30 +0000 UTC <nil> <nil> map[pod-template-hash:854fdc678 test-deployment-static:true] map[cni.projectcalico.org/containerID:572c9c3e598392e69afd66fc4ae0882ad3f7053857a8d06220d3e928464f2c44 cni.projectcalico.org/podIP:172.25.1.7/32 cni.projectcalico.org/podIPs:172.25.1.7/32] [{apps/v1 ReplicaSet test-deployment-854fdc678 a1d7013f-a586-4e18-8332-c04ab95431be 0xc004929e57 0xc004929e58}] []  [{calico Update v1 2022-05-31 13:08:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-05-31 13:08:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a1d7013f-a586-4e18-8332-c04ab95431be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-31 13:08:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.7\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x472b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x472b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-8-180.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 13:08:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 13:08:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 13:08:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 13:08:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.8.180,PodIP:172.25.1.7,StartTime:2022-05-31 13:08:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-31 13:08:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://3ef294fe917c8cac0c8d663292bbf9d50e6e977e275f89a0c37656ea1b143d5b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.7,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

May 31 13:08:54.150: INFO: pod: "test-deployment-854fdc678-m92hl":
&Pod{ObjectMeta:{test-deployment-854fdc678-m92hl test-deployment-854fdc678- deployment-3077  bc1405b3-66b2-4608-afa5-41cde0a6321c 4344 0 2022-05-31 13:08:45 +0000 UTC <nil> <nil> map[pod-template-hash:854fdc678 test-deployment-static:true] map[cni.projectcalico.org/containerID:7e1c5e30cda572a84a894544450fae477af0e5e7a265c20ca58727e186331526 cni.projectcalico.org/podIP:172.25.2.6/32 cni.projectcalico.org/podIPs:172.25.2.6/32] [{apps/v1 ReplicaSet test-deployment-854fdc678 a1d7013f-a586-4e18-8332-c04ab95431be 0xc002216077 0xc002216078}] []  [{calico Update v1 2022-05-31 13:08:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-05-31 13:08:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a1d7013f-a586-4e18-8332-c04ab95431be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-31 13:08:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.6\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qg4pn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qg4pn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-160.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 13:08:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 13:08:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 13:08:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 13:08:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.160,PodIP:172.25.2.6,StartTime:2022-05-31 13:08:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-31 13:08:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://31f06e60277cdd33d0688add6837f6dd4335ef5a51996cc03c5dfdffd3a73e44,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.6,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:08:54.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3077" for this suite.

• [SLOW TEST:37.760 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":346,"completed":2,"skipped":47,"failed":0}
SSSSS
------------------------------
[sig-node] Security Context 
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:08:54.197: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
May 31 13:08:54.326: INFO: Waiting up to 5m0s for pod "security-context-f127de16-7ae6-4ca1-86e9-5b77f478631c" in namespace "security-context-1305" to be "Succeeded or Failed"
May 31 13:08:54.340: INFO: Pod "security-context-f127de16-7ae6-4ca1-86e9-5b77f478631c": Phase="Pending", Reason="", readiness=false. Elapsed: 13.677258ms
May 31 13:08:56.356: INFO: Pod "security-context-f127de16-7ae6-4ca1-86e9-5b77f478631c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029777358s
May 31 13:08:58.364: INFO: Pod "security-context-f127de16-7ae6-4ca1-86e9-5b77f478631c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037936615s
May 31 13:09:00.382: INFO: Pod "security-context-f127de16-7ae6-4ca1-86e9-5b77f478631c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.055559737s
STEP: Saw pod success
May 31 13:09:00.382: INFO: Pod "security-context-f127de16-7ae6-4ca1-86e9-5b77f478631c" satisfied condition "Succeeded or Failed"
May 31 13:09:00.388: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod security-context-f127de16-7ae6-4ca1-86e9-5b77f478631c container test-container: <nil>
STEP: delete the pod
May 31 13:09:00.476: INFO: Waiting for pod security-context-f127de16-7ae6-4ca1-86e9-5b77f478631c to disappear
May 31 13:09:00.483: INFO: Pod security-context-f127de16-7ae6-4ca1-86e9-5b77f478631c no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:09:00.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-1305" for this suite.

• [SLOW TEST:6.307 seconds]
[sig-node] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":3,"skipped":52,"failed":0}
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:09:00.506: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 13:09:00.564: INFO: Creating ReplicaSet my-hostname-basic-0067d51f-b836-41b5-8cfd-62631a059078
May 31 13:09:00.582: INFO: Pod name my-hostname-basic-0067d51f-b836-41b5-8cfd-62631a059078: Found 0 pods out of 1
May 31 13:09:05.596: INFO: Pod name my-hostname-basic-0067d51f-b836-41b5-8cfd-62631a059078: Found 1 pods out of 1
May 31 13:09:05.596: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-0067d51f-b836-41b5-8cfd-62631a059078" is running
May 31 13:09:05.603: INFO: Pod "my-hostname-basic-0067d51f-b836-41b5-8cfd-62631a059078-ds5vh" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-05-31 13:09:00 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-05-31 13:09:02 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-05-31 13:09:02 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-05-31 13:09:00 +0000 UTC Reason: Message:}])
May 31 13:09:05.603: INFO: Trying to dial the pod
May 31 13:09:10.684: INFO: Controller my-hostname-basic-0067d51f-b836-41b5-8cfd-62631a059078: Got expected result from replica 1 [my-hostname-basic-0067d51f-b836-41b5-8cfd-62631a059078-ds5vh]: "my-hostname-basic-0067d51f-b836-41b5-8cfd-62631a059078-ds5vh", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:09:10.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4558" for this suite.

• [SLOW TEST:10.201 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":4,"skipped":52,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:09:10.713: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:09:10.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6289" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":346,"completed":5,"skipped":89,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:09:10.853: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 31 13:09:12.131: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 31 13:09:14.160: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 31, 13, 9, 12, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 31, 13, 9, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 31, 13, 9, 12, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 31, 13, 9, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 31 13:09:17.207: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 13:09:17.216: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:09:20.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4822" for this suite.
STEP: Destroying namespace "webhook-4822-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.990 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":346,"completed":6,"skipped":93,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:09:20.848: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 31 13:09:21.819: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 31 13:09:24.875: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:09:25.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6799" for this suite.
STEP: Destroying namespace "webhook-6799-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":346,"completed":7,"skipped":97,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:09:25.779: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:09:25.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8816" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":346,"completed":8,"skipped":111,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:09:25.926: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Starting the proxy
May 31 13:09:25.976: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-9804 proxy --unix-socket=/tmp/kubectl-proxy-unix798675285/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:09:26.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9804" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":346,"completed":9,"skipped":168,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:09:26.098: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with secret that has name projected-secret-test-b2cf696a-4c1e-4720-a5f2-55b1bf979de1
STEP: Creating a pod to test consume secrets
May 31 13:09:26.233: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-98dfd648-a654-4c11-a83b-130fa26af8d4" in namespace "projected-6706" to be "Succeeded or Failed"
May 31 13:09:26.241: INFO: Pod "pod-projected-secrets-98dfd648-a654-4c11-a83b-130fa26af8d4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.92008ms
May 31 13:09:28.257: INFO: Pod "pod-projected-secrets-98dfd648-a654-4c11-a83b-130fa26af8d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024608155s
May 31 13:09:30.267: INFO: Pod "pod-projected-secrets-98dfd648-a654-4c11-a83b-130fa26af8d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034441444s
STEP: Saw pod success
May 31 13:09:30.267: INFO: Pod "pod-projected-secrets-98dfd648-a654-4c11-a83b-130fa26af8d4" satisfied condition "Succeeded or Failed"
May 31 13:09:30.279: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod pod-projected-secrets-98dfd648-a654-4c11-a83b-130fa26af8d4 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 31 13:09:30.358: INFO: Waiting for pod pod-projected-secrets-98dfd648-a654-4c11-a83b-130fa26af8d4 to disappear
May 31 13:09:30.365: INFO: Pod pod-projected-secrets-98dfd648-a654-4c11-a83b-130fa26af8d4 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:09:30.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6706" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":10,"skipped":245,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:09:30.390: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0644 on node default medium
May 31 13:09:30.486: INFO: Waiting up to 5m0s for pod "pod-d74ff085-8d68-4ed9-95a2-446ccd3ba4e3" in namespace "emptydir-6286" to be "Succeeded or Failed"
May 31 13:09:30.492: INFO: Pod "pod-d74ff085-8d68-4ed9-95a2-446ccd3ba4e3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.70476ms
May 31 13:09:32.503: INFO: Pod "pod-d74ff085-8d68-4ed9-95a2-446ccd3ba4e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017766384s
May 31 13:09:34.515: INFO: Pod "pod-d74ff085-8d68-4ed9-95a2-446ccd3ba4e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02937496s
May 31 13:09:36.528: INFO: Pod "pod-d74ff085-8d68-4ed9-95a2-446ccd3ba4e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.042276714s
STEP: Saw pod success
May 31 13:09:36.528: INFO: Pod "pod-d74ff085-8d68-4ed9-95a2-446ccd3ba4e3" satisfied condition "Succeeded or Failed"
May 31 13:09:36.538: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod pod-d74ff085-8d68-4ed9-95a2-446ccd3ba4e3 container test-container: <nil>
STEP: delete the pod
May 31 13:09:36.647: INFO: Waiting for pod pod-d74ff085-8d68-4ed9-95a2-446ccd3ba4e3 to disappear
May 31 13:09:36.659: INFO: Pod pod-d74ff085-8d68-4ed9-95a2-446ccd3ba4e3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:09:36.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6286" for this suite.

• [SLOW TEST:6.293 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":11,"skipped":246,"failed":0}
SSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:09:36.684: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2344.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-2344.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2344.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-2344.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 31 13:09:50.990: INFO: DNS probes using dns-2344/dns-test-2f848c47-67fc-4a52-9cc9-02a4aff7a4d6 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:09:51.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2344" for this suite.

• [SLOW TEST:14.347 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":346,"completed":12,"skipped":249,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:09:51.035: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward api env vars
May 31 13:09:51.102: INFO: Waiting up to 5m0s for pod "downward-api-beffb3b1-83d9-49e1-b5d2-06bca3160076" in namespace "downward-api-7836" to be "Succeeded or Failed"
May 31 13:09:51.116: INFO: Pod "downward-api-beffb3b1-83d9-49e1-b5d2-06bca3160076": Phase="Pending", Reason="", readiness=false. Elapsed: 14.422704ms
May 31 13:09:53.130: INFO: Pod "downward-api-beffb3b1-83d9-49e1-b5d2-06bca3160076": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027951575s
May 31 13:09:55.143: INFO: Pod "downward-api-beffb3b1-83d9-49e1-b5d2-06bca3160076": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041100807s
STEP: Saw pod success
May 31 13:09:55.143: INFO: Pod "downward-api-beffb3b1-83d9-49e1-b5d2-06bca3160076" satisfied condition "Succeeded or Failed"
May 31 13:09:55.151: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod downward-api-beffb3b1-83d9-49e1-b5d2-06bca3160076 container dapi-container: <nil>
STEP: delete the pod
May 31 13:09:55.214: INFO: Waiting for pod downward-api-beffb3b1-83d9-49e1-b5d2-06bca3160076 to disappear
May 31 13:09:55.223: INFO: Pod downward-api-beffb3b1-83d9-49e1-b5d2-06bca3160076 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:09:55.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7836" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":346,"completed":13,"skipped":300,"failed":0}
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:09:55.258: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-upd-d965a48d-06fd-4b9c-b29d-c1d5b8d5d4c8
STEP: Creating the pod
May 31 13:09:55.367: INFO: The status of Pod pod-configmaps-7d4b822c-dd39-424e-b45e-24cf137885fb is Pending, waiting for it to be Running (with Ready = true)
May 31 13:09:57.383: INFO: The status of Pod pod-configmaps-7d4b822c-dd39-424e-b45e-24cf137885fb is Running (Ready = true)
STEP: Updating configmap configmap-test-upd-d965a48d-06fd-4b9c-b29d-c1d5b8d5d4c8
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:11:08.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1841" for this suite.

• [SLOW TEST:73.282 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":14,"skipped":306,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:11:08.543: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 31 13:11:08.643: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b8509b73-abe1-40a0-9970-f44687034720" in namespace "downward-api-3795" to be "Succeeded or Failed"
May 31 13:11:08.656: INFO: Pod "downwardapi-volume-b8509b73-abe1-40a0-9970-f44687034720": Phase="Pending", Reason="", readiness=false. Elapsed: 12.675614ms
May 31 13:11:10.668: INFO: Pod "downwardapi-volume-b8509b73-abe1-40a0-9970-f44687034720": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023724397s
May 31 13:11:12.681: INFO: Pod "downwardapi-volume-b8509b73-abe1-40a0-9970-f44687034720": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037217945s
STEP: Saw pod success
May 31 13:11:12.681: INFO: Pod "downwardapi-volume-b8509b73-abe1-40a0-9970-f44687034720" satisfied condition "Succeeded or Failed"
May 31 13:11:12.689: INFO: Trying to get logs from node ip-172-31-3-160.eu-central-1.compute.internal pod downwardapi-volume-b8509b73-abe1-40a0-9970-f44687034720 container client-container: <nil>
STEP: delete the pod
May 31 13:11:12.776: INFO: Waiting for pod downwardapi-volume-b8509b73-abe1-40a0-9970-f44687034720 to disappear
May 31 13:11:12.784: INFO: Pod downwardapi-volume-b8509b73-abe1-40a0-9970-f44687034720 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:11:12.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3795" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":15,"skipped":317,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:11:12.811: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap configmap-4789/configmap-test-068dacc7-7c5f-405c-8b7d-94333341def6
STEP: Creating a pod to test consume configMaps
May 31 13:11:12.899: INFO: Waiting up to 5m0s for pod "pod-configmaps-c5a304f0-6a8f-4ea1-88ee-a9677d3414aa" in namespace "configmap-4789" to be "Succeeded or Failed"
May 31 13:11:12.916: INFO: Pod "pod-configmaps-c5a304f0-6a8f-4ea1-88ee-a9677d3414aa": Phase="Pending", Reason="", readiness=false. Elapsed: 16.871143ms
May 31 13:11:14.932: INFO: Pod "pod-configmaps-c5a304f0-6a8f-4ea1-88ee-a9677d3414aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032172464s
May 31 13:11:16.942: INFO: Pod "pod-configmaps-c5a304f0-6a8f-4ea1-88ee-a9677d3414aa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043026032s
May 31 13:11:18.976: INFO: Pod "pod-configmaps-c5a304f0-6a8f-4ea1-88ee-a9677d3414aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.07628093s
STEP: Saw pod success
May 31 13:11:18.976: INFO: Pod "pod-configmaps-c5a304f0-6a8f-4ea1-88ee-a9677d3414aa" satisfied condition "Succeeded or Failed"
May 31 13:11:18.983: INFO: Trying to get logs from node ip-172-31-3-160.eu-central-1.compute.internal pod pod-configmaps-c5a304f0-6a8f-4ea1-88ee-a9677d3414aa container env-test: <nil>
STEP: delete the pod
May 31 13:11:19.055: INFO: Waiting for pod pod-configmaps-c5a304f0-6a8f-4ea1-88ee-a9677d3414aa to disappear
May 31 13:11:19.073: INFO: Pod pod-configmaps-c5a304f0-6a8f-4ea1-88ee-a9677d3414aa no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:11:19.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4789" for this suite.

• [SLOW TEST:6.304 seconds]
[sig-node] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":346,"completed":16,"skipped":348,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:11:19.118: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:11:47.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9675" for this suite.

• [SLOW TEST:28.212 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":346,"completed":17,"skipped":355,"failed":0}
SSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:11:47.331: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 13:11:47.419: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-1f19e882-035d-403b-915b-5461cde912d1" in namespace "security-context-test-9210" to be "Succeeded or Failed"
May 31 13:11:47.434: INFO: Pod "busybox-readonly-false-1f19e882-035d-403b-915b-5461cde912d1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.472754ms
May 31 13:11:49.448: INFO: Pod "busybox-readonly-false-1f19e882-035d-403b-915b-5461cde912d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028202269s
May 31 13:11:51.459: INFO: Pod "busybox-readonly-false-1f19e882-035d-403b-915b-5461cde912d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039483281s
May 31 13:11:51.459: INFO: Pod "busybox-readonly-false-1f19e882-035d-403b-915b-5461cde912d1" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:11:51.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9210" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":346,"completed":18,"skipped":361,"failed":0}

------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:11:51.481: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 13:11:51.640: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"ed10eb26-915a-4083-a6ac-be82b6f2abdb", Controller:(*bool)(0xc0039c3dc6), BlockOwnerDeletion:(*bool)(0xc0039c3dc7)}}
May 31 13:11:51.659: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"846c9d2d-363b-4ee4-8396-5e6bc32bb015", Controller:(*bool)(0xc0045a3b26), BlockOwnerDeletion:(*bool)(0xc0045a3b27)}}
May 31 13:11:51.677: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"839e493e-5dd1-454b-8239-51549aa69b0c", Controller:(*bool)(0xc003468b7e), BlockOwnerDeletion:(*bool)(0xc003468b7f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:11:56.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6634" for this suite.

• [SLOW TEST:5.252 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":346,"completed":19,"skipped":361,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:11:56.743: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 13:11:56.800: INFO: Pod name rollover-pod: Found 0 pods out of 1
May 31 13:12:01.820: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 31 13:12:01.820: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
May 31 13:12:03.833: INFO: Creating deployment "test-rollover-deployment"
May 31 13:12:03.852: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
May 31 13:12:05.870: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
May 31 13:12:05.883: INFO: Ensure that both replica sets have 1 created replica
May 31 13:12:05.896: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
May 31 13:12:05.918: INFO: Updating deployment test-rollover-deployment
May 31 13:12:05.919: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
May 31 13:12:07.939: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
May 31 13:12:07.957: INFO: Make sure deployment "test-rollover-deployment" is complete
May 31 13:12:07.992: INFO: all replica sets need to contain the pod-template-hash label
May 31 13:12:07.992: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.May, 31, 13, 12, 3, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 31, 13, 12, 3, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 31, 13, 12, 7, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 31, 13, 12, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 31 13:12:10.016: INFO: all replica sets need to contain the pod-template-hash label
May 31 13:12:10.016: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.May, 31, 13, 12, 3, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 31, 13, 12, 3, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 31, 13, 12, 7, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 31, 13, 12, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 31 13:12:12.015: INFO: all replica sets need to contain the pod-template-hash label
May 31 13:12:12.015: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.May, 31, 13, 12, 3, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 31, 13, 12, 3, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 31, 13, 12, 7, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 31, 13, 12, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 31 13:12:14.013: INFO: all replica sets need to contain the pod-template-hash label
May 31 13:12:14.014: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.May, 31, 13, 12, 3, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 31, 13, 12, 3, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 31, 13, 12, 7, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 31, 13, 12, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 31 13:12:16.014: INFO: all replica sets need to contain the pod-template-hash label
May 31 13:12:16.014: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.May, 31, 13, 12, 3, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 31, 13, 12, 3, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 31, 13, 12, 7, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 31, 13, 12, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668b7f667d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 31 13:12:18.011: INFO: 
May 31 13:12:18.011: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
May 31 13:12:18.039: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-2567  1dfa6596-b6c5-4ae7-a53c-f14777bacf46 5951 2 2022-05-31 13:12:03 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-05-31 13:12:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-31 13:12:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00406b0a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-05-31 13:12:03 +0000 UTC,LastTransitionTime:2022-05-31 13:12:03 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-668b7f667d" has successfully progressed.,LastUpdateTime:2022-05-31 13:12:17 +0000 UTC,LastTransitionTime:2022-05-31 13:12:03 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 31 13:12:18.048: INFO: New ReplicaSet "test-rollover-deployment-668b7f667d" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-668b7f667d  deployment-2567  3fbd4c98-d2aa-4399-8fb8-0ed6790fee92 5941 2 2022-05-31 13:12:05 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668b7f667d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 1dfa6596-b6c5-4ae7-a53c-f14777bacf46 0xc00406b557 0xc00406b558}] []  [{kube-controller-manager Update apps/v1 2022-05-31 13:12:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1dfa6596-b6c5-4ae7-a53c-f14777bacf46\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-31 13:12:17 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 668b7f667d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668b7f667d] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00406b608 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 31 13:12:18.049: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
May 31 13:12:18.049: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-2567  c84ac3cb-d2dd-4c0c-9f60-26eeea9535c9 5950 2 2022-05-31 13:11:56 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 1dfa6596-b6c5-4ae7-a53c-f14777bacf46 0xc00406b42f 0xc00406b440}] []  [{e2e.test Update apps/v1 2022-05-31 13:11:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-31 13:12:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1dfa6596-b6c5-4ae7-a53c-f14777bacf46\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-05-31 13:12:17 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00406b4f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 31 13:12:18.050: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-784bc44b77  deployment-2567  1d5d95dd-40fd-4942-85c4-61d9b751f491 5890 2 2022-05-31 13:12:03 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:784bc44b77] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 1dfa6596-b6c5-4ae7-a53c-f14777bacf46 0xc00406b667 0xc00406b668}] []  [{kube-controller-manager Update apps/v1 2022-05-31 13:12:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1dfa6596-b6c5-4ae7-a53c-f14777bacf46\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-31 13:12:06 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 784bc44b77,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:784bc44b77] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00406b718 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 31 13:12:18.058: INFO: Pod "test-rollover-deployment-668b7f667d-gbpvp" is available:
&Pod{ObjectMeta:{test-rollover-deployment-668b7f667d-gbpvp test-rollover-deployment-668b7f667d- deployment-2567  06c8f1f1-279f-45ea-82d3-0b6ad2f22f1d 5905 0 2022-05-31 13:12:05 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668b7f667d] map[cni.projectcalico.org/containerID:6b31f8b4b93122b048ae39cd580cd3c8dca4d166b169d686fe716304ddb72953 cni.projectcalico.org/podIP:172.25.1.19/32 cni.projectcalico.org/podIPs:172.25.1.19/32] [{apps/v1 ReplicaSet test-rollover-deployment-668b7f667d 3fbd4c98-d2aa-4399-8fb8-0ed6790fee92 0xc0042fe7f7 0xc0042fe7f8}] []  [{kube-controller-manager Update v1 2022-05-31 13:12:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3fbd4c98-d2aa-4399-8fb8-0ed6790fee92\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-05-31 13:12:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-05-31 13:12:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.19\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lbzng,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lbzng,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-8-180.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 13:12:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 13:12:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 13:12:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 13:12:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.8.180,PodIP:172.25.1.19,StartTime:2022-05-31 13:12:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-31 13:12:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:5b3a9f1c71c09c00649d8374224642ff7029ce91a721ec9132e6ed45fa73fd43,ContainerID:containerd://6e8f61199d78b38a384bb7c14d9077c89d96822e9a3018f38059964a9fd85e43,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.19,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:12:18.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2567" for this suite.

• [SLOW TEST:21.347 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":346,"completed":20,"skipped":421,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:12:18.102: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 31 13:12:18.170: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b32dac13-72d9-42ed-a3b7-53933dc87f52" in namespace "projected-1127" to be "Succeeded or Failed"
May 31 13:12:18.183: INFO: Pod "downwardapi-volume-b32dac13-72d9-42ed-a3b7-53933dc87f52": Phase="Pending", Reason="", readiness=false. Elapsed: 7.776432ms
May 31 13:12:20.196: INFO: Pod "downwardapi-volume-b32dac13-72d9-42ed-a3b7-53933dc87f52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021047424s
May 31 13:12:22.220: INFO: Pod "downwardapi-volume-b32dac13-72d9-42ed-a3b7-53933dc87f52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044989273s
STEP: Saw pod success
May 31 13:12:22.220: INFO: Pod "downwardapi-volume-b32dac13-72d9-42ed-a3b7-53933dc87f52" satisfied condition "Succeeded or Failed"
May 31 13:12:22.228: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod downwardapi-volume-b32dac13-72d9-42ed-a3b7-53933dc87f52 container client-container: <nil>
STEP: delete the pod
May 31 13:12:22.317: INFO: Waiting for pod downwardapi-volume-b32dac13-72d9-42ed-a3b7-53933dc87f52 to disappear
May 31 13:12:22.324: INFO: Pod downwardapi-volume-b32dac13-72d9-42ed-a3b7-53933dc87f52 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:12:22.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1127" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":21,"skipped":460,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:12:22.354: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service in namespace services-9769
STEP: creating service affinity-clusterip in namespace services-9769
STEP: creating replication controller affinity-clusterip in namespace services-9769
I0531 13:12:22.484208      19 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-9769, replica count: 3
I0531 13:12:25.535556      19 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0531 13:12:28.536703      19 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0531 13:12:31.537200      19 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0531 13:12:34.537552      19 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 31 13:12:34.560: INFO: Creating new exec pod
May 31 13:12:37.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-9769 exec execpod-affinitycw6fc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
May 31 13:12:38.347: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
May 31 13:12:38.347: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 31 13:12:38.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-9769 exec execpod-affinitycw6fc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.29.219 80'
May 31 13:12:38.835: INFO: stderr: "+ + nc -v -t -w 2 10.240.29.219 80\necho hostName\nConnection to 10.240.29.219 80 port [tcp/http] succeeded!\n"
May 31 13:12:38.835: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 31 13:12:38.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-9769 exec execpod-affinitycw6fc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.29.219:80/ ; done'
May 31 13:12:39.356: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.219:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.29.219:80/\n"
May 31 13:12:39.356: INFO: stdout: "\naffinity-clusterip-vdpfg\naffinity-clusterip-vdpfg\naffinity-clusterip-vdpfg\naffinity-clusterip-vdpfg\naffinity-clusterip-vdpfg\naffinity-clusterip-vdpfg\naffinity-clusterip-vdpfg\naffinity-clusterip-vdpfg\naffinity-clusterip-vdpfg\naffinity-clusterip-vdpfg\naffinity-clusterip-vdpfg\naffinity-clusterip-vdpfg\naffinity-clusterip-vdpfg\naffinity-clusterip-vdpfg\naffinity-clusterip-vdpfg\naffinity-clusterip-vdpfg"
May 31 13:12:39.356: INFO: Received response from host: affinity-clusterip-vdpfg
May 31 13:12:39.356: INFO: Received response from host: affinity-clusterip-vdpfg
May 31 13:12:39.356: INFO: Received response from host: affinity-clusterip-vdpfg
May 31 13:12:39.356: INFO: Received response from host: affinity-clusterip-vdpfg
May 31 13:12:39.356: INFO: Received response from host: affinity-clusterip-vdpfg
May 31 13:12:39.356: INFO: Received response from host: affinity-clusterip-vdpfg
May 31 13:12:39.356: INFO: Received response from host: affinity-clusterip-vdpfg
May 31 13:12:39.356: INFO: Received response from host: affinity-clusterip-vdpfg
May 31 13:12:39.356: INFO: Received response from host: affinity-clusterip-vdpfg
May 31 13:12:39.356: INFO: Received response from host: affinity-clusterip-vdpfg
May 31 13:12:39.356: INFO: Received response from host: affinity-clusterip-vdpfg
May 31 13:12:39.356: INFO: Received response from host: affinity-clusterip-vdpfg
May 31 13:12:39.356: INFO: Received response from host: affinity-clusterip-vdpfg
May 31 13:12:39.356: INFO: Received response from host: affinity-clusterip-vdpfg
May 31 13:12:39.356: INFO: Received response from host: affinity-clusterip-vdpfg
May 31 13:12:39.356: INFO: Received response from host: affinity-clusterip-vdpfg
May 31 13:12:39.356: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-9769, will wait for the garbage collector to delete the pods
May 31 13:12:39.465: INFO: Deleting ReplicationController affinity-clusterip took: 17.478416ms
May 31 13:12:39.565: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.360704ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:12:41.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9769" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:19.634 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":22,"skipped":474,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:12:41.988: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 13:12:42.072: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
May 31 13:12:42.110: INFO: Pod name sample-pod: Found 0 pods out of 1
May 31 13:12:47.121: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 31 13:12:47.121: INFO: Creating deployment "test-rolling-update-deployment"
May 31 13:12:47.129: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
May 31 13:12:47.148: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
May 31 13:12:49.171: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
May 31 13:12:49.178: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
May 31 13:12:49.203: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-68  e01180cf-cd72-43a1-ac74-4f1a40b1b46d 6330 1 2022-05-31 13:12:47 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2022-05-31 13:12:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-31 13:12:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004929598 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-05-31 13:12:47 +0000 UTC,LastTransitionTime:2022-05-31 13:12:47 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-796dbc4547" has successfully progressed.,LastUpdateTime:2022-05-31 13:12:48 +0000 UTC,LastTransitionTime:2022-05-31 13:12:47 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 31 13:12:49.210: INFO: New ReplicaSet "test-rolling-update-deployment-796dbc4547" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-796dbc4547  deployment-68  f2a0cc53-d6ae-4c3a-a7dc-99a99823a949 6319 1 2022-05-31 13:12:47 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:796dbc4547] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment e01180cf-cd72-43a1-ac74-4f1a40b1b46d 0xc004929a47 0xc004929a48}] []  [{kube-controller-manager Update apps/v1 2022-05-31 13:12:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e01180cf-cd72-43a1-ac74-4f1a40b1b46d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-31 13:12:48 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 796dbc4547,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:796dbc4547] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004929af8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 31 13:12:49.210: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
May 31 13:12:49.211: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-68  55c0a77e-647d-476d-a8a8-b77f0da17eb5 6329 2 2022-05-31 13:12:42 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment e01180cf-cd72-43a1-ac74-4f1a40b1b46d 0xc00492991f 0xc004929930}] []  [{e2e.test Update apps/v1 2022-05-31 13:12:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-31 13:12:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e01180cf-cd72-43a1-ac74-4f1a40b1b46d\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-05-31 13:12:48 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0049299e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 31 13:12:49.218: INFO: Pod "test-rolling-update-deployment-796dbc4547-kwqqv" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-796dbc4547-kwqqv test-rolling-update-deployment-796dbc4547- deployment-68  251b47bb-c9d4-4eae-a43d-6cc05bd19855 6318 0 2022-05-31 13:12:47 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:796dbc4547] map[cni.projectcalico.org/containerID:5073ca167f9016d2246a4e330e21c0a182544695b69deeefbf4be3bf64dce90e cni.projectcalico.org/podIP:172.25.2.11/32 cni.projectcalico.org/podIPs:172.25.2.11/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-796dbc4547 f2a0cc53-d6ae-4c3a-a7dc-99a99823a949 0xc0031a2257 0xc0031a2258}] []  [{calico Update v1 2022-05-31 13:12:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-05-31 13:12:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f2a0cc53-d6ae-4c3a-a7dc-99a99823a949\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-31 13:12:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.11\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gcb5k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gcb5k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-160.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 13:12:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 13:12:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 13:12:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 13:12:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.160,PodIP:172.25.2.11,StartTime:2022-05-31 13:12:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-31 13:12:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,ImageID:k8s.gcr.io/e2e-test-images/agnhost@sha256:5b3a9f1c71c09c00649d8374224642ff7029ce91a721ec9132e6ed45fa73fd43,ContainerID:containerd://392028cb7038896549edfb167f7c1c5239a44e94603c85834f89b4bfa0065365,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.11,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:12:49.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-68" for this suite.

• [SLOW TEST:7.253 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":23,"skipped":494,"failed":0}
SSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:12:49.243: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward api env vars
May 31 13:12:49.342: INFO: Waiting up to 5m0s for pod "downward-api-d1b01a62-f402-4381-8c06-46c9364177f7" in namespace "downward-api-1571" to be "Succeeded or Failed"
May 31 13:12:49.353: INFO: Pod "downward-api-d1b01a62-f402-4381-8c06-46c9364177f7": Phase="Pending", Reason="", readiness=false. Elapsed: 11.262464ms
May 31 13:12:51.362: INFO: Pod "downward-api-d1b01a62-f402-4381-8c06-46c9364177f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020057484s
May 31 13:12:53.373: INFO: Pod "downward-api-d1b01a62-f402-4381-8c06-46c9364177f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030519938s
STEP: Saw pod success
May 31 13:12:53.373: INFO: Pod "downward-api-d1b01a62-f402-4381-8c06-46c9364177f7" satisfied condition "Succeeded or Failed"
May 31 13:12:53.379: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod downward-api-d1b01a62-f402-4381-8c06-46c9364177f7 container dapi-container: <nil>
STEP: delete the pod
May 31 13:12:53.457: INFO: Waiting for pod downward-api-d1b01a62-f402-4381-8c06-46c9364177f7 to disappear
May 31 13:12:53.471: INFO: Pod downward-api-d1b01a62-f402-4381-8c06-46c9364177f7 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:12:53.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1571" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":346,"completed":24,"skipped":499,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:12:53.494: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 13:12:53.592: INFO: Waiting up to 5m0s for pod "busybox-user-65534-8684f1ff-e21d-4bc0-b504-26100789091d" in namespace "security-context-test-3176" to be "Succeeded or Failed"
May 31 13:12:53.600: INFO: Pod "busybox-user-65534-8684f1ff-e21d-4bc0-b504-26100789091d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.814492ms
May 31 13:12:55.611: INFO: Pod "busybox-user-65534-8684f1ff-e21d-4bc0-b504-26100789091d": Phase="Running", Reason="", readiness=true. Elapsed: 2.018816407s
May 31 13:12:57.624: INFO: Pod "busybox-user-65534-8684f1ff-e21d-4bc0-b504-26100789091d": Phase="Running", Reason="", readiness=false. Elapsed: 4.0320429s
May 31 13:12:59.635: INFO: Pod "busybox-user-65534-8684f1ff-e21d-4bc0-b504-26100789091d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.042390014s
May 31 13:12:59.635: INFO: Pod "busybox-user-65534-8684f1ff-e21d-4bc0-b504-26100789091d" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:12:59.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3176" for this suite.

• [SLOW TEST:6.165 seconds]
[sig-node] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:50
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":25,"skipped":512,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:12:59.660: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
May 31 13:13:01.772: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-3784 PodName:var-expansion-f71f17b8-1862-42b6-bfda-5f0a8a76ec5b ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 31 13:13:01.772: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
May 31 13:13:01.773: INFO: ExecWithOptions: Clientset creation
May 31 13:13:01.773: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/var-expansion-3784/pods/var-expansion-f71f17b8-1862-42b6-bfda-5f0a8a76ec5b/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true %!s(MISSING))
STEP: test for file in mounted path
May 31 13:13:02.029: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-3784 PodName:var-expansion-f71f17b8-1862-42b6-bfda-5f0a8a76ec5b ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 31 13:13:02.029: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
May 31 13:13:02.030: INFO: ExecWithOptions: Clientset creation
May 31 13:13:02.030: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/var-expansion-3784/pods/var-expansion-f71f17b8-1862-42b6-bfda-5f0a8a76ec5b/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true %!s(MISSING))
STEP: updating the annotation value
May 31 13:13:02.986: INFO: Successfully updated pod "var-expansion-f71f17b8-1862-42b6-bfda-5f0a8a76ec5b"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
May 31 13:13:02.997: INFO: Deleting pod "var-expansion-f71f17b8-1862-42b6-bfda-5f0a8a76ec5b" in namespace "var-expansion-3784"
May 31 13:13:03.020: INFO: Wait up to 5m0s for pod "var-expansion-f71f17b8-1862-42b6-bfda-5f0a8a76ec5b" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:13:37.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3784" for this suite.

• [SLOW TEST:37.410 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","total":346,"completed":26,"skipped":560,"failed":0}
SSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:13:37.072: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 13:13:37.126: INFO: Creating deployment "test-recreate-deployment"
May 31 13:13:37.141: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
May 31 13:13:37.158: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
May 31 13:13:39.185: INFO: Waiting deployment "test-recreate-deployment" to complete
May 31 13:13:39.194: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
May 31 13:13:39.212: INFO: Updating deployment test-recreate-deployment
May 31 13:13:39.212: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
May 31 13:13:39.380: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-6497  11e7cba9-c38c-44fc-8a8a-633a58053c14 6680 2 2022-05-31 13:13:37 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-05-31 13:13:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-31 13:13:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00406af88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-05-31 13:13:39 +0000 UTC,LastTransitionTime:2022-05-31 13:13:39 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5b99bd5487" is progressing.,LastUpdateTime:2022-05-31 13:13:39 +0000 UTC,LastTransitionTime:2022-05-31 13:13:37 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

May 31 13:13:39.390: INFO: New ReplicaSet "test-recreate-deployment-5b99bd5487" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5b99bd5487  deployment-6497  a6d3d2a8-7980-429e-a40f-11888d20fbc2 6679 1 2022-05-31 13:13:39 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5b99bd5487] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 11e7cba9-c38c-44fc-8a8a-633a58053c14 0xc00406b357 0xc00406b358}] []  [{kube-controller-manager Update apps/v1 2022-05-31 13:13:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"11e7cba9-c38c-44fc-8a8a-633a58053c14\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-31 13:13:39 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5b99bd5487,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5b99bd5487] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00406b3f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 31 13:13:39.390: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
May 31 13:13:39.390: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d659f7dc9  deployment-6497  4f40bd8c-1e78-4823-a4b5-9a3c78fd19af 6668 2 2022-05-31 13:13:37 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d659f7dc9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 11e7cba9-c38c-44fc-8a8a-633a58053c14 0xc00406b457 0xc00406b458}] []  [{kube-controller-manager Update apps/v1 2022-05-31 13:13:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"11e7cba9-c38c-44fc-8a8a-633a58053c14\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-31 13:13:39 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d659f7dc9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d659f7dc9] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00406b508 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 31 13:13:39.397: INFO: Pod "test-recreate-deployment-5b99bd5487-8748v" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5b99bd5487-8748v test-recreate-deployment-5b99bd5487- deployment-6497  dffb8b8a-5610-44ee-a24a-6839d172993c 6678 0 2022-05-31 13:13:39 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5b99bd5487] map[] [{apps/v1 ReplicaSet test-recreate-deployment-5b99bd5487 a6d3d2a8-7980-429e-a40f-11888d20fbc2 0xc0041ff287 0xc0041ff288}] []  [{kube-controller-manager Update v1 2022-05-31 13:13:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a6d3d2a8-7980-429e-a40f-11888d20fbc2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-31 13:13:39 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xq8hq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xq8hq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-8-180.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 13:13:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 13:13:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 13:13:39 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 13:13:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.8.180,PodIP:,StartTime:2022-05-31 13:13:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:13:39.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6497" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":27,"skipped":563,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:13:39.431: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 13:13:39.489: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: creating the pod
STEP: submitting the pod to kubernetes
May 31 13:13:39.514: INFO: The status of Pod pod-logs-websocket-3c03f30f-8087-41b3-ab9d-2f62c1dc7088 is Pending, waiting for it to be Running (with Ready = true)
May 31 13:13:41.526: INFO: The status of Pod pod-logs-websocket-3c03f30f-8087-41b3-ab9d-2f62c1dc7088 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:13:41.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1570" for this suite.
•{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":346,"completed":28,"skipped":641,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:13:41.631: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
May 31 13:13:41.744: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
May 31 13:13:58.987: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
May 31 13:14:02.469: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:14:17.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-800" for this suite.

• [SLOW TEST:36.354 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":346,"completed":29,"skipped":663,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:14:17.986: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-42c51db2-1839-4d3c-87d0-5dadc1ad6675
STEP: Creating a pod to test consume secrets
May 31 13:14:18.135: INFO: Waiting up to 5m0s for pod "pod-secrets-24b06e98-1150-474d-81b4-d4fc1480ffe8" in namespace "secrets-6312" to be "Succeeded or Failed"
May 31 13:14:18.145: INFO: Pod "pod-secrets-24b06e98-1150-474d-81b4-d4fc1480ffe8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.176231ms
May 31 13:14:20.152: INFO: Pod "pod-secrets-24b06e98-1150-474d-81b4-d4fc1480ffe8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017949499s
May 31 13:14:22.163: INFO: Pod "pod-secrets-24b06e98-1150-474d-81b4-d4fc1480ffe8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028380802s
STEP: Saw pod success
May 31 13:14:22.163: INFO: Pod "pod-secrets-24b06e98-1150-474d-81b4-d4fc1480ffe8" satisfied condition "Succeeded or Failed"
May 31 13:14:22.172: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod pod-secrets-24b06e98-1150-474d-81b4-d4fc1480ffe8 container secret-volume-test: <nil>
STEP: delete the pod
May 31 13:14:22.265: INFO: Waiting for pod pod-secrets-24b06e98-1150-474d-81b4-d4fc1480ffe8 to disappear
May 31 13:14:22.272: INFO: Pod pod-secrets-24b06e98-1150-474d-81b4-d4fc1480ffe8 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:14:22.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6312" for this suite.
STEP: Destroying namespace "secret-namespace-5749" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":346,"completed":30,"skipped":689,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:14:22.318: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name cm-test-opt-del-323891fe-7f61-4579-acc0-5e8b9af56346
STEP: Creating configMap with name cm-test-opt-upd-545245de-6e14-49e4-b12d-bcf8d2de5c1d
STEP: Creating the pod
May 31 13:14:22.418: INFO: The status of Pod pod-configmaps-7de7510c-5eae-470e-8576-4ecc473aba79 is Pending, waiting for it to be Running (with Ready = true)
May 31 13:14:24.433: INFO: The status of Pod pod-configmaps-7de7510c-5eae-470e-8576-4ecc473aba79 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-323891fe-7f61-4579-acc0-5e8b9af56346
STEP: Updating configmap cm-test-opt-upd-545245de-6e14-49e4-b12d-bcf8d2de5c1d
STEP: Creating configMap with name cm-test-opt-create-fa628c6f-6ea0-41c3-bd9c-8fc9ef688073
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:14:26.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2215" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":31,"skipped":707,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:14:26.821: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with secret that has name projected-secret-test-map-7d485a95-e809-473b-9988-5bf29468b652
STEP: Creating a pod to test consume secrets
May 31 13:14:26.899: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-eac0fe24-9cc0-4e09-bebd-1e001115f8bd" in namespace "projected-8792" to be "Succeeded or Failed"
May 31 13:14:26.910: INFO: Pod "pod-projected-secrets-eac0fe24-9cc0-4e09-bebd-1e001115f8bd": Phase="Pending", Reason="", readiness=false. Elapsed: 11.381875ms
May 31 13:14:28.926: INFO: Pod "pod-projected-secrets-eac0fe24-9cc0-4e09-bebd-1e001115f8bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026641621s
May 31 13:14:30.935: INFO: Pod "pod-projected-secrets-eac0fe24-9cc0-4e09-bebd-1e001115f8bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036147519s
STEP: Saw pod success
May 31 13:14:30.935: INFO: Pod "pod-projected-secrets-eac0fe24-9cc0-4e09-bebd-1e001115f8bd" satisfied condition "Succeeded or Failed"
May 31 13:14:30.955: INFO: Trying to get logs from node ip-172-31-3-160.eu-central-1.compute.internal pod pod-projected-secrets-eac0fe24-9cc0-4e09-bebd-1e001115f8bd container projected-secret-volume-test: <nil>
STEP: delete the pod
May 31 13:14:31.041: INFO: Waiting for pod pod-projected-secrets-eac0fe24-9cc0-4e09-bebd-1e001115f8bd to disappear
May 31 13:14:31.046: INFO: Pod pod-projected-secrets-eac0fe24-9cc0-4e09-bebd-1e001115f8bd no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:14:31.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8792" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":32,"skipped":726,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:14:31.070: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-map-251d2359-3475-4a03-b1e9-fd0efe53f871
STEP: Creating a pod to test consume configMaps
May 31 13:14:31.149: INFO: Waiting up to 5m0s for pod "pod-configmaps-715831b0-6aaf-4d3e-b5e8-a90e0444c923" in namespace "configmap-89" to be "Succeeded or Failed"
May 31 13:14:31.158: INFO: Pod "pod-configmaps-715831b0-6aaf-4d3e-b5e8-a90e0444c923": Phase="Pending", Reason="", readiness=false. Elapsed: 8.870217ms
May 31 13:14:33.168: INFO: Pod "pod-configmaps-715831b0-6aaf-4d3e-b5e8-a90e0444c923": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019599458s
May 31 13:14:35.176: INFO: Pod "pod-configmaps-715831b0-6aaf-4d3e-b5e8-a90e0444c923": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027128597s
May 31 13:14:37.186: INFO: Pod "pod-configmaps-715831b0-6aaf-4d3e-b5e8-a90e0444c923": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036749437s
STEP: Saw pod success
May 31 13:14:37.186: INFO: Pod "pod-configmaps-715831b0-6aaf-4d3e-b5e8-a90e0444c923" satisfied condition "Succeeded or Failed"
May 31 13:14:37.192: INFO: Trying to get logs from node ip-172-31-3-160.eu-central-1.compute.internal pod pod-configmaps-715831b0-6aaf-4d3e-b5e8-a90e0444c923 container agnhost-container: <nil>
STEP: delete the pod
May 31 13:14:37.274: INFO: Waiting for pod pod-configmaps-715831b0-6aaf-4d3e-b5e8-a90e0444c923 to disappear
May 31 13:14:37.279: INFO: Pod pod-configmaps-715831b0-6aaf-4d3e-b5e8-a90e0444c923 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:14:37.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-89" for this suite.

• [SLOW TEST:6.227 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":33,"skipped":744,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:14:37.315: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
May 31 13:14:37.356: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
May 31 13:14:40.768: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:14:58.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6036" for this suite.

• [SLOW TEST:20.832 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":346,"completed":34,"skipped":787,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:14:58.149: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service in namespace services-8163
May 31 13:14:58.222: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
May 31 13:15:00.229: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
May 31 13:15:00.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-8163 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
May 31 13:15:00.906: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
May 31 13:15:00.906: INFO: stdout: "ipvs"
May 31 13:15:00.906: INFO: proxyMode: ipvs
May 31 13:15:00.974: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 31 13:15:00.996: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-8163
STEP: creating replication controller affinity-clusterip-timeout in namespace services-8163
I0531 13:15:01.068765      19 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-8163, replica count: 3
I0531 13:15:04.120297      19 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 31 13:15:04.136: INFO: Creating new exec pod
May 31 13:15:07.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-8163 exec execpod-affinity7jsdv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
May 31 13:15:07.693: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip-timeout 80\n+ echo hostName\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
May 31 13:15:07.693: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 31 13:15:07.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-8163 exec execpod-affinity7jsdv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.30.209 80'
May 31 13:15:08.286: INFO: stderr: "+ nc -v -t -w 2 10.240.30.209 80\n+ echo hostName\nConnection to 10.240.30.209 80 port [tcp/http] succeeded!\n"
May 31 13:15:08.286: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 31 13:15:08.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-8163 exec execpod-affinity7jsdv -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.30.209:80/ ; done'
May 31 13:15:08.911: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.30.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.30.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.30.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.30.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.30.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.30.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.30.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.30.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.30.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.30.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.30.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.30.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.30.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.30.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.30.209:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.30.209:80/\n"
May 31 13:15:08.911: INFO: stdout: "\naffinity-clusterip-timeout-5pgtn\naffinity-clusterip-timeout-5pgtn\naffinity-clusterip-timeout-5pgtn\naffinity-clusterip-timeout-5pgtn\naffinity-clusterip-timeout-5pgtn\naffinity-clusterip-timeout-5pgtn\naffinity-clusterip-timeout-5pgtn\naffinity-clusterip-timeout-5pgtn\naffinity-clusterip-timeout-5pgtn\naffinity-clusterip-timeout-5pgtn\naffinity-clusterip-timeout-5pgtn\naffinity-clusterip-timeout-5pgtn\naffinity-clusterip-timeout-5pgtn\naffinity-clusterip-timeout-5pgtn\naffinity-clusterip-timeout-5pgtn\naffinity-clusterip-timeout-5pgtn"
May 31 13:15:08.911: INFO: Received response from host: affinity-clusterip-timeout-5pgtn
May 31 13:15:08.911: INFO: Received response from host: affinity-clusterip-timeout-5pgtn
May 31 13:15:08.911: INFO: Received response from host: affinity-clusterip-timeout-5pgtn
May 31 13:15:08.911: INFO: Received response from host: affinity-clusterip-timeout-5pgtn
May 31 13:15:08.911: INFO: Received response from host: affinity-clusterip-timeout-5pgtn
May 31 13:15:08.911: INFO: Received response from host: affinity-clusterip-timeout-5pgtn
May 31 13:15:08.911: INFO: Received response from host: affinity-clusterip-timeout-5pgtn
May 31 13:15:08.911: INFO: Received response from host: affinity-clusterip-timeout-5pgtn
May 31 13:15:08.911: INFO: Received response from host: affinity-clusterip-timeout-5pgtn
May 31 13:15:08.911: INFO: Received response from host: affinity-clusterip-timeout-5pgtn
May 31 13:15:08.911: INFO: Received response from host: affinity-clusterip-timeout-5pgtn
May 31 13:15:08.911: INFO: Received response from host: affinity-clusterip-timeout-5pgtn
May 31 13:15:08.911: INFO: Received response from host: affinity-clusterip-timeout-5pgtn
May 31 13:15:08.911: INFO: Received response from host: affinity-clusterip-timeout-5pgtn
May 31 13:15:08.911: INFO: Received response from host: affinity-clusterip-timeout-5pgtn
May 31 13:15:08.911: INFO: Received response from host: affinity-clusterip-timeout-5pgtn
May 31 13:15:08.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-8163 exec execpod-affinity7jsdv -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.240.30.209:80/'
May 31 13:15:09.300: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.240.30.209:80/\n"
May 31 13:15:09.300: INFO: stdout: "affinity-clusterip-timeout-5pgtn"
May 31 13:17:19.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-8163 exec execpod-affinity7jsdv -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.240.30.209:80/'
May 31 13:17:19.713: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.240.30.209:80/\n"
May 31 13:17:19.714: INFO: stdout: "affinity-clusterip-timeout-knlgm"
May 31 13:17:19.714: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-8163, will wait for the garbage collector to delete the pods
May 31 13:17:19.817: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 14.183839ms
May 31 13:17:19.919: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 102.223913ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:17:22.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8163" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:144.249 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":35,"skipped":824,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:17:22.398: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 31 13:17:23.774: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 31 13:17:26.829: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:17:26.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-238" for this suite.
STEP: Destroying namespace "webhook-238-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":346,"completed":36,"skipped":852,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:17:26.973: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
May 31 13:17:27.057: INFO: observed Pod pod-test in namespace pods-8613 in phase Pending with labels: map[test-pod-static:true] & conditions []
May 31 13:17:27.057: INFO: observed Pod pod-test in namespace pods-8613 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-31 13:17:27 +0000 UTC  }]
May 31 13:17:27.078: INFO: observed Pod pod-test in namespace pods-8613 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-31 13:17:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-05-31 13:17:27 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-05-31 13:17:27 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-31 13:17:27 +0000 UTC  }]
May 31 13:17:27.577: INFO: observed Pod pod-test in namespace pods-8613 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-31 13:17:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-05-31 13:17:27 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-05-31 13:17:27 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-31 13:17:27 +0000 UTC  }]
May 31 13:17:28.508: INFO: Found Pod pod-test in namespace pods-8613 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-31 13:17:27 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2022-05-31 13:17:28 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2022-05-31 13:17:28 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-31 13:17:27 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
May 31 13:17:28.536: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
May 31 13:17:28.588: INFO: observed event type ADDED
May 31 13:17:28.589: INFO: observed event type MODIFIED
May 31 13:17:28.589: INFO: observed event type MODIFIED
May 31 13:17:28.589: INFO: observed event type MODIFIED
May 31 13:17:28.590: INFO: observed event type MODIFIED
May 31 13:17:28.590: INFO: observed event type MODIFIED
May 31 13:17:28.590: INFO: observed event type MODIFIED
May 31 13:17:28.590: INFO: observed event type MODIFIED
May 31 13:17:30.515: INFO: observed event type MODIFIED
May 31 13:17:30.863: INFO: observed event type MODIFIED
May 31 13:17:31.516: INFO: observed event type MODIFIED
May 31 13:17:31.533: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:17:31.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8613" for this suite.
•{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":346,"completed":37,"skipped":862,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:17:31.560: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod liveness-133c0882-10be-4a90-9704-6b5e87ef5ac8 in namespace container-probe-4814
May 31 13:17:33.625: INFO: Started pod liveness-133c0882-10be-4a90-9704-6b5e87ef5ac8 in namespace container-probe-4814
STEP: checking the pod's current state and verifying that restartCount is present
May 31 13:17:33.632: INFO: Initial restart count of pod liveness-133c0882-10be-4a90-9704-6b5e87ef5ac8 is 0
May 31 13:17:53.772: INFO: Restart count of pod container-probe-4814/liveness-133c0882-10be-4a90-9704-6b5e87ef5ac8 is now 1 (20.140473624s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:17:53.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4814" for this suite.

• [SLOW TEST:22.255 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":38,"skipped":900,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:17:53.818: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with secret that has name secret-emptykey-test-442bf944-3b17-4241-8edc-4aa820c9f5ad
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:17:53.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1038" for this suite.
•{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","total":346,"completed":39,"skipped":951,"failed":0}
SSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:17:53.905: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
May 31 13:17:54.003: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May 31 13:17:56.020: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the pod with lifecycle hook
May 31 13:17:56.050: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
May 31 13:17:58.071: INFO: The status of Pod pod-with-prestop-http-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
May 31 13:17:58.099: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 31 13:17:58.106: INFO: Pod pod-with-prestop-http-hook still exists
May 31 13:18:00.108: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 31 13:18:00.120: INFO: Pod pod-with-prestop-http-hook still exists
May 31 13:18:02.108: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 31 13:18:02.120: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:18:02.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9902" for this suite.

• [SLOW TEST:8.309 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":346,"completed":40,"skipped":957,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:18:02.215: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:18:02.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7999" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":346,"completed":41,"skipped":980,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:18:02.302: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 31 13:18:02.377: INFO: Waiting up to 5m0s for pod "downwardapi-volume-51e2674c-aff2-4343-8603-d59589524eee" in namespace "projected-924" to be "Succeeded or Failed"
May 31 13:18:02.390: INFO: Pod "downwardapi-volume-51e2674c-aff2-4343-8603-d59589524eee": Phase="Pending", Reason="", readiness=false. Elapsed: 12.551812ms
May 31 13:18:04.403: INFO: Pod "downwardapi-volume-51e2674c-aff2-4343-8603-d59589524eee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025644876s
May 31 13:18:06.416: INFO: Pod "downwardapi-volume-51e2674c-aff2-4343-8603-d59589524eee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038590792s
STEP: Saw pod success
May 31 13:18:06.416: INFO: Pod "downwardapi-volume-51e2674c-aff2-4343-8603-d59589524eee" satisfied condition "Succeeded or Failed"
May 31 13:18:06.426: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod downwardapi-volume-51e2674c-aff2-4343-8603-d59589524eee container client-container: <nil>
STEP: delete the pod
May 31 13:18:06.512: INFO: Waiting for pod downwardapi-volume-51e2674c-aff2-4343-8603-d59589524eee to disappear
May 31 13:18:06.518: INFO: Pod downwardapi-volume-51e2674c-aff2-4343-8603-d59589524eee no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:18:06.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-924" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":42,"skipped":983,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces 
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:18:06.547: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:18:06.598: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename disruption-2
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: listing a collection of PDBs across all namespaces
STEP: listing a collection of PDBs in namespace disruption-5053
STEP: deleting a collection of PDBs
STEP: Waiting for the PDB collection to be deleted
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:18:08.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-5143" for this suite.
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:18:08.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-5053" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","total":346,"completed":43,"skipped":1041,"failed":0}
S
------------------------------
[sig-node] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:18:08.886: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
STEP: submitting the pod to kubernetes
May 31 13:18:08.958: INFO: The status of Pod pod-update-activedeadlineseconds-b62a8a23-403b-4e23-9b98-447c6958a574 is Pending, waiting for it to be Running (with Ready = true)
May 31 13:18:10.969: INFO: The status of Pod pod-update-activedeadlineseconds-b62a8a23-403b-4e23-9b98-447c6958a574 is Pending, waiting for it to be Running (with Ready = true)
May 31 13:18:12.970: INFO: The status of Pod pod-update-activedeadlineseconds-b62a8a23-403b-4e23-9b98-447c6958a574 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 31 13:18:13.528: INFO: Successfully updated pod "pod-update-activedeadlineseconds-b62a8a23-403b-4e23-9b98-447c6958a574"
May 31 13:18:13.530: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-b62a8a23-403b-4e23-9b98-447c6958a574" in namespace "pods-2844" to be "terminated due to deadline exceeded"
May 31 13:18:13.548: INFO: Pod "pod-update-activedeadlineseconds-b62a8a23-403b-4e23-9b98-447c6958a574": Phase="Running", Reason="", readiness=true. Elapsed: 17.736185ms
May 31 13:18:15.557: INFO: Pod "pod-update-activedeadlineseconds-b62a8a23-403b-4e23-9b98-447c6958a574": Phase="Running", Reason="", readiness=true. Elapsed: 2.026512229s
May 31 13:18:17.591: INFO: Pod "pod-update-activedeadlineseconds-b62a8a23-403b-4e23-9b98-447c6958a574": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.060867435s
May 31 13:18:17.591: INFO: Pod "pod-update-activedeadlineseconds-b62a8a23-403b-4e23-9b98-447c6958a574" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:18:17.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2844" for this suite.

• [SLOW TEST:8.750 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":346,"completed":44,"skipped":1042,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:18:17.648: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating replication controller my-hostname-basic-58e36966-2dcb-404b-9f33-5f8d8ecf26ff
May 31 13:18:17.761: INFO: Pod name my-hostname-basic-58e36966-2dcb-404b-9f33-5f8d8ecf26ff: Found 0 pods out of 1
May 31 13:18:22.778: INFO: Pod name my-hostname-basic-58e36966-2dcb-404b-9f33-5f8d8ecf26ff: Found 1 pods out of 1
May 31 13:18:22.778: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-58e36966-2dcb-404b-9f33-5f8d8ecf26ff" are running
May 31 13:18:22.792: INFO: Pod "my-hostname-basic-58e36966-2dcb-404b-9f33-5f8d8ecf26ff-9k9zf" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-05-31 13:18:17 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-05-31 13:18:19 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-05-31 13:18:19 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-05-31 13:18:17 +0000 UTC Reason: Message:}])
May 31 13:18:22.792: INFO: Trying to dial the pod
May 31 13:18:27.933: INFO: Controller my-hostname-basic-58e36966-2dcb-404b-9f33-5f8d8ecf26ff: Got expected result from replica 1 [my-hostname-basic-58e36966-2dcb-404b-9f33-5f8d8ecf26ff-9k9zf]: "my-hostname-basic-58e36966-2dcb-404b-9f33-5f8d8ecf26ff-9k9zf", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:18:27.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6045" for this suite.

• [SLOW TEST:10.315 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":45,"skipped":1059,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:18:27.979: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 13:18:28.038: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
May 31 13:18:29.114: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:18:29.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5531" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":346,"completed":46,"skipped":1067,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:18:29.143: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name cm-test-opt-del-8ed96033-6415-49ea-80a2-d21f8817aed7
STEP: Creating configMap with name cm-test-opt-upd-0c98da77-ae3c-4b17-b0ae-05d2959e68fe
STEP: Creating the pod
May 31 13:18:29.238: INFO: The status of Pod pod-projected-configmaps-560d01e1-bcdd-4e6e-89ef-f8fe651b84e0 is Pending, waiting for it to be Running (with Ready = true)
May 31 13:18:31.253: INFO: The status of Pod pod-projected-configmaps-560d01e1-bcdd-4e6e-89ef-f8fe651b84e0 is Pending, waiting for it to be Running (with Ready = true)
May 31 13:18:33.262: INFO: The status of Pod pod-projected-configmaps-560d01e1-bcdd-4e6e-89ef-f8fe651b84e0 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-8ed96033-6415-49ea-80a2-d21f8817aed7
STEP: Updating configmap cm-test-opt-upd-0c98da77-ae3c-4b17-b0ae-05d2959e68fe
STEP: Creating configMap with name cm-test-opt-create-a6407ef0-0080-4243-9882-9c8815810175
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:19:38.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6181" for this suite.

• [SLOW TEST:69.425 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":47,"skipped":1070,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:19:38.575: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
May 31 13:19:38.635: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
May 31 13:19:38.658: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
May 31 13:19:38.658: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
May 31 13:19:38.674: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
May 31 13:19:38.674: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
May 31 13:19:38.700: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
May 31 13:19:38.700: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
May 31 13:19:45.787: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:19:45.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-9665" for this suite.

• [SLOW TEST:7.281 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":346,"completed":48,"skipped":1083,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:19:45.856: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:20:00.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3366" for this suite.
STEP: Destroying namespace "nsdeletetest-3292" for this suite.
May 31 13:20:00.217: INFO: Namespace nsdeletetest-3292 was already deleted
STEP: Destroying namespace "nsdeletetest-7385" for this suite.

• [SLOW TEST:14.381 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":346,"completed":49,"skipped":1094,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:20:00.239: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 31 13:20:01.292: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 31 13:20:04.353: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:20:04.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1299" for this suite.
STEP: Destroying namespace "webhook-1299-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":346,"completed":50,"skipped":1155,"failed":0}
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:20:04.720: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 13:20:04.803: INFO: created pod
May 31 13:20:04.803: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-3211" to be "Succeeded or Failed"
May 31 13:20:04.813: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 10.107366ms
May 31 13:20:06.827: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024248265s
May 31 13:20:08.836: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033426842s
May 31 13:20:10.848: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.045790459s
STEP: Saw pod success
May 31 13:20:10.849: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
May 31 13:20:40.849: INFO: polling logs
May 31 13:20:40.896: INFO: Pod logs: 
2022/05/31 13:20:06 OK: Got token
2022/05/31 13:20:06 validating with in-cluster discovery
2022/05/31 13:20:06 OK: got issuer https://kz7h9l58lv.captain.captain.k8c.io:31059
2022/05/31 13:20:06 Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kz7h9l58lv.captain.captain.k8c.io:31059", Subject:"system:serviceaccount:svcaccounts-3211:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1654003804, NotBefore:1654003204, IssuedAt:1654003204, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-3211", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"31fd3a95-ce90-4289-9cd5-5942253db627"}}}
2022/05/31 13:20:06 OK: Constructed OIDC provider for issuer https://kz7h9l58lv.captain.captain.k8c.io:31059
2022/05/31 13:20:06 OK: Validated signature on JWT
2022/05/31 13:20:06 OK: Got valid claims from token!
2022/05/31 13:20:06 Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kz7h9l58lv.captain.captain.k8c.io:31059", Subject:"system:serviceaccount:svcaccounts-3211:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1654003804, NotBefore:1654003204, IssuedAt:1654003204, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-3211", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"31fd3a95-ce90-4289-9cd5-5942253db627"}}}

May 31 13:20:40.896: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:20:40.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3211" for this suite.

• [SLOW TEST:36.234 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","total":346,"completed":51,"skipped":1163,"failed":0}
SS
------------------------------
[sig-node] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:20:40.961: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod liveness-45cce931-8985-40f6-9c42-a3d83f8ffd9b in namespace container-probe-4000
May 31 13:20:43.053: INFO: Started pod liveness-45cce931-8985-40f6-9c42-a3d83f8ffd9b in namespace container-probe-4000
STEP: checking the pod's current state and verifying that restartCount is present
May 31 13:20:43.061: INFO: Initial restart count of pod liveness-45cce931-8985-40f6-9c42-a3d83f8ffd9b is 0
May 31 13:21:03.197: INFO: Restart count of pod container-probe-4000/liveness-45cce931-8985-40f6-9c42-a3d83f8ffd9b is now 1 (20.135778581s elapsed)
May 31 13:21:23.350: INFO: Restart count of pod container-probe-4000/liveness-45cce931-8985-40f6-9c42-a3d83f8ffd9b is now 2 (40.28831975s elapsed)
May 31 13:21:43.491: INFO: Restart count of pod container-probe-4000/liveness-45cce931-8985-40f6-9c42-a3d83f8ffd9b is now 3 (1m0.42915472s elapsed)
May 31 13:22:03.619: INFO: Restart count of pod container-probe-4000/liveness-45cce931-8985-40f6-9c42-a3d83f8ffd9b is now 4 (1m20.557276154s elapsed)
May 31 13:23:04.027: INFO: Restart count of pod container-probe-4000/liveness-45cce931-8985-40f6-9c42-a3d83f8ffd9b is now 5 (2m20.965777229s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:23:04.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4000" for this suite.

• [SLOW TEST:143.115 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":346,"completed":52,"skipped":1165,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:23:04.084: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with secret that has name projected-secret-test-734067b1-b1d8-4632-b79a-89f3a675d2ef
STEP: Creating a pod to test consume secrets
May 31 13:23:04.197: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-dfcb16f1-6547-47e6-923b-1a7f5be68e3a" in namespace "projected-7732" to be "Succeeded or Failed"
May 31 13:23:04.206: INFO: Pod "pod-projected-secrets-dfcb16f1-6547-47e6-923b-1a7f5be68e3a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.532222ms
May 31 13:23:06.217: INFO: Pod "pod-projected-secrets-dfcb16f1-6547-47e6-923b-1a7f5be68e3a": Phase="Running", Reason="", readiness=true. Elapsed: 2.01986575s
May 31 13:23:08.228: INFO: Pod "pod-projected-secrets-dfcb16f1-6547-47e6-923b-1a7f5be68e3a": Phase="Running", Reason="", readiness=false. Elapsed: 4.030594415s
May 31 13:23:10.241: INFO: Pod "pod-projected-secrets-dfcb16f1-6547-47e6-923b-1a7f5be68e3a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.044008794s
STEP: Saw pod success
May 31 13:23:10.242: INFO: Pod "pod-projected-secrets-dfcb16f1-6547-47e6-923b-1a7f5be68e3a" satisfied condition "Succeeded or Failed"
May 31 13:23:10.250: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod pod-projected-secrets-dfcb16f1-6547-47e6-923b-1a7f5be68e3a container projected-secret-volume-test: <nil>
STEP: delete the pod
May 31 13:23:10.333: INFO: Waiting for pod pod-projected-secrets-dfcb16f1-6547-47e6-923b-1a7f5be68e3a to disappear
May 31 13:23:10.340: INFO: Pod pod-projected-secrets-dfcb16f1-6547-47e6-923b-1a7f5be68e3a no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:23:10.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7732" for this suite.

• [SLOW TEST:6.279 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":53,"skipped":1184,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:23:10.369: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
May 31 13:23:20.591: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0531 13:23:20.591120      19 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:23:20.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5210" for this suite.

• [SLOW TEST:10.247 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":346,"completed":54,"skipped":1213,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:23:20.617: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pdb
STEP: Waiting for the pdb to be processed
STEP: updating the pdb
STEP: Waiting for the pdb to be processed
STEP: patching the pdb
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be deleted
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:23:24.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-4336" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","total":346,"completed":55,"skipped":1222,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring 
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:23:24.841: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename endpointslicemirroring
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslicemirroring.go:39
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: mirroring a new custom Endpoint
May 31 13:23:24.986: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint
May 31 13:23:27.017: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint
May 31 13:23:29.069: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:23:31.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-6456" for this suite.

• [SLOW TEST:6.276 seconds]
[sig-network] EndpointSliceMirroring
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","total":346,"completed":56,"skipped":1238,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:23:31.117: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-map-00668e9e-0ec2-4ee5-83c3-b7410fa528fa
STEP: Creating a pod to test consume configMaps
May 31 13:23:31.262: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bd7ec9fe-300a-4be4-b294-6ce4be27acce" in namespace "projected-1340" to be "Succeeded or Failed"
May 31 13:23:31.271: INFO: Pod "pod-projected-configmaps-bd7ec9fe-300a-4be4-b294-6ce4be27acce": Phase="Pending", Reason="", readiness=false. Elapsed: 9.207889ms
May 31 13:23:33.285: INFO: Pod "pod-projected-configmaps-bd7ec9fe-300a-4be4-b294-6ce4be27acce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022968666s
May 31 13:23:35.302: INFO: Pod "pod-projected-configmaps-bd7ec9fe-300a-4be4-b294-6ce4be27acce": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039896113s
May 31 13:23:37.313: INFO: Pod "pod-projected-configmaps-bd7ec9fe-300a-4be4-b294-6ce4be27acce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.051002467s
STEP: Saw pod success
May 31 13:23:37.314: INFO: Pod "pod-projected-configmaps-bd7ec9fe-300a-4be4-b294-6ce4be27acce" satisfied condition "Succeeded or Failed"
May 31 13:23:37.321: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod pod-projected-configmaps-bd7ec9fe-300a-4be4-b294-6ce4be27acce container agnhost-container: <nil>
STEP: delete the pod
May 31 13:23:37.375: INFO: Waiting for pod pod-projected-configmaps-bd7ec9fe-300a-4be4-b294-6ce4be27acce to disappear
May 31 13:23:37.388: INFO: Pod pod-projected-configmaps-bd7ec9fe-300a-4be4-b294-6ce4be27acce no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:23:37.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1340" for this suite.

• [SLOW TEST:6.295 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":57,"skipped":1246,"failed":0}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:23:37.414: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0666 on node default medium
May 31 13:23:37.503: INFO: Waiting up to 5m0s for pod "pod-0b690422-8fb4-492e-8c28-1806b6a0e48c" in namespace "emptydir-1258" to be "Succeeded or Failed"
May 31 13:23:37.511: INFO: Pod "pod-0b690422-8fb4-492e-8c28-1806b6a0e48c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.213047ms
May 31 13:23:39.530: INFO: Pod "pod-0b690422-8fb4-492e-8c28-1806b6a0e48c": Phase="Running", Reason="", readiness=false. Elapsed: 2.027946939s
May 31 13:23:41.543: INFO: Pod "pod-0b690422-8fb4-492e-8c28-1806b6a0e48c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040077497s
STEP: Saw pod success
May 31 13:23:41.543: INFO: Pod "pod-0b690422-8fb4-492e-8c28-1806b6a0e48c" satisfied condition "Succeeded or Failed"
May 31 13:23:41.554: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod pod-0b690422-8fb4-492e-8c28-1806b6a0e48c container test-container: <nil>
STEP: delete the pod
May 31 13:23:41.630: INFO: Waiting for pod pod-0b690422-8fb4-492e-8c28-1806b6a0e48c to disappear
May 31 13:23:41.640: INFO: Pod pod-0b690422-8fb4-492e-8c28-1806b6a0e48c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:23:41.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1258" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":58,"skipped":1252,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:23:41.671: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test substitution in container's command
May 31 13:23:41.753: INFO: Waiting up to 5m0s for pod "var-expansion-4290c0e7-6e4e-4391-94e5-1d78872aebe0" in namespace "var-expansion-1884" to be "Succeeded or Failed"
May 31 13:23:41.760: INFO: Pod "var-expansion-4290c0e7-6e4e-4391-94e5-1d78872aebe0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.130159ms
May 31 13:23:43.776: INFO: Pod "var-expansion-4290c0e7-6e4e-4391-94e5-1d78872aebe0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023529608s
May 31 13:23:45.785: INFO: Pod "var-expansion-4290c0e7-6e4e-4391-94e5-1d78872aebe0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031985678s
STEP: Saw pod success
May 31 13:23:45.785: INFO: Pod "var-expansion-4290c0e7-6e4e-4391-94e5-1d78872aebe0" satisfied condition "Succeeded or Failed"
May 31 13:23:45.792: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod var-expansion-4290c0e7-6e4e-4391-94e5-1d78872aebe0 container dapi-container: <nil>
STEP: delete the pod
May 31 13:23:45.870: INFO: Waiting for pod var-expansion-4290c0e7-6e4e-4391-94e5-1d78872aebe0 to disappear
May 31 13:23:45.875: INFO: Pod var-expansion-4290c0e7-6e4e-4391-94e5-1d78872aebe0 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:23:45.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1884" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":346,"completed":59,"skipped":1288,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:23:45.905: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-6555
[It] should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating statefulset ss in namespace statefulset-6555
May 31 13:23:46.074: INFO: Found 0 stateful pods, waiting for 1
May 31 13:23:56.085: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label
STEP: Getting /status
May 31 13:23:56.133: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status
May 31 13:23:56.164: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated
May 31 13:23:56.170: INFO: Observed &StatefulSet event: ADDED
May 31 13:23:56.170: INFO: Found Statefulset ss in namespace statefulset-6555 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May 31 13:23:56.170: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status
May 31 13:23:56.171: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
May 31 13:23:56.188: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched
May 31 13:23:56.195: INFO: Observed &StatefulSet event: ADDED
May 31 13:23:56.195: INFO: Observed Statefulset ss in namespace statefulset-6555 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May 31 13:23:56.195: INFO: Observed &StatefulSet event: MODIFIED
May 31 13:23:56.195: INFO: Found Statefulset ss in namespace statefulset-6555 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
May 31 13:23:56.196: INFO: Deleting all statefulset in ns statefulset-6555
May 31 13:23:56.203: INFO: Scaling statefulset ss to 0
May 31 13:24:06.251: INFO: Waiting for statefulset status.replicas updated to 0
May 31 13:24:06.260: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:24:06.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6555" for this suite.

• [SLOW TEST:20.412 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should validate Statefulset Status endpoints [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","total":346,"completed":60,"skipped":1299,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:24:06.320: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 31 13:24:06.388: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e3daf6e3-92b2-41d8-a077-7d1b039b9a1e" in namespace "downward-api-2811" to be "Succeeded or Failed"
May 31 13:24:06.399: INFO: Pod "downwardapi-volume-e3daf6e3-92b2-41d8-a077-7d1b039b9a1e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.586948ms
May 31 13:24:08.416: INFO: Pod "downwardapi-volume-e3daf6e3-92b2-41d8-a077-7d1b039b9a1e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028046594s
May 31 13:24:10.428: INFO: Pod "downwardapi-volume-e3daf6e3-92b2-41d8-a077-7d1b039b9a1e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040082853s
May 31 13:24:12.437: INFO: Pod "downwardapi-volume-e3daf6e3-92b2-41d8-a077-7d1b039b9a1e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.049467264s
STEP: Saw pod success
May 31 13:24:12.438: INFO: Pod "downwardapi-volume-e3daf6e3-92b2-41d8-a077-7d1b039b9a1e" satisfied condition "Succeeded or Failed"
May 31 13:24:12.445: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod downwardapi-volume-e3daf6e3-92b2-41d8-a077-7d1b039b9a1e container client-container: <nil>
STEP: delete the pod
May 31 13:24:12.535: INFO: Waiting for pod downwardapi-volume-e3daf6e3-92b2-41d8-a077-7d1b039b9a1e to disappear
May 31 13:24:12.545: INFO: Pod downwardapi-volume-e3daf6e3-92b2-41d8-a077-7d1b039b9a1e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:24:12.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2811" for this suite.

• [SLOW TEST:6.248 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":61,"skipped":1340,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:24:12.568: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod test-webserver-94303429-c4d9-4746-a995-25f90f8aacf4 in namespace container-probe-7979
May 31 13:24:14.665: INFO: Started pod test-webserver-94303429-c4d9-4746-a995-25f90f8aacf4 in namespace container-probe-7979
STEP: checking the pod's current state and verifying that restartCount is present
May 31 13:24:14.672: INFO: Initial restart count of pod test-webserver-94303429-c4d9-4746-a995-25f90f8aacf4 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:28:16.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7979" for this suite.

• [SLOW TEST:243.896 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":62,"skipped":1368,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:28:16.466: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:28:16.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3188" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":63,"skipped":1392,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:28:16.699: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 31 13:28:17.658: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 31 13:28:19.690: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 31, 13, 28, 17, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 31, 13, 28, 17, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 31, 13, 28, 17, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 31, 13, 28, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 31 13:28:22.724: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:28:23.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4766" for this suite.
STEP: Destroying namespace "webhook-4766-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.415 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":346,"completed":64,"skipped":1417,"failed":0}
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:28:23.114: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 31 13:28:25.384: INFO: DNS probes using dns-1605/dns-test-d94edec4-beec-4548-81a2-c6e967e3c400 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:28:25.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1605" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":346,"completed":65,"skipped":1417,"failed":0}
SSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:28:25.449: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-f9b79 in namespace proxy-3404
I0531 13:28:25.546612      19 runners.go:193] Created replication controller with name: proxy-service-f9b79, namespace: proxy-3404, replica count: 1
I0531 13:28:26.599033      19 runners.go:193] proxy-service-f9b79 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0531 13:28:27.599370      19 runners.go:193] proxy-service-f9b79 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 31 13:28:27.612: INFO: setup took 2.093721065s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
May 31 13:28:27.641: INFO: (0) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:162/proxy/: bar (200; 28.686151ms)
May 31 13:28:27.641: INFO: (0) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:160/proxy/: foo (200; 28.445761ms)
May 31 13:28:27.641: INFO: (0) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:1080/proxy/rewriteme">... (200; 28.644499ms)
May 31 13:28:27.641: INFO: (0) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:160/proxy/: foo (200; 28.574886ms)
May 31 13:28:27.642: INFO: (0) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:1080/proxy/rewriteme">test<... (200; 29.307324ms)
May 31 13:28:27.642: INFO: (0) /api/v1/namespaces/proxy-3404/services/proxy-service-f9b79:portname1/proxy/: foo (200; 29.093366ms)
May 31 13:28:27.646: INFO: (0) /api/v1/namespaces/proxy-3404/services/http:proxy-service-f9b79:portname1/proxy/: foo (200; 33.535283ms)
May 31 13:28:27.650: INFO: (0) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r/proxy/rewriteme">test</a> (200; 37.477292ms)
May 31 13:28:27.652: INFO: (0) /api/v1/namespaces/proxy-3404/services/http:proxy-service-f9b79:portname2/proxy/: bar (200; 39.253529ms)
May 31 13:28:27.652: INFO: (0) /api/v1/namespaces/proxy-3404/services/proxy-service-f9b79:portname2/proxy/: bar (200; 39.764699ms)
May 31 13:28:27.652: INFO: (0) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:162/proxy/: bar (200; 39.436176ms)
May 31 13:28:27.655: INFO: (0) /api/v1/namespaces/proxy-3404/services/https:proxy-service-f9b79:tlsportname1/proxy/: tls baz (200; 42.47183ms)
May 31 13:28:27.655: INFO: (0) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:462/proxy/: tls qux (200; 42.186219ms)
May 31 13:28:27.659: INFO: (0) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:460/proxy/: tls baz (200; 47.443448ms)
May 31 13:28:27.660: INFO: (0) /api/v1/namespaces/proxy-3404/services/https:proxy-service-f9b79:tlsportname2/proxy/: tls qux (200; 47.342904ms)
May 31 13:28:27.729: INFO: (0) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:443/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:443/proxy/tlsrewritem... (200; 116.560631ms)
May 31 13:28:27.746: INFO: (1) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:160/proxy/: foo (200; 17.017351ms)
May 31 13:28:27.746: INFO: (1) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:1080/proxy/rewriteme">test<... (200; 15.540356ms)
May 31 13:28:27.746: INFO: (1) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:160/proxy/: foo (200; 16.050324ms)
May 31 13:28:27.746: INFO: (1) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:443/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:443/proxy/tlsrewritem... (200; 16.591175ms)
May 31 13:28:27.748: INFO: (1) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r/proxy/rewriteme">test</a> (200; 17.367484ms)
May 31 13:28:27.750: INFO: (1) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:1080/proxy/rewriteme">... (200; 19.695332ms)
May 31 13:28:27.750: INFO: (1) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:162/proxy/: bar (200; 19.072479ms)
May 31 13:28:27.750: INFO: (1) /api/v1/namespaces/proxy-3404/services/http:proxy-service-f9b79:portname2/proxy/: bar (200; 19.933791ms)
May 31 13:28:27.750: INFO: (1) /api/v1/namespaces/proxy-3404/services/https:proxy-service-f9b79:tlsportname1/proxy/: tls baz (200; 19.588358ms)
May 31 13:28:27.750: INFO: (1) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:462/proxy/: tls qux (200; 19.083439ms)
May 31 13:28:27.750: INFO: (1) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:460/proxy/: tls baz (200; 19.776436ms)
May 31 13:28:27.770: INFO: (1) /api/v1/namespaces/proxy-3404/services/https:proxy-service-f9b79:tlsportname2/proxy/: tls qux (200; 39.28516ms)
May 31 13:28:27.770: INFO: (1) /api/v1/namespaces/proxy-3404/services/proxy-service-f9b79:portname1/proxy/: foo (200; 39.858072ms)
May 31 13:28:27.770: INFO: (1) /api/v1/namespaces/proxy-3404/services/http:proxy-service-f9b79:portname1/proxy/: foo (200; 40.651812ms)
May 31 13:28:27.770: INFO: (1) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:162/proxy/: bar (200; 40.924832ms)
May 31 13:28:27.770: INFO: (1) /api/v1/namespaces/proxy-3404/services/proxy-service-f9b79:portname2/proxy/: bar (200; 39.457177ms)
May 31 13:28:27.785: INFO: (2) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:1080/proxy/rewriteme">test<... (200; 12.940198ms)
May 31 13:28:27.785: INFO: (2) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:162/proxy/: bar (200; 13.996918ms)
May 31 13:28:27.785: INFO: (2) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r/proxy/rewriteme">test</a> (200; 14.611261ms)
May 31 13:28:27.785: INFO: (2) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:160/proxy/: foo (200; 14.360821ms)
May 31 13:28:27.786: INFO: (2) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:462/proxy/: tls qux (200; 14.250987ms)
May 31 13:28:27.786: INFO: (2) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:460/proxy/: tls baz (200; 15.212343ms)
May 31 13:28:27.786: INFO: (2) /api/v1/namespaces/proxy-3404/services/https:proxy-service-f9b79:tlsportname1/proxy/: tls baz (200; 15.354898ms)
May 31 13:28:27.786: INFO: (2) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:160/proxy/: foo (200; 15.142481ms)
May 31 13:28:27.788: INFO: (2) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:1080/proxy/rewriteme">... (200; 15.825266ms)
May 31 13:28:27.797: INFO: (2) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:162/proxy/: bar (200; 24.873847ms)
May 31 13:28:27.797: INFO: (2) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:443/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:443/proxy/tlsrewritem... (200; 24.398259ms)
May 31 13:28:27.797: INFO: (2) /api/v1/namespaces/proxy-3404/services/https:proxy-service-f9b79:tlsportname2/proxy/: tls qux (200; 25.400317ms)
May 31 13:28:27.797: INFO: (2) /api/v1/namespaces/proxy-3404/services/http:proxy-service-f9b79:portname2/proxy/: bar (200; 24.791863ms)
May 31 13:28:27.797: INFO: (2) /api/v1/namespaces/proxy-3404/services/proxy-service-f9b79:portname1/proxy/: foo (200; 25.826483ms)
May 31 13:28:27.797: INFO: (2) /api/v1/namespaces/proxy-3404/services/http:proxy-service-f9b79:portname1/proxy/: foo (200; 25.009022ms)
May 31 13:28:27.841: INFO: (2) /api/v1/namespaces/proxy-3404/services/proxy-service-f9b79:portname2/proxy/: bar (200; 68.99508ms)
May 31 13:28:27.857: INFO: (3) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:162/proxy/: bar (200; 15.082539ms)
May 31 13:28:27.858: INFO: (3) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:1080/proxy/rewriteme">... (200; 15.166662ms)
May 31 13:28:27.858: INFO: (3) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r/proxy/rewriteme">test</a> (200; 16.654717ms)
May 31 13:28:27.858: INFO: (3) /api/v1/namespaces/proxy-3404/services/http:proxy-service-f9b79:portname1/proxy/: foo (200; 15.626029ms)
May 31 13:28:27.858: INFO: (3) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:460/proxy/: tls baz (200; 15.269845ms)
May 31 13:28:27.858: INFO: (3) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:443/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:443/proxy/tlsrewritem... (200; 15.407041ms)
May 31 13:28:27.858: INFO: (3) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:462/proxy/: tls qux (200; 16.397068ms)
May 31 13:28:27.858: INFO: (3) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:160/proxy/: foo (200; 16.648068ms)
May 31 13:28:27.860: INFO: (3) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:162/proxy/: bar (200; 17.390045ms)
May 31 13:28:27.860: INFO: (3) /api/v1/namespaces/proxy-3404/services/https:proxy-service-f9b79:tlsportname1/proxy/: tls baz (200; 19.548467ms)
May 31 13:28:27.860: INFO: (3) /api/v1/namespaces/proxy-3404/services/https:proxy-service-f9b79:tlsportname2/proxy/: tls qux (200; 18.413714ms)
May 31 13:28:27.860: INFO: (3) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:1080/proxy/rewriteme">test<... (200; 18.333531ms)
May 31 13:28:27.862: INFO: (3) /api/v1/namespaces/proxy-3404/services/proxy-service-f9b79:portname2/proxy/: bar (200; 19.927561ms)
May 31 13:28:27.864: INFO: (3) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:160/proxy/: foo (200; 22.578571ms)
May 31 13:28:27.864: INFO: (3) /api/v1/namespaces/proxy-3404/services/proxy-service-f9b79:portname1/proxy/: foo (200; 22.03473ms)
May 31 13:28:27.867: INFO: (3) /api/v1/namespaces/proxy-3404/services/http:proxy-service-f9b79:portname2/proxy/: bar (200; 24.92766ms)
May 31 13:28:27.889: INFO: (4) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:1080/proxy/rewriteme">... (200; 20.993901ms)
May 31 13:28:27.889: INFO: (4) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:160/proxy/: foo (200; 20.728971ms)
May 31 13:28:27.889: INFO: (4) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:462/proxy/: tls qux (200; 21.351834ms)
May 31 13:28:27.889: INFO: (4) /api/v1/namespaces/proxy-3404/services/http:proxy-service-f9b79:portname2/proxy/: bar (200; 20.487472ms)
May 31 13:28:27.889: INFO: (4) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:460/proxy/: tls baz (200; 20.96845ms)
May 31 13:28:27.889: INFO: (4) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:162/proxy/: bar (200; 20.682649ms)
May 31 13:28:27.889: INFO: (4) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r/proxy/rewriteme">test</a> (200; 20.890587ms)
May 31 13:28:27.889: INFO: (4) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:1080/proxy/rewriteme">test<... (200; 20.570845ms)
May 31 13:28:27.889: INFO: (4) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:160/proxy/: foo (200; 20.850245ms)
May 31 13:28:27.894: INFO: (4) /api/v1/namespaces/proxy-3404/services/https:proxy-service-f9b79:tlsportname1/proxy/: tls baz (200; 26.107743ms)
May 31 13:28:27.894: INFO: (4) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:443/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:443/proxy/tlsrewritem... (200; 26.480957ms)
May 31 13:28:27.894: INFO: (4) /api/v1/namespaces/proxy-3404/services/http:proxy-service-f9b79:portname1/proxy/: foo (200; 25.940507ms)
May 31 13:28:27.894: INFO: (4) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:162/proxy/: bar (200; 26.254748ms)
May 31 13:28:27.894: INFO: (4) /api/v1/namespaces/proxy-3404/services/https:proxy-service-f9b79:tlsportname2/proxy/: tls qux (200; 26.152895ms)
May 31 13:28:27.894: INFO: (4) /api/v1/namespaces/proxy-3404/services/proxy-service-f9b79:portname1/proxy/: foo (200; 26.738187ms)
May 31 13:28:27.894: INFO: (4) /api/v1/namespaces/proxy-3404/services/proxy-service-f9b79:portname2/proxy/: bar (200; 26.270799ms)
May 31 13:28:27.914: INFO: (5) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:460/proxy/: tls baz (200; 17.954007ms)
May 31 13:28:27.914: INFO: (5) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:443/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:443/proxy/tlsrewritem... (200; 18.33459ms)
May 31 13:28:27.914: INFO: (5) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:162/proxy/: bar (200; 17.695987ms)
May 31 13:28:27.914: INFO: (5) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:462/proxy/: tls qux (200; 18.321061ms)
May 31 13:28:27.914: INFO: (5) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:162/proxy/: bar (200; 17.931385ms)
May 31 13:28:27.915: INFO: (5) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r/proxy/rewriteme">test</a> (200; 18.275249ms)
May 31 13:28:27.915: INFO: (5) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:1080/proxy/rewriteme">... (200; 18.802778ms)
May 31 13:28:27.915: INFO: (5) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:160/proxy/: foo (200; 18.633152ms)
May 31 13:28:27.915: INFO: (5) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:1080/proxy/rewriteme">test<... (200; 18.962934ms)
May 31 13:28:27.921: INFO: (5) /api/v1/namespaces/proxy-3404/services/https:proxy-service-f9b79:tlsportname1/proxy/: tls baz (200; 24.39663ms)
May 31 13:28:27.921: INFO: (5) /api/v1/namespaces/proxy-3404/services/http:proxy-service-f9b79:portname1/proxy/: foo (200; 24.650819ms)
May 31 13:28:27.921: INFO: (5) /api/v1/namespaces/proxy-3404/services/https:proxy-service-f9b79:tlsportname2/proxy/: tls qux (200; 24.969671ms)
May 31 13:28:27.921: INFO: (5) /api/v1/namespaces/proxy-3404/services/proxy-service-f9b79:portname2/proxy/: bar (200; 24.560335ms)
May 31 13:28:27.921: INFO: (5) /api/v1/namespaces/proxy-3404/services/http:proxy-service-f9b79:portname2/proxy/: bar (200; 24.589717ms)
May 31 13:28:27.961: INFO: (5) /api/v1/namespaces/proxy-3404/services/proxy-service-f9b79:portname1/proxy/: foo (200; 64.889875ms)
May 31 13:28:27.970: INFO: (5) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:160/proxy/: foo (200; 73.742709ms)
May 31 13:28:27.995: INFO: (6) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r/proxy/rewriteme">test</a> (200; 24.291606ms)
May 31 13:28:27.996: INFO: (6) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:160/proxy/: foo (200; 23.588429ms)
May 31 13:28:28.000: INFO: (6) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:460/proxy/: tls baz (200; 28.027006ms)
May 31 13:28:28.000: INFO: (6) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:162/proxy/: bar (200; 28.586827ms)
May 31 13:28:28.002: INFO: (6) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:1080/proxy/rewriteme">test<... (200; 31.244087ms)
May 31 13:28:28.003: INFO: (6) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:160/proxy/: foo (200; 30.707157ms)
May 31 13:28:28.008: INFO: (6) /api/v1/namespaces/proxy-3404/services/proxy-service-f9b79:portname2/proxy/: bar (200; 36.650061ms)
May 31 13:28:28.008: INFO: (6) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:443/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:443/proxy/tlsrewritem... (200; 35.972405ms)
May 31 13:28:28.008: INFO: (6) /api/v1/namespaces/proxy-3404/services/proxy-service-f9b79:portname1/proxy/: foo (200; 36.452493ms)
May 31 13:28:28.008: INFO: (6) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:462/proxy/: tls qux (200; 35.675204ms)
May 31 13:28:28.009: INFO: (6) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:162/proxy/: bar (200; 36.517536ms)
May 31 13:28:28.012: INFO: (6) /api/v1/namespaces/proxy-3404/services/https:proxy-service-f9b79:tlsportname1/proxy/: tls baz (200; 40.228535ms)
May 31 13:28:28.012: INFO: (6) /api/v1/namespaces/proxy-3404/services/https:proxy-service-f9b79:tlsportname2/proxy/: tls qux (200; 41.350628ms)
May 31 13:28:28.014: INFO: (6) /api/v1/namespaces/proxy-3404/services/http:proxy-service-f9b79:portname2/proxy/: bar (200; 42.384027ms)
May 31 13:28:28.014: INFO: (6) /api/v1/namespaces/proxy-3404/services/http:proxy-service-f9b79:portname1/proxy/: foo (200; 41.899919ms)
May 31 13:28:28.014: INFO: (6) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:1080/proxy/rewriteme">... (200; 42.342415ms)
May 31 13:28:28.036: INFO: (7) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:462/proxy/: tls qux (200; 21.782951ms)
May 31 13:28:28.038: INFO: (7) /api/v1/namespaces/proxy-3404/services/http:proxy-service-f9b79:portname1/proxy/: foo (200; 21.883845ms)
May 31 13:28:28.038: INFO: (7) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r/proxy/rewriteme">test</a> (200; 21.343884ms)
May 31 13:28:28.039: INFO: (7) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:162/proxy/: bar (200; 24.211423ms)
May 31 13:28:28.039: INFO: (7) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:1080/proxy/rewriteme">test<... (200; 21.48413ms)
May 31 13:28:28.039: INFO: (7) /api/v1/namespaces/proxy-3404/services/https:proxy-service-f9b79:tlsportname1/proxy/: tls baz (200; 25.115326ms)
May 31 13:28:28.039: INFO: (7) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:443/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:443/proxy/tlsrewritem... (200; 23.652882ms)
May 31 13:28:28.039: INFO: (7) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:460/proxy/: tls baz (200; 23.539107ms)
May 31 13:28:28.040: INFO: (7) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:162/proxy/: bar (200; 22.269369ms)
May 31 13:28:28.040: INFO: (7) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:1080/proxy/rewriteme">... (200; 25.579474ms)
May 31 13:28:28.040: INFO: (7) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:160/proxy/: foo (200; 22.57188ms)
May 31 13:28:28.040: INFO: (7) /api/v1/namespaces/proxy-3404/services/https:proxy-service-f9b79:tlsportname2/proxy/: tls qux (200; 24.226413ms)
May 31 13:28:28.131: INFO: (7) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:160/proxy/: foo (200; 114.430981ms)
May 31 13:28:28.131: INFO: (7) /api/v1/namespaces/proxy-3404/services/http:proxy-service-f9b79:portname2/proxy/: bar (200; 114.825016ms)
May 31 13:28:28.131: INFO: (7) /api/v1/namespaces/proxy-3404/services/proxy-service-f9b79:portname2/proxy/: bar (200; 115.842794ms)
May 31 13:28:28.132: INFO: (7) /api/v1/namespaces/proxy-3404/services/proxy-service-f9b79:portname1/proxy/: foo (200; 116.547111ms)
May 31 13:28:28.169: INFO: (8) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:160/proxy/: foo (200; 32.65049ms)
May 31 13:28:28.169: INFO: (8) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:162/proxy/: bar (200; 29.592955ms)
May 31 13:28:28.169: INFO: (8) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r/proxy/rewriteme">test</a> (200; 30.128085ms)
May 31 13:28:28.169: INFO: (8) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:460/proxy/: tls baz (200; 31.759016ms)
May 31 13:28:28.169: INFO: (8) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:1080/proxy/rewriteme">... (200; 32.11142ms)
May 31 13:28:28.169: INFO: (8) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:1080/proxy/rewriteme">test<... (200; 26.898004ms)
May 31 13:28:28.169: INFO: (8) /api/v1/namespaces/proxy-3404/services/https:proxy-service-f9b79:tlsportname2/proxy/: tls qux (200; 29.440799ms)
May 31 13:28:28.183: INFO: (8) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:160/proxy/: foo (200; 44.271298ms)
May 31 13:28:28.184: INFO: (8) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:443/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:443/proxy/tlsrewritem... (200; 46.994341ms)
May 31 13:28:28.184: INFO: (8) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:462/proxy/: tls qux (200; 44.953794ms)
May 31 13:28:28.185: INFO: (8) /api/v1/namespaces/proxy-3404/services/https:proxy-service-f9b79:tlsportname1/proxy/: tls baz (200; 46.117997ms)
May 31 13:28:28.185: INFO: (8) /api/v1/namespaces/proxy-3404/services/proxy-service-f9b79:portname1/proxy/: foo (200; 46.441199ms)
May 31 13:28:28.188: INFO: (8) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:162/proxy/: bar (200; 46.635257ms)
May 31 13:28:28.174: INFO: (8) /api/v1/namespaces/proxy-3404/services/http:proxy-service-f9b79:portname1/proxy/: foo (200; 37.244552ms)
May 31 13:28:28.272: INFO: (8) /api/v1/namespaces/proxy-3404/services/http:proxy-service-f9b79:portname2/proxy/: bar (200; 135.659841ms)
May 31 13:28:28.273: INFO: (8) /api/v1/namespaces/proxy-3404/services/proxy-service-f9b79:portname2/proxy/: bar (200; 130.358491ms)
May 31 13:28:28.292: INFO: (9) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:162/proxy/: bar (200; 18.540098ms)
May 31 13:28:28.292: INFO: (9) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:160/proxy/: foo (200; 18.374452ms)
May 31 13:28:28.292: INFO: (9) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:162/proxy/: bar (200; 17.937846ms)
May 31 13:28:28.292: INFO: (9) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:462/proxy/: tls qux (200; 19.050568ms)
May 31 13:28:28.296: INFO: (9) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:1080/proxy/rewriteme">... (200; 22.377593ms)
May 31 13:28:28.297: INFO: (9) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r/proxy/rewriteme">test</a> (200; 23.599229ms)
May 31 13:28:28.297: INFO: (9) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:443/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:443/proxy/tlsrewritem... (200; 22.618153ms)
May 31 13:28:28.297: INFO: (9) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:160/proxy/: foo (200; 23.283777ms)
May 31 13:28:28.297: INFO: (9) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:460/proxy/: tls baz (200; 22.936014ms)
May 31 13:28:28.311: INFO: (9) /api/v1/namespaces/proxy-3404/services/https:proxy-service-f9b79:tlsportname1/proxy/: tls baz (200; 38.084225ms)
May 31 13:28:28.311: INFO: (9) /api/v1/namespaces/proxy-3404/services/proxy-service-f9b79:portname2/proxy/: bar (200; 37.603426ms)
May 31 13:28:28.311: INFO: (9) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:1080/proxy/rewriteme">test<... (200; 37.445611ms)
May 31 13:28:28.311: INFO: (9) /api/v1/namespaces/proxy-3404/services/http:proxy-service-f9b79:portname2/proxy/: bar (200; 36.767455ms)
May 31 13:28:28.312: INFO: (9) /api/v1/namespaces/proxy-3404/services/https:proxy-service-f9b79:tlsportname2/proxy/: tls qux (200; 37.504203ms)
May 31 13:28:28.397: INFO: (9) /api/v1/namespaces/proxy-3404/services/http:proxy-service-f9b79:portname1/proxy/: foo (200; 122.917862ms)
May 31 13:28:28.397: INFO: (9) /api/v1/namespaces/proxy-3404/services/proxy-service-f9b79:portname1/proxy/: foo (200; 123.360538ms)
May 31 13:28:28.417: INFO: (10) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:1080/proxy/rewriteme">... (200; 17.479769ms)
May 31 13:28:28.417: INFO: (10) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:462/proxy/: tls qux (200; 17.775099ms)
May 31 13:28:28.417: INFO: (10) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:160/proxy/: foo (200; 18.626252ms)
May 31 13:28:28.417: INFO: (10) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:162/proxy/: bar (200; 18.367372ms)
May 31 13:28:28.417: INFO: (10) /api/v1/namespaces/proxy-3404/services/https:proxy-service-f9b79:tlsportname2/proxy/: tls qux (200; 19.296417ms)
May 31 13:28:28.417: INFO: (10) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:160/proxy/: foo (200; 18.888172ms)
May 31 13:28:28.418: INFO: (10) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:460/proxy/: tls baz (200; 18.948464ms)
May 31 13:28:28.422: INFO: (10) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r/proxy/rewriteme">test</a> (200; 24.198992ms)
May 31 13:28:28.423: INFO: (10) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:1080/proxy/rewriteme">test<... (200; 23.683423ms)
May 31 13:28:28.423: INFO: (10) /api/v1/namespaces/proxy-3404/services/https:proxy-service-f9b79:tlsportname1/proxy/: tls baz (200; 24.817626ms)
May 31 13:28:28.423: INFO: (10) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:162/proxy/: bar (200; 23.319159ms)
May 31 13:28:28.423: INFO: (10) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:443/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:443/proxy/tlsrewritem... (200; 23.724674ms)
May 31 13:28:28.423: INFO: (10) /api/v1/namespaces/proxy-3404/services/http:proxy-service-f9b79:portname1/proxy/: foo (200; 24.41598ms)
May 31 13:28:28.423: INFO: (10) /api/v1/namespaces/proxy-3404/services/proxy-service-f9b79:portname1/proxy/: foo (200; 23.927502ms)
May 31 13:28:28.423: INFO: (10) /api/v1/namespaces/proxy-3404/services/http:proxy-service-f9b79:portname2/proxy/: bar (200; 24.391629ms)
May 31 13:28:28.436: INFO: (10) /api/v1/namespaces/proxy-3404/services/proxy-service-f9b79:portname2/proxy/: bar (200; 36.726404ms)
May 31 13:28:28.464: INFO: (11) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:160/proxy/: foo (200; 27.005967ms)
May 31 13:28:28.464: INFO: (11) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r/proxy/rewriteme">test</a> (200; 27.211434ms)
May 31 13:28:28.464: INFO: (11) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:160/proxy/: foo (200; 27.013017ms)
May 31 13:28:28.464: INFO: (11) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:1080/proxy/rewriteme">... (200; 27.623271ms)
May 31 13:28:28.465: INFO: (11) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:462/proxy/: tls qux (200; 28.38951ms)
May 31 13:28:28.465: INFO: (11) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:460/proxy/: tls baz (200; 27.86207ms)
May 31 13:28:28.465: INFO: (11) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:1080/proxy/rewriteme">test<... (200; 28.097238ms)
May 31 13:28:28.465: INFO: (11) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:443/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:443/proxy/tlsrewritem... (200; 28.303746ms)
May 31 13:28:28.509: INFO: (11) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:162/proxy/: bar (200; 72.099817ms)
May 31 13:28:28.509: INFO: (11) /api/v1/namespaces/proxy-3404/services/http:proxy-service-f9b79:portname2/proxy/: bar (200; 72.533483ms)
May 31 13:28:28.513: INFO: (11) /api/v1/namespaces/proxy-3404/services/http:proxy-service-f9b79:portname1/proxy/: foo (200; 76.008784ms)
May 31 13:28:28.513: INFO: (11) /api/v1/namespaces/proxy-3404/services/https:proxy-service-f9b79:tlsportname1/proxy/: tls baz (200; 75.935351ms)
May 31 13:28:28.513: INFO: (11) /api/v1/namespaces/proxy-3404/services/https:proxy-service-f9b79:tlsportname2/proxy/: tls qux (200; 76.233102ms)
May 31 13:28:28.513: INFO: (11) /api/v1/namespaces/proxy-3404/services/proxy-service-f9b79:portname1/proxy/: foo (200; 76.235252ms)
May 31 13:28:28.606: INFO: (11) /api/v1/namespaces/proxy-3404/services/proxy-service-f9b79:portname2/proxy/: bar (200; 169.683413ms)
May 31 13:28:28.606: INFO: (11) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:162/proxy/: bar (200; 168.938715ms)
May 31 13:28:28.627: INFO: (12) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:160/proxy/: foo (200; 19.670972ms)
May 31 13:28:28.628: INFO: (12) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:160/proxy/: foo (200; 20.813895ms)
May 31 13:28:28.628: INFO: (12) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:162/proxy/: bar (200; 19.789707ms)
May 31 13:28:28.628: INFO: (12) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:1080/proxy/rewriteme">... (200; 20.512253ms)
May 31 13:28:28.628: INFO: (12) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:1080/proxy/rewriteme">test<... (200; 19.338459ms)
May 31 13:28:28.639: INFO: (12) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:462/proxy/: tls qux (200; 30.970658ms)
May 31 13:28:28.644: INFO: (12) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r/proxy/rewriteme">test</a> (200; 36.142713ms)
May 31 13:28:28.644: INFO: (12) /api/v1/namespaces/proxy-3404/services/https:proxy-service-f9b79:tlsportname2/proxy/: tls qux (200; 35.755158ms)
May 31 13:28:28.644: INFO: (12) /api/v1/namespaces/proxy-3404/services/http:proxy-service-f9b79:portname1/proxy/: foo (200; 36.770927ms)
May 31 13:28:28.644: INFO: (12) /api/v1/namespaces/proxy-3404/services/proxy-service-f9b79:portname1/proxy/: foo (200; 36.08964ms)
May 31 13:28:28.644: INFO: (12) /api/v1/namespaces/proxy-3404/services/proxy-service-f9b79:portname2/proxy/: bar (200; 35.643913ms)
May 31 13:28:28.644: INFO: (12) /api/v1/namespaces/proxy-3404/services/https:proxy-service-f9b79:tlsportname1/proxy/: tls baz (200; 36.337499ms)
May 31 13:28:28.644: INFO: (12) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:162/proxy/: bar (200; 37.211653ms)
May 31 13:28:28.644: INFO: (12) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:443/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:443/proxy/tlsrewritem... (200; 36.843169ms)
May 31 13:28:28.644: INFO: (12) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:460/proxy/: tls baz (200; 36.762706ms)
May 31 13:28:28.647: INFO: (12) /api/v1/namespaces/proxy-3404/services/http:proxy-service-f9b79:portname2/proxy/: bar (200; 39.482869ms)
May 31 13:28:28.666: INFO: (13) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:1080/proxy/rewriteme">... (200; 18.626413ms)
May 31 13:28:28.666: INFO: (13) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:160/proxy/: foo (200; 18.794479ms)
May 31 13:28:28.666: INFO: (13) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:162/proxy/: bar (200; 18.614292ms)
May 31 13:28:28.666: INFO: (13) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:160/proxy/: foo (200; 18.807589ms)
May 31 13:28:28.666: INFO: (13) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:1080/proxy/rewriteme">test<... (200; 18.540849ms)
May 31 13:28:28.666: INFO: (13) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:462/proxy/: tls qux (200; 18.687274ms)
May 31 13:28:28.667: INFO: (13) /api/v1/namespaces/proxy-3404/services/http:proxy-service-f9b79:portname2/proxy/: bar (200; 19.800506ms)
May 31 13:28:28.667: INFO: (13) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:460/proxy/: tls baz (200; 19.738994ms)
May 31 13:28:28.667: INFO: (13) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:443/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:443/proxy/tlsrewritem... (200; 19.796296ms)
May 31 13:28:28.667: INFO: (13) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r/proxy/rewriteme">test</a> (200; 20.017135ms)
May 31 13:28:28.669: INFO: (13) /api/v1/namespaces/proxy-3404/services/https:proxy-service-f9b79:tlsportname1/proxy/: tls baz (200; 21.651346ms)
May 31 13:28:28.710: INFO: (13) /api/v1/namespaces/proxy-3404/services/proxy-service-f9b79:portname1/proxy/: foo (200; 62.474805ms)
May 31 13:28:28.710: INFO: (13) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:162/proxy/: bar (200; 63.073597ms)
May 31 13:28:28.710: INFO: (13) /api/v1/namespaces/proxy-3404/services/http:proxy-service-f9b79:portname1/proxy/: foo (200; 63.160551ms)
May 31 13:28:28.710: INFO: (13) /api/v1/namespaces/proxy-3404/services/https:proxy-service-f9b79:tlsportname2/proxy/: tls qux (200; 62.669542ms)
May 31 13:28:28.713: INFO: (13) /api/v1/namespaces/proxy-3404/services/proxy-service-f9b79:portname2/proxy/: bar (200; 65.424686ms)
May 31 13:28:28.740: INFO: (14) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:162/proxy/: bar (200; 27.105422ms)
May 31 13:28:28.741: INFO: (14) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r/proxy/rewriteme">test</a> (200; 27.268608ms)
May 31 13:28:28.741: INFO: (14) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:160/proxy/: foo (200; 27.412993ms)
May 31 13:28:28.741: INFO: (14) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:460/proxy/: tls baz (200; 27.428444ms)
May 31 13:28:28.743: INFO: (14) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:162/proxy/: bar (200; 29.539483ms)
May 31 13:28:28.747: INFO: (14) /api/v1/namespaces/proxy-3404/services/https:proxy-service-f9b79:tlsportname1/proxy/: tls baz (200; 33.883447ms)
May 31 13:28:28.747: INFO: (14) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:443/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:443/proxy/tlsrewritem... (200; 33.375758ms)
May 31 13:28:28.748: INFO: (14) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:160/proxy/: foo (200; 34.604864ms)
May 31 13:28:28.748: INFO: (14) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:1080/proxy/rewriteme">test<... (200; 34.840223ms)
May 31 13:28:28.750: INFO: (14) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:1080/proxy/rewriteme">... (200; 35.996786ms)
May 31 13:28:28.750: INFO: (14) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:462/proxy/: tls qux (200; 36.426483ms)
May 31 13:28:28.750: INFO: (14) /api/v1/namespaces/proxy-3404/services/https:proxy-service-f9b79:tlsportname2/proxy/: tls qux (200; 36.262757ms)
May 31 13:28:28.755: INFO: (14) /api/v1/namespaces/proxy-3404/services/proxy-service-f9b79:portname1/proxy/: foo (200; 41.245684ms)
May 31 13:28:28.755: INFO: (14) /api/v1/namespaces/proxy-3404/services/http:proxy-service-f9b79:portname2/proxy/: bar (200; 41.157321ms)
May 31 13:28:28.755: INFO: (14) /api/v1/namespaces/proxy-3404/services/proxy-service-f9b79:portname2/proxy/: bar (200; 41.265925ms)
May 31 13:28:28.755: INFO: (14) /api/v1/namespaces/proxy-3404/services/http:proxy-service-f9b79:portname1/proxy/: foo (200; 41.095029ms)
May 31 13:28:28.770: INFO: (15) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:162/proxy/: bar (200; 13.582852ms)
May 31 13:28:28.770: INFO: (15) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:460/proxy/: tls baz (200; 14.695724ms)
May 31 13:28:28.771: INFO: (15) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r/proxy/rewriteme">test</a> (200; 15.041957ms)
May 31 13:28:28.771: INFO: (15) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:462/proxy/: tls qux (200; 16.18612ms)
May 31 13:28:28.772: INFO: (15) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:162/proxy/: bar (200; 16.737641ms)
May 31 13:28:28.772: INFO: (15) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:160/proxy/: foo (200; 16.301284ms)
May 31 13:28:28.774: INFO: (15) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:1080/proxy/rewriteme">test<... (200; 18.418104ms)
May 31 13:28:28.774: INFO: (15) /api/v1/namespaces/proxy-3404/services/proxy-service-f9b79:portname1/proxy/: foo (200; 18.701135ms)
May 31 13:28:28.776: INFO: (15) /api/v1/namespaces/proxy-3404/services/https:proxy-service-f9b79:tlsportname2/proxy/: tls qux (200; 21.22334ms)
May 31 13:28:28.777: INFO: (15) /api/v1/namespaces/proxy-3404/services/https:proxy-service-f9b79:tlsportname1/proxy/: tls baz (200; 20.665109ms)
May 31 13:28:28.777: INFO: (15) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:1080/proxy/rewriteme">... (200; 21.022762ms)
May 31 13:28:28.777: INFO: (15) /api/v1/namespaces/proxy-3404/services/proxy-service-f9b79:portname2/proxy/: bar (200; 21.689208ms)
May 31 13:28:28.777: INFO: (15) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:443/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:443/proxy/tlsrewritem... (200; 21.171188ms)
May 31 13:28:28.780: INFO: (15) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:160/proxy/: foo (200; 24.003555ms)
May 31 13:28:28.816: INFO: (15) /api/v1/namespaces/proxy-3404/services/http:proxy-service-f9b79:portname1/proxy/: foo (200; 60.756089ms)
May 31 13:28:28.816: INFO: (15) /api/v1/namespaces/proxy-3404/services/http:proxy-service-f9b79:portname2/proxy/: bar (200; 60.683146ms)
May 31 13:28:28.834: INFO: (16) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:160/proxy/: foo (200; 17.481108ms)
May 31 13:28:28.834: INFO: (16) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:1080/proxy/rewriteme">test<... (200; 16.883027ms)
May 31 13:28:28.834: INFO: (16) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r/proxy/rewriteme">test</a> (200; 17.595683ms)
May 31 13:28:28.835: INFO: (16) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:160/proxy/: foo (200; 17.707677ms)
May 31 13:28:28.835: INFO: (16) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:1080/proxy/rewriteme">... (200; 17.320633ms)
May 31 13:28:28.835: INFO: (16) /api/v1/namespaces/proxy-3404/services/https:proxy-service-f9b79:tlsportname1/proxy/: tls baz (200; 18.729546ms)
May 31 13:28:28.836: INFO: (16) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:162/proxy/: bar (200; 17.968948ms)
May 31 13:28:28.836: INFO: (16) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:460/proxy/: tls baz (200; 17.811651ms)
May 31 13:28:28.836: INFO: (16) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:443/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:443/proxy/tlsrewritem... (200; 18.137084ms)
May 31 13:28:28.836: INFO: (16) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:462/proxy/: tls qux (200; 18.719125ms)
May 31 13:28:28.836: INFO: (16) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:162/proxy/: bar (200; 18.734836ms)
May 31 13:28:28.846: INFO: (16) /api/v1/namespaces/proxy-3404/services/http:proxy-service-f9b79:portname2/proxy/: bar (200; 28.810805ms)
May 31 13:28:28.846: INFO: (16) /api/v1/namespaces/proxy-3404/services/https:proxy-service-f9b79:tlsportname2/proxy/: tls qux (200; 28.90619ms)
May 31 13:28:28.846: INFO: (16) /api/v1/namespaces/proxy-3404/services/proxy-service-f9b79:portname1/proxy/: foo (200; 29.097886ms)
May 31 13:28:28.846: INFO: (16) /api/v1/namespaces/proxy-3404/services/http:proxy-service-f9b79:portname1/proxy/: foo (200; 28.746223ms)
May 31 13:28:28.846: INFO: (16) /api/v1/namespaces/proxy-3404/services/proxy-service-f9b79:portname2/proxy/: bar (200; 29.072226ms)
May 31 13:28:28.863: INFO: (17) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:1080/proxy/rewriteme">... (200; 14.610691ms)
May 31 13:28:28.863: INFO: (17) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:162/proxy/: bar (200; 15.91403ms)
May 31 13:28:28.863: INFO: (17) /api/v1/namespaces/proxy-3404/services/http:proxy-service-f9b79:portname2/proxy/: bar (200; 16.404168ms)
May 31 13:28:28.864: INFO: (17) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r/proxy/rewriteme">test</a> (200; 15.489293ms)
May 31 13:28:28.864: INFO: (17) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:160/proxy/: foo (200; 17.031231ms)
May 31 13:28:28.864: INFO: (17) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:1080/proxy/rewriteme">test<... (200; 16.157118ms)
May 31 13:28:28.867: INFO: (17) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:160/proxy/: foo (200; 18.661703ms)
May 31 13:28:28.867: INFO: (17) /api/v1/namespaces/proxy-3404/services/https:proxy-service-f9b79:tlsportname2/proxy/: tls qux (200; 18.569419ms)
May 31 13:28:28.867: INFO: (17) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:462/proxy/: tls qux (200; 19.194543ms)
May 31 13:28:28.867: INFO: (17) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:443/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:443/proxy/tlsrewritem... (200; 18.523888ms)
May 31 13:28:28.868: INFO: (17) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:460/proxy/: tls baz (200; 18.60441ms)
May 31 13:28:28.868: INFO: (17) /api/v1/namespaces/proxy-3404/services/https:proxy-service-f9b79:tlsportname1/proxy/: tls baz (200; 20.350527ms)
May 31 13:28:28.869: INFO: (17) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:162/proxy/: bar (200; 20.084357ms)
May 31 13:28:28.869: INFO: (17) /api/v1/namespaces/proxy-3404/services/proxy-service-f9b79:portname2/proxy/: bar (200; 20.635648ms)
May 31 13:28:28.872: INFO: (17) /api/v1/namespaces/proxy-3404/services/http:proxy-service-f9b79:portname1/proxy/: foo (200; 22.627562ms)
May 31 13:28:28.873: INFO: (17) /api/v1/namespaces/proxy-3404/services/proxy-service-f9b79:portname1/proxy/: foo (200; 24.723152ms)
May 31 13:28:28.896: INFO: (18) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:162/proxy/: bar (200; 21.983168ms)
May 31 13:28:28.896: INFO: (18) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r/proxy/rewriteme">test</a> (200; 22.397464ms)
May 31 13:28:28.897: INFO: (18) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:462/proxy/: tls qux (200; 22.990707ms)
May 31 13:28:28.897: INFO: (18) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:160/proxy/: foo (200; 23.132532ms)
May 31 13:28:28.897: INFO: (18) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:1080/proxy/rewriteme">test<... (200; 22.926543ms)
May 31 13:28:28.897: INFO: (18) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:460/proxy/: tls baz (200; 23.509316ms)
May 31 13:28:28.899: INFO: (18) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:1080/proxy/rewriteme">... (200; 24.963941ms)
May 31 13:28:28.899: INFO: (18) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:162/proxy/: bar (200; 25.548713ms)
May 31 13:28:28.900: INFO: (18) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:160/proxy/: foo (200; 26.087943ms)
May 31 13:28:28.900: INFO: (18) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:443/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:443/proxy/tlsrewritem... (200; 25.521892ms)
May 31 13:28:28.900: INFO: (18) /api/v1/namespaces/proxy-3404/services/https:proxy-service-f9b79:tlsportname1/proxy/: tls baz (200; 26.873373ms)
May 31 13:28:28.902: INFO: (18) /api/v1/namespaces/proxy-3404/services/https:proxy-service-f9b79:tlsportname2/proxy/: tls qux (200; 28.096059ms)
May 31 13:28:28.905: INFO: (18) /api/v1/namespaces/proxy-3404/services/http:proxy-service-f9b79:portname2/proxy/: bar (200; 30.949866ms)
May 31 13:28:28.906: INFO: (18) /api/v1/namespaces/proxy-3404/services/http:proxy-service-f9b79:portname1/proxy/: foo (200; 31.764867ms)
May 31 13:28:28.909: INFO: (18) /api/v1/namespaces/proxy-3404/services/proxy-service-f9b79:portname2/proxy/: bar (200; 35.0399ms)
May 31 13:28:28.909: INFO: (18) /api/v1/namespaces/proxy-3404/services/proxy-service-f9b79:portname1/proxy/: foo (200; 34.706237ms)
May 31 13:28:28.937: INFO: (19) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:1080/proxy/rewriteme">... (200; 25.554723ms)
May 31 13:28:28.937: INFO: (19) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:1080/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:1080/proxy/rewriteme">test<... (200; 24.484112ms)
May 31 13:28:28.937: INFO: (19) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:160/proxy/: foo (200; 26.165245ms)
May 31 13:28:28.937: INFO: (19) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r:162/proxy/: bar (200; 26.397645ms)
May 31 13:28:28.937: INFO: (19) /api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/proxy-service-f9b79-t4p6r/proxy/rewriteme">test</a> (200; 25.335094ms)
May 31 13:28:28.938: INFO: (19) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:162/proxy/: bar (200; 25.589584ms)
May 31 13:28:28.938: INFO: (19) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:443/proxy/: <a href="/api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:443/proxy/tlsrewritem... (200; 26.274709ms)
May 31 13:28:28.938: INFO: (19) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:462/proxy/: tls qux (200; 25.541862ms)
May 31 13:28:28.939: INFO: (19) /api/v1/namespaces/proxy-3404/services/https:proxy-service-f9b79:tlsportname2/proxy/: tls qux (200; 26.151685ms)
May 31 13:28:28.941: INFO: (19) /api/v1/namespaces/proxy-3404/services/http:proxy-service-f9b79:portname1/proxy/: foo (200; 29.168579ms)
May 31 13:28:28.941: INFO: (19) /api/v1/namespaces/proxy-3404/pods/https:proxy-service-f9b79-t4p6r:460/proxy/: tls baz (200; 29.098746ms)
May 31 13:28:28.945: INFO: (19) /api/v1/namespaces/proxy-3404/services/https:proxy-service-f9b79:tlsportname1/proxy/: tls baz (200; 32.758404ms)
May 31 13:28:28.945: INFO: (19) /api/v1/namespaces/proxy-3404/services/proxy-service-f9b79:portname2/proxy/: bar (200; 32.13813ms)
May 31 13:28:28.945: INFO: (19) /api/v1/namespaces/proxy-3404/pods/http:proxy-service-f9b79-t4p6r:160/proxy/: foo (200; 32.696661ms)
May 31 13:28:28.945: INFO: (19) /api/v1/namespaces/proxy-3404/services/http:proxy-service-f9b79:portname2/proxy/: bar (200; 33.583605ms)
May 31 13:28:28.948: INFO: (19) /api/v1/namespaces/proxy-3404/services/proxy-service-f9b79:portname1/proxy/: foo (200; 35.629682ms)
STEP: deleting ReplicationController proxy-service-f9b79 in namespace proxy-3404, will wait for the garbage collector to delete the pods
May 31 13:28:29.031: INFO: Deleting ReplicationController proxy-service-f9b79 took: 19.959462ms
May 31 13:28:29.132: INFO: Terminating ReplicationController proxy-service-f9b79 pods took: 101.048798ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:28:31.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3404" for this suite.

• [SLOW TEST:5.715 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":346,"completed":66,"skipped":1424,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:28:31.164: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0531 13:29:11.417193      19 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
May 31 13:29:11.417: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

May 31 13:29:11.417: INFO: Deleting pod "simpletest.rc-292rz" in namespace "gc-4566"
May 31 13:29:11.449: INFO: Deleting pod "simpletest.rc-29v2j" in namespace "gc-4566"
May 31 13:29:11.471: INFO: Deleting pod "simpletest.rc-2g99m" in namespace "gc-4566"
May 31 13:29:11.489: INFO: Deleting pod "simpletest.rc-2pgz4" in namespace "gc-4566"
May 31 13:29:11.513: INFO: Deleting pod "simpletest.rc-2r4xb" in namespace "gc-4566"
May 31 13:29:11.541: INFO: Deleting pod "simpletest.rc-2vfr9" in namespace "gc-4566"
May 31 13:29:11.601: INFO: Deleting pod "simpletest.rc-4hlk4" in namespace "gc-4566"
May 31 13:29:11.631: INFO: Deleting pod "simpletest.rc-5jhfn" in namespace "gc-4566"
May 31 13:29:11.664: INFO: Deleting pod "simpletest.rc-5wjtb" in namespace "gc-4566"
May 31 13:29:11.769: INFO: Deleting pod "simpletest.rc-66rx9" in namespace "gc-4566"
May 31 13:29:11.798: INFO: Deleting pod "simpletest.rc-687zk" in namespace "gc-4566"
May 31 13:29:11.830: INFO: Deleting pod "simpletest.rc-69nv8" in namespace "gc-4566"
May 31 13:29:11.860: INFO: Deleting pod "simpletest.rc-6qclt" in namespace "gc-4566"
May 31 13:29:11.922: INFO: Deleting pod "simpletest.rc-74tnq" in namespace "gc-4566"
May 31 13:29:11.952: INFO: Deleting pod "simpletest.rc-7kv5q" in namespace "gc-4566"
May 31 13:29:11.990: INFO: Deleting pod "simpletest.rc-7sxlg" in namespace "gc-4566"
May 31 13:29:12.008: INFO: Deleting pod "simpletest.rc-87sqq" in namespace "gc-4566"
May 31 13:29:12.044: INFO: Deleting pod "simpletest.rc-8l5j2" in namespace "gc-4566"
May 31 13:29:12.193: INFO: Deleting pod "simpletest.rc-8sc7l" in namespace "gc-4566"
May 31 13:29:12.221: INFO: Deleting pod "simpletest.rc-8v9k6" in namespace "gc-4566"
May 31 13:29:12.238: INFO: Deleting pod "simpletest.rc-8zbjj" in namespace "gc-4566"
May 31 13:29:12.270: INFO: Deleting pod "simpletest.rc-9cfw8" in namespace "gc-4566"
May 31 13:29:12.297: INFO: Deleting pod "simpletest.rc-9qfjv" in namespace "gc-4566"
May 31 13:29:12.317: INFO: Deleting pod "simpletest.rc-bbwgz" in namespace "gc-4566"
May 31 13:29:12.334: INFO: Deleting pod "simpletest.rc-c7pl7" in namespace "gc-4566"
May 31 13:29:12.355: INFO: Deleting pod "simpletest.rc-c9bk5" in namespace "gc-4566"
May 31 13:29:12.396: INFO: Deleting pod "simpletest.rc-cd6dj" in namespace "gc-4566"
May 31 13:29:12.434: INFO: Deleting pod "simpletest.rc-cp4wn" in namespace "gc-4566"
May 31 13:29:12.466: INFO: Deleting pod "simpletest.rc-d2g8r" in namespace "gc-4566"
May 31 13:29:12.510: INFO: Deleting pod "simpletest.rc-d5qlz" in namespace "gc-4566"
May 31 13:29:12.540: INFO: Deleting pod "simpletest.rc-dt4lh" in namespace "gc-4566"
May 31 13:29:12.561: INFO: Deleting pod "simpletest.rc-dvh6r" in namespace "gc-4566"
May 31 13:29:12.586: INFO: Deleting pod "simpletest.rc-dwnng" in namespace "gc-4566"
May 31 13:29:12.624: INFO: Deleting pod "simpletest.rc-dx9sp" in namespace "gc-4566"
May 31 13:29:12.662: INFO: Deleting pod "simpletest.rc-fhx7w" in namespace "gc-4566"
May 31 13:29:12.688: INFO: Deleting pod "simpletest.rc-fmnns" in namespace "gc-4566"
May 31 13:29:12.729: INFO: Deleting pod "simpletest.rc-fwrbk" in namespace "gc-4566"
May 31 13:29:12.755: INFO: Deleting pod "simpletest.rc-g62pw" in namespace "gc-4566"
May 31 13:29:12.776: INFO: Deleting pod "simpletest.rc-gq7rp" in namespace "gc-4566"
May 31 13:29:12.794: INFO: Deleting pod "simpletest.rc-h5m59" in namespace "gc-4566"
May 31 13:29:12.848: INFO: Deleting pod "simpletest.rc-hjwzq" in namespace "gc-4566"
May 31 13:29:12.878: INFO: Deleting pod "simpletest.rc-hlt9s" in namespace "gc-4566"
May 31 13:29:12.895: INFO: Deleting pod "simpletest.rc-hrnn6" in namespace "gc-4566"
May 31 13:29:12.914: INFO: Deleting pod "simpletest.rc-jdf5s" in namespace "gc-4566"
May 31 13:29:12.944: INFO: Deleting pod "simpletest.rc-jm5xx" in namespace "gc-4566"
May 31 13:29:12.968: INFO: Deleting pod "simpletest.rc-jmjml" in namespace "gc-4566"
May 31 13:29:13.007: INFO: Deleting pod "simpletest.rc-kcksm" in namespace "gc-4566"
May 31 13:29:13.035: INFO: Deleting pod "simpletest.rc-kcxbf" in namespace "gc-4566"
May 31 13:29:13.054: INFO: Deleting pod "simpletest.rc-kd2zz" in namespace "gc-4566"
May 31 13:29:13.095: INFO: Deleting pod "simpletest.rc-kp8lc" in namespace "gc-4566"
May 31 13:29:13.152: INFO: Deleting pod "simpletest.rc-krq22" in namespace "gc-4566"
May 31 13:29:13.201: INFO: Deleting pod "simpletest.rc-kschb" in namespace "gc-4566"
May 31 13:29:13.234: INFO: Deleting pod "simpletest.rc-lcbvb" in namespace "gc-4566"
May 31 13:29:13.262: INFO: Deleting pod "simpletest.rc-lg7mc" in namespace "gc-4566"
May 31 13:29:13.281: INFO: Deleting pod "simpletest.rc-lh878" in namespace "gc-4566"
May 31 13:29:13.310: INFO: Deleting pod "simpletest.rc-lhwkw" in namespace "gc-4566"
May 31 13:29:13.360: INFO: Deleting pod "simpletest.rc-llkfb" in namespace "gc-4566"
May 31 13:29:13.383: INFO: Deleting pod "simpletest.rc-lvvls" in namespace "gc-4566"
May 31 13:29:13.404: INFO: Deleting pod "simpletest.rc-m65jf" in namespace "gc-4566"
May 31 13:29:13.442: INFO: Deleting pod "simpletest.rc-mclr9" in namespace "gc-4566"
May 31 13:29:13.475: INFO: Deleting pod "simpletest.rc-mcncq" in namespace "gc-4566"
May 31 13:29:13.510: INFO: Deleting pod "simpletest.rc-mgjck" in namespace "gc-4566"
May 31 13:29:13.563: INFO: Deleting pod "simpletest.rc-mjp5x" in namespace "gc-4566"
May 31 13:29:13.589: INFO: Deleting pod "simpletest.rc-mpd85" in namespace "gc-4566"
May 31 13:29:13.626: INFO: Deleting pod "simpletest.rc-np29b" in namespace "gc-4566"
May 31 13:29:13.642: INFO: Deleting pod "simpletest.rc-nt7qw" in namespace "gc-4566"
May 31 13:29:13.657: INFO: Deleting pod "simpletest.rc-ntbgj" in namespace "gc-4566"
May 31 13:29:13.673: INFO: Deleting pod "simpletest.rc-pn7wc" in namespace "gc-4566"
May 31 13:29:13.709: INFO: Deleting pod "simpletest.rc-q4bb9" in namespace "gc-4566"
May 31 13:29:13.740: INFO: Deleting pod "simpletest.rc-qdb4z" in namespace "gc-4566"
May 31 13:29:13.762: INFO: Deleting pod "simpletest.rc-r8q5r" in namespace "gc-4566"
May 31 13:29:13.780: INFO: Deleting pod "simpletest.rc-rgnx6" in namespace "gc-4566"
May 31 13:29:13.796: INFO: Deleting pod "simpletest.rc-rnbr6" in namespace "gc-4566"
May 31 13:29:13.819: INFO: Deleting pod "simpletest.rc-rzmh7" in namespace "gc-4566"
May 31 13:29:13.854: INFO: Deleting pod "simpletest.rc-s2zbf" in namespace "gc-4566"
May 31 13:29:13.886: INFO: Deleting pod "simpletest.rc-s8gz5" in namespace "gc-4566"
May 31 13:29:13.912: INFO: Deleting pod "simpletest.rc-sk77n" in namespace "gc-4566"
May 31 13:29:13.957: INFO: Deleting pod "simpletest.rc-sn5fx" in namespace "gc-4566"
May 31 13:29:13.986: INFO: Deleting pod "simpletest.rc-svxg4" in namespace "gc-4566"
May 31 13:29:14.009: INFO: Deleting pod "simpletest.rc-tfdrx" in namespace "gc-4566"
May 31 13:29:14.046: INFO: Deleting pod "simpletest.rc-tfprx" in namespace "gc-4566"
May 31 13:29:14.066: INFO: Deleting pod "simpletest.rc-tmvdf" in namespace "gc-4566"
May 31 13:29:14.094: INFO: Deleting pod "simpletest.rc-tqzcn" in namespace "gc-4566"
May 31 13:29:14.120: INFO: Deleting pod "simpletest.rc-v6m9j" in namespace "gc-4566"
May 31 13:29:14.149: INFO: Deleting pod "simpletest.rc-vf4f8" in namespace "gc-4566"
May 31 13:29:14.166: INFO: Deleting pod "simpletest.rc-vj5dp" in namespace "gc-4566"
May 31 13:29:14.185: INFO: Deleting pod "simpletest.rc-w9shg" in namespace "gc-4566"
May 31 13:29:14.200: INFO: Deleting pod "simpletest.rc-wf6bb" in namespace "gc-4566"
May 31 13:29:14.214: INFO: Deleting pod "simpletest.rc-wpdmm" in namespace "gc-4566"
May 31 13:29:14.232: INFO: Deleting pod "simpletest.rc-wqzzl" in namespace "gc-4566"
May 31 13:29:14.254: INFO: Deleting pod "simpletest.rc-xf759" in namespace "gc-4566"
May 31 13:29:14.276: INFO: Deleting pod "simpletest.rc-xvnmt" in namespace "gc-4566"
May 31 13:29:14.298: INFO: Deleting pod "simpletest.rc-z4gpf" in namespace "gc-4566"
May 31 13:29:14.324: INFO: Deleting pod "simpletest.rc-z62bh" in namespace "gc-4566"
May 31 13:29:14.363: INFO: Deleting pod "simpletest.rc-z7v6f" in namespace "gc-4566"
May 31 13:29:14.378: INFO: Deleting pod "simpletest.rc-z96ph" in namespace "gc-4566"
May 31 13:29:14.422: INFO: Deleting pod "simpletest.rc-zh7tt" in namespace "gc-4566"
May 31 13:29:14.444: INFO: Deleting pod "simpletest.rc-zjdqd" in namespace "gc-4566"
May 31 13:29:14.470: INFO: Deleting pod "simpletest.rc-zvhpn" in namespace "gc-4566"
May 31 13:29:14.492: INFO: Deleting pod "simpletest.rc-zxlbk" in namespace "gc-4566"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:29:14.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4566" for this suite.

• [SLOW TEST:43.382 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":346,"completed":67,"skipped":1433,"failed":0}
S
------------------------------
[sig-network] EndpointSlice 
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:29:14.549: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: referencing a single matching pod
STEP: referencing matching pods with named port
STEP: creating empty Endpoints and EndpointSlices for no matching Pods
STEP: recreating EndpointSlices after they've been deleted
May 31 13:29:34.830: INFO: EndpointSlice for Service endpointslice-9761/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:29:44.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-9761" for this suite.

• [SLOW TEST:30.331 seconds]
[sig-network] EndpointSlice
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","total":346,"completed":68,"skipped":1434,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:29:44.884: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a replication controller
May 31 13:29:44.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-4691 create -f -'
May 31 13:29:45.608: INFO: stderr: ""
May 31 13:29:45.608: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 31 13:29:45.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-4691 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 31 13:29:45.769: INFO: stderr: ""
May 31 13:29:45.769: INFO: stdout: "update-demo-nautilus-cvd5f update-demo-nautilus-d2f8x "
May 31 13:29:45.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-4691 get pods update-demo-nautilus-cvd5f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 31 13:29:45.876: INFO: stderr: ""
May 31 13:29:45.876: INFO: stdout: ""
May 31 13:29:45.876: INFO: update-demo-nautilus-cvd5f is created but not running
May 31 13:29:50.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-4691 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 31 13:29:51.224: INFO: stderr: ""
May 31 13:29:51.224: INFO: stdout: "update-demo-nautilus-cvd5f update-demo-nautilus-d2f8x "
May 31 13:29:51.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-4691 get pods update-demo-nautilus-cvd5f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 31 13:29:51.434: INFO: stderr: ""
May 31 13:29:51.434: INFO: stdout: "true"
May 31 13:29:51.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-4691 get pods update-demo-nautilus-cvd5f -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 31 13:29:51.598: INFO: stderr: ""
May 31 13:29:51.598: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
May 31 13:29:51.598: INFO: validating pod update-demo-nautilus-cvd5f
May 31 13:29:51.702: INFO: got data: {
  "image": "nautilus.jpg"
}

May 31 13:29:51.702: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 31 13:29:51.702: INFO: update-demo-nautilus-cvd5f is verified up and running
May 31 13:29:51.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-4691 get pods update-demo-nautilus-d2f8x -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 31 13:29:51.817: INFO: stderr: ""
May 31 13:29:51.817: INFO: stdout: "true"
May 31 13:29:51.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-4691 get pods update-demo-nautilus-d2f8x -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 31 13:29:51.911: INFO: stderr: ""
May 31 13:29:51.911: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
May 31 13:29:51.911: INFO: validating pod update-demo-nautilus-d2f8x
May 31 13:29:51.972: INFO: got data: {
  "image": "nautilus.jpg"
}

May 31 13:29:51.972: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 31 13:29:51.972: INFO: update-demo-nautilus-d2f8x is verified up and running
STEP: using delete to clean up resources
May 31 13:29:51.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-4691 delete --grace-period=0 --force -f -'
May 31 13:29:52.077: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 31 13:29:52.077: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 31 13:29:52.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-4691 get rc,svc -l name=update-demo --no-headers'
May 31 13:29:52.243: INFO: stderr: "No resources found in kubectl-4691 namespace.\n"
May 31 13:29:52.243: INFO: stdout: ""
May 31 13:29:52.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-4691 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 31 13:29:52.438: INFO: stderr: ""
May 31 13:29:52.438: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:29:52.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4691" for this suite.

• [SLOW TEST:7.603 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":346,"completed":69,"skipped":1458,"failed":0}
SSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:29:52.487: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:29:56.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7388" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":346,"completed":70,"skipped":1461,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:29:56.239: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
May 31 13:29:56.284: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 31 13:29:56.308: INFO: Waiting for terminating namespaces to be deleted...
May 31 13:29:56.315: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-11-247.eu-central-1.compute.internal before test
May 31 13:29:56.336: INFO: calico-kube-controllers-786b7976d6-qvmrx from kube-system started at 2022-05-31 12:59:25 +0000 UTC (1 container statuses recorded)
May 31 13:29:56.337: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May 31 13:29:56.337: INFO: canal-jkvwq from kube-system started at 2022-05-31 12:58:35 +0000 UTC (2 container statuses recorded)
May 31 13:29:56.337: INFO: 	Container calico-node ready: true, restart count 0
May 31 13:29:56.338: INFO: 	Container kube-flannel ready: true, restart count 0
May 31 13:29:56.338: INFO: coredns-767874cf84-bl7km from kube-system started at 2022-05-31 12:59:25 +0000 UTC (1 container statuses recorded)
May 31 13:29:56.338: INFO: 	Container coredns ready: true, restart count 0
May 31 13:29:56.338: INFO: coredns-767874cf84-wsllw from kube-system started at 2022-05-31 12:59:25 +0000 UTC (1 container statuses recorded)
May 31 13:29:56.338: INFO: 	Container coredns ready: true, restart count 0
May 31 13:29:56.338: INFO: kube-proxy-7tkx8 from kube-system started at 2022-05-31 12:58:35 +0000 UTC (1 container statuses recorded)
May 31 13:29:56.338: INFO: 	Container kube-proxy ready: true, restart count 0
May 31 13:29:56.338: INFO: node-local-dns-2f4s9 from kube-system started at 2022-05-31 12:58:35 +0000 UTC (1 container statuses recorded)
May 31 13:29:56.338: INFO: 	Container node-cache ready: true, restart count 0
May 31 13:29:56.338: INFO: openvpn-client-76b67b68f8-rllqf from kube-system started at 2022-05-31 12:59:25 +0000 UTC (2 container statuses recorded)
May 31 13:29:56.338: INFO: 	Container dnat-controller ready: true, restart count 0
May 31 13:29:56.338: INFO: 	Container openvpn-client ready: true, restart count 0
May 31 13:29:56.338: INFO: user-ssh-keys-agent-79f9n from kube-system started at 2022-05-31 12:58:35 +0000 UTC (1 container statuses recorded)
May 31 13:29:56.338: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
May 31 13:29:56.338: INFO: dashboard-metrics-scraper-75d68f84c9-78rk7 from kubernetes-dashboard started at 2022-05-31 12:59:25 +0000 UTC (1 container statuses recorded)
May 31 13:29:56.338: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
May 31 13:29:56.338: INFO: dashboard-metrics-scraper-75d68f84c9-d4cl8 from kubernetes-dashboard started at 2022-05-31 12:59:25 +0000 UTC (1 container statuses recorded)
May 31 13:29:56.339: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
May 31 13:29:56.339: INFO: sonobuoy-systemd-logs-daemon-set-fb59706c47b44a31-jzqd2 from sonobuoy started at 2022-05-31 13:07:09 +0000 UTC (2 container statuses recorded)
May 31 13:29:56.339: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 31 13:29:56.339: INFO: 	Container systemd-logs ready: true, restart count 0
May 31 13:29:56.339: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-3-160.eu-central-1.compute.internal before test
May 31 13:29:56.352: INFO: canal-ddb54 from kube-system started at 2022-05-31 12:59:30 +0000 UTC (2 container statuses recorded)
May 31 13:29:56.352: INFO: 	Container calico-node ready: true, restart count 0
May 31 13:29:56.352: INFO: 	Container kube-flannel ready: true, restart count 0
May 31 13:29:56.352: INFO: kube-proxy-5w228 from kube-system started at 2022-05-31 12:59:30 +0000 UTC (1 container statuses recorded)
May 31 13:29:56.352: INFO: 	Container kube-proxy ready: true, restart count 0
May 31 13:29:56.352: INFO: node-local-dns-bnwqt from kube-system started at 2022-05-31 12:59:30 +0000 UTC (1 container statuses recorded)
May 31 13:29:56.352: INFO: 	Container node-cache ready: true, restart count 0
May 31 13:29:56.352: INFO: user-ssh-keys-agent-msrlk from kube-system started at 2022-05-31 12:59:30 +0000 UTC (1 container statuses recorded)
May 31 13:29:56.352: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
May 31 13:29:56.352: INFO: rc-test-g48w2 from replication-controller-7388 started at 2022-05-31 13:29:54 +0000 UTC (1 container statuses recorded)
May 31 13:29:56.352: INFO: 	Container rc-test ready: true, restart count 0
May 31 13:29:56.352: INFO: sonobuoy-e2e-job-701195ac2dd44242 from sonobuoy started at 2022-05-31 13:07:09 +0000 UTC (2 container statuses recorded)
May 31 13:29:56.352: INFO: 	Container e2e ready: true, restart count 0
May 31 13:29:56.352: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 31 13:29:56.352: INFO: sonobuoy-systemd-logs-daemon-set-fb59706c47b44a31-6t75j from sonobuoy started at 2022-05-31 13:07:09 +0000 UTC (2 container statuses recorded)
May 31 13:29:56.352: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 31 13:29:56.352: INFO: 	Container systemd-logs ready: true, restart count 0
May 31 13:29:56.352: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-8-180.eu-central-1.compute.internal before test
May 31 13:29:56.364: INFO: canal-5bcsn from kube-system started at 2022-05-31 12:58:46 +0000 UTC (2 container statuses recorded)
May 31 13:29:56.364: INFO: 	Container calico-node ready: true, restart count 0
May 31 13:29:56.364: INFO: 	Container kube-flannel ready: true, restart count 0
May 31 13:29:56.364: INFO: kube-proxy-78w8t from kube-system started at 2022-05-31 12:58:46 +0000 UTC (1 container statuses recorded)
May 31 13:29:56.364: INFO: 	Container kube-proxy ready: true, restart count 0
May 31 13:29:56.364: INFO: node-local-dns-l65lm from kube-system started at 2022-05-31 12:58:46 +0000 UTC (1 container statuses recorded)
May 31 13:29:56.364: INFO: 	Container node-cache ready: true, restart count 0
May 31 13:29:56.364: INFO: user-ssh-keys-agent-dztdn from kube-system started at 2022-05-31 12:58:46 +0000 UTC (1 container statuses recorded)
May 31 13:29:56.364: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
May 31 13:29:56.364: INFO: rc-test-kwvdl from replication-controller-7388 started at 2022-05-31 13:29:52 +0000 UTC (1 container statuses recorded)
May 31 13:29:56.364: INFO: 	Container rc-test ready: true, restart count 0
May 31 13:29:56.364: INFO: sonobuoy from sonobuoy started at 2022-05-31 13:07:02 +0000 UTC (1 container statuses recorded)
May 31 13:29:56.364: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 31 13:29:56.364: INFO: sonobuoy-systemd-logs-daemon-set-fb59706c47b44a31-qx4ld from sonobuoy started at 2022-05-31 13:07:09 +0000 UTC (2 container statuses recorded)
May 31 13:29:56.364: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 31 13:29:56.364: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16f433b085c8b8b9], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:29:57.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2243" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":346,"completed":71,"skipped":1480,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:29:57.472: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 13:29:57.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-2979 version'
May 31 13:29:57.691: INFO: stderr: ""
May 31 13:29:57.691: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.6\", GitCommit:\"ad3338546da947756e8a88aa6822e9c11e7eac22\", GitTreeState:\"clean\", BuildDate:\"2022-04-14T08:49:13Z\", GoVersion:\"go1.17.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.6\", GitCommit:\"ad3338546da947756e8a88aa6822e9c11e7eac22\", GitTreeState:\"clean\", BuildDate:\"2022-04-14T08:43:11Z\", GoVersion:\"go1.17.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:29:57.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2979" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":346,"completed":72,"skipped":1496,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:29:57.712: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-6139
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-6139
STEP: creating replication controller externalsvc in namespace services-6139
I0531 13:29:57.816025      19 runners.go:193] Created replication controller with name: externalsvc, namespace: services-6139, replica count: 2
I0531 13:30:00.872290      19 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
May 31 13:30:00.924: INFO: Creating new exec pod
May 31 13:30:04.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-6139 exec execpodvmtjt -- /bin/sh -x -c nslookup clusterip-service.services-6139.svc.cluster.local'
May 31 13:30:05.451: INFO: stderr: "+ nslookup clusterip-service.services-6139.svc.cluster.local\n"
May 31 13:30:05.451: INFO: stdout: "Server:\t\t169.254.20.10\nAddress:\t169.254.20.10#53\n\nclusterip-service.services-6139.svc.cluster.local\tcanonical name = externalsvc.services-6139.svc.cluster.local.\nName:\texternalsvc.services-6139.svc.cluster.local\nAddress: 10.240.17.83\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-6139, will wait for the garbage collector to delete the pods
May 31 13:30:05.541: INFO: Deleting ReplicationController externalsvc took: 25.741596ms
May 31 13:30:05.642: INFO: Terminating ReplicationController externalsvc pods took: 100.9474ms
May 31 13:30:09.186: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:30:09.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6139" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:11.541 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":346,"completed":73,"skipped":1507,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:30:09.272: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 31 13:30:13.423: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:30:13.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1371" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]","total":346,"completed":74,"skipped":1550,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:30:13.510: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pdb that targets all three pods in a test replica set
STEP: Waiting for the pdb to be processed
STEP: First trying to evict a pod which shouldn't be evictable
STEP: Waiting for all pods to be running
May 31 13:30:13.687: INFO: pods: 0 < 3
May 31 13:30:15.699: INFO: running pods: 2 < 3
STEP: locating a running pod
STEP: Updating the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
STEP: Waiting for the pdb to observed all healthy pods
STEP: Patching the pdb to disallow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
STEP: locating a running pod
STEP: Deleting the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be deleted
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:30:20.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-344" for this suite.

• [SLOW TEST:6.534 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","total":346,"completed":75,"skipped":1558,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:30:20.044: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-cf863ce9-254c-442f-aa48-555c38f1b619
STEP: Creating a pod to test consume configMaps
May 31 13:30:20.148: INFO: Waiting up to 5m0s for pod "pod-configmaps-b42166b9-6c84-4bda-9042-aad3f8c6c7f3" in namespace "configmap-4038" to be "Succeeded or Failed"
May 31 13:30:20.158: INFO: Pod "pod-configmaps-b42166b9-6c84-4bda-9042-aad3f8c6c7f3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.121497ms
May 31 13:30:22.174: INFO: Pod "pod-configmaps-b42166b9-6c84-4bda-9042-aad3f8c6c7f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026290862s
May 31 13:30:24.188: INFO: Pod "pod-configmaps-b42166b9-6c84-4bda-9042-aad3f8c6c7f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040206231s
STEP: Saw pod success
May 31 13:30:24.190: INFO: Pod "pod-configmaps-b42166b9-6c84-4bda-9042-aad3f8c6c7f3" satisfied condition "Succeeded or Failed"
May 31 13:30:24.202: INFO: Trying to get logs from node ip-172-31-3-160.eu-central-1.compute.internal pod pod-configmaps-b42166b9-6c84-4bda-9042-aad3f8c6c7f3 container configmap-volume-test: <nil>
STEP: delete the pod
May 31 13:30:24.253: INFO: Waiting for pod pod-configmaps-b42166b9-6c84-4bda-9042-aad3f8c6c7f3 to disappear
May 31 13:30:24.261: INFO: Pod pod-configmaps-b42166b9-6c84-4bda-9042-aad3f8c6c7f3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:30:24.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4038" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":76,"skipped":1562,"failed":0}
SSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:30:24.313: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test substitution in volume subpath
May 31 13:30:24.398: INFO: Waiting up to 5m0s for pod "var-expansion-53772cda-d96d-4066-8142-a7e83c1039ae" in namespace "var-expansion-2500" to be "Succeeded or Failed"
May 31 13:30:24.408: INFO: Pod "var-expansion-53772cda-d96d-4066-8142-a7e83c1039ae": Phase="Pending", Reason="", readiness=false. Elapsed: 10.016081ms
May 31 13:30:26.418: INFO: Pod "var-expansion-53772cda-d96d-4066-8142-a7e83c1039ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02039421s
May 31 13:30:28.432: INFO: Pod "var-expansion-53772cda-d96d-4066-8142-a7e83c1039ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034159585s
STEP: Saw pod success
May 31 13:30:28.432: INFO: Pod "var-expansion-53772cda-d96d-4066-8142-a7e83c1039ae" satisfied condition "Succeeded or Failed"
May 31 13:30:28.438: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod var-expansion-53772cda-d96d-4066-8142-a7e83c1039ae container dapi-container: <nil>
STEP: delete the pod
May 31 13:30:28.525: INFO: Waiting for pod var-expansion-53772cda-d96d-4066-8142-a7e83c1039ae to disappear
May 31 13:30:28.535: INFO: Pod var-expansion-53772cda-d96d-4066-8142-a7e83c1039ae no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:30:28.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2500" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","total":346,"completed":77,"skipped":1567,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:30:28.570: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-2639ac5d-6f07-4822-978c-faf29c257bbe
STEP: Creating a pod to test consume configMaps
May 31 13:30:28.656: INFO: Waiting up to 5m0s for pod "pod-configmaps-556d4f0e-dcb4-4882-9b52-d432fed13688" in namespace "configmap-460" to be "Succeeded or Failed"
May 31 13:30:28.671: INFO: Pod "pod-configmaps-556d4f0e-dcb4-4882-9b52-d432fed13688": Phase="Pending", Reason="", readiness=false. Elapsed: 14.847626ms
May 31 13:30:30.684: INFO: Pod "pod-configmaps-556d4f0e-dcb4-4882-9b52-d432fed13688": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027115456s
May 31 13:30:32.694: INFO: Pod "pod-configmaps-556d4f0e-dcb4-4882-9b52-d432fed13688": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037177062s
STEP: Saw pod success
May 31 13:30:32.694: INFO: Pod "pod-configmaps-556d4f0e-dcb4-4882-9b52-d432fed13688" satisfied condition "Succeeded or Failed"
May 31 13:30:32.703: INFO: Trying to get logs from node ip-172-31-3-160.eu-central-1.compute.internal pod pod-configmaps-556d4f0e-dcb4-4882-9b52-d432fed13688 container agnhost-container: <nil>
STEP: delete the pod
May 31 13:30:32.794: INFO: Waiting for pod pod-configmaps-556d4f0e-dcb4-4882-9b52-d432fed13688 to disappear
May 31 13:30:32.801: INFO: Pod pod-configmaps-556d4f0e-dcb4-4882-9b52-d432fed13688 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:30:32.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-460" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":78,"skipped":1604,"failed":0}
SS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:30:32.827: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7372.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-7372.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7372.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-7372.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 31 13:30:35.202: INFO: DNS probes using dns-7372/dns-test-02e4f1a9-047b-44f3-b959-1a2a01430eab succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:30:35.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7372" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":346,"completed":79,"skipped":1606,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:30:35.286: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap that has name configmap-test-emptyKey-b3590217-c4a6-4751-9af3-18fc9df0ceed
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:30:35.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9804" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":346,"completed":80,"skipped":1657,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:30:35.438: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create deployment with httpd image
May 31 13:30:35.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-7971 create -f -'
May 31 13:30:36.130: INFO: stderr: ""
May 31 13:30:36.130: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
May 31 13:30:36.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-7971 diff -f -'
May 31 13:30:36.736: INFO: rc: 1
May 31 13:30:36.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-7971 delete -f -'
May 31 13:30:37.001: INFO: stderr: ""
May 31 13:30:37.001: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:30:37.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7971" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":346,"completed":81,"skipped":1671,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:30:37.027: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Request ServerVersion
STEP: Confirm major version
May 31 13:30:37.085: INFO: Major version: 1
STEP: Confirm minor version
May 31 13:30:37.098: INFO: cleanMinorVersion: 23
May 31 13:30:37.098: INFO: Minor version: 23
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:30:37.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-787" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":346,"completed":82,"skipped":1691,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:30:37.120: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
STEP: submitting the pod to kubernetes
May 31 13:30:37.200: INFO: The status of Pod pod-update-39bbe75e-eb48-4b37-bf68-5336ca561e30 is Pending, waiting for it to be Running (with Ready = true)
May 31 13:30:39.212: INFO: The status of Pod pod-update-39bbe75e-eb48-4b37-bf68-5336ca561e30 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 31 13:30:39.787: INFO: Successfully updated pod "pod-update-39bbe75e-eb48-4b37-bf68-5336ca561e30"
STEP: verifying the updated pod is in kubernetes
May 31 13:30:39.808: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:30:39.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2286" for this suite.
•{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","total":346,"completed":83,"skipped":1724,"failed":0}
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:30:39.859: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 31 13:30:40.492: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 31 13:30:43.545: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 13:30:43.554: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6105-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:30:47.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-473" for this suite.
STEP: Destroying namespace "webhook-473-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.341 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":346,"completed":84,"skipped":1726,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:30:47.206: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod pod-subpath-test-secret-hcjf
STEP: Creating a pod to test atomic-volume-subpath
May 31 13:30:47.365: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-hcjf" in namespace "subpath-2443" to be "Succeeded or Failed"
May 31 13:30:47.374: INFO: Pod "pod-subpath-test-secret-hcjf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.470318ms
May 31 13:30:49.387: INFO: Pod "pod-subpath-test-secret-hcjf": Phase="Running", Reason="", readiness=true. Elapsed: 2.021893368s
May 31 13:30:51.403: INFO: Pod "pod-subpath-test-secret-hcjf": Phase="Running", Reason="", readiness=true. Elapsed: 4.037453386s
May 31 13:30:53.412: INFO: Pod "pod-subpath-test-secret-hcjf": Phase="Running", Reason="", readiness=true. Elapsed: 6.04669837s
May 31 13:30:55.425: INFO: Pod "pod-subpath-test-secret-hcjf": Phase="Running", Reason="", readiness=true. Elapsed: 8.059507941s
May 31 13:30:57.439: INFO: Pod "pod-subpath-test-secret-hcjf": Phase="Running", Reason="", readiness=true. Elapsed: 10.073533933s
May 31 13:30:59.449: INFO: Pod "pod-subpath-test-secret-hcjf": Phase="Running", Reason="", readiness=true. Elapsed: 12.08378668s
May 31 13:31:01.458: INFO: Pod "pod-subpath-test-secret-hcjf": Phase="Running", Reason="", readiness=true. Elapsed: 14.092971163s
May 31 13:31:03.470: INFO: Pod "pod-subpath-test-secret-hcjf": Phase="Running", Reason="", readiness=true. Elapsed: 16.104951984s
May 31 13:31:05.537: INFO: Pod "pod-subpath-test-secret-hcjf": Phase="Running", Reason="", readiness=true. Elapsed: 18.172117921s
May 31 13:31:07.548: INFO: Pod "pod-subpath-test-secret-hcjf": Phase="Running", Reason="", readiness=true. Elapsed: 20.183000977s
May 31 13:31:09.565: INFO: Pod "pod-subpath-test-secret-hcjf": Phase="Running", Reason="", readiness=false. Elapsed: 22.199820384s
May 31 13:31:11.574: INFO: Pod "pod-subpath-test-secret-hcjf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.208378515s
STEP: Saw pod success
May 31 13:31:11.574: INFO: Pod "pod-subpath-test-secret-hcjf" satisfied condition "Succeeded or Failed"
May 31 13:31:11.580: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod pod-subpath-test-secret-hcjf container test-container-subpath-secret-hcjf: <nil>
STEP: delete the pod
May 31 13:31:11.627: INFO: Waiting for pod pod-subpath-test-secret-hcjf to disappear
May 31 13:31:11.635: INFO: Pod pod-subpath-test-secret-hcjf no longer exists
STEP: Deleting pod pod-subpath-test-secret-hcjf
May 31 13:31:11.635: INFO: Deleting pod "pod-subpath-test-secret-hcjf" in namespace "subpath-2443"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:31:11.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2443" for this suite.

• [SLOW TEST:24.462 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":85,"skipped":1813,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:31:11.669: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 31 13:31:11.754: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6c1ee7d3-a0cb-4ecb-83bb-546d6304ac28" in namespace "downward-api-1500" to be "Succeeded or Failed"
May 31 13:31:11.768: INFO: Pod "downwardapi-volume-6c1ee7d3-a0cb-4ecb-83bb-546d6304ac28": Phase="Pending", Reason="", readiness=false. Elapsed: 13.614781ms
May 31 13:31:13.782: INFO: Pod "downwardapi-volume-6c1ee7d3-a0cb-4ecb-83bb-546d6304ac28": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028396233s
May 31 13:31:15.796: INFO: Pod "downwardapi-volume-6c1ee7d3-a0cb-4ecb-83bb-546d6304ac28": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042474345s
STEP: Saw pod success
May 31 13:31:15.796: INFO: Pod "downwardapi-volume-6c1ee7d3-a0cb-4ecb-83bb-546d6304ac28" satisfied condition "Succeeded or Failed"
May 31 13:31:15.807: INFO: Trying to get logs from node ip-172-31-3-160.eu-central-1.compute.internal pod downwardapi-volume-6c1ee7d3-a0cb-4ecb-83bb-546d6304ac28 container client-container: <nil>
STEP: delete the pod
May 31 13:31:15.921: INFO: Waiting for pod downwardapi-volume-6c1ee7d3-a0cb-4ecb-83bb-546d6304ac28 to disappear
May 31 13:31:15.931: INFO: Pod downwardapi-volume-6c1ee7d3-a0cb-4ecb-83bb-546d6304ac28 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:31:15.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1500" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":346,"completed":86,"skipped":1828,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:31:15.970: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 31 13:31:17.035: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
May 31 13:31:19.081: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 31, 13, 31, 17, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 31, 13, 31, 17, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 31, 13, 31, 17, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 31, 13, 31, 17, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 31 13:31:22.111: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
May 31 13:31:22.210: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:31:22.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8506" for this suite.
STEP: Destroying namespace "webhook-8506-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.510 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":346,"completed":87,"skipped":1840,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:31:22.481: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
May 31 13:31:22.550: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:31:27.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9577" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":346,"completed":88,"skipped":1853,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:31:27.439: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
May 31 13:31:27.633: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May 31 13:31:29.643: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the pod with lifecycle hook
May 31 13:31:29.669: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
May 31 13:31:31.681: INFO: The status of Pod pod-with-poststart-http-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 31 13:31:31.768: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 31 13:31:31.774: INFO: Pod pod-with-poststart-http-hook still exists
May 31 13:31:33.778: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 31 13:31:33.791: INFO: Pod pod-with-poststart-http-hook still exists
May 31 13:31:35.779: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 31 13:31:35.792: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:31:35.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7196" for this suite.

• [SLOW TEST:8.380 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":346,"completed":89,"skipped":1875,"failed":0}
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:31:35.819: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating Agnhost RC
May 31 13:31:35.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-8000 create -f -'
May 31 13:31:37.793: INFO: stderr: ""
May 31 13:31:37.793: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
May 31 13:31:38.801: INFO: Selector matched 1 pods for map[app:agnhost]
May 31 13:31:38.801: INFO: Found 0 / 1
May 31 13:31:39.807: INFO: Selector matched 1 pods for map[app:agnhost]
May 31 13:31:39.807: INFO: Found 1 / 1
May 31 13:31:39.807: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
May 31 13:31:39.815: INFO: Selector matched 1 pods for map[app:agnhost]
May 31 13:31:39.815: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 31 13:31:39.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-8000 patch pod agnhost-primary-md8gp -p {"metadata":{"annotations":{"x":"y"}}}'
May 31 13:31:40.176: INFO: stderr: ""
May 31 13:31:40.176: INFO: stdout: "pod/agnhost-primary-md8gp patched\n"
STEP: checking annotations
May 31 13:31:40.188: INFO: Selector matched 1 pods for map[app:agnhost]
May 31 13:31:40.188: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:31:40.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8000" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":346,"completed":90,"skipped":1884,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:31:40.283: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 31 13:31:41.763: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 31 13:31:44.807: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:31:45.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3410" for this suite.
STEP: Destroying namespace "webhook-3410-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.056 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":346,"completed":91,"skipped":1912,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:31:45.341: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-5021
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-5021
STEP: Waiting until pod test-pod will start running in namespace statefulset-5021
STEP: Creating statefulset with conflicting port in namespace statefulset-5021
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5021
May 31 13:31:47.575: INFO: Observed stateful pod in namespace: statefulset-5021, name: ss-0, uid: 19ae4f46-32c7-4946-a2f5-d16baba077ed, status phase: Pending. Waiting for statefulset controller to delete.
May 31 13:31:47.606: INFO: Observed stateful pod in namespace: statefulset-5021, name: ss-0, uid: 19ae4f46-32c7-4946-a2f5-d16baba077ed, status phase: Failed. Waiting for statefulset controller to delete.
May 31 13:31:47.639: INFO: Observed stateful pod in namespace: statefulset-5021, name: ss-0, uid: 19ae4f46-32c7-4946-a2f5-d16baba077ed, status phase: Failed. Waiting for statefulset controller to delete.
May 31 13:31:47.659: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-5021
STEP: Removing pod with conflicting port in namespace statefulset-5021
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5021 and will be in running state
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
May 31 13:31:49.724: INFO: Deleting all statefulset in ns statefulset-5021
May 31 13:31:49.731: INFO: Scaling statefulset ss to 0
May 31 13:31:59.775: INFO: Waiting for statefulset status.replicas updated to 0
May 31 13:31:59.780: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:31:59.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5021" for this suite.

• [SLOW TEST:14.499 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":346,"completed":92,"skipped":1934,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:31:59.840: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 13:31:59.943: INFO: The status of Pod test-webserver-3d77993f-47cf-4e01-868d-95777bbceed8 is Pending, waiting for it to be Running (with Ready = true)
May 31 13:32:01.961: INFO: The status of Pod test-webserver-3d77993f-47cf-4e01-868d-95777bbceed8 is Running (Ready = false)
May 31 13:32:03.955: INFO: The status of Pod test-webserver-3d77993f-47cf-4e01-868d-95777bbceed8 is Running (Ready = false)
May 31 13:32:05.953: INFO: The status of Pod test-webserver-3d77993f-47cf-4e01-868d-95777bbceed8 is Running (Ready = false)
May 31 13:32:07.951: INFO: The status of Pod test-webserver-3d77993f-47cf-4e01-868d-95777bbceed8 is Running (Ready = false)
May 31 13:32:09.952: INFO: The status of Pod test-webserver-3d77993f-47cf-4e01-868d-95777bbceed8 is Running (Ready = false)
May 31 13:32:11.956: INFO: The status of Pod test-webserver-3d77993f-47cf-4e01-868d-95777bbceed8 is Running (Ready = false)
May 31 13:32:13.960: INFO: The status of Pod test-webserver-3d77993f-47cf-4e01-868d-95777bbceed8 is Running (Ready = false)
May 31 13:32:15.957: INFO: The status of Pod test-webserver-3d77993f-47cf-4e01-868d-95777bbceed8 is Running (Ready = false)
May 31 13:32:17.957: INFO: The status of Pod test-webserver-3d77993f-47cf-4e01-868d-95777bbceed8 is Running (Ready = false)
May 31 13:32:19.952: INFO: The status of Pod test-webserver-3d77993f-47cf-4e01-868d-95777bbceed8 is Running (Ready = false)
May 31 13:32:21.958: INFO: The status of Pod test-webserver-3d77993f-47cf-4e01-868d-95777bbceed8 is Running (Ready = true)
May 31 13:32:21.969: INFO: Container started at 2022-05-31 13:32:01 +0000 UTC, pod became ready at 2022-05-31 13:32:19 +0000 UTC
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:32:21.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9680" for this suite.

• [SLOW TEST:22.157 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":346,"completed":93,"skipped":1968,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:32:22.000: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-91446ed2-9483-4ef1-8984-f3d7c49aacfd
STEP: Creating a pod to test consume configMaps
May 31 13:32:22.078: INFO: Waiting up to 5m0s for pod "pod-configmaps-3b455ef5-f450-42a5-a0eb-9bd49274790b" in namespace "configmap-1784" to be "Succeeded or Failed"
May 31 13:32:22.091: INFO: Pod "pod-configmaps-3b455ef5-f450-42a5-a0eb-9bd49274790b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.343547ms
May 31 13:32:24.107: INFO: Pod "pod-configmaps-3b455ef5-f450-42a5-a0eb-9bd49274790b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028573133s
May 31 13:32:26.118: INFO: Pod "pod-configmaps-3b455ef5-f450-42a5-a0eb-9bd49274790b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039244234s
May 31 13:32:28.153: INFO: Pod "pod-configmaps-3b455ef5-f450-42a5-a0eb-9bd49274790b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.074397472s
STEP: Saw pod success
May 31 13:32:28.186: INFO: Pod "pod-configmaps-3b455ef5-f450-42a5-a0eb-9bd49274790b" satisfied condition "Succeeded or Failed"
May 31 13:32:28.214: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod pod-configmaps-3b455ef5-f450-42a5-a0eb-9bd49274790b container agnhost-container: <nil>
STEP: delete the pod
May 31 13:32:28.302: INFO: Waiting for pod pod-configmaps-3b455ef5-f450-42a5-a0eb-9bd49274790b to disappear
May 31 13:32:28.323: INFO: Pod pod-configmaps-3b455ef5-f450-42a5-a0eb-9bd49274790b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:32:28.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1784" for this suite.

• [SLOW TEST:6.368 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":94,"skipped":1999,"failed":0}
SSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:32:28.473: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward api env vars
May 31 13:32:28.593: INFO: Waiting up to 5m0s for pod "downward-api-c06d76a6-c4d2-419f-bd0f-6b272430fe06" in namespace "downward-api-9215" to be "Succeeded or Failed"
May 31 13:32:28.601: INFO: Pod "downward-api-c06d76a6-c4d2-419f-bd0f-6b272430fe06": Phase="Pending", Reason="", readiness=false. Elapsed: 5.181634ms
May 31 13:32:30.612: INFO: Pod "downward-api-c06d76a6-c4d2-419f-bd0f-6b272430fe06": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016418792s
May 31 13:32:32.630: INFO: Pod "downward-api-c06d76a6-c4d2-419f-bd0f-6b272430fe06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034476962s
STEP: Saw pod success
May 31 13:32:32.630: INFO: Pod "downward-api-c06d76a6-c4d2-419f-bd0f-6b272430fe06" satisfied condition "Succeeded or Failed"
May 31 13:32:32.641: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod downward-api-c06d76a6-c4d2-419f-bd0f-6b272430fe06 container dapi-container: <nil>
STEP: delete the pod
May 31 13:32:32.690: INFO: Waiting for pod downward-api-c06d76a6-c4d2-419f-bd0f-6b272430fe06 to disappear
May 31 13:32:32.700: INFO: Pod downward-api-c06d76a6-c4d2-419f-bd0f-6b272430fe06 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:32:32.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9215" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":346,"completed":95,"skipped":2004,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:32:32.725: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 31 13:32:32.848: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 31 13:32:32.848: INFO: Node ip-172-31-11-247.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 31 13:32:33.868: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 31 13:32:33.869: INFO: Node ip-172-31-11-247.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 31 13:32:34.885: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 31 13:32:34.885: INFO: Node ip-172-31-11-247.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 31 13:32:35.874: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 31 13:32:35.874: INFO: Node ip-172-31-11-247.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 31 13:32:36.868: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 31 13:32:36.868: INFO: Node ip-172-31-11-247.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 31 13:32:37.875: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 31 13:32:37.875: INFO: Node ip-172-31-11-247.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 31 13:32:38.866: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 31 13:32:38.866: INFO: Node ip-172-31-11-247.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 31 13:32:39.871: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 31 13:32:39.871: INFO: Node ip-172-31-11-247.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 31 13:32:40.874: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May 31 13:32:40.874: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived.
May 31 13:32:40.923: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 31 13:32:40.923: INFO: Node ip-172-31-8-180.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 31 13:32:41.958: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 31 13:32:41.958: INFO: Node ip-172-31-8-180.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 31 13:32:42.946: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 31 13:32:42.946: INFO: Node ip-172-31-8-180.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 31 13:32:43.944: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 31 13:32:43.945: INFO: Node ip-172-31-8-180.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 31 13:32:44.942: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May 31 13:32:44.942: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5233, will wait for the garbage collector to delete the pods
May 31 13:32:45.015: INFO: Deleting DaemonSet.extensions daemon-set took: 10.785582ms
May 31 13:32:45.215: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.47223ms
May 31 13:32:47.330: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 31 13:32:47.330: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May 31 13:32:47.338: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"16208"},"items":null}

May 31 13:32:47.347: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"16208"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:32:47.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5233" for this suite.

• [SLOW TEST:14.712 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":346,"completed":96,"skipped":2023,"failed":0}
SSSS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:32:47.438: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:186
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 31 13:32:47.599: INFO: starting watch
STEP: patching
STEP: updating
May 31 13:32:47.628: INFO: waiting for watch events with expected annotations
May 31 13:32:47.628: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:32:47.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-9515" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":346,"completed":97,"skipped":2027,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:32:47.744: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
May 31 13:32:47.839: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:32:53.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-183" for this suite.

• [SLOW TEST:5.599 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":346,"completed":98,"skipped":2059,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:32:53.346: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 31 13:32:55.136: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 31 13:32:58.188: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:32:58.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7374" for this suite.
STEP: Destroying namespace "webhook-7374-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.257 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":346,"completed":99,"skipped":2064,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:32:58.604: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-ca658e68-b5ca-499d-b0a9-6fbe34778411
STEP: Creating a pod to test consume configMaps
May 31 13:32:58.691: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ccd90ee6-6b82-4904-94b1-ec567c906c5d" in namespace "projected-5000" to be "Succeeded or Failed"
May 31 13:32:58.703: INFO: Pod "pod-projected-configmaps-ccd90ee6-6b82-4904-94b1-ec567c906c5d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.619502ms
May 31 13:33:00.715: INFO: Pod "pod-projected-configmaps-ccd90ee6-6b82-4904-94b1-ec567c906c5d": Phase="Running", Reason="", readiness=true. Elapsed: 2.024180311s
May 31 13:33:02.725: INFO: Pod "pod-projected-configmaps-ccd90ee6-6b82-4904-94b1-ec567c906c5d": Phase="Running", Reason="", readiness=false. Elapsed: 4.034022692s
May 31 13:33:04.737: INFO: Pod "pod-projected-configmaps-ccd90ee6-6b82-4904-94b1-ec567c906c5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.045724336s
STEP: Saw pod success
May 31 13:33:04.737: INFO: Pod "pod-projected-configmaps-ccd90ee6-6b82-4904-94b1-ec567c906c5d" satisfied condition "Succeeded or Failed"
May 31 13:33:04.746: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod pod-projected-configmaps-ccd90ee6-6b82-4904-94b1-ec567c906c5d container agnhost-container: <nil>
STEP: delete the pod
May 31 13:33:04.782: INFO: Waiting for pod pod-projected-configmaps-ccd90ee6-6b82-4904-94b1-ec567c906c5d to disappear
May 31 13:33:04.789: INFO: Pod pod-projected-configmaps-ccd90ee6-6b82-4904-94b1-ec567c906c5d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:33:04.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5000" for this suite.

• [SLOW TEST:6.206 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":100,"skipped":2076,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:33:04.810: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Waiting for the pdb to be processed
STEP: Updating PodDisruptionBudget status
STEP: Waiting for all pods to be running
May 31 13:33:06.925: INFO: running pods: 0 < 1
STEP: locating a running pod
STEP: Waiting for the pdb to be processed
STEP: Patching PodDisruptionBudget status
STEP: Waiting for the pdb to be processed
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:33:09.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-551" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","total":346,"completed":101,"skipped":2085,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:33:09.059: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0531 13:33:10.239930      19 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
May 31 13:33:10.240: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:33:10.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1344" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":346,"completed":102,"skipped":2095,"failed":0}

------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:33:10.267: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1573
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
May 31 13:33:10.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-4486 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
May 31 13:33:10.440: INFO: stderr: ""
May 31 13:33:10.440: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
May 31 13:33:15.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-4486 get pod e2e-test-httpd-pod -o json'
May 31 13:33:15.595: INFO: stderr: ""
May 31 13:33:15.595: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"5d75bd43ff346b31ae6382813223fd9af80a5ec071afaf1d3e9a4890395ec142\",\n            \"cni.projectcalico.org/podIP\": \"172.25.2.72/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.25.2.72/32\"\n        },\n        \"creationTimestamp\": \"2022-05-31T13:33:10Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4486\",\n        \"resourceVersion\": \"16618\",\n        \"uid\": \"8c0e76a9-d989-4e86-be02-293d64b52a6d\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-5lx66\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-172-31-3-160.eu-central-1.compute.internal\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-5lx66\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-05-31T13:33:10Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-05-31T13:33:12Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-05-31T13:33:12Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-05-31T13:33:10Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://755432a1dd04ed49d3ffd437fc8c59d4d6167907a4c8e281526355e9512a3964\",\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2022-05-31T13:33:11Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.31.3.160\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.25.2.72\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.25.2.72\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2022-05-31T13:33:10Z\"\n    }\n}\n"
STEP: replace the image in the pod
May 31 13:33:15.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-4486 replace -f -'
May 31 13:33:16.891: INFO: stderr: ""
May 31 13:33:16.891: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/busybox:1.29-2
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1577
May 31 13:33:16.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-4486 delete pods e2e-test-httpd-pod'
May 31 13:33:18.927: INFO: stderr: ""
May 31 13:33:18.927: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:33:18.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4486" for this suite.

• [SLOW TEST:8.705 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1570
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":346,"completed":103,"skipped":2095,"failed":0}
S
------------------------------
[sig-apps] CronJob 
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:33:18.973: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a ForbidConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring no more jobs are scheduled
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:39:01.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-4770" for this suite.

• [SLOW TEST:342.217 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","total":346,"completed":104,"skipped":2096,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:39:01.190: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 31 13:39:02.295: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 31 13:39:05.373: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
May 31 13:39:07.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=webhook-3131 attach --namespace=webhook-3131 to-be-attached-pod -i -c=container1'
May 31 13:39:07.730: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:39:07.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3131" for this suite.
STEP: Destroying namespace "webhook-3131-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.657 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":346,"completed":105,"skipped":2109,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:39:07.856: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 13:39:07.898: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: creating replication controller svc-latency-rc in namespace svc-latency-2358
I0531 13:39:07.908445      19 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-2358, replica count: 1
I0531 13:39:08.960711      19 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0531 13:39:09.961053      19 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0531 13:39:10.961399      19 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0531 13:39:11.961994      19 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 31 13:39:12.089: INFO: Created: latency-svc-z8mp6
May 31 13:39:12.102: INFO: Got endpoints: latency-svc-z8mp6 [39.806836ms]
May 31 13:39:12.129: INFO: Created: latency-svc-wc6x4
May 31 13:39:12.138: INFO: Got endpoints: latency-svc-wc6x4 [36.158329ms]
May 31 13:39:12.165: INFO: Created: latency-svc-ctb57
May 31 13:39:12.165: INFO: Got endpoints: latency-svc-ctb57 [62.298493ms]
May 31 13:39:12.190: INFO: Created: latency-svc-4zgst
May 31 13:39:12.201: INFO: Got endpoints: latency-svc-4zgst [98.280098ms]
May 31 13:39:12.202: INFO: Created: latency-svc-8t59b
May 31 13:39:12.212: INFO: Got endpoints: latency-svc-8t59b [108.613649ms]
May 31 13:39:12.219: INFO: Created: latency-svc-x5lvs
May 31 13:39:12.229: INFO: Got endpoints: latency-svc-x5lvs [125.637313ms]
May 31 13:39:12.235: INFO: Created: latency-svc-ww2wh
May 31 13:39:12.243: INFO: Created: latency-svc-z6dp9
May 31 13:39:12.251: INFO: Got endpoints: latency-svc-z6dp9 [144.934121ms]
May 31 13:39:12.251: INFO: Got endpoints: latency-svc-ww2wh [146.413858ms]
May 31 13:39:12.262: INFO: Created: latency-svc-z8rd7
May 31 13:39:12.284: INFO: Got endpoints: latency-svc-z8rd7 [177.786869ms]
May 31 13:39:12.285: INFO: Created: latency-svc-mtp2d
May 31 13:39:12.298: INFO: Got endpoints: latency-svc-mtp2d [190.191997ms]
May 31 13:39:12.304: INFO: Created: latency-svc-29wc7
May 31 13:39:12.305: INFO: Got endpoints: latency-svc-29wc7 [198.334088ms]
May 31 13:39:12.307: INFO: Created: latency-svc-gh8ln
May 31 13:39:12.322: INFO: Got endpoints: latency-svc-gh8ln [215.688119ms]
May 31 13:39:12.324: INFO: Created: latency-svc-jvfvh
May 31 13:39:12.325: INFO: Got endpoints: latency-svc-jvfvh [218.250937ms]
May 31 13:39:12.336: INFO: Created: latency-svc-xwbhg
May 31 13:39:12.336: INFO: Created: latency-svc-k4m8g
May 31 13:39:12.336: INFO: Got endpoints: latency-svc-k4m8g [229.374483ms]
May 31 13:39:12.347: INFO: Got endpoints: latency-svc-xwbhg [239.281436ms]
May 31 13:39:12.353: INFO: Created: latency-svc-d95tr
May 31 13:39:12.375: INFO: Got endpoints: latency-svc-d95tr [267.745752ms]
May 31 13:39:12.377: INFO: Created: latency-svc-qcvn8
May 31 13:39:12.377: INFO: Got endpoints: latency-svc-qcvn8 [238.618523ms]
May 31 13:39:12.382: INFO: Created: latency-svc-wf9wb
May 31 13:39:12.403: INFO: Created: latency-svc-hf2hn
May 31 13:39:12.403: INFO: Got endpoints: latency-svc-hf2hn [202.074267ms]
May 31 13:39:12.403: INFO: Got endpoints: latency-svc-wf9wb [238.098384ms]
May 31 13:39:12.407: INFO: Created: latency-svc-v2d2j
May 31 13:39:12.416: INFO: Created: latency-svc-8p86f
May 31 13:39:12.417: INFO: Got endpoints: latency-svc-v2d2j [205.41668ms]
May 31 13:39:12.424: INFO: Got endpoints: latency-svc-8p86f [194.934675ms]
May 31 13:39:12.428: INFO: Created: latency-svc-d9ggb
May 31 13:39:12.434: INFO: Got endpoints: latency-svc-d9ggb [182.179329ms]
May 31 13:39:12.436: INFO: Created: latency-svc-9pqt7
May 31 13:39:12.447: INFO: Got endpoints: latency-svc-9pqt7 [196.003215ms]
May 31 13:39:12.453: INFO: Created: latency-svc-g29wc
May 31 13:39:12.463: INFO: Got endpoints: latency-svc-g29wc [178.710615ms]
May 31 13:39:12.472: INFO: Created: latency-svc-pz4nw
May 31 13:39:12.482: INFO: Got endpoints: latency-svc-pz4nw [184.458882ms]
May 31 13:39:12.494: INFO: Created: latency-svc-9z4jh
May 31 13:39:12.503: INFO: Created: latency-svc-hdjx5
May 31 13:39:12.513: INFO: Got endpoints: latency-svc-9z4jh [207.765923ms]
May 31 13:39:12.525: INFO: Got endpoints: latency-svc-hdjx5 [200.58898ms]
May 31 13:39:12.528: INFO: Created: latency-svc-qznqb
May 31 13:39:12.534: INFO: Got endpoints: latency-svc-qznqb [208.811973ms]
May 31 13:39:12.536: INFO: Created: latency-svc-p7lsr
May 31 13:39:12.542: INFO: Got endpoints: latency-svc-p7lsr [206.429388ms]
May 31 13:39:12.548: INFO: Created: latency-svc-ww5x6
May 31 13:39:12.553: INFO: Got endpoints: latency-svc-ww5x6 [205.548702ms]
May 31 13:39:12.558: INFO: Created: latency-svc-vwrtt
May 31 13:39:12.564: INFO: Created: latency-svc-8b78g
May 31 13:39:12.565: INFO: Got endpoints: latency-svc-vwrtt [189.639869ms]
May 31 13:39:12.573: INFO: Got endpoints: latency-svc-8b78g [196.057717ms]
May 31 13:39:12.575: INFO: Created: latency-svc-6slkx
May 31 13:39:12.585: INFO: Got endpoints: latency-svc-6slkx [182.041657ms]
May 31 13:39:12.590: INFO: Created: latency-svc-rzr74
May 31 13:39:12.596: INFO: Got endpoints: latency-svc-rzr74 [192.320108ms]
May 31 13:39:12.621: INFO: Created: latency-svc-7qcll
May 31 13:39:12.625: INFO: Created: latency-svc-p4d57
May 31 13:39:12.633: INFO: Got endpoints: latency-svc-7qcll [209.2396ms]
May 31 13:39:12.640: INFO: Got endpoints: latency-svc-p4d57 [222.949814ms]
May 31 13:39:12.642: INFO: Created: latency-svc-x48xl
May 31 13:39:12.658: INFO: Created: latency-svc-njg68
May 31 13:39:12.669: INFO: Created: latency-svc-4vkmt
May 31 13:39:12.704: INFO: Created: latency-svc-nlnh7
May 31 13:39:12.708: INFO: Got endpoints: latency-svc-4vkmt [243.442783ms]
May 31 13:39:12.709: INFO: Got endpoints: latency-svc-njg68 [261.960636ms]
May 31 13:39:12.711: INFO: Got endpoints: latency-svc-x48xl [277.300369ms]
May 31 13:39:12.712: INFO: Got endpoints: latency-svc-nlnh7 [229.26315ms]
May 31 13:39:12.748: INFO: Created: latency-svc-whzfm
May 31 13:39:12.767: INFO: Got endpoints: latency-svc-whzfm [253.55905ms]
May 31 13:39:12.787: INFO: Created: latency-svc-f7v6b
May 31 13:39:12.798: INFO: Created: latency-svc-dnsbq
May 31 13:39:12.815: INFO: Got endpoints: latency-svc-dnsbq [281.133429ms]
May 31 13:39:12.832: INFO: Created: latency-svc-klj78
May 31 13:39:12.836: INFO: Got endpoints: latency-svc-f7v6b [309.448294ms]
May 31 13:39:12.840: INFO: Got endpoints: latency-svc-klj78 [291.587343ms]
May 31 13:39:12.853: INFO: Created: latency-svc-pbmkp
May 31 13:39:12.861: INFO: Got endpoints: latency-svc-pbmkp [305.956809ms]
May 31 13:39:12.868: INFO: Created: latency-svc-p5c7l
May 31 13:39:12.873: INFO: Created: latency-svc-zghgt
May 31 13:39:12.881: INFO: Created: latency-svc-d62wp
May 31 13:39:12.892: INFO: Created: latency-svc-rbglg
May 31 13:39:12.895: INFO: Got endpoints: latency-svc-p5c7l [329.613486ms]
May 31 13:39:12.900: INFO: Created: latency-svc-f6lbb
May 31 13:39:12.907: INFO: Created: latency-svc-l74l6
May 31 13:39:12.923: INFO: Created: latency-svc-kxj8j
May 31 13:39:12.925: INFO: Created: latency-svc-8m6fw
May 31 13:39:12.937: INFO: Created: latency-svc-xm7fq
May 31 13:39:12.950: INFO: Created: latency-svc-qr5qm
May 31 13:39:12.959: INFO: Got endpoints: latency-svc-zghgt [385.960809ms]
May 31 13:39:12.960: INFO: Created: latency-svc-mzsjc
May 31 13:39:12.968: INFO: Created: latency-svc-4r2rh
May 31 13:39:12.979: INFO: Created: latency-svc-j5pn6
May 31 13:39:12.984: INFO: Created: latency-svc-5shd5
May 31 13:39:12.998: INFO: Got endpoints: latency-svc-d62wp [412.386608ms]
May 31 13:39:13.000: INFO: Created: latency-svc-wh4zl
May 31 13:39:13.010: INFO: Created: latency-svc-mrdzz
May 31 13:39:13.025: INFO: Created: latency-svc-8mr6p
May 31 13:39:13.030: INFO: Created: latency-svc-mxc64
May 31 13:39:13.053: INFO: Got endpoints: latency-svc-rbglg [457.057645ms]
May 31 13:39:13.072: INFO: Created: latency-svc-pjfvq
May 31 13:39:13.096: INFO: Got endpoints: latency-svc-f6lbb [463.055077ms]
May 31 13:39:13.112: INFO: Created: latency-svc-z57ch
May 31 13:39:13.144: INFO: Got endpoints: latency-svc-l74l6 [504.360012ms]
May 31 13:39:13.176: INFO: Created: latency-svc-js2z6
May 31 13:39:13.195: INFO: Got endpoints: latency-svc-kxj8j [486.482853ms]
May 31 13:39:13.211: INFO: Created: latency-svc-sl7bf
May 31 13:39:13.246: INFO: Got endpoints: latency-svc-8m6fw [535.916069ms]
May 31 13:39:13.260: INFO: Created: latency-svc-vpgnr
May 31 13:39:13.297: INFO: Got endpoints: latency-svc-xm7fq [584.776264ms]
May 31 13:39:13.316: INFO: Created: latency-svc-gk5h5
May 31 13:39:13.347: INFO: Got endpoints: latency-svc-qr5qm [635.398341ms]
May 31 13:39:13.365: INFO: Created: latency-svc-tbpbh
May 31 13:39:13.401: INFO: Got endpoints: latency-svc-mzsjc [628.2017ms]
May 31 13:39:13.416: INFO: Created: latency-svc-s72dd
May 31 13:39:13.445: INFO: Got endpoints: latency-svc-4r2rh [629.568817ms]
May 31 13:39:13.462: INFO: Created: latency-svc-hjqf4
May 31 13:39:13.495: INFO: Got endpoints: latency-svc-j5pn6 [659.089634ms]
May 31 13:39:13.511: INFO: Created: latency-svc-7bdmd
May 31 13:39:13.545: INFO: Got endpoints: latency-svc-5shd5 [705.569585ms]
May 31 13:39:13.562: INFO: Created: latency-svc-4nxwq
May 31 13:39:13.597: INFO: Got endpoints: latency-svc-wh4zl [734.495481ms]
May 31 13:39:13.610: INFO: Created: latency-svc-2xxwt
May 31 13:39:13.645: INFO: Got endpoints: latency-svc-mrdzz [750.00264ms]
May 31 13:39:13.661: INFO: Created: latency-svc-tb5rn
May 31 13:39:13.698: INFO: Got endpoints: latency-svc-8mr6p [738.755882ms]
May 31 13:39:13.711: INFO: Created: latency-svc-xwt5l
May 31 13:39:13.743: INFO: Got endpoints: latency-svc-mxc64 [745.539109ms]
May 31 13:39:13.759: INFO: Created: latency-svc-tgkfj
May 31 13:39:13.795: INFO: Got endpoints: latency-svc-pjfvq [741.441794ms]
May 31 13:39:13.810: INFO: Created: latency-svc-fm46w
May 31 13:39:13.846: INFO: Got endpoints: latency-svc-z57ch [749.409132ms]
May 31 13:39:13.863: INFO: Created: latency-svc-cnjvs
May 31 13:39:13.895: INFO: Got endpoints: latency-svc-js2z6 [750.324578ms]
May 31 13:39:13.912: INFO: Created: latency-svc-n2hvs
May 31 13:39:13.947: INFO: Got endpoints: latency-svc-sl7bf [751.259416ms]
May 31 13:39:13.966: INFO: Created: latency-svc-sh6lv
May 31 13:39:13.999: INFO: Got endpoints: latency-svc-vpgnr [753.461307ms]
May 31 13:39:14.014: INFO: Created: latency-svc-xprm4
May 31 13:39:14.046: INFO: Got endpoints: latency-svc-gk5h5 [749.301901ms]
May 31 13:39:14.066: INFO: Created: latency-svc-lddsg
May 31 13:39:14.098: INFO: Got endpoints: latency-svc-tbpbh [750.663998ms]
May 31 13:39:14.115: INFO: Created: latency-svc-86m4b
May 31 13:39:14.147: INFO: Got endpoints: latency-svc-s72dd [746.114505ms]
May 31 13:39:14.164: INFO: Created: latency-svc-28rxn
May 31 13:39:14.198: INFO: Got endpoints: latency-svc-hjqf4 [752.706989ms]
May 31 13:39:14.215: INFO: Created: latency-svc-s86k9
May 31 13:39:14.248: INFO: Got endpoints: latency-svc-7bdmd [752.358713ms]
May 31 13:39:14.264: INFO: Created: latency-svc-b2j47
May 31 13:39:14.295: INFO: Got endpoints: latency-svc-4nxwq [749.234427ms]
May 31 13:39:14.318: INFO: Created: latency-svc-6l9xw
May 31 13:39:14.344: INFO: Got endpoints: latency-svc-2xxwt [747.515886ms]
May 31 13:39:14.361: INFO: Created: latency-svc-sn9lz
May 31 13:39:14.398: INFO: Got endpoints: latency-svc-tb5rn [752.008112ms]
May 31 13:39:14.413: INFO: Created: latency-svc-64b9x
May 31 13:39:14.448: INFO: Got endpoints: latency-svc-xwt5l [749.560657ms]
May 31 13:39:14.467: INFO: Created: latency-svc-8d7zl
May 31 13:39:14.496: INFO: Got endpoints: latency-svc-tgkfj [752.402021ms]
May 31 13:39:14.515: INFO: Created: latency-svc-5hjwq
May 31 13:39:14.545: INFO: Got endpoints: latency-svc-fm46w [749.984887ms]
May 31 13:39:14.563: INFO: Created: latency-svc-5x7sq
May 31 13:39:14.596: INFO: Got endpoints: latency-svc-cnjvs [749.638802ms]
May 31 13:39:14.610: INFO: Created: latency-svc-xklsg
May 31 13:39:14.648: INFO: Got endpoints: latency-svc-n2hvs [751.848994ms]
May 31 13:39:14.665: INFO: Created: latency-svc-zfbbc
May 31 13:39:14.697: INFO: Got endpoints: latency-svc-sh6lv [749.290878ms]
May 31 13:39:14.710: INFO: Created: latency-svc-lpkjv
May 31 13:39:14.745: INFO: Got endpoints: latency-svc-xprm4 [745.130162ms]
May 31 13:39:14.760: INFO: Created: latency-svc-t9745
May 31 13:39:14.796: INFO: Got endpoints: latency-svc-lddsg [749.184097ms]
May 31 13:39:14.810: INFO: Created: latency-svc-jjlsf
May 31 13:39:14.846: INFO: Got endpoints: latency-svc-86m4b [747.7202ms]
May 31 13:39:14.866: INFO: Created: latency-svc-jkttm
May 31 13:39:14.897: INFO: Got endpoints: latency-svc-28rxn [748.889452ms]
May 31 13:39:14.913: INFO: Created: latency-svc-k49cm
May 31 13:39:14.945: INFO: Got endpoints: latency-svc-s86k9 [746.721902ms]
May 31 13:39:14.961: INFO: Created: latency-svc-479br
May 31 13:39:14.998: INFO: Got endpoints: latency-svc-b2j47 [750.058083ms]
May 31 13:39:15.014: INFO: Created: latency-svc-25bbs
May 31 13:39:15.059: INFO: Got endpoints: latency-svc-6l9xw [763.070115ms]
May 31 13:39:15.074: INFO: Created: latency-svc-crv9s
May 31 13:39:15.101: INFO: Got endpoints: latency-svc-sn9lz [755.634299ms]
May 31 13:39:15.122: INFO: Created: latency-svc-lzcx6
May 31 13:39:15.151: INFO: Got endpoints: latency-svc-64b9x [753.172465ms]
May 31 13:39:15.168: INFO: Created: latency-svc-9w798
May 31 13:39:15.196: INFO: Got endpoints: latency-svc-8d7zl [747.695485ms]
May 31 13:39:15.212: INFO: Created: latency-svc-24j5h
May 31 13:39:15.248: INFO: Got endpoints: latency-svc-5hjwq [750.223123ms]
May 31 13:39:15.266: INFO: Created: latency-svc-kp484
May 31 13:39:15.295: INFO: Got endpoints: latency-svc-5x7sq [748.945781ms]
May 31 13:39:15.309: INFO: Created: latency-svc-kbx5w
May 31 13:39:15.349: INFO: Got endpoints: latency-svc-xklsg [753.332255ms]
May 31 13:39:15.365: INFO: Created: latency-svc-jrmkw
May 31 13:39:15.394: INFO: Got endpoints: latency-svc-zfbbc [745.619753ms]
May 31 13:39:15.409: INFO: Created: latency-svc-c2vt2
May 31 13:39:15.447: INFO: Got endpoints: latency-svc-lpkjv [750.186979ms]
May 31 13:39:15.462: INFO: Created: latency-svc-l7kfx
May 31 13:39:15.496: INFO: Got endpoints: latency-svc-t9745 [750.243602ms]
May 31 13:39:15.515: INFO: Created: latency-svc-fs6jf
May 31 13:39:15.546: INFO: Got endpoints: latency-svc-jjlsf [749.943877ms]
May 31 13:39:15.560: INFO: Created: latency-svc-ssh7v
May 31 13:39:15.599: INFO: Got endpoints: latency-svc-jkttm [751.394546ms]
May 31 13:39:15.618: INFO: Created: latency-svc-wj6s4
May 31 13:39:15.648: INFO: Got endpoints: latency-svc-k49cm [751.232525ms]
May 31 13:39:15.669: INFO: Created: latency-svc-vn8ds
May 31 13:39:15.694: INFO: Got endpoints: latency-svc-479br [748.338302ms]
May 31 13:39:15.708: INFO: Created: latency-svc-pwwdx
May 31 13:39:15.747: INFO: Got endpoints: latency-svc-25bbs [748.884744ms]
May 31 13:39:15.766: INFO: Created: latency-svc-smnx8
May 31 13:39:15.796: INFO: Got endpoints: latency-svc-crv9s [737.496122ms]
May 31 13:39:15.810: INFO: Created: latency-svc-jqqwf
May 31 13:39:15.849: INFO: Got endpoints: latency-svc-lzcx6 [747.851896ms]
May 31 13:39:15.869: INFO: Created: latency-svc-gzr9n
May 31 13:39:15.895: INFO: Got endpoints: latency-svc-9w798 [743.206029ms]
May 31 13:39:15.911: INFO: Created: latency-svc-jzmzj
May 31 13:39:15.948: INFO: Got endpoints: latency-svc-24j5h [751.407131ms]
May 31 13:39:15.962: INFO: Created: latency-svc-wk7g7
May 31 13:39:15.994: INFO: Got endpoints: latency-svc-kp484 [743.557185ms]
May 31 13:39:16.009: INFO: Created: latency-svc-sbbh6
May 31 13:39:16.059: INFO: Got endpoints: latency-svc-kbx5w [764.454966ms]
May 31 13:39:16.082: INFO: Created: latency-svc-zz7pn
May 31 13:39:16.097: INFO: Got endpoints: latency-svc-jrmkw [746.483823ms]
May 31 13:39:16.144: INFO: Got endpoints: latency-svc-c2vt2 [750.598271ms]
May 31 13:39:16.161: INFO: Created: latency-svc-nnhlz
May 31 13:39:16.165: INFO: Created: latency-svc-d89fr
May 31 13:39:16.194: INFO: Got endpoints: latency-svc-l7kfx [746.642519ms]
May 31 13:39:16.210: INFO: Created: latency-svc-lf25m
May 31 13:39:16.246: INFO: Got endpoints: latency-svc-fs6jf [750.090984ms]
May 31 13:39:16.267: INFO: Created: latency-svc-qck6x
May 31 13:39:16.296: INFO: Got endpoints: latency-svc-ssh7v [749.939023ms]
May 31 13:39:16.314: INFO: Created: latency-svc-rg89g
May 31 13:39:16.345: INFO: Got endpoints: latency-svc-wj6s4 [746.313347ms]
May 31 13:39:16.369: INFO: Created: latency-svc-c8sdr
May 31 13:39:16.399: INFO: Got endpoints: latency-svc-vn8ds [751.007137ms]
May 31 13:39:16.420: INFO: Created: latency-svc-q2rzj
May 31 13:39:16.447: INFO: Got endpoints: latency-svc-pwwdx [753.566116ms]
May 31 13:39:16.465: INFO: Created: latency-svc-74v62
May 31 13:39:16.494: INFO: Got endpoints: latency-svc-smnx8 [747.185388ms]
May 31 13:39:16.513: INFO: Created: latency-svc-sqw45
May 31 13:39:16.545: INFO: Got endpoints: latency-svc-jqqwf [748.713389ms]
May 31 13:39:16.567: INFO: Created: latency-svc-c2j22
May 31 13:39:16.595: INFO: Got endpoints: latency-svc-gzr9n [745.404618ms]
May 31 13:39:16.610: INFO: Created: latency-svc-kkqqx
May 31 13:39:16.648: INFO: Got endpoints: latency-svc-jzmzj [752.517073ms]
May 31 13:39:16.668: INFO: Created: latency-svc-f968l
May 31 13:39:16.696: INFO: Got endpoints: latency-svc-wk7g7 [747.703524ms]
May 31 13:39:16.711: INFO: Created: latency-svc-wpft4
May 31 13:39:16.745: INFO: Got endpoints: latency-svc-sbbh6 [750.874445ms]
May 31 13:39:16.763: INFO: Created: latency-svc-7kqm9
May 31 13:39:16.798: INFO: Got endpoints: latency-svc-zz7pn [738.218899ms]
May 31 13:39:16.818: INFO: Created: latency-svc-249nl
May 31 13:39:16.848: INFO: Got endpoints: latency-svc-nnhlz [751.395844ms]
May 31 13:39:16.871: INFO: Created: latency-svc-zmqpb
May 31 13:39:16.900: INFO: Got endpoints: latency-svc-d89fr [755.401219ms]
May 31 13:39:16.933: INFO: Created: latency-svc-4tfkv
May 31 13:39:16.951: INFO: Got endpoints: latency-svc-lf25m [756.196943ms]
May 31 13:39:16.968: INFO: Created: latency-svc-l4btq
May 31 13:39:16.996: INFO: Got endpoints: latency-svc-qck6x [749.394646ms]
May 31 13:39:17.013: INFO: Created: latency-svc-vrh7j
May 31 13:39:17.044: INFO: Got endpoints: latency-svc-rg89g [747.647315ms]
May 31 13:39:17.058: INFO: Created: latency-svc-dphrr
May 31 13:39:17.096: INFO: Got endpoints: latency-svc-c8sdr [750.338237ms]
May 31 13:39:17.114: INFO: Created: latency-svc-27qb4
May 31 13:39:17.145: INFO: Got endpoints: latency-svc-q2rzj [746.004897ms]
May 31 13:39:17.167: INFO: Created: latency-svc-cl2rj
May 31 13:39:17.196: INFO: Got endpoints: latency-svc-74v62 [748.621937ms]
May 31 13:39:17.213: INFO: Created: latency-svc-dc8hm
May 31 13:39:17.246: INFO: Got endpoints: latency-svc-sqw45 [750.987763ms]
May 31 13:39:17.264: INFO: Created: latency-svc-pwvvj
May 31 13:39:17.296: INFO: Got endpoints: latency-svc-c2j22 [749.271723ms]
May 31 13:39:17.311: INFO: Created: latency-svc-5kk9h
May 31 13:39:17.348: INFO: Got endpoints: latency-svc-kkqqx [753.063384ms]
May 31 13:39:17.363: INFO: Created: latency-svc-ck9ss
May 31 13:39:17.397: INFO: Got endpoints: latency-svc-f968l [749.183832ms]
May 31 13:39:17.412: INFO: Created: latency-svc-p755c
May 31 13:39:17.450: INFO: Got endpoints: latency-svc-wpft4 [753.571206ms]
May 31 13:39:17.469: INFO: Created: latency-svc-fbtbc
May 31 13:39:17.494: INFO: Got endpoints: latency-svc-7kqm9 [748.605085ms]
May 31 13:39:17.509: INFO: Created: latency-svc-m448r
May 31 13:39:17.547: INFO: Got endpoints: latency-svc-249nl [748.425812ms]
May 31 13:39:17.564: INFO: Created: latency-svc-ppf2n
May 31 13:39:17.595: INFO: Got endpoints: latency-svc-zmqpb [746.393157ms]
May 31 13:39:17.609: INFO: Created: latency-svc-srq5q
May 31 13:39:17.646: INFO: Got endpoints: latency-svc-4tfkv [745.091533ms]
May 31 13:39:17.661: INFO: Created: latency-svc-wm2v8
May 31 13:39:17.697: INFO: Got endpoints: latency-svc-l4btq [744.988273ms]
May 31 13:39:17.711: INFO: Created: latency-svc-n4jt8
May 31 13:39:17.747: INFO: Got endpoints: latency-svc-vrh7j [750.986286ms]
May 31 13:39:17.763: INFO: Created: latency-svc-9pp99
May 31 13:39:17.796: INFO: Got endpoints: latency-svc-dphrr [752.314031ms]
May 31 13:39:17.824: INFO: Created: latency-svc-jhkrs
May 31 13:39:17.862: INFO: Got endpoints: latency-svc-27qb4 [765.622779ms]
May 31 13:39:17.879: INFO: Created: latency-svc-drcmm
May 31 13:39:17.895: INFO: Got endpoints: latency-svc-cl2rj [749.336656ms]
May 31 13:39:17.913: INFO: Created: latency-svc-lxh65
May 31 13:39:17.950: INFO: Got endpoints: latency-svc-dc8hm [753.950102ms]
May 31 13:39:17.973: INFO: Created: latency-svc-v8lgz
May 31 13:39:17.996: INFO: Got endpoints: latency-svc-pwvvj [749.027249ms]
May 31 13:39:18.012: INFO: Created: latency-svc-2s9qt
May 31 13:39:18.045: INFO: Got endpoints: latency-svc-5kk9h [749.724284ms]
May 31 13:39:18.066: INFO: Created: latency-svc-7bnqc
May 31 13:39:18.106: INFO: Got endpoints: latency-svc-ck9ss [757.270376ms]
May 31 13:39:18.139: INFO: Created: latency-svc-vxw98
May 31 13:39:18.148: INFO: Got endpoints: latency-svc-p755c [750.308397ms]
May 31 13:39:18.175: INFO: Created: latency-svc-nrwwd
May 31 13:39:18.199: INFO: Got endpoints: latency-svc-fbtbc [748.511576ms]
May 31 13:39:18.222: INFO: Created: latency-svc-zb5wx
May 31 13:39:18.248: INFO: Got endpoints: latency-svc-m448r [754.061311ms]
May 31 13:39:18.264: INFO: Created: latency-svc-2jwmd
May 31 13:39:18.296: INFO: Got endpoints: latency-svc-ppf2n [748.483409ms]
May 31 13:39:18.324: INFO: Created: latency-svc-hbfn4
May 31 13:39:18.356: INFO: Got endpoints: latency-svc-srq5q [760.181038ms]
May 31 13:39:18.371: INFO: Created: latency-svc-vzt4h
May 31 13:39:18.395: INFO: Got endpoints: latency-svc-wm2v8 [749.282087ms]
May 31 13:39:18.411: INFO: Created: latency-svc-q7sks
May 31 13:39:18.447: INFO: Got endpoints: latency-svc-n4jt8 [749.524242ms]
May 31 13:39:18.465: INFO: Created: latency-svc-2hwj4
May 31 13:39:18.498: INFO: Got endpoints: latency-svc-9pp99 [751.395439ms]
May 31 13:39:18.513: INFO: Created: latency-svc-8lxdn
May 31 13:39:18.572: INFO: Got endpoints: latency-svc-jhkrs [775.371857ms]
May 31 13:39:18.587: INFO: Created: latency-svc-nfnnn
May 31 13:39:18.595: INFO: Got endpoints: latency-svc-drcmm [733.310584ms]
May 31 13:39:18.613: INFO: Created: latency-svc-kt8d2
May 31 13:39:18.647: INFO: Got endpoints: latency-svc-lxh65 [752.020634ms]
May 31 13:39:18.662: INFO: Created: latency-svc-kmtr2
May 31 13:39:18.695: INFO: Got endpoints: latency-svc-v8lgz [744.582236ms]
May 31 13:39:18.712: INFO: Created: latency-svc-2r8m5
May 31 13:39:18.746: INFO: Got endpoints: latency-svc-2s9qt [750.61868ms]
May 31 13:39:18.764: INFO: Created: latency-svc-nbjgs
May 31 13:39:18.794: INFO: Got endpoints: latency-svc-7bnqc [748.594713ms]
May 31 13:39:18.811: INFO: Created: latency-svc-4jxk4
May 31 13:39:18.847: INFO: Got endpoints: latency-svc-vxw98 [740.721916ms]
May 31 13:39:18.863: INFO: Created: latency-svc-p8n89
May 31 13:39:18.898: INFO: Got endpoints: latency-svc-nrwwd [749.899407ms]
May 31 13:39:18.914: INFO: Created: latency-svc-gl6t8
May 31 13:39:18.948: INFO: Got endpoints: latency-svc-zb5wx [748.830867ms]
May 31 13:39:18.964: INFO: Created: latency-svc-7hgmw
May 31 13:39:18.998: INFO: Got endpoints: latency-svc-2jwmd [749.350536ms]
May 31 13:39:19.016: INFO: Created: latency-svc-j6g4s
May 31 13:39:19.046: INFO: Got endpoints: latency-svc-hbfn4 [750.52452ms]
May 31 13:39:19.066: INFO: Created: latency-svc-l997t
May 31 13:39:19.096: INFO: Got endpoints: latency-svc-vzt4h [739.845791ms]
May 31 13:39:19.110: INFO: Created: latency-svc-bxd8f
May 31 13:39:19.144: INFO: Got endpoints: latency-svc-q7sks [748.347052ms]
May 31 13:39:19.161: INFO: Created: latency-svc-6jddq
May 31 13:39:19.202: INFO: Got endpoints: latency-svc-2hwj4 [754.727373ms]
May 31 13:39:19.223: INFO: Created: latency-svc-fks2l
May 31 13:39:19.247: INFO: Got endpoints: latency-svc-8lxdn [748.65278ms]
May 31 13:39:19.280: INFO: Created: latency-svc-wfz4j
May 31 13:39:19.295: INFO: Got endpoints: latency-svc-nfnnn [723.434921ms]
May 31 13:39:19.313: INFO: Created: latency-svc-n5mxx
May 31 13:39:19.346: INFO: Got endpoints: latency-svc-kt8d2 [750.618159ms]
May 31 13:39:19.367: INFO: Created: latency-svc-jm7z5
May 31 13:39:19.397: INFO: Got endpoints: latency-svc-kmtr2 [748.97364ms]
May 31 13:39:19.415: INFO: Created: latency-svc-5h8sm
May 31 13:39:19.447: INFO: Got endpoints: latency-svc-2r8m5 [751.770204ms]
May 31 13:39:19.464: INFO: Created: latency-svc-pdvhf
May 31 13:39:19.497: INFO: Got endpoints: latency-svc-nbjgs [750.124275ms]
May 31 13:39:19.516: INFO: Created: latency-svc-jqr5j
May 31 13:39:19.551: INFO: Got endpoints: latency-svc-4jxk4 [756.811221ms]
May 31 13:39:19.570: INFO: Created: latency-svc-nnxkp
May 31 13:39:19.594: INFO: Got endpoints: latency-svc-p8n89 [746.666023ms]
May 31 13:39:19.614: INFO: Created: latency-svc-nd4sb
May 31 13:39:19.646: INFO: Got endpoints: latency-svc-gl6t8 [748.338635ms]
May 31 13:39:19.668: INFO: Created: latency-svc-8swfp
May 31 13:39:19.703: INFO: Got endpoints: latency-svc-7hgmw [754.684745ms]
May 31 13:39:19.720: INFO: Created: latency-svc-h5gg8
May 31 13:39:19.748: INFO: Got endpoints: latency-svc-j6g4s [749.993269ms]
May 31 13:39:19.764: INFO: Created: latency-svc-26z4q
May 31 13:39:19.798: INFO: Got endpoints: latency-svc-l997t [751.868544ms]
May 31 13:39:19.831: INFO: Created: latency-svc-wt6k4
May 31 13:39:19.855: INFO: Got endpoints: latency-svc-bxd8f [758.931126ms]
May 31 13:39:19.881: INFO: Created: latency-svc-lf29j
May 31 13:39:19.897: INFO: Got endpoints: latency-svc-6jddq [753.535815ms]
May 31 13:39:19.916: INFO: Created: latency-svc-gxw92
May 31 13:39:19.947: INFO: Got endpoints: latency-svc-fks2l [745.287731ms]
May 31 13:39:19.994: INFO: Got endpoints: latency-svc-wfz4j [745.905312ms]
May 31 13:39:20.048: INFO: Got endpoints: latency-svc-n5mxx [752.953546ms]
May 31 13:39:20.094: INFO: Got endpoints: latency-svc-jm7z5 [747.072106ms]
May 31 13:39:20.147: INFO: Got endpoints: latency-svc-5h8sm [749.986523ms]
May 31 13:39:20.197: INFO: Got endpoints: latency-svc-pdvhf [749.928884ms]
May 31 13:39:20.250: INFO: Got endpoints: latency-svc-jqr5j [753.014832ms]
May 31 13:39:20.297: INFO: Got endpoints: latency-svc-nnxkp [745.243009ms]
May 31 13:39:20.359: INFO: Got endpoints: latency-svc-nd4sb [765.02917ms]
May 31 13:39:20.398: INFO: Got endpoints: latency-svc-8swfp [751.323556ms]
May 31 13:39:20.446: INFO: Got endpoints: latency-svc-h5gg8 [742.552774ms]
May 31 13:39:20.497: INFO: Got endpoints: latency-svc-26z4q [748.608069ms]
May 31 13:39:20.550: INFO: Got endpoints: latency-svc-wt6k4 [751.071796ms]
May 31 13:39:20.597: INFO: Got endpoints: latency-svc-lf29j [741.893066ms]
May 31 13:39:20.648: INFO: Got endpoints: latency-svc-gxw92 [750.03085ms]
May 31 13:39:20.650: INFO: Latencies: [36.158329ms 62.298493ms 98.280098ms 108.613649ms 125.637313ms 144.934121ms 146.413858ms 177.786869ms 178.710615ms 182.041657ms 182.179329ms 184.458882ms 189.639869ms 190.191997ms 192.320108ms 194.934675ms 196.003215ms 196.057717ms 198.334088ms 200.58898ms 202.074267ms 205.41668ms 205.548702ms 206.429388ms 207.765923ms 208.811973ms 209.2396ms 215.688119ms 218.250937ms 222.949814ms 229.26315ms 229.374483ms 238.098384ms 238.618523ms 239.281436ms 243.442783ms 253.55905ms 261.960636ms 267.745752ms 277.300369ms 281.133429ms 291.587343ms 305.956809ms 309.448294ms 329.613486ms 385.960809ms 412.386608ms 457.057645ms 463.055077ms 486.482853ms 504.360012ms 535.916069ms 584.776264ms 628.2017ms 629.568817ms 635.398341ms 659.089634ms 705.569585ms 723.434921ms 733.310584ms 734.495481ms 737.496122ms 738.218899ms 738.755882ms 739.845791ms 740.721916ms 741.441794ms 741.893066ms 742.552774ms 743.206029ms 743.557185ms 744.582236ms 744.988273ms 745.091533ms 745.130162ms 745.243009ms 745.287731ms 745.404618ms 745.539109ms 745.619753ms 745.905312ms 746.004897ms 746.114505ms 746.313347ms 746.393157ms 746.483823ms 746.642519ms 746.666023ms 746.721902ms 747.072106ms 747.185388ms 747.515886ms 747.647315ms 747.695485ms 747.703524ms 747.7202ms 747.851896ms 748.338302ms 748.338635ms 748.347052ms 748.425812ms 748.483409ms 748.511576ms 748.594713ms 748.605085ms 748.608069ms 748.621937ms 748.65278ms 748.713389ms 748.830867ms 748.884744ms 748.889452ms 748.945781ms 748.97364ms 749.027249ms 749.183832ms 749.184097ms 749.234427ms 749.271723ms 749.282087ms 749.290878ms 749.301901ms 749.336656ms 749.350536ms 749.394646ms 749.409132ms 749.524242ms 749.560657ms 749.638802ms 749.724284ms 749.899407ms 749.928884ms 749.939023ms 749.943877ms 749.984887ms 749.986523ms 749.993269ms 750.00264ms 750.03085ms 750.058083ms 750.090984ms 750.124275ms 750.186979ms 750.223123ms 750.243602ms 750.308397ms 750.324578ms 750.338237ms 750.52452ms 750.598271ms 750.618159ms 750.61868ms 750.663998ms 750.874445ms 750.986286ms 750.987763ms 751.007137ms 751.071796ms 751.232525ms 751.259416ms 751.323556ms 751.394546ms 751.395439ms 751.395844ms 751.407131ms 751.770204ms 751.848994ms 751.868544ms 752.008112ms 752.020634ms 752.314031ms 752.358713ms 752.402021ms 752.517073ms 752.706989ms 752.953546ms 753.014832ms 753.063384ms 753.172465ms 753.332255ms 753.461307ms 753.535815ms 753.566116ms 753.571206ms 753.950102ms 754.061311ms 754.684745ms 754.727373ms 755.401219ms 755.634299ms 756.196943ms 756.811221ms 757.270376ms 758.931126ms 760.181038ms 763.070115ms 764.454966ms 765.02917ms 765.622779ms 775.371857ms]
May 31 13:39:20.651: INFO: 50 %ile: 748.425812ms
May 31 13:39:20.651: INFO: 90 %ile: 753.461307ms
May 31 13:39:20.651: INFO: 99 %ile: 765.622779ms
May 31 13:39:20.651: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:39:20.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-2358" for this suite.

• [SLOW TEST:12.825 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":346,"completed":106,"skipped":2129,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:39:20.682: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 31 13:39:20.760: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c349ac7c-61bd-4f01-be34-956c6ac9de16" in namespace "projected-6121" to be "Succeeded or Failed"
May 31 13:39:20.770: INFO: Pod "downwardapi-volume-c349ac7c-61bd-4f01-be34-956c6ac9de16": Phase="Pending", Reason="", readiness=false. Elapsed: 9.862264ms
May 31 13:39:22.784: INFO: Pod "downwardapi-volume-c349ac7c-61bd-4f01-be34-956c6ac9de16": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023898232s
May 31 13:39:24.795: INFO: Pod "downwardapi-volume-c349ac7c-61bd-4f01-be34-956c6ac9de16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034749184s
STEP: Saw pod success
May 31 13:39:24.795: INFO: Pod "downwardapi-volume-c349ac7c-61bd-4f01-be34-956c6ac9de16" satisfied condition "Succeeded or Failed"
May 31 13:39:24.803: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod downwardapi-volume-c349ac7c-61bd-4f01-be34-956c6ac9de16 container client-container: <nil>
STEP: delete the pod
May 31 13:39:24.843: INFO: Waiting for pod downwardapi-volume-c349ac7c-61bd-4f01-be34-956c6ac9de16 to disappear
May 31 13:39:24.854: INFO: Pod downwardapi-volume-c349ac7c-61bd-4f01-be34-956c6ac9de16 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:39:24.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6121" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":107,"skipped":2172,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:39:24.877: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2367.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2367.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2367.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2367.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2367.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2367.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2367.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2367.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2367.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2367.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2367.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2367.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 68.18.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.18.68_udp@PTR;check="$$(dig +tcp +noall +answer +search 68.18.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.18.68_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2367.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2367.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2367.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2367.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2367.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2367.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2367.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2367.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2367.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2367.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2367.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2367.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 68.18.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.18.68_udp@PTR;check="$$(dig +tcp +noall +answer +search 68.18.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.18.68_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 31 13:39:29.204: INFO: Unable to read wheezy_udp@dns-test-service.dns-2367.svc.cluster.local from pod dns-2367/dns-test-641422e4-9dc3-44a1-a3d6-90948b8c7b78: the server could not find the requested resource (get pods dns-test-641422e4-9dc3-44a1-a3d6-90948b8c7b78)
May 31 13:39:29.214: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2367.svc.cluster.local from pod dns-2367/dns-test-641422e4-9dc3-44a1-a3d6-90948b8c7b78: the server could not find the requested resource (get pods dns-test-641422e4-9dc3-44a1-a3d6-90948b8c7b78)
May 31 13:39:29.223: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2367.svc.cluster.local from pod dns-2367/dns-test-641422e4-9dc3-44a1-a3d6-90948b8c7b78: the server could not find the requested resource (get pods dns-test-641422e4-9dc3-44a1-a3d6-90948b8c7b78)
May 31 13:39:29.273: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2367.svc.cluster.local from pod dns-2367/dns-test-641422e4-9dc3-44a1-a3d6-90948b8c7b78: the server could not find the requested resource (get pods dns-test-641422e4-9dc3-44a1-a3d6-90948b8c7b78)
May 31 13:39:29.336: INFO: Unable to read jessie_udp@dns-test-service.dns-2367.svc.cluster.local from pod dns-2367/dns-test-641422e4-9dc3-44a1-a3d6-90948b8c7b78: the server could not find the requested resource (get pods dns-test-641422e4-9dc3-44a1-a3d6-90948b8c7b78)
May 31 13:39:29.347: INFO: Unable to read jessie_tcp@dns-test-service.dns-2367.svc.cluster.local from pod dns-2367/dns-test-641422e4-9dc3-44a1-a3d6-90948b8c7b78: the server could not find the requested resource (get pods dns-test-641422e4-9dc3-44a1-a3d6-90948b8c7b78)
May 31 13:39:29.360: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2367.svc.cluster.local from pod dns-2367/dns-test-641422e4-9dc3-44a1-a3d6-90948b8c7b78: the server could not find the requested resource (get pods dns-test-641422e4-9dc3-44a1-a3d6-90948b8c7b78)
May 31 13:39:29.371: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2367.svc.cluster.local from pod dns-2367/dns-test-641422e4-9dc3-44a1-a3d6-90948b8c7b78: the server could not find the requested resource (get pods dns-test-641422e4-9dc3-44a1-a3d6-90948b8c7b78)
May 31 13:39:29.427: INFO: Lookups using dns-2367/dns-test-641422e4-9dc3-44a1-a3d6-90948b8c7b78 failed for: [wheezy_udp@dns-test-service.dns-2367.svc.cluster.local wheezy_tcp@dns-test-service.dns-2367.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2367.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2367.svc.cluster.local jessie_udp@dns-test-service.dns-2367.svc.cluster.local jessie_tcp@dns-test-service.dns-2367.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2367.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2367.svc.cluster.local]

May 31 13:39:34.675: INFO: DNS probes using dns-2367/dns-test-641422e4-9dc3-44a1-a3d6-90948b8c7b78 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:39:34.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2367" for this suite.

• [SLOW TEST:9.907 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":346,"completed":108,"skipped":2191,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:39:34.793: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:39:36.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6346" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":346,"completed":109,"skipped":2213,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:39:36.990: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir volume type on node default medium
May 31 13:39:37.057: INFO: Waiting up to 5m0s for pod "pod-233f459d-8783-4588-bcdb-33115b868b6d" in namespace "emptydir-9389" to be "Succeeded or Failed"
May 31 13:39:37.067: INFO: Pod "pod-233f459d-8783-4588-bcdb-33115b868b6d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.561154ms
May 31 13:39:39.078: INFO: Pod "pod-233f459d-8783-4588-bcdb-33115b868b6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020489699s
May 31 13:39:41.091: INFO: Pod "pod-233f459d-8783-4588-bcdb-33115b868b6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033223019s
STEP: Saw pod success
May 31 13:39:41.092: INFO: Pod "pod-233f459d-8783-4588-bcdb-33115b868b6d" satisfied condition "Succeeded or Failed"
May 31 13:39:41.099: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod pod-233f459d-8783-4588-bcdb-33115b868b6d container test-container: <nil>
STEP: delete the pod
May 31 13:39:41.182: INFO: Waiting for pod pod-233f459d-8783-4588-bcdb-33115b868b6d to disappear
May 31 13:39:41.202: INFO: Pod pod-233f459d-8783-4588-bcdb-33115b868b6d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:39:41.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9389" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":110,"skipped":2236,"failed":0}
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:39:41.235: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 31 13:39:41.473: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 31 13:39:41.473: INFO: Node ip-172-31-11-247.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 31 13:39:42.506: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 31 13:39:42.506: INFO: Node ip-172-31-11-247.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 31 13:39:43.501: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May 31 13:39:43.501: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status
May 31 13:39:43.514: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status
May 31 13:39:43.535: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated
May 31 13:39:43.540: INFO: Observed &DaemonSet event: ADDED
May 31 13:39:43.540: INFO: Observed &DaemonSet event: MODIFIED
May 31 13:39:43.540: INFO: Observed &DaemonSet event: MODIFIED
May 31 13:39:43.540: INFO: Observed &DaemonSet event: MODIFIED
May 31 13:39:43.541: INFO: Observed &DaemonSet event: MODIFIED
May 31 13:39:43.541: INFO: Observed &DaemonSet event: MODIFIED
May 31 13:39:43.541: INFO: Found daemon set daemon-set in namespace daemonsets-9984 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
May 31 13:39:43.541: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status
STEP: watching for the daemon set status to be patched
May 31 13:39:43.557: INFO: Observed &DaemonSet event: ADDED
May 31 13:39:43.558: INFO: Observed &DaemonSet event: MODIFIED
May 31 13:39:43.558: INFO: Observed &DaemonSet event: MODIFIED
May 31 13:39:43.558: INFO: Observed &DaemonSet event: MODIFIED
May 31 13:39:43.559: INFO: Observed &DaemonSet event: MODIFIED
May 31 13:39:43.559: INFO: Observed &DaemonSet event: MODIFIED
May 31 13:39:43.559: INFO: Observed daemon set daemon-set in namespace daemonsets-9984 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
May 31 13:39:43.559: INFO: Observed &DaemonSet event: MODIFIED
May 31 13:39:43.559: INFO: Found daemon set daemon-set in namespace daemonsets-9984 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
May 31 13:39:43.559: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9984, will wait for the garbage collector to delete the pods
May 31 13:39:43.635: INFO: Deleting DaemonSet.extensions daemon-set took: 9.478043ms
May 31 13:39:43.736: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.688589ms
May 31 13:39:46.447: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 31 13:39:46.447: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May 31 13:39:46.456: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20298"},"items":null}

May 31 13:39:46.462: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20298"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:39:46.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9984" for this suite.

• [SLOW TEST:5.302 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","total":346,"completed":111,"skipped":2239,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:39:46.535: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating the pod
May 31 13:39:46.671: INFO: The status of Pod labelsupdate36bafa45-e712-41fb-8023-e0617ec31382 is Pending, waiting for it to be Running (with Ready = true)
May 31 13:39:48.682: INFO: The status of Pod labelsupdate36bafa45-e712-41fb-8023-e0617ec31382 is Running (Ready = true)
May 31 13:39:49.277: INFO: Successfully updated pod "labelsupdate36bafa45-e712-41fb-8023-e0617ec31382"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:39:51.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1463" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":112,"skipped":2253,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:39:51.362: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
May 31 13:39:51.438: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:39:56.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1053" for this suite.

• [SLOW TEST:5.642 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","total":346,"completed":113,"skipped":2277,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:39:57.004: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-projected-all-test-volume-689a0c05-ea73-49a2-a189-2b438bc93298
STEP: Creating secret with name secret-projected-all-test-volume-0a9f63c5-c593-446a-bb34-9ae2e1728595
STEP: Creating a pod to test Check all projections for projected volume plugin
May 31 13:39:57.122: INFO: Waiting up to 5m0s for pod "projected-volume-2a27dd02-a8fb-4156-a244-3c8a769a4099" in namespace "projected-4432" to be "Succeeded or Failed"
May 31 13:39:57.146: INFO: Pod "projected-volume-2a27dd02-a8fb-4156-a244-3c8a769a4099": Phase="Pending", Reason="", readiness=false. Elapsed: 23.602146ms
May 31 13:39:59.157: INFO: Pod "projected-volume-2a27dd02-a8fb-4156-a244-3c8a769a4099": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034326325s
May 31 13:40:01.172: INFO: Pod "projected-volume-2a27dd02-a8fb-4156-a244-3c8a769a4099": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049726634s
STEP: Saw pod success
May 31 13:40:01.172: INFO: Pod "projected-volume-2a27dd02-a8fb-4156-a244-3c8a769a4099" satisfied condition "Succeeded or Failed"
May 31 13:40:01.180: INFO: Trying to get logs from node ip-172-31-3-160.eu-central-1.compute.internal pod projected-volume-2a27dd02-a8fb-4156-a244-3c8a769a4099 container projected-all-volume-test: <nil>
STEP: delete the pod
May 31 13:40:01.279: INFO: Waiting for pod projected-volume-2a27dd02-a8fb-4156-a244-3c8a769a4099 to disappear
May 31 13:40:01.293: INFO: Pod projected-volume-2a27dd02-a8fb-4156-a244-3c8a769a4099 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:40:01.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4432" for this suite.
•{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":346,"completed":114,"skipped":2299,"failed":0}

------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:40:01.328: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name projected-secret-test-7515f94d-5a63-44b4-a673-46f16bb01120
STEP: Creating a pod to test consume secrets
May 31 13:40:01.434: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-aa65a88b-75ae-4a2d-8f87-4643a18db5fb" in namespace "projected-8178" to be "Succeeded or Failed"
May 31 13:40:01.446: INFO: Pod "pod-projected-secrets-aa65a88b-75ae-4a2d-8f87-4643a18db5fb": Phase="Pending", Reason="", readiness=false. Elapsed: 12.010278ms
May 31 13:40:03.461: INFO: Pod "pod-projected-secrets-aa65a88b-75ae-4a2d-8f87-4643a18db5fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026677881s
May 31 13:40:05.478: INFO: Pod "pod-projected-secrets-aa65a88b-75ae-4a2d-8f87-4643a18db5fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044067382s
STEP: Saw pod success
May 31 13:40:05.479: INFO: Pod "pod-projected-secrets-aa65a88b-75ae-4a2d-8f87-4643a18db5fb" satisfied condition "Succeeded or Failed"
May 31 13:40:05.496: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod pod-projected-secrets-aa65a88b-75ae-4a2d-8f87-4643a18db5fb container secret-volume-test: <nil>
STEP: delete the pod
May 31 13:40:05.548: INFO: Waiting for pod pod-projected-secrets-aa65a88b-75ae-4a2d-8f87-4643a18db5fb to disappear
May 31 13:40:05.556: INFO: Pod pod-projected-secrets-aa65a88b-75ae-4a2d-8f87-4643a18db5fb no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:40:05.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8178" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":115,"skipped":2299,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:40:05.590: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 31 13:40:05.668: INFO: Waiting up to 5m0s for pod "downwardapi-volume-84b86832-be4f-4bd1-9e6a-0da014d8558b" in namespace "downward-api-1414" to be "Succeeded or Failed"
May 31 13:40:05.686: INFO: Pod "downwardapi-volume-84b86832-be4f-4bd1-9e6a-0da014d8558b": Phase="Pending", Reason="", readiness=false. Elapsed: 18.421257ms
May 31 13:40:07.702: INFO: Pod "downwardapi-volume-84b86832-be4f-4bd1-9e6a-0da014d8558b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034074238s
May 31 13:40:09.713: INFO: Pod "downwardapi-volume-84b86832-be4f-4bd1-9e6a-0da014d8558b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045221847s
STEP: Saw pod success
May 31 13:40:09.713: INFO: Pod "downwardapi-volume-84b86832-be4f-4bd1-9e6a-0da014d8558b" satisfied condition "Succeeded or Failed"
May 31 13:40:09.721: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod downwardapi-volume-84b86832-be4f-4bd1-9e6a-0da014d8558b container client-container: <nil>
STEP: delete the pod
May 31 13:40:09.797: INFO: Waiting for pod downwardapi-volume-84b86832-be4f-4bd1-9e6a-0da014d8558b to disappear
May 31 13:40:09.803: INFO: Pod downwardapi-volume-84b86832-be4f-4bd1-9e6a-0da014d8558b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:40:09.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1414" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":116,"skipped":2328,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:40:09.837: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-570aacff-b0ba-4464-b215-f1fbd9b6b44c
STEP: Creating a pod to test consume secrets
May 31 13:40:09.918: INFO: Waiting up to 5m0s for pod "pod-secrets-a1e69510-3556-46e2-9b62-7f66cfcfdfe3" in namespace "secrets-3781" to be "Succeeded or Failed"
May 31 13:40:09.926: INFO: Pod "pod-secrets-a1e69510-3556-46e2-9b62-7f66cfcfdfe3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.623022ms
May 31 13:40:11.936: INFO: Pod "pod-secrets-a1e69510-3556-46e2-9b62-7f66cfcfdfe3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017767064s
May 31 13:40:13.946: INFO: Pod "pod-secrets-a1e69510-3556-46e2-9b62-7f66cfcfdfe3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028014869s
STEP: Saw pod success
May 31 13:40:13.946: INFO: Pod "pod-secrets-a1e69510-3556-46e2-9b62-7f66cfcfdfe3" satisfied condition "Succeeded or Failed"
May 31 13:40:13.952: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod pod-secrets-a1e69510-3556-46e2-9b62-7f66cfcfdfe3 container secret-volume-test: <nil>
STEP: delete the pod
May 31 13:40:14.038: INFO: Waiting for pod pod-secrets-a1e69510-3556-46e2-9b62-7f66cfcfdfe3 to disappear
May 31 13:40:14.053: INFO: Pod pod-secrets-a1e69510-3556-46e2-9b62-7f66cfcfdfe3 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:40:14.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3781" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":117,"skipped":2348,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:40:14.094: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 31 13:40:14.185: INFO: Waiting up to 5m0s for pod "pod-13cca804-47f8-40b5-aad2-f1254c44798e" in namespace "emptydir-1167" to be "Succeeded or Failed"
May 31 13:40:14.190: INFO: Pod "pod-13cca804-47f8-40b5-aad2-f1254c44798e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.49905ms
May 31 13:40:16.208: INFO: Pod "pod-13cca804-47f8-40b5-aad2-f1254c44798e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023017969s
May 31 13:40:18.226: INFO: Pod "pod-13cca804-47f8-40b5-aad2-f1254c44798e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041137342s
STEP: Saw pod success
May 31 13:40:18.226: INFO: Pod "pod-13cca804-47f8-40b5-aad2-f1254c44798e" satisfied condition "Succeeded or Failed"
May 31 13:40:18.233: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod pod-13cca804-47f8-40b5-aad2-f1254c44798e container test-container: <nil>
STEP: delete the pod
May 31 13:40:18.278: INFO: Waiting for pod pod-13cca804-47f8-40b5-aad2-f1254c44798e to disappear
May 31 13:40:18.286: INFO: Pod pod-13cca804-47f8-40b5-aad2-f1254c44798e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:40:18.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1167" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":118,"skipped":2359,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:40:18.310: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
May 31 13:40:29.358: INFO: 74 pods remaining
May 31 13:40:29.358: INFO: 74 pods has nil DeletionTimestamp
May 31 13:40:29.358: INFO: 
STEP: Gathering metrics
W0531 13:40:34.444976      19 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
May 31 13:40:34.447: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

May 31 13:40:34.447: INFO: Deleting pod "simpletest-rc-to-be-deleted-222s4" in namespace "gc-375"
May 31 13:40:34.479: INFO: Deleting pod "simpletest-rc-to-be-deleted-2cxjz" in namespace "gc-375"
May 31 13:40:34.526: INFO: Deleting pod "simpletest-rc-to-be-deleted-2lctd" in namespace "gc-375"
May 31 13:40:34.555: INFO: Deleting pod "simpletest-rc-to-be-deleted-2qwwf" in namespace "gc-375"
May 31 13:40:34.575: INFO: Deleting pod "simpletest-rc-to-be-deleted-42dk4" in namespace "gc-375"
May 31 13:40:34.598: INFO: Deleting pod "simpletest-rc-to-be-deleted-494x6" in namespace "gc-375"
May 31 13:40:34.620: INFO: Deleting pod "simpletest-rc-to-be-deleted-4cz4p" in namespace "gc-375"
May 31 13:40:34.671: INFO: Deleting pod "simpletest-rc-to-be-deleted-4fhxd" in namespace "gc-375"
May 31 13:40:34.697: INFO: Deleting pod "simpletest-rc-to-be-deleted-4lbxw" in namespace "gc-375"
May 31 13:40:34.730: INFO: Deleting pod "simpletest-rc-to-be-deleted-4v2zw" in namespace "gc-375"
May 31 13:40:34.760: INFO: Deleting pod "simpletest-rc-to-be-deleted-4xbqz" in namespace "gc-375"
May 31 13:40:34.815: INFO: Deleting pod "simpletest-rc-to-be-deleted-5229f" in namespace "gc-375"
May 31 13:40:34.840: INFO: Deleting pod "simpletest-rc-to-be-deleted-527dc" in namespace "gc-375"
May 31 13:40:34.866: INFO: Deleting pod "simpletest-rc-to-be-deleted-5n8pp" in namespace "gc-375"
May 31 13:40:34.896: INFO: Deleting pod "simpletest-rc-to-be-deleted-5wsgc" in namespace "gc-375"
May 31 13:40:34.914: INFO: Deleting pod "simpletest-rc-to-be-deleted-6dbrv" in namespace "gc-375"
May 31 13:40:34.942: INFO: Deleting pod "simpletest-rc-to-be-deleted-6j27f" in namespace "gc-375"
May 31 13:40:34.961: INFO: Deleting pod "simpletest-rc-to-be-deleted-6ps22" in namespace "gc-375"
May 31 13:40:35.010: INFO: Deleting pod "simpletest-rc-to-be-deleted-6ps96" in namespace "gc-375"
May 31 13:40:35.039: INFO: Deleting pod "simpletest-rc-to-be-deleted-6qstb" in namespace "gc-375"
May 31 13:40:35.074: INFO: Deleting pod "simpletest-rc-to-be-deleted-6t247" in namespace "gc-375"
May 31 13:40:35.096: INFO: Deleting pod "simpletest-rc-to-be-deleted-7299v" in namespace "gc-375"
May 31 13:40:35.115: INFO: Deleting pod "simpletest-rc-to-be-deleted-72vns" in namespace "gc-375"
May 31 13:40:35.138: INFO: Deleting pod "simpletest-rc-to-be-deleted-77phm" in namespace "gc-375"
May 31 13:40:35.160: INFO: Deleting pod "simpletest-rc-to-be-deleted-7pbqs" in namespace "gc-375"
May 31 13:40:35.183: INFO: Deleting pod "simpletest-rc-to-be-deleted-7wrs5" in namespace "gc-375"
May 31 13:40:35.211: INFO: Deleting pod "simpletest-rc-to-be-deleted-86d9t" in namespace "gc-375"
May 31 13:40:35.247: INFO: Deleting pod "simpletest-rc-to-be-deleted-8gzpb" in namespace "gc-375"
May 31 13:40:35.278: INFO: Deleting pod "simpletest-rc-to-be-deleted-8rjcx" in namespace "gc-375"
May 31 13:40:35.304: INFO: Deleting pod "simpletest-rc-to-be-deleted-8x8m2" in namespace "gc-375"
May 31 13:40:35.325: INFO: Deleting pod "simpletest-rc-to-be-deleted-9565b" in namespace "gc-375"
May 31 13:40:35.343: INFO: Deleting pod "simpletest-rc-to-be-deleted-9cm78" in namespace "gc-375"
May 31 13:40:35.362: INFO: Deleting pod "simpletest-rc-to-be-deleted-9mnb7" in namespace "gc-375"
May 31 13:40:35.398: INFO: Deleting pod "simpletest-rc-to-be-deleted-9mtw2" in namespace "gc-375"
May 31 13:40:35.434: INFO: Deleting pod "simpletest-rc-to-be-deleted-9rb95" in namespace "gc-375"
May 31 13:40:35.494: INFO: Deleting pod "simpletest-rc-to-be-deleted-bgkgq" in namespace "gc-375"
May 31 13:40:35.536: INFO: Deleting pod "simpletest-rc-to-be-deleted-cj4ww" in namespace "gc-375"
May 31 13:40:35.557: INFO: Deleting pod "simpletest-rc-to-be-deleted-cjhw5" in namespace "gc-375"
May 31 13:40:35.605: INFO: Deleting pod "simpletest-rc-to-be-deleted-cl54n" in namespace "gc-375"
May 31 13:40:35.627: INFO: Deleting pod "simpletest-rc-to-be-deleted-cnvzp" in namespace "gc-375"
May 31 13:40:35.650: INFO: Deleting pod "simpletest-rc-to-be-deleted-cvkpt" in namespace "gc-375"
May 31 13:40:35.676: INFO: Deleting pod "simpletest-rc-to-be-deleted-czw2n" in namespace "gc-375"
May 31 13:40:35.700: INFO: Deleting pod "simpletest-rc-to-be-deleted-dbq9z" in namespace "gc-375"
May 31 13:40:35.722: INFO: Deleting pod "simpletest-rc-to-be-deleted-dd7v6" in namespace "gc-375"
May 31 13:40:35.752: INFO: Deleting pod "simpletest-rc-to-be-deleted-ddhnw" in namespace "gc-375"
May 31 13:40:35.777: INFO: Deleting pod "simpletest-rc-to-be-deleted-dtl2j" in namespace "gc-375"
May 31 13:40:35.822: INFO: Deleting pod "simpletest-rc-to-be-deleted-dz6jz" in namespace "gc-375"
May 31 13:40:35.850: INFO: Deleting pod "simpletest-rc-to-be-deleted-dzdm4" in namespace "gc-375"
May 31 13:40:35.894: INFO: Deleting pod "simpletest-rc-to-be-deleted-dzrkj" in namespace "gc-375"
May 31 13:40:35.928: INFO: Deleting pod "simpletest-rc-to-be-deleted-fwnd8" in namespace "gc-375"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:40:35.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-375" for this suite.

• [SLOW TEST:17.674 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":346,"completed":119,"skipped":2368,"failed":0}
SSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:40:35.987: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test override command
May 31 13:40:36.062: INFO: Waiting up to 5m0s for pod "client-containers-1e810fe4-6618-45d0-b678-1bd9d807618a" in namespace "containers-689" to be "Succeeded or Failed"
May 31 13:40:36.070: INFO: Pod "client-containers-1e810fe4-6618-45d0-b678-1bd9d807618a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.579777ms
May 31 13:40:38.088: INFO: Pod "client-containers-1e810fe4-6618-45d0-b678-1bd9d807618a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026462567s
May 31 13:40:40.099: INFO: Pod "client-containers-1e810fe4-6618-45d0-b678-1bd9d807618a": Phase="Running", Reason="", readiness=true. Elapsed: 4.037349648s
May 31 13:40:42.111: INFO: Pod "client-containers-1e810fe4-6618-45d0-b678-1bd9d807618a": Phase="Running", Reason="", readiness=false. Elapsed: 6.048865261s
May 31 13:40:44.123: INFO: Pod "client-containers-1e810fe4-6618-45d0-b678-1bd9d807618a": Phase="Running", Reason="", readiness=false. Elapsed: 8.060673256s
May 31 13:40:46.137: INFO: Pod "client-containers-1e810fe4-6618-45d0-b678-1bd9d807618a": Phase="Running", Reason="", readiness=false. Elapsed: 10.074681208s
May 31 13:40:48.148: INFO: Pod "client-containers-1e810fe4-6618-45d0-b678-1bd9d807618a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.08614807s
STEP: Saw pod success
May 31 13:40:48.149: INFO: Pod "client-containers-1e810fe4-6618-45d0-b678-1bd9d807618a" satisfied condition "Succeeded or Failed"
May 31 13:40:48.165: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod client-containers-1e810fe4-6618-45d0-b678-1bd9d807618a container agnhost-container: <nil>
STEP: delete the pod
May 31 13:40:48.277: INFO: Waiting for pod client-containers-1e810fe4-6618-45d0-b678-1bd9d807618a to disappear
May 31 13:40:48.285: INFO: Pod client-containers-1e810fe4-6618-45d0-b678-1bd9d807618a no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:40:48.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-689" for this suite.

• [SLOW TEST:12.323 seconds]
[sig-node] Docker Containers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":346,"completed":120,"skipped":2374,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:40:48.314: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
May 31 13:40:50.967: INFO: Successfully updated pod "adopt-release-gknvr"
STEP: Checking that the Job readopts the Pod
May 31 13:40:50.967: INFO: Waiting up to 15m0s for pod "adopt-release-gknvr" in namespace "job-7709" to be "adopted"
May 31 13:40:50.974: INFO: Pod "adopt-release-gknvr": Phase="Running", Reason="", readiness=true. Elapsed: 7.15853ms
May 31 13:40:52.985: INFO: Pod "adopt-release-gknvr": Phase="Running", Reason="", readiness=true. Elapsed: 2.018182585s
May 31 13:40:52.985: INFO: Pod "adopt-release-gknvr" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
May 31 13:40:53.507: INFO: Successfully updated pod "adopt-release-gknvr"
STEP: Checking that the Job releases the Pod
May 31 13:40:53.508: INFO: Waiting up to 15m0s for pod "adopt-release-gknvr" in namespace "job-7709" to be "released"
May 31 13:40:53.519: INFO: Pod "adopt-release-gknvr": Phase="Running", Reason="", readiness=true. Elapsed: 11.749739ms
May 31 13:40:55.531: INFO: Pod "adopt-release-gknvr": Phase="Running", Reason="", readiness=true. Elapsed: 2.023191141s
May 31 13:40:55.531: INFO: Pod "adopt-release-gknvr" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:40:55.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7709" for this suite.

• [SLOW TEST:7.254 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":346,"completed":121,"skipped":2424,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:40:55.570: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod with failed condition
STEP: updating the pod
May 31 13:42:56.226: INFO: Successfully updated pod "var-expansion-f933b525-464a-45dc-9e65-6578e597659a"
STEP: waiting for pod running
STEP: deleting the pod gracefully
May 31 13:42:58.262: INFO: Deleting pod "var-expansion-f933b525-464a-45dc-9e65-6578e597659a" in namespace "var-expansion-2245"
May 31 13:42:58.283: INFO: Wait up to 5m0s for pod "var-expansion-f933b525-464a-45dc-9e65-6578e597659a" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:43:30.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2245" for this suite.

• [SLOW TEST:154.762 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","total":346,"completed":122,"skipped":2437,"failed":0}
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:43:30.333: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-map-260d4ab5-1621-4d29-9761-5ffba5a3ccc3
STEP: Creating a pod to test consume configMaps
May 31 13:43:30.437: INFO: Waiting up to 5m0s for pod "pod-configmaps-922dcbf1-1dc0-4841-bbc5-3079e8d8873e" in namespace "configmap-9797" to be "Succeeded or Failed"
May 31 13:43:30.446: INFO: Pod "pod-configmaps-922dcbf1-1dc0-4841-bbc5-3079e8d8873e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.360887ms
May 31 13:43:32.458: INFO: Pod "pod-configmaps-922dcbf1-1dc0-4841-bbc5-3079e8d8873e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021451913s
May 31 13:43:34.469: INFO: Pod "pod-configmaps-922dcbf1-1dc0-4841-bbc5-3079e8d8873e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032152794s
STEP: Saw pod success
May 31 13:43:34.469: INFO: Pod "pod-configmaps-922dcbf1-1dc0-4841-bbc5-3079e8d8873e" satisfied condition "Succeeded or Failed"
May 31 13:43:34.475: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod pod-configmaps-922dcbf1-1dc0-4841-bbc5-3079e8d8873e container agnhost-container: <nil>
STEP: delete the pod
May 31 13:43:34.519: INFO: Waiting for pod pod-configmaps-922dcbf1-1dc0-4841-bbc5-3079e8d8873e to disappear
May 31 13:43:34.526: INFO: Pod pod-configmaps-922dcbf1-1dc0-4841-bbc5-3079e8d8873e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:43:34.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9797" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":123,"skipped":2439,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:43:34.555: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating pod
May 31 13:43:34.623: INFO: The status of Pod pod-hostip-f7c59c87-bd42-43ea-8748-eb792450b172 is Pending, waiting for it to be Running (with Ready = true)
May 31 13:43:36.634: INFO: The status of Pod pod-hostip-f7c59c87-bd42-43ea-8748-eb792450b172 is Running (Ready = true)
May 31 13:43:36.647: INFO: Pod pod-hostip-f7c59c87-bd42-43ea-8748-eb792450b172 has hostIP: 172.31.8.180
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:43:36.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6066" for this suite.
•{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","total":346,"completed":124,"skipped":2448,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:43:36.672: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 31 13:43:36.784: INFO: Waiting up to 5m0s for pod "downwardapi-volume-07e9021c-0b12-421d-85ff-928bb5002e2c" in namespace "downward-api-1151" to be "Succeeded or Failed"
May 31 13:43:36.794: INFO: Pod "downwardapi-volume-07e9021c-0b12-421d-85ff-928bb5002e2c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.992653ms
May 31 13:43:38.807: INFO: Pod "downwardapi-volume-07e9021c-0b12-421d-85ff-928bb5002e2c": Phase="Running", Reason="", readiness=true. Elapsed: 2.023050885s
May 31 13:43:40.819: INFO: Pod "downwardapi-volume-07e9021c-0b12-421d-85ff-928bb5002e2c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035368372s
STEP: Saw pod success
May 31 13:43:40.820: INFO: Pod "downwardapi-volume-07e9021c-0b12-421d-85ff-928bb5002e2c" satisfied condition "Succeeded or Failed"
May 31 13:43:40.826: INFO: Trying to get logs from node ip-172-31-3-160.eu-central-1.compute.internal pod downwardapi-volume-07e9021c-0b12-421d-85ff-928bb5002e2c container client-container: <nil>
STEP: delete the pod
May 31 13:43:40.862: INFO: Waiting for pod downwardapi-volume-07e9021c-0b12-421d-85ff-928bb5002e2c to disappear
May 31 13:43:40.868: INFO: Pod downwardapi-volume-07e9021c-0b12-421d-85ff-928bb5002e2c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:43:40.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1151" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":125,"skipped":2454,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:43:40.887: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
May 31 13:43:41.348: INFO: Pod name wrapped-volume-race-4b36c3d7-138f-4393-972d-d855b2d17c6f: Found 0 pods out of 5
May 31 13:43:46.368: INFO: Pod name wrapped-volume-race-4b36c3d7-138f-4393-972d-d855b2d17c6f: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-4b36c3d7-138f-4393-972d-d855b2d17c6f in namespace emptydir-wrapper-4434, will wait for the garbage collector to delete the pods
May 31 13:43:56.494: INFO: Deleting ReplicationController wrapped-volume-race-4b36c3d7-138f-4393-972d-d855b2d17c6f took: 13.229288ms
May 31 13:43:56.596: INFO: Terminating ReplicationController wrapped-volume-race-4b36c3d7-138f-4393-972d-d855b2d17c6f pods took: 101.784226ms
STEP: Creating RC which spawns configmap-volume pods
May 31 13:44:01.639: INFO: Pod name wrapped-volume-race-f5fedbc2-7b0a-4758-9a2a-a8966e38ad9e: Found 0 pods out of 5
May 31 13:44:06.660: INFO: Pod name wrapped-volume-race-f5fedbc2-7b0a-4758-9a2a-a8966e38ad9e: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-f5fedbc2-7b0a-4758-9a2a-a8966e38ad9e in namespace emptydir-wrapper-4434, will wait for the garbage collector to delete the pods
May 31 13:44:18.794: INFO: Deleting ReplicationController wrapped-volume-race-f5fedbc2-7b0a-4758-9a2a-a8966e38ad9e took: 17.658442ms
May 31 13:44:18.995: INFO: Terminating ReplicationController wrapped-volume-race-f5fedbc2-7b0a-4758-9a2a-a8966e38ad9e pods took: 200.879058ms
STEP: Creating RC which spawns configmap-volume pods
May 31 13:44:21.548: INFO: Pod name wrapped-volume-race-d441928e-c70d-49fe-93d3-333f67ca34dd: Found 0 pods out of 5
May 31 13:44:26.566: INFO: Pod name wrapped-volume-race-d441928e-c70d-49fe-93d3-333f67ca34dd: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-d441928e-c70d-49fe-93d3-333f67ca34dd in namespace emptydir-wrapper-4434, will wait for the garbage collector to delete the pods
May 31 13:44:36.703: INFO: Deleting ReplicationController wrapped-volume-race-d441928e-c70d-49fe-93d3-333f67ca34dd took: 17.77945ms
May 31 13:44:36.804: INFO: Terminating ReplicationController wrapped-volume-race-d441928e-c70d-49fe-93d3-333f67ca34dd pods took: 101.3013ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:44:41.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4434" for this suite.

• [SLOW TEST:60.174 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":346,"completed":126,"skipped":2456,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:44:41.062: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 13:44:41.099: INFO: Creating pod...
May 31 13:44:41.129: INFO: Pod Quantity: 1 Status: Pending
May 31 13:44:42.143: INFO: Pod Quantity: 1 Status: Pending
May 31 13:44:43.140: INFO: Pod Status: Running
May 31 13:44:43.140: INFO: Creating service...
May 31 13:44:43.159: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-7582/pods/agnhost/proxy/some/path/with/DELETE
May 31 13:44:43.222: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
May 31 13:44:43.223: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-7582/pods/agnhost/proxy/some/path/with/GET
May 31 13:44:43.240: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
May 31 13:44:43.240: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-7582/pods/agnhost/proxy/some/path/with/HEAD
May 31 13:44:43.329: INFO: http.Client request:HEAD | StatusCode:200
May 31 13:44:43.329: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-7582/pods/agnhost/proxy/some/path/with/OPTIONS
May 31 13:44:43.339: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
May 31 13:44:43.339: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-7582/pods/agnhost/proxy/some/path/with/PATCH
May 31 13:44:43.349: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
May 31 13:44:43.349: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-7582/pods/agnhost/proxy/some/path/with/POST
May 31 13:44:43.362: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
May 31 13:44:43.362: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-7582/pods/agnhost/proxy/some/path/with/PUT
May 31 13:44:43.372: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
May 31 13:44:43.372: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-7582/services/test-service/proxy/some/path/with/DELETE
May 31 13:44:43.398: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
May 31 13:44:43.398: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-7582/services/test-service/proxy/some/path/with/GET
May 31 13:44:43.417: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
May 31 13:44:43.417: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-7582/services/test-service/proxy/some/path/with/HEAD
May 31 13:44:43.430: INFO: http.Client request:HEAD | StatusCode:200
May 31 13:44:43.430: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-7582/services/test-service/proxy/some/path/with/OPTIONS
May 31 13:44:43.445: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
May 31 13:44:43.445: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-7582/services/test-service/proxy/some/path/with/PATCH
May 31 13:44:43.471: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
May 31 13:44:43.474: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-7582/services/test-service/proxy/some/path/with/POST
May 31 13:44:43.496: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
May 31 13:44:43.496: INFO: Starting http.Client for https://10.240.16.1:443/api/v1/namespaces/proxy-7582/services/test-service/proxy/some/path/with/PUT
May 31 13:44:43.533: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:44:43.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7582" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","total":346,"completed":127,"skipped":2480,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:44:43.564: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 13:44:43.609: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:44:44.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7680" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":346,"completed":128,"skipped":2489,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:44:44.675: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 31 13:44:44.745: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e93c6cfe-1903-4694-a0b6-97e3ae53aecc" in namespace "projected-7634" to be "Succeeded or Failed"
May 31 13:44:44.753: INFO: Pod "downwardapi-volume-e93c6cfe-1903-4694-a0b6-97e3ae53aecc": Phase="Pending", Reason="", readiness=false. Elapsed: 7.485251ms
May 31 13:44:46.781: INFO: Pod "downwardapi-volume-e93c6cfe-1903-4694-a0b6-97e3ae53aecc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035335448s
May 31 13:44:48.803: INFO: Pod "downwardapi-volume-e93c6cfe-1903-4694-a0b6-97e3ae53aecc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.057278256s
STEP: Saw pod success
May 31 13:44:48.814: INFO: Pod "downwardapi-volume-e93c6cfe-1903-4694-a0b6-97e3ae53aecc" satisfied condition "Succeeded or Failed"
May 31 13:44:48.832: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod downwardapi-volume-e93c6cfe-1903-4694-a0b6-97e3ae53aecc container client-container: <nil>
STEP: delete the pod
May 31 13:44:48.905: INFO: Waiting for pod downwardapi-volume-e93c6cfe-1903-4694-a0b6-97e3ae53aecc to disappear
May 31 13:44:48.914: INFO: Pod downwardapi-volume-e93c6cfe-1903-4694-a0b6-97e3ae53aecc no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:44:48.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7634" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":129,"skipped":2497,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:44:48.964: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir volume type on tmpfs
May 31 13:44:49.101: INFO: Waiting up to 5m0s for pod "pod-5c7a7807-112d-44e7-8c0c-d00f5cd6c50a" in namespace "emptydir-6490" to be "Succeeded or Failed"
May 31 13:44:49.117: INFO: Pod "pod-5c7a7807-112d-44e7-8c0c-d00f5cd6c50a": Phase="Pending", Reason="", readiness=false. Elapsed: 15.926408ms
May 31 13:44:51.128: INFO: Pod "pod-5c7a7807-112d-44e7-8c0c-d00f5cd6c50a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026947256s
May 31 13:44:53.140: INFO: Pod "pod-5c7a7807-112d-44e7-8c0c-d00f5cd6c50a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039014751s
STEP: Saw pod success
May 31 13:44:53.140: INFO: Pod "pod-5c7a7807-112d-44e7-8c0c-d00f5cd6c50a" satisfied condition "Succeeded or Failed"
May 31 13:44:53.146: INFO: Trying to get logs from node ip-172-31-3-160.eu-central-1.compute.internal pod pod-5c7a7807-112d-44e7-8c0c-d00f5cd6c50a container test-container: <nil>
STEP: delete the pod
May 31 13:44:53.251: INFO: Waiting for pod pod-5c7a7807-112d-44e7-8c0c-d00f5cd6c50a to disappear
May 31 13:44:53.257: INFO: Pod pod-5c7a7807-112d-44e7-8c0c-d00f5cd6c50a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:44:53.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6490" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":130,"skipped":2504,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:44:53.279: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 13:44:53.353: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 31 13:44:56.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=crd-publish-openapi-4277 --namespace=crd-publish-openapi-4277 create -f -'
May 31 13:44:57.820: INFO: stderr: ""
May 31 13:44:57.820: INFO: stdout: "e2e-test-crd-publish-openapi-7360-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May 31 13:44:57.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=crd-publish-openapi-4277 --namespace=crd-publish-openapi-4277 delete e2e-test-crd-publish-openapi-7360-crds test-cr'
May 31 13:44:57.948: INFO: stderr: ""
May 31 13:44:57.948: INFO: stdout: "e2e-test-crd-publish-openapi-7360-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
May 31 13:44:57.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=crd-publish-openapi-4277 --namespace=crd-publish-openapi-4277 apply -f -'
May 31 13:44:58.303: INFO: stderr: ""
May 31 13:44:58.303: INFO: stdout: "e2e-test-crd-publish-openapi-7360-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
May 31 13:44:58.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=crd-publish-openapi-4277 --namespace=crd-publish-openapi-4277 delete e2e-test-crd-publish-openapi-7360-crds test-cr'
May 31 13:44:58.397: INFO: stderr: ""
May 31 13:44:58.397: INFO: stdout: "e2e-test-crd-publish-openapi-7360-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
May 31 13:44:58.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=crd-publish-openapi-4277 explain e2e-test-crd-publish-openapi-7360-crds'
May 31 13:44:59.312: INFO: stderr: ""
May 31 13:44:59.312: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7360-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:45:03.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4277" for this suite.

• [SLOW TEST:9.919 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":346,"completed":131,"skipped":2510,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:45:03.199: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Performing setup for networking test in namespace pod-network-test-4047
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 31 13:45:03.257: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 31 13:45:03.353: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 31 13:45:05.371: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 31 13:45:07.383: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 31 13:45:09.387: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 31 13:45:11.376: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 31 13:45:13.371: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 31 13:45:15.371: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 31 13:45:17.376: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 31 13:45:19.374: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 31 13:45:21.372: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 31 13:45:23.382: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 31 13:45:23.401: INFO: The status of Pod netserver-1 is Running (Ready = true)
May 31 13:45:23.421: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
May 31 13:45:25.537: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May 31 13:45:25.537: INFO: Going to poll 172.25.0.77 on port 8083 at least 0 times, with a maximum of 39 tries before failing
May 31 13:45:25.543: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.0.77:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4047 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 31 13:45:25.544: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
May 31 13:45:25.544: INFO: ExecWithOptions: Clientset creation
May 31 13:45:25.545: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-4047/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.25.0.77%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
May 31 13:45:25.905: INFO: Found all 1 expected endpoints: [netserver-0]
May 31 13:45:25.905: INFO: Going to poll 172.25.2.121 on port 8083 at least 0 times, with a maximum of 39 tries before failing
May 31 13:45:25.913: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.2.121:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4047 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 31 13:45:25.913: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
May 31 13:45:25.914: INFO: ExecWithOptions: Clientset creation
May 31 13:45:25.915: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-4047/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.25.2.121%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
May 31 13:45:26.296: INFO: Found all 1 expected endpoints: [netserver-1]
May 31 13:45:26.296: INFO: Going to poll 172.25.1.168 on port 8083 at least 0 times, with a maximum of 39 tries before failing
May 31 13:45:26.309: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.1.168:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4047 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 31 13:45:26.309: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
May 31 13:45:26.311: INFO: ExecWithOptions: Clientset creation
May 31 13:45:26.311: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-4047/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.25.1.168%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
May 31 13:45:26.654: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:45:26.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4047" for this suite.

• [SLOW TEST:23.478 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":132,"skipped":2557,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:45:26.679: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
May 31 13:45:26.758: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1531  3d6cdb04-5f16-4020-899f-37a8f5ed11f1 24710 0 2022-05-31 13:45:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-05-31 13:45:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 31 13:45:26.758: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1531  3d6cdb04-5f16-4020-899f-37a8f5ed11f1 24710 0 2022-05-31 13:45:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-05-31 13:45:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
May 31 13:45:26.776: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1531  3d6cdb04-5f16-4020-899f-37a8f5ed11f1 24711 0 2022-05-31 13:45:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-05-31 13:45:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
May 31 13:45:26.776: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1531  3d6cdb04-5f16-4020-899f-37a8f5ed11f1 24711 0 2022-05-31 13:45:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-05-31 13:45:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
May 31 13:45:26.794: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1531  3d6cdb04-5f16-4020-899f-37a8f5ed11f1 24712 0 2022-05-31 13:45:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-05-31 13:45:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 31 13:45:26.794: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1531  3d6cdb04-5f16-4020-899f-37a8f5ed11f1 24712 0 2022-05-31 13:45:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-05-31 13:45:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
May 31 13:45:26.821: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1531  3d6cdb04-5f16-4020-899f-37a8f5ed11f1 24713 0 2022-05-31 13:45:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-05-31 13:45:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 31 13:45:26.821: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1531  3d6cdb04-5f16-4020-899f-37a8f5ed11f1 24713 0 2022-05-31 13:45:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-05-31 13:45:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
May 31 13:45:26.847: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1531  d5f2f601-e236-482d-88d3-7f6aaab3529b 24714 0 2022-05-31 13:45:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-05-31 13:45:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 31 13:45:26.848: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1531  d5f2f601-e236-482d-88d3-7f6aaab3529b 24714 0 2022-05-31 13:45:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-05-31 13:45:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
May 31 13:45:36.872: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1531  d5f2f601-e236-482d-88d3-7f6aaab3529b 24801 0 2022-05-31 13:45:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-05-31 13:45:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 31 13:45:36.872: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1531  d5f2f601-e236-482d-88d3-7f6aaab3529b 24801 0 2022-05-31 13:45:26 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-05-31 13:45:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:45:46.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1531" for this suite.

• [SLOW TEST:20.225 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":346,"completed":133,"skipped":2571,"failed":0}
SSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:45:46.905: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Given a Pod with a 'name' label pod-adoption-release is created
May 31 13:45:46.999: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
May 31 13:45:49.010: INFO: The status of Pod pod-adoption-release is Running (Ready = true)
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
May 31 13:45:50.063: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:45:51.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9692" for this suite.
•{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":346,"completed":134,"skipped":2577,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:45:51.140: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a replication controller
May 31 13:45:51.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-2231 create -f -'
May 31 13:45:51.721: INFO: stderr: ""
May 31 13:45:51.721: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 31 13:45:51.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-2231 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 31 13:45:51.839: INFO: stderr: ""
May 31 13:45:51.839: INFO: stdout: "update-demo-nautilus-d8j8s update-demo-nautilus-rvmsz "
May 31 13:45:51.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-2231 get pods update-demo-nautilus-d8j8s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 31 13:45:51.940: INFO: stderr: ""
May 31 13:45:51.940: INFO: stdout: ""
May 31 13:45:51.940: INFO: update-demo-nautilus-d8j8s is created but not running
May 31 13:45:56.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-2231 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 31 13:45:57.110: INFO: stderr: ""
May 31 13:45:57.110: INFO: stdout: "update-demo-nautilus-d8j8s update-demo-nautilus-rvmsz "
May 31 13:45:57.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-2231 get pods update-demo-nautilus-d8j8s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 31 13:45:57.208: INFO: stderr: ""
May 31 13:45:57.208: INFO: stdout: "true"
May 31 13:45:57.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-2231 get pods update-demo-nautilus-d8j8s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 31 13:45:57.298: INFO: stderr: ""
May 31 13:45:57.298: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
May 31 13:45:57.298: INFO: validating pod update-demo-nautilus-d8j8s
May 31 13:45:57.356: INFO: got data: {
  "image": "nautilus.jpg"
}

May 31 13:45:57.356: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 31 13:45:57.356: INFO: update-demo-nautilus-d8j8s is verified up and running
May 31 13:45:57.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-2231 get pods update-demo-nautilus-rvmsz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 31 13:45:57.431: INFO: stderr: ""
May 31 13:45:57.431: INFO: stdout: "true"
May 31 13:45:57.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-2231 get pods update-demo-nautilus-rvmsz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 31 13:45:57.511: INFO: stderr: ""
May 31 13:45:57.511: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
May 31 13:45:57.511: INFO: validating pod update-demo-nautilus-rvmsz
May 31 13:45:57.612: INFO: got data: {
  "image": "nautilus.jpg"
}

May 31 13:45:57.612: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 31 13:45:57.612: INFO: update-demo-nautilus-rvmsz is verified up and running
STEP: scaling down the replication controller
May 31 13:45:57.615: INFO: scanned /root for discovery docs: <nil>
May 31 13:45:57.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-2231 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
May 31 13:45:58.755: INFO: stderr: ""
May 31 13:45:58.755: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 31 13:45:58.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-2231 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 31 13:45:58.862: INFO: stderr: ""
May 31 13:45:58.862: INFO: stdout: "update-demo-nautilus-rvmsz "
May 31 13:45:58.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-2231 get pods update-demo-nautilus-rvmsz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 31 13:45:58.977: INFO: stderr: ""
May 31 13:45:58.977: INFO: stdout: "true"
May 31 13:45:58.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-2231 get pods update-demo-nautilus-rvmsz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 31 13:45:59.105: INFO: stderr: ""
May 31 13:45:59.105: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
May 31 13:45:59.105: INFO: validating pod update-demo-nautilus-rvmsz
May 31 13:45:59.160: INFO: got data: {
  "image": "nautilus.jpg"
}

May 31 13:45:59.160: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 31 13:45:59.160: INFO: update-demo-nautilus-rvmsz is verified up and running
STEP: scaling up the replication controller
May 31 13:45:59.162: INFO: scanned /root for discovery docs: <nil>
May 31 13:45:59.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-2231 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
May 31 13:46:00.334: INFO: stderr: ""
May 31 13:46:00.334: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 31 13:46:00.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-2231 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 31 13:46:00.511: INFO: stderr: ""
May 31 13:46:00.511: INFO: stdout: "update-demo-nautilus-qf6n7 update-demo-nautilus-rvmsz "
May 31 13:46:00.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-2231 get pods update-demo-nautilus-qf6n7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 31 13:46:00.638: INFO: stderr: ""
May 31 13:46:00.638: INFO: stdout: ""
May 31 13:46:00.638: INFO: update-demo-nautilus-qf6n7 is created but not running
May 31 13:46:05.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-2231 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
May 31 13:46:05.813: INFO: stderr: ""
May 31 13:46:05.813: INFO: stdout: "update-demo-nautilus-qf6n7 update-demo-nautilus-rvmsz "
May 31 13:46:05.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-2231 get pods update-demo-nautilus-qf6n7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 31 13:46:06.049: INFO: stderr: ""
May 31 13:46:06.049: INFO: stdout: "true"
May 31 13:46:06.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-2231 get pods update-demo-nautilus-qf6n7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 31 13:46:06.221: INFO: stderr: ""
May 31 13:46:06.221: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
May 31 13:46:06.221: INFO: validating pod update-demo-nautilus-qf6n7
May 31 13:46:06.276: INFO: got data: {
  "image": "nautilus.jpg"
}

May 31 13:46:06.277: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 31 13:46:06.277: INFO: update-demo-nautilus-qf6n7 is verified up and running
May 31 13:46:06.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-2231 get pods update-demo-nautilus-rvmsz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
May 31 13:46:06.394: INFO: stderr: ""
May 31 13:46:06.394: INFO: stdout: "true"
May 31 13:46:06.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-2231 get pods update-demo-nautilus-rvmsz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
May 31 13:46:06.543: INFO: stderr: ""
May 31 13:46:06.543: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
May 31 13:46:06.543: INFO: validating pod update-demo-nautilus-rvmsz
May 31 13:46:06.601: INFO: got data: {
  "image": "nautilus.jpg"
}

May 31 13:46:06.601: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 31 13:46:06.601: INFO: update-demo-nautilus-rvmsz is verified up and running
STEP: using delete to clean up resources
May 31 13:46:06.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-2231 delete --grace-period=0 --force -f -'
May 31 13:46:06.720: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 31 13:46:06.720: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 31 13:46:06.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-2231 get rc,svc -l name=update-demo --no-headers'
May 31 13:46:06.892: INFO: stderr: "No resources found in kubectl-2231 namespace.\n"
May 31 13:46:06.892: INFO: stdout: ""
May 31 13:46:06.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-2231 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 31 13:46:07.057: INFO: stderr: ""
May 31 13:46:07.057: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:46:07.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2231" for this suite.

• [SLOW TEST:15.944 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":346,"completed":135,"skipped":2596,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:46:07.084: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
May 31 13:46:07.268: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8015  396b36a5-f52a-4bc0-ba68-d698e6e7a5a0 25055 0 2022-05-31 13:46:07 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-05-31 13:46:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 31 13:46:07.269: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8015  396b36a5-f52a-4bc0-ba68-d698e6e7a5a0 25057 0 2022-05-31 13:46:07 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-05-31 13:46:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:46:07.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8015" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":346,"completed":136,"skipped":2603,"failed":0}
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:46:07.294: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-a1eb5565-b9d7-49a9-8795-77ba407158e8
STEP: Creating a pod to test consume configMaps
May 31 13:46:07.362: INFO: Waiting up to 5m0s for pod "pod-configmaps-60e4e8d1-4d34-4fa5-a996-948fe90d9d13" in namespace "configmap-1834" to be "Succeeded or Failed"
May 31 13:46:07.370: INFO: Pod "pod-configmaps-60e4e8d1-4d34-4fa5-a996-948fe90d9d13": Phase="Pending", Reason="", readiness=false. Elapsed: 7.89693ms
May 31 13:46:09.383: INFO: Pod "pod-configmaps-60e4e8d1-4d34-4fa5-a996-948fe90d9d13": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020955141s
May 31 13:46:11.393: INFO: Pod "pod-configmaps-60e4e8d1-4d34-4fa5-a996-948fe90d9d13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031140312s
STEP: Saw pod success
May 31 13:46:11.394: INFO: Pod "pod-configmaps-60e4e8d1-4d34-4fa5-a996-948fe90d9d13" satisfied condition "Succeeded or Failed"
May 31 13:46:11.401: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod pod-configmaps-60e4e8d1-4d34-4fa5-a996-948fe90d9d13 container agnhost-container: <nil>
STEP: delete the pod
May 31 13:46:11.434: INFO: Waiting for pod pod-configmaps-60e4e8d1-4d34-4fa5-a996-948fe90d9d13 to disappear
May 31 13:46:11.439: INFO: Pod pod-configmaps-60e4e8d1-4d34-4fa5-a996-948fe90d9d13 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:46:11.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1834" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":137,"skipped":2606,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:46:11.466: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating the pod
May 31 13:46:11.559: INFO: The status of Pod annotationupdate12ea74ed-d46e-40aa-a3b3-98b16b05f03d is Pending, waiting for it to be Running (with Ready = true)
May 31 13:46:13.570: INFO: The status of Pod annotationupdate12ea74ed-d46e-40aa-a3b3-98b16b05f03d is Running (Ready = true)
May 31 13:46:14.162: INFO: Successfully updated pod "annotationupdate12ea74ed-d46e-40aa-a3b3-98b16b05f03d"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:46:16.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5594" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":138,"skipped":2652,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:46:16.222: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 13:46:16.362: INFO: Create a RollingUpdate DaemonSet
May 31 13:46:16.376: INFO: Check that daemon pods launch on every node of the cluster
May 31 13:46:16.403: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 31 13:46:16.404: INFO: Node ip-172-31-11-247.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 31 13:46:17.423: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 31 13:46:17.423: INFO: Node ip-172-31-11-247.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 31 13:46:18.423: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May 31 13:46:18.423: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
May 31 13:46:18.424: INFO: Update the DaemonSet to trigger a rollout
May 31 13:46:18.441: INFO: Updating DaemonSet daemon-set
May 31 13:46:21.492: INFO: Roll back the DaemonSet before rollout is complete
May 31 13:46:21.526: INFO: Updating DaemonSet daemon-set
May 31 13:46:21.526: INFO: Make sure DaemonSet rollback is complete
May 31 13:46:21.540: INFO: Wrong image for pod: daemon-set-7mlfb. Expected: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
May 31 13:46:21.540: INFO: Pod daemon-set-7mlfb is not available
May 31 13:46:25.586: INFO: Pod daemon-set-hlggh is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-915, will wait for the garbage collector to delete the pods
May 31 13:46:25.684: INFO: Deleting DaemonSet.extensions daemon-set took: 14.440388ms
May 31 13:46:25.785: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.358581ms
May 31 13:46:27.797: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 31 13:46:27.797: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May 31 13:46:27.803: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"25357"},"items":null}

May 31 13:46:27.808: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"25357"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:46:27.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-915" for this suite.

• [SLOW TEST:11.640 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":346,"completed":139,"skipped":2665,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:46:27.866: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 31 13:46:28.908: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
May 31 13:46:30.968: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 31, 13, 46, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 31, 13, 46, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 31, 13, 46, 28, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 31, 13, 46, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 31 13:46:33.995: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:46:34.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4463" for this suite.
STEP: Destroying namespace "webhook-4463-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.454 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":346,"completed":140,"skipped":2686,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:46:34.321: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:46:47.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3566" for this suite.

• [SLOW TEST:13.263 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":346,"completed":141,"skipped":2732,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:46:47.598: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 13:46:49.705: INFO: Deleting pod "var-expansion-bfcf05e5-41fc-400f-ab2c-40f427da3901" in namespace "var-expansion-3915"
May 31 13:46:49.727: INFO: Wait up to 5m0s for pod "var-expansion-bfcf05e5-41fc-400f-ab2c-40f427da3901" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:46:53.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3915" for this suite.

• [SLOW TEST:6.178 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","total":346,"completed":142,"skipped":2758,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:46:53.779: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with configMap that has name projected-configmap-test-upd-409dac2e-8a7a-4c8a-b3f1-ae67ea204c33
STEP: Creating the pod
May 31 13:46:53.874: INFO: The status of Pod pod-projected-configmaps-af5bce46-50cb-4a37-a833-f7bdcc422813 is Pending, waiting for it to be Running (with Ready = true)
May 31 13:46:55.884: INFO: The status of Pod pod-projected-configmaps-af5bce46-50cb-4a37-a833-f7bdcc422813 is Running (Ready = true)
STEP: Updating configmap projected-configmap-test-upd-409dac2e-8a7a-4c8a-b3f1-ae67ea204c33
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:46:58.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-522" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":143,"skipped":2797,"failed":0}
SSSSSSS
------------------------------
[sig-network] HostPort 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:46:58.104: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename hostport
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/hostport.go:47
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled
May 31 13:46:58.234: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May 31 13:47:00.243: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.31.3.160 on the node which pod1 resides and expect scheduled
May 31 13:47:00.267: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
May 31 13:47:02.284: INFO: The status of Pod pod2 is Running (Ready = false)
May 31 13:47:04.286: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.31.3.160 but use UDP protocol on the node which pod2 resides
May 31 13:47:04.307: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
May 31 13:47:06.319: INFO: The status of Pod pod3 is Running (Ready = true)
May 31 13:47:06.337: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
May 31 13:47:08.344: INFO: The status of Pod e2e-host-exec is Running (Ready = true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
May 31 13:47:08.355: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.31.3.160 http://127.0.0.1:54323/hostname] Namespace:hostport-9192 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 31 13:47:08.355: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
May 31 13:47:08.356: INFO: ExecWithOptions: Clientset creation
May 31 13:47:08.356: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/hostport-9192/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+172.31.3.160+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true %!s(MISSING))
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.3.160, port: 54323
May 31 13:47:08.729: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.31.3.160:54323/hostname] Namespace:hostport-9192 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 31 13:47:08.729: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
May 31 13:47:08.730: INFO: ExecWithOptions: Clientset creation
May 31 13:47:08.730: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/hostport-9192/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F172.31.3.160%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true %!s(MISSING))
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.31.3.160, port: 54323 UDP
May 31 13:47:09.094: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 172.31.3.160 54323] Namespace:hostport-9192 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 31 13:47:09.094: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
May 31 13:47:09.095: INFO: ExecWithOptions: Clientset creation
May 31 13:47:09.095: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/hostport-9192/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=nc+-vuz+-w+5+172.31.3.160+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true %!s(MISSING))
[AfterEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:47:14.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-9192" for this suite.

• [SLOW TEST:16.402 seconds]
[sig-network] HostPort
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","total":346,"completed":144,"skipped":2804,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:47:14.513: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:47:14.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8418" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":346,"completed":145,"skipped":2810,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:47:14.711: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
May 31 13:47:14.782: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
May 31 13:47:16.798: INFO: The status of Pod test-pod is Running (Ready = true)
STEP: Creating hostNetwork=true pod
May 31 13:47:16.831: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
May 31 13:47:18.842: INFO: The status of Pod test-host-network-pod is Running (Ready = true)
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
May 31 13:47:18.849: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-89 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 31 13:47:18.849: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
May 31 13:47:18.849: INFO: ExecWithOptions: Clientset creation
May 31 13:47:18.849: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-89/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
May 31 13:47:19.107: INFO: Exec stderr: ""
May 31 13:47:19.107: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-89 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 31 13:47:19.107: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
May 31 13:47:19.108: INFO: ExecWithOptions: Clientset creation
May 31 13:47:19.108: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-89/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
May 31 13:47:19.392: INFO: Exec stderr: ""
May 31 13:47:19.393: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-89 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 31 13:47:19.393: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
May 31 13:47:19.394: INFO: ExecWithOptions: Clientset creation
May 31 13:47:19.394: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-89/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
May 31 13:47:19.725: INFO: Exec stderr: ""
May 31 13:47:19.737: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-89 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 31 13:47:19.737: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
May 31 13:47:19.738: INFO: ExecWithOptions: Clientset creation
May 31 13:47:19.739: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-89/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
May 31 13:47:20.037: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
May 31 13:47:20.037: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-89 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 31 13:47:20.038: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
May 31 13:47:20.048: INFO: ExecWithOptions: Clientset creation
May 31 13:47:20.048: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-89/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true %!s(MISSING))
May 31 13:47:20.368: INFO: Exec stderr: ""
May 31 13:47:20.368: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-89 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 31 13:47:20.368: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
May 31 13:47:20.369: INFO: ExecWithOptions: Clientset creation
May 31 13:47:20.369: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-89/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true %!s(MISSING))
May 31 13:47:20.765: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
May 31 13:47:20.765: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-89 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 31 13:47:20.765: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
May 31 13:47:20.766: INFO: ExecWithOptions: Clientset creation
May 31 13:47:20.766: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-89/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
May 31 13:47:21.125: INFO: Exec stderr: ""
May 31 13:47:21.125: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-89 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 31 13:47:21.126: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
May 31 13:47:21.126: INFO: ExecWithOptions: Clientset creation
May 31 13:47:21.126: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-89/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
May 31 13:47:21.478: INFO: Exec stderr: ""
May 31 13:47:21.478: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-89 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 31 13:47:21.478: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
May 31 13:47:21.479: INFO: ExecWithOptions: Clientset creation
May 31 13:47:21.480: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-89/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
May 31 13:47:21.784: INFO: Exec stderr: ""
May 31 13:47:21.785: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-89 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 31 13:47:21.785: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
May 31 13:47:21.787: INFO: ExecWithOptions: Clientset creation
May 31 13:47:21.787: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-89/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
May 31 13:47:22.072: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:47:22.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-89" for this suite.

• [SLOW TEST:7.396 seconds]
[sig-node] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":146,"skipped":2823,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:47:22.107: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
May 31 13:47:22.152: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:47:27.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4771" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":346,"completed":147,"skipped":2836,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:47:27.057: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name s-test-opt-del-d2bbde19-8a01-44bb-aeb4-e79049ae6376
STEP: Creating secret with name s-test-opt-upd-70f21c9f-3c2f-4c50-9d7d-8de5e73f2ced
STEP: Creating the pod
May 31 13:47:27.212: INFO: The status of Pod pod-secrets-d2ba98a7-6d4f-4b34-8949-4dc39886a897 is Pending, waiting for it to be Running (with Ready = true)
May 31 13:47:29.223: INFO: The status of Pod pod-secrets-d2ba98a7-6d4f-4b34-8949-4dc39886a897 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-d2bbde19-8a01-44bb-aeb4-e79049ae6376
STEP: Updating secret s-test-opt-upd-70f21c9f-3c2f-4c50-9d7d-8de5e73f2ced
STEP: Creating secret with name s-test-opt-create-202a9e7e-4785-45e7-a758-36a156ecf9f9
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:47:31.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3910" for this suite.
•{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":148,"skipped":2852,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:47:31.626: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 31 13:47:31.779: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 31 13:47:31.779: INFO: Node ip-172-31-11-247.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 31 13:47:32.806: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 31 13:47:32.806: INFO: Node ip-172-31-11-247.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 31 13:47:33.799: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 31 13:47:33.799: INFO: Node ip-172-31-3-160.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 31 13:47:34.807: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May 31 13:47:34.807: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets
STEP: DeleteCollection of the DaemonSets
STEP: Verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
May 31 13:47:34.873: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"26114"},"items":null}

May 31 13:47:34.886: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"26114"},"items":[{"metadata":{"name":"daemon-set-25z4p","generateName":"daemon-set-","namespace":"daemonsets-5516","uid":"0cce3bd8-e068-4312-b661-d6ea65be6fa2","resourceVersion":"26096","creationTimestamp":"2022-05-31T13:47:31Z","labels":{"controller-revision-hash":"5b46c58f6f","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"0d8f09a2c03ef69deb54527e4f961bf1f21b3144ccac9140e5e99496d7838c5e","cni.projectcalico.org/podIP":"172.25.0.79/32","cni.projectcalico.org/podIPs":"172.25.0.79/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"4e36505c-c1c0-4925-8423-292c1376cf94","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-05-31T13:47:31Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4e36505c-c1c0-4925-8423-292c1376cf94\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2022-05-31T13:47:32Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-05-31T13:47:32Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.0.79\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-v44zh","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-v44zh","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-11-247.eu-central-1.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-11-247.eu-central-1.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-05-31T13:47:31Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-05-31T13:47:32Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-05-31T13:47:32Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-05-31T13:47:31Z"}],"hostIP":"172.31.11.247","podIP":"172.25.0.79","podIPs":[{"ip":"172.25.0.79"}],"startTime":"2022-05-31T13:47:31Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-05-31T13:47:32Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://20db5a256913825d20c60b4f99a1f00c18b9c550b4a56aa32ccd3adae4dadaa8","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-5ddx9","generateName":"daemon-set-","namespace":"daemonsets-5516","uid":"22d14470-5358-4318-922e-fbf0b935a1a5","resourceVersion":"26100","creationTimestamp":"2022-05-31T13:47:31Z","labels":{"controller-revision-hash":"5b46c58f6f","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"78be664c34c33004cc6e8a10a6e29647b253a03006ca74b81bd000c469a1dd19","cni.projectcalico.org/podIP":"172.25.1.180/32","cni.projectcalico.org/podIPs":"172.25.1.180/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"4e36505c-c1c0-4925-8423-292c1376cf94","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-05-31T13:47:31Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4e36505c-c1c0-4925-8423-292c1376cf94\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2022-05-31T13:47:32Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-05-31T13:47:33Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.180\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-t2wn9","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-t2wn9","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-8-180.eu-central-1.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-8-180.eu-central-1.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-05-31T13:47:31Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-05-31T13:47:33Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-05-31T13:47:33Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-05-31T13:47:31Z"}],"hostIP":"172.31.8.180","podIP":"172.25.1.180","podIPs":[{"ip":"172.25.1.180"}],"startTime":"2022-05-31T13:47:31Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-05-31T13:47:32Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://818c4b362861634a65a0a09ae86b0b89349bb963401f9bf696fbea990a651a7c","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-5f46d","generateName":"daemon-set-","namespace":"daemonsets-5516","uid":"e2d96249-4d19-42a6-8790-193aeff80e13","resourceVersion":"26105","creationTimestamp":"2022-05-31T13:47:31Z","labels":{"controller-revision-hash":"5b46c58f6f","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"04324215f2f515a3be7df831cf73d429fbf5ec74634117f728793635ec1760d2","cni.projectcalico.org/podIP":"172.25.2.132/32","cni.projectcalico.org/podIPs":"172.25.2.132/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"4e36505c-c1c0-4925-8423-292c1376cf94","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-05-31T13:47:31Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4e36505c-c1c0-4925-8423-292c1376cf94\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2022-05-31T13:47:32Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-05-31T13:47:34Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.132\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-4xzqq","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-4xzqq","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-172-31-3-160.eu-central-1.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-172-31-3-160.eu-central-1.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-05-31T13:47:31Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-05-31T13:47:34Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-05-31T13:47:34Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-05-31T13:47:31Z"}],"hostIP":"172.31.3.160","podIP":"172.25.2.132","podIPs":[{"ip":"172.25.2.132"}],"startTime":"2022-05-31T13:47:31Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-05-31T13:47:33Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://e84b4efc88b6e157e393732bd1ba38e3ce73fc1891b3f90aa8dd43b0085fc3ee","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:47:34.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5516" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","total":346,"completed":149,"skipped":2866,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:47:34.967: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-7240
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 13:47:35.073: INFO: Found 0 stateful pods, waiting for 1
May 31 13:47:45.081: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet
May 31 13:47:45.127: INFO: Found 1 stateful pods, waiting for 2
May 31 13:47:55.140: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 31 13:47:55.140: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets
STEP: Delete all of the StatefulSets
STEP: Verify that StatefulSets have been deleted
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
May 31 13:47:55.181: INFO: Deleting all statefulset in ns statefulset-7240
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:47:55.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7240" for this suite.

• [SLOW TEST:20.292 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should list, patch and delete a collection of StatefulSets [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","total":346,"completed":150,"skipped":2885,"failed":0}
S
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:47:55.272: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4022
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-4022
I0531 13:47:55.442508      19 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4022, replica count: 2
May 31 13:47:58.504: INFO: Creating new exec pod
I0531 13:47:58.504837      19 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 31 13:48:01.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-4022 exec execpod4csq9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 31 13:48:02.038: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 31 13:48:02.038: INFO: stdout: "externalname-service-24fc7"
May 31 13:48:02.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-4022 exec execpod4csq9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.16.121 80'
May 31 13:48:02.513: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.16.121 80\nConnection to 10.240.16.121 80 port [tcp/http] succeeded!\n"
May 31 13:48:02.513: INFO: stdout: "externalname-service-bdr9w"
May 31 13:48:02.513: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:48:02.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4022" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:7.295 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":346,"completed":151,"skipped":2886,"failed":0}
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:48:02.567: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 31 13:48:02.726: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 31 13:48:02.726: INFO: Node ip-172-31-11-247.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 31 13:48:03.746: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 31 13:48:03.746: INFO: Node ip-172-31-11-247.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 31 13:48:04.746: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 31 13:48:04.746: INFO: Node ip-172-31-3-160.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 31 13:48:05.745: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May 31 13:48:05.745: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
May 31 13:48:05.814: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 31 13:48:05.814: INFO: Node ip-172-31-3-160.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 31 13:48:06.849: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 31 13:48:06.849: INFO: Node ip-172-31-3-160.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 31 13:48:07.847: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May 31 13:48:07.847: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6085, will wait for the garbage collector to delete the pods
May 31 13:48:07.961: INFO: Deleting DaemonSet.extensions daemon-set took: 14.347241ms
May 31 13:48:08.064: INFO: Terminating DaemonSet.extensions daemon-set pods took: 103.196839ms
May 31 13:48:10.374: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 31 13:48:10.374: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May 31 13:48:10.380: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"26619"},"items":null}

May 31 13:48:10.385: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"26619"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:48:10.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6085" for this suite.

• [SLOW TEST:7.874 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":346,"completed":152,"skipped":2891,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:48:10.442: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4996.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-4996.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4996.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4996.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4996.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-4996.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4996.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-4996.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4996.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-4996.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4996.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-4996.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4996.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-4996.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4996.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-4996.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 31 13:48:14.718: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4996.svc.cluster.local from pod dns-4996/dns-test-5f00c812-adea-4523-8d6c-74376fe80030: the server could not find the requested resource (get pods dns-test-5f00c812-adea-4523-8d6c-74376fe80030)
May 31 13:48:14.729: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4996.svc.cluster.local from pod dns-4996/dns-test-5f00c812-adea-4523-8d6c-74376fe80030: the server could not find the requested resource (get pods dns-test-5f00c812-adea-4523-8d6c-74376fe80030)
May 31 13:48:14.786: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4996.svc.cluster.local from pod dns-4996/dns-test-5f00c812-adea-4523-8d6c-74376fe80030: the server could not find the requested resource (get pods dns-test-5f00c812-adea-4523-8d6c-74376fe80030)
May 31 13:48:14.814: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4996.svc.cluster.local from pod dns-4996/dns-test-5f00c812-adea-4523-8d6c-74376fe80030: the server could not find the requested resource (get pods dns-test-5f00c812-adea-4523-8d6c-74376fe80030)
May 31 13:48:14.825: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4996.svc.cluster.local from pod dns-4996/dns-test-5f00c812-adea-4523-8d6c-74376fe80030: the server could not find the requested resource (get pods dns-test-5f00c812-adea-4523-8d6c-74376fe80030)
May 31 13:48:14.840: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4996.svc.cluster.local from pod dns-4996/dns-test-5f00c812-adea-4523-8d6c-74376fe80030: the server could not find the requested resource (get pods dns-test-5f00c812-adea-4523-8d6c-74376fe80030)
May 31 13:48:14.851: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4996.svc.cluster.local from pod dns-4996/dns-test-5f00c812-adea-4523-8d6c-74376fe80030: the server could not find the requested resource (get pods dns-test-5f00c812-adea-4523-8d6c-74376fe80030)
May 31 13:48:14.863: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4996.svc.cluster.local from pod dns-4996/dns-test-5f00c812-adea-4523-8d6c-74376fe80030: the server could not find the requested resource (get pods dns-test-5f00c812-adea-4523-8d6c-74376fe80030)
May 31 13:48:14.863: INFO: Lookups using dns-4996/dns-test-5f00c812-adea-4523-8d6c-74376fe80030 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4996.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4996.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4996.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4996.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4996.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4996.svc.cluster.local jessie_udp@dns-test-service-2.dns-4996.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4996.svc.cluster.local]

May 31 13:48:20.018: INFO: DNS probes using dns-4996/dns-test-5f00c812-adea-4523-8d6c-74376fe80030 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:48:20.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4996" for this suite.

• [SLOW TEST:9.669 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":346,"completed":153,"skipped":2945,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:48:20.111: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: starting the proxy server
May 31 13:48:20.161: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-9667 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:48:20.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9667" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":346,"completed":154,"skipped":2970,"failed":0}
SSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:48:20.265: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test env composition
May 31 13:48:20.346: INFO: Waiting up to 5m0s for pod "var-expansion-5cd5fd08-fe5a-42a1-9bea-fc27cb859be1" in namespace "var-expansion-3221" to be "Succeeded or Failed"
May 31 13:48:20.358: INFO: Pod "var-expansion-5cd5fd08-fe5a-42a1-9bea-fc27cb859be1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.023408ms
May 31 13:48:22.372: INFO: Pod "var-expansion-5cd5fd08-fe5a-42a1-9bea-fc27cb859be1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02628231s
May 31 13:48:24.382: INFO: Pod "var-expansion-5cd5fd08-fe5a-42a1-9bea-fc27cb859be1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036308104s
STEP: Saw pod success
May 31 13:48:24.382: INFO: Pod "var-expansion-5cd5fd08-fe5a-42a1-9bea-fc27cb859be1" satisfied condition "Succeeded or Failed"
May 31 13:48:24.390: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod var-expansion-5cd5fd08-fe5a-42a1-9bea-fc27cb859be1 container dapi-container: <nil>
STEP: delete the pod
May 31 13:48:24.438: INFO: Waiting for pod var-expansion-5cd5fd08-fe5a-42a1-9bea-fc27cb859be1 to disappear
May 31 13:48:24.444: INFO: Pod var-expansion-5cd5fd08-fe5a-42a1-9bea-fc27cb859be1 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:48:24.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3221" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":346,"completed":155,"skipped":2978,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:48:24.473: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a cronjob
STEP: Ensuring more than one job is running at a time
STEP: Ensuring at least two running jobs exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:50:00.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-6746" for this suite.

• [SLOW TEST:96.161 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","total":346,"completed":156,"skipped":2991,"failed":0}
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:50:00.637: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service multi-endpoint-test in namespace services-9410
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9410 to expose endpoints map[]
May 31 13:50:00.732: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
May 31 13:50:01.767: INFO: successfully validated that service multi-endpoint-test in namespace services-9410 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-9410
May 31 13:50:01.797: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May 31 13:50:03.808: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9410 to expose endpoints map[pod1:[100]]
May 31 13:50:03.837: INFO: successfully validated that service multi-endpoint-test in namespace services-9410 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-9410
May 31 13:50:03.859: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
May 31 13:50:05.872: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9410 to expose endpoints map[pod1:[100] pod2:[101]]
May 31 13:50:05.917: INFO: successfully validated that service multi-endpoint-test in namespace services-9410 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods
May 31 13:50:05.917: INFO: Creating new exec pod
May 31 13:50:08.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-9410 exec execpodlszkq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
May 31 13:50:09.337: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
May 31 13:50:09.337: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 31 13:50:09.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-9410 exec execpodlszkq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.27.232 80'
May 31 13:50:09.741: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.27.232 80\nConnection to 10.240.27.232 80 port [tcp/http] succeeded!\n"
May 31 13:50:09.741: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 31 13:50:09.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-9410 exec execpodlszkq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
May 31 13:50:10.185: INFO: stderr: "+ nc -v -t -w 2 multi-endpoint-test 81\n+ echo hostName\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
May 31 13:50:10.185: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 31 13:50:10.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-9410 exec execpodlszkq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.27.232 81'
May 31 13:50:10.659: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.27.232 81\nConnection to 10.240.27.232 81 port [tcp/*] succeeded!\n"
May 31 13:50:10.659: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-9410
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9410 to expose endpoints map[pod2:[101]]
May 31 13:50:11.745: INFO: successfully validated that service multi-endpoint-test in namespace services-9410 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-9410
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9410 to expose endpoints map[]
May 31 13:50:11.823: INFO: successfully validated that service multi-endpoint-test in namespace services-9410 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:50:11.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9410" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:11.339 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":346,"completed":157,"skipped":2991,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:50:11.980: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
May 31 13:50:12.196: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May 31 13:50:14.206: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the pod with lifecycle hook
May 31 13:50:14.233: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
May 31 13:50:16.244: INFO: The status of Pod pod-with-poststart-exec-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 31 13:50:16.295: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 31 13:50:16.302: INFO: Pod pod-with-poststart-exec-hook still exists
May 31 13:50:18.310: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 31 13:50:18.322: INFO: Pod pod-with-poststart-exec-hook still exists
May 31 13:50:20.305: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 31 13:50:20.313: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:50:20.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7900" for this suite.

• [SLOW TEST:8.359 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":346,"completed":158,"skipped":3020,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:50:20.340: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:50:20.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5181" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":346,"completed":159,"skipped":3038,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:50:20.497: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 13:50:20.618: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
May 31 13:50:20.649: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 31 13:50:20.649: INFO: Node ip-172-31-11-247.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 31 13:50:21.674: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 31 13:50:21.675: INFO: Node ip-172-31-11-247.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 31 13:50:22.671: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May 31 13:50:22.671: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
May 31 13:50:22.744: INFO: Wrong image for pod: daemon-set-5djqc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May 31 13:50:22.744: INFO: Wrong image for pod: daemon-set-f88jr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May 31 13:50:22.744: INFO: Wrong image for pod: daemon-set-jk8rz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May 31 13:50:23.790: INFO: Wrong image for pod: daemon-set-f88jr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May 31 13:50:23.790: INFO: Wrong image for pod: daemon-set-jk8rz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May 31 13:50:24.794: INFO: Wrong image for pod: daemon-set-f88jr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May 31 13:50:24.794: INFO: Wrong image for pod: daemon-set-jk8rz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May 31 13:50:25.794: INFO: Wrong image for pod: daemon-set-f88jr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May 31 13:50:25.794: INFO: Wrong image for pod: daemon-set-jk8rz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May 31 13:50:25.794: INFO: Pod daemon-set-wjlfl is not available
May 31 13:50:26.790: INFO: Wrong image for pod: daemon-set-f88jr. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May 31 13:50:26.791: INFO: Wrong image for pod: daemon-set-jk8rz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May 31 13:50:26.791: INFO: Pod daemon-set-wjlfl is not available
May 31 13:50:27.794: INFO: Wrong image for pod: daemon-set-jk8rz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May 31 13:50:28.796: INFO: Pod daemon-set-bmpwk is not available
May 31 13:50:28.796: INFO: Wrong image for pod: daemon-set-jk8rz. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.33, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
May 31 13:50:30.796: INFO: Pod daemon-set-nhgkz is not available
STEP: Check that daemon pods are still running on every node of the cluster.
May 31 13:50:30.829: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
May 31 13:50:30.829: INFO: Node ip-172-31-11-247.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 31 13:50:31.849: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
May 31 13:50:31.849: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7333, will wait for the garbage collector to delete the pods
May 31 13:50:31.962: INFO: Deleting DaemonSet.extensions daemon-set took: 11.736733ms
May 31 13:50:32.062: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.142559ms
May 31 13:50:34.485: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 31 13:50:34.485: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May 31 13:50:34.491: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"27640"},"items":null}

May 31 13:50:34.510: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"27640"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:50:34.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7333" for this suite.

• [SLOW TEST:14.072 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":346,"completed":160,"skipped":3065,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:50:34.569: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
May 31 13:50:35.664: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 31 13:50:38.724: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 13:50:38.733: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:50:42.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-3929" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:7.910 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":346,"completed":161,"skipped":3068,"failed":0}
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:50:42.480: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting the auto-created API token
STEP: reading a file in the container
May 31 13:50:45.117: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4200 pod-service-account-5d374cee-006c-4a70-9d66-1f22a25a7551 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
May 31 13:50:45.524: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4200 pod-service-account-5d374cee-006c-4a70-9d66-1f22a25a7551 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
May 31 13:50:45.976: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4200 pod-service-account-5d374cee-006c-4a70-9d66-1f22a25a7551 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:50:46.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4200" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":346,"completed":162,"skipped":3068,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:50:46.381: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 31 13:50:47.269: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 31 13:50:50.317: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:51:02.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-338" for this suite.
STEP: Destroying namespace "webhook-338-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:16.618 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":346,"completed":163,"skipped":3083,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:51:03.000: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 31 13:51:03.092: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c56e6a10-c306-4cdc-9f5e-c84f6be622af" in namespace "downward-api-2280" to be "Succeeded or Failed"
May 31 13:51:03.105: INFO: Pod "downwardapi-volume-c56e6a10-c306-4cdc-9f5e-c84f6be622af": Phase="Pending", Reason="", readiness=false. Elapsed: 13.300102ms
May 31 13:51:05.115: INFO: Pod "downwardapi-volume-c56e6a10-c306-4cdc-9f5e-c84f6be622af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022584432s
May 31 13:51:07.127: INFO: Pod "downwardapi-volume-c56e6a10-c306-4cdc-9f5e-c84f6be622af": Phase="Running", Reason="", readiness=false. Elapsed: 4.035084382s
May 31 13:51:09.136: INFO: Pod "downwardapi-volume-c56e6a10-c306-4cdc-9f5e-c84f6be622af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.044203167s
STEP: Saw pod success
May 31 13:51:09.136: INFO: Pod "downwardapi-volume-c56e6a10-c306-4cdc-9f5e-c84f6be622af" satisfied condition "Succeeded or Failed"
May 31 13:51:09.142: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod downwardapi-volume-c56e6a10-c306-4cdc-9f5e-c84f6be622af container client-container: <nil>
STEP: delete the pod
May 31 13:51:09.228: INFO: Waiting for pod downwardapi-volume-c56e6a10-c306-4cdc-9f5e-c84f6be622af to disappear
May 31 13:51:09.234: INFO: Pod downwardapi-volume-c56e6a10-c306-4cdc-9f5e-c84f6be622af no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:51:09.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2280" for this suite.

• [SLOW TEST:6.256 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":164,"skipped":3089,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:51:09.258: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 31 13:51:09.356: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
May 31 13:51:09.368: INFO: starting watch
STEP: patching
STEP: updating
May 31 13:51:09.398: INFO: waiting for watch events with expected annotations
May 31 13:51:09.398: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:51:09.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-9686" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":346,"completed":165,"skipped":3118,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:51:09.519: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:51:25.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1772" for this suite.

• [SLOW TEST:16.270 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":346,"completed":166,"skipped":3154,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:51:25.791: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a Service
STEP: watching for the Service to be added
May 31 13:51:25.868: INFO: Found Service test-service-krc74 in namespace services-4710 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
May 31 13:51:25.868: INFO: Service test-service-krc74 created
STEP: Getting /status
May 31 13:51:25.875: INFO: Service test-service-krc74 has LoadBalancer: {[]}
STEP: patching the ServiceStatus
STEP: watching for the Service to be patched
May 31 13:51:25.890: INFO: observed Service test-service-krc74 in namespace services-4710 with annotations: map[] & LoadBalancer: {[]}
May 31 13:51:25.890: INFO: Found Service test-service-krc74 in namespace services-4710 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
May 31 13:51:25.890: INFO: Service test-service-krc74 has service status patched
STEP: updating the ServiceStatus
May 31 13:51:25.905: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated
May 31 13:51:25.909: INFO: Observed Service test-service-krc74 in namespace services-4710 with annotations: map[] & Conditions: {[]}
May 31 13:51:25.909: INFO: Observed event: &Service{ObjectMeta:{test-service-krc74  services-4710  5404031c-6cec-4b8e-b513-259c0c911a6c 28188 0 2022-05-31 13:51:25 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] []  [{e2e.test Update v1 2022-05-31 13:51:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2022-05-31 13:51:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.240.21.206,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.240.21.206],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
May 31 13:51:25.910: INFO: Found Service test-service-krc74 in namespace services-4710 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
May 31 13:51:25.910: INFO: Service test-service-krc74 has service status updated
STEP: patching the service
STEP: watching for the Service to be patched
May 31 13:51:25.931: INFO: observed Service test-service-krc74 in namespace services-4710 with labels: map[test-service-static:true]
May 31 13:51:25.931: INFO: observed Service test-service-krc74 in namespace services-4710 with labels: map[test-service-static:true]
May 31 13:51:25.931: INFO: observed Service test-service-krc74 in namespace services-4710 with labels: map[test-service-static:true]
May 31 13:51:25.931: INFO: Found Service test-service-krc74 in namespace services-4710 with labels: map[test-service:patched test-service-static:true]
May 31 13:51:25.931: INFO: Service test-service-krc74 patched
STEP: deleting the service
STEP: watching for the Service to be deleted
May 31 13:51:25.961: INFO: Observed event: ADDED
May 31 13:51:25.961: INFO: Observed event: MODIFIED
May 31 13:51:25.961: INFO: Observed event: MODIFIED
May 31 13:51:25.961: INFO: Observed event: MODIFIED
May 31 13:51:25.961: INFO: Found Service test-service-krc74 in namespace services-4710 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
May 31 13:51:25.961: INFO: Service test-service-krc74 deleted
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:51:25.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4710" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","total":346,"completed":167,"skipped":3167,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:51:26.002: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 31 13:51:26.076: INFO: Waiting up to 5m0s for pod "downwardapi-volume-122e5ac4-8801-4289-a457-a5096cc26c63" in namespace "projected-9942" to be "Succeeded or Failed"
May 31 13:51:26.084: INFO: Pod "downwardapi-volume-122e5ac4-8801-4289-a457-a5096cc26c63": Phase="Pending", Reason="", readiness=false. Elapsed: 7.75905ms
May 31 13:51:28.106: INFO: Pod "downwardapi-volume-122e5ac4-8801-4289-a457-a5096cc26c63": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029561479s
May 31 13:51:30.129: INFO: Pod "downwardapi-volume-122e5ac4-8801-4289-a457-a5096cc26c63": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052342377s
STEP: Saw pod success
May 31 13:51:30.129: INFO: Pod "downwardapi-volume-122e5ac4-8801-4289-a457-a5096cc26c63" satisfied condition "Succeeded or Failed"
May 31 13:51:30.137: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod downwardapi-volume-122e5ac4-8801-4289-a457-a5096cc26c63 container client-container: <nil>
STEP: delete the pod
May 31 13:51:30.238: INFO: Waiting for pod downwardapi-volume-122e5ac4-8801-4289-a457-a5096cc26c63 to disappear
May 31 13:51:30.252: INFO: Pod downwardapi-volume-122e5ac4-8801-4289-a457-a5096cc26c63 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:51:30.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9942" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":168,"skipped":3172,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:51:30.283: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:51:30.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8418" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":346,"completed":169,"skipped":3228,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:51:30.441: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 31 13:51:31.094: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 31 13:51:34.139: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 13:51:34.149: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7288-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:51:37.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3587" for this suite.
STEP: Destroying namespace "webhook-3587-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.230 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":346,"completed":170,"skipped":3268,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:51:37.673: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
May 31 13:51:37.798: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-530  cb9921ef-fc32-4c5f-8ad6-58b10dc6c9db 28388 0 2022-05-31 13:51:37 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2022-05-31 13:51:37 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gwjwp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gwjwp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 31 13:51:37.814: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
May 31 13:51:39.830: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
May 31 13:51:39.830: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-530 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 31 13:51:39.830: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
May 31 13:51:39.831: INFO: ExecWithOptions: Clientset creation
May 31 13:51:39.831: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/dns-530/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
STEP: Verifying customized DNS server is configured on pod...
May 31 13:51:40.150: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-530 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 31 13:51:40.150: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
May 31 13:51:40.151: INFO: ExecWithOptions: Clientset creation
May 31 13:51:40.151: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/dns-530/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
May 31 13:51:40.579: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:51:40.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-530" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":346,"completed":171,"skipped":3299,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:51:40.626: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 31 13:51:40.695: INFO: Waiting up to 5m0s for pod "pod-a245fcba-7b22-4785-bea1-bc72ce3767c1" in namespace "emptydir-1150" to be "Succeeded or Failed"
May 31 13:51:40.706: INFO: Pod "pod-a245fcba-7b22-4785-bea1-bc72ce3767c1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.673957ms
May 31 13:51:42.742: INFO: Pod "pod-a245fcba-7b22-4785-bea1-bc72ce3767c1": Phase="Running", Reason="", readiness=true. Elapsed: 2.047207768s
May 31 13:51:44.756: INFO: Pod "pod-a245fcba-7b22-4785-bea1-bc72ce3767c1": Phase="Running", Reason="", readiness=false. Elapsed: 4.061016277s
May 31 13:51:46.769: INFO: Pod "pod-a245fcba-7b22-4785-bea1-bc72ce3767c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.073444214s
STEP: Saw pod success
May 31 13:51:46.769: INFO: Pod "pod-a245fcba-7b22-4785-bea1-bc72ce3767c1" satisfied condition "Succeeded or Failed"
May 31 13:51:46.777: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod pod-a245fcba-7b22-4785-bea1-bc72ce3767c1 container test-container: <nil>
STEP: delete the pod
May 31 13:51:46.820: INFO: Waiting for pod pod-a245fcba-7b22-4785-bea1-bc72ce3767c1 to disappear
May 31 13:51:46.826: INFO: Pod pod-a245fcba-7b22-4785-bea1-bc72ce3767c1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:51:46.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1150" for this suite.

• [SLOW TEST:6.223 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":172,"skipped":3301,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:51:46.849: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 13:51:46.892: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:51:53.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5911" for this suite.

• [SLOW TEST:6.551 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":346,"completed":173,"skipped":3310,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:51:53.428: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1411
STEP: creating an pod
May 31 13:51:53.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-888 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.33 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
May 31 13:51:53.885: INFO: stderr: ""
May 31 13:51:53.885: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Waiting for log generator to start.
May 31 13:51:53.885: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
May 31 13:51:53.886: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-888" to be "running and ready, or succeeded"
May 31 13:51:53.894: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 7.668789ms
May 31 13:51:55.911: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.024814388s
May 31 13:51:55.911: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
May 31 13:51:55.911: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
May 31 13:51:55.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-888 logs logs-generator logs-generator'
May 31 13:51:56.083: INFO: stderr: ""
May 31 13:51:56.083: INFO: stdout: "I0531 13:51:54.774591       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/dhtb 384\nI0531 13:51:54.974975       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/g6t 255\nI0531 13:51:55.175569       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/b64 226\nI0531 13:51:55.375212       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/qgf2 433\nI0531 13:51:55.574655       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/dd4 434\nI0531 13:51:55.775299       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/68mp 326\nI0531 13:51:55.974617       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/xnt8 552\n"
STEP: limiting log lines
May 31 13:51:56.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-888 logs logs-generator logs-generator --tail=1'
May 31 13:51:56.208: INFO: stderr: ""
May 31 13:51:56.208: INFO: stdout: "I0531 13:51:56.174986       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/s6p 303\n"
May 31 13:51:56.208: INFO: got output "I0531 13:51:56.174986       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/s6p 303\n"
STEP: limiting log bytes
May 31 13:51:56.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-888 logs logs-generator logs-generator --limit-bytes=1'
May 31 13:51:56.374: INFO: stderr: ""
May 31 13:51:56.374: INFO: stdout: "I"
May 31 13:51:56.374: INFO: got output "I"
STEP: exposing timestamps
May 31 13:51:56.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-888 logs logs-generator logs-generator --tail=1 --timestamps'
May 31 13:51:56.531: INFO: stderr: ""
May 31 13:51:56.531: INFO: stdout: "2022-05-31T13:51:56.375591936Z I0531 13:51:56.375378       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/gl45 279\n"
May 31 13:51:56.531: INFO: got output "2022-05-31T13:51:56.375591936Z I0531 13:51:56.375378       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/gl45 279\n"
STEP: restricting to a time range
May 31 13:51:59.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-888 logs logs-generator logs-generator --since=1s'
May 31 13:51:59.253: INFO: stderr: ""
May 31 13:51:59.253: INFO: stdout: "I0531 13:51:58.375394       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/vc4 367\nI0531 13:51:58.574997       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/94j 278\nI0531 13:51:58.775312       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/bhh 556\nI0531 13:51:58.974629       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/default/pods/l7kl 449\nI0531 13:51:59.175046       1 logs_generator.go:76] 22 GET /api/v1/namespaces/kube-system/pods/rx2 339\n"
May 31 13:51:59.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-888 logs logs-generator logs-generator --since=24h'
May 31 13:51:59.462: INFO: stderr: ""
May 31 13:51:59.462: INFO: stdout: "I0531 13:51:54.774591       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/dhtb 384\nI0531 13:51:54.974975       1 logs_generator.go:76] 1 GET /api/v1/namespaces/ns/pods/g6t 255\nI0531 13:51:55.175569       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/kube-system/pods/b64 226\nI0531 13:51:55.375212       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/qgf2 433\nI0531 13:51:55.574655       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/dd4 434\nI0531 13:51:55.775299       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/68mp 326\nI0531 13:51:55.974617       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/xnt8 552\nI0531 13:51:56.174986       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/s6p 303\nI0531 13:51:56.375378       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/gl45 279\nI0531 13:51:56.574670       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/kube-system/pods/jmr 498\nI0531 13:51:56.775092       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/77r 374\nI0531 13:51:56.975555       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/zj5m 566\nI0531 13:51:57.174988       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/x4rg 446\nI0531 13:51:57.375299       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/rj9 208\nI0531 13:51:57.574656       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/kube-system/pods/4q5 407\nI0531 13:51:57.775186       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/v976 208\nI0531 13:51:57.975520       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/9nh 299\nI0531 13:51:58.175095       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/b26w 577\nI0531 13:51:58.375394       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/vc4 367\nI0531 13:51:58.574997       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/94j 278\nI0531 13:51:58.775312       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/bhh 556\nI0531 13:51:58.974629       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/default/pods/l7kl 449\nI0531 13:51:59.175046       1 logs_generator.go:76] 22 GET /api/v1/namespaces/kube-system/pods/rx2 339\nI0531 13:51:59.375338       1 logs_generator.go:76] 23 POST /api/v1/namespaces/kube-system/pods/c8nb 391\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1416
May 31 13:51:59.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-888 delete pod logs-generator'
May 31 13:52:00.842: INFO: stderr: ""
May 31 13:52:00.842: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:52:00.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-888" for this suite.

• [SLOW TEST:7.452 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1408
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":346,"completed":174,"skipped":3312,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:52:00.874: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 31 13:52:00.953: INFO: Waiting up to 5m0s for pod "downwardapi-volume-02eb4dd0-d17c-4b7f-ba1e-a619d2a6edfd" in namespace "projected-9360" to be "Succeeded or Failed"
May 31 13:52:00.971: INFO: Pod "downwardapi-volume-02eb4dd0-d17c-4b7f-ba1e-a619d2a6edfd": Phase="Pending", Reason="", readiness=false. Elapsed: 17.297056ms
May 31 13:52:03.002: INFO: Pod "downwardapi-volume-02eb4dd0-d17c-4b7f-ba1e-a619d2a6edfd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049090674s
May 31 13:52:05.011: INFO: Pod "downwardapi-volume-02eb4dd0-d17c-4b7f-ba1e-a619d2a6edfd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.057231352s
STEP: Saw pod success
May 31 13:52:05.011: INFO: Pod "downwardapi-volume-02eb4dd0-d17c-4b7f-ba1e-a619d2a6edfd" satisfied condition "Succeeded or Failed"
May 31 13:52:05.021: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod downwardapi-volume-02eb4dd0-d17c-4b7f-ba1e-a619d2a6edfd container client-container: <nil>
STEP: delete the pod
May 31 13:52:05.055: INFO: Waiting for pod downwardapi-volume-02eb4dd0-d17c-4b7f-ba1e-a619d2a6edfd to disappear
May 31 13:52:05.062: INFO: Pod downwardapi-volume-02eb4dd0-d17c-4b7f-ba1e-a619d2a6edfd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:52:05.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9360" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":346,"completed":175,"skipped":3335,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:52:05.088: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
May 31 13:52:05.155: INFO: Waiting up to 5m0s for pod "security-context-7ec198c5-ddbe-416b-b442-1ed461eb9aaf" in namespace "security-context-4700" to be "Succeeded or Failed"
May 31 13:52:05.163: INFO: Pod "security-context-7ec198c5-ddbe-416b-b442-1ed461eb9aaf": Phase="Pending", Reason="", readiness=false. Elapsed: 7.115883ms
May 31 13:52:07.175: INFO: Pod "security-context-7ec198c5-ddbe-416b-b442-1ed461eb9aaf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019984709s
May 31 13:52:09.189: INFO: Pod "security-context-7ec198c5-ddbe-416b-b442-1ed461eb9aaf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033602834s
STEP: Saw pod success
May 31 13:52:09.189: INFO: Pod "security-context-7ec198c5-ddbe-416b-b442-1ed461eb9aaf" satisfied condition "Succeeded or Failed"
May 31 13:52:09.197: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod security-context-7ec198c5-ddbe-416b-b442-1ed461eb9aaf container test-container: <nil>
STEP: delete the pod
May 31 13:52:09.231: INFO: Waiting for pod security-context-7ec198c5-ddbe-416b-b442-1ed461eb9aaf to disappear
May 31 13:52:09.244: INFO: Pod security-context-7ec198c5-ddbe-416b-b442-1ed461eb9aaf no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:52:09.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-4700" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":176,"skipped":3348,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:52:09.267: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Performing setup for networking test in namespace pod-network-test-9781
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 31 13:52:09.313: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 31 13:52:09.400: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 31 13:52:11.413: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 31 13:52:13.413: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 31 13:52:15.410: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 31 13:52:17.413: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 31 13:52:19.420: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 31 13:52:21.416: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 31 13:52:23.416: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 31 13:52:25.413: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 31 13:52:27.414: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 31 13:52:29.415: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 31 13:52:29.435: INFO: The status of Pod netserver-1 is Running (Ready = true)
May 31 13:52:29.463: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
May 31 13:52:31.513: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May 31 13:52:31.513: INFO: Breadth first check of 172.25.0.84 on host 172.31.11.247...
May 31 13:52:31.521: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.206:9080/dial?request=hostname&protocol=http&host=172.25.0.84&port=8083&tries=1'] Namespace:pod-network-test-9781 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 31 13:52:31.521: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
May 31 13:52:31.522: INFO: ExecWithOptions: Clientset creation
May 31 13:52:31.522: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-9781/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.1.206%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.25.0.84%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
May 31 13:52:32.305: INFO: Waiting for responses: map[]
May 31 13:52:32.305: INFO: reached 172.25.0.84 after 0/1 tries
May 31 13:52:32.305: INFO: Breadth first check of 172.25.2.141 on host 172.31.3.160...
May 31 13:52:32.330: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.206:9080/dial?request=hostname&protocol=http&host=172.25.2.141&port=8083&tries=1'] Namespace:pod-network-test-9781 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 31 13:52:32.330: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
May 31 13:52:32.330: INFO: ExecWithOptions: Clientset creation
May 31 13:52:32.330: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-9781/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.1.206%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.25.2.141%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
May 31 13:52:32.616: INFO: Waiting for responses: map[]
May 31 13:52:32.616: INFO: reached 172.25.2.141 after 0/1 tries
May 31 13:52:32.616: INFO: Breadth first check of 172.25.1.205 on host 172.31.8.180...
May 31 13:52:32.625: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.206:9080/dial?request=hostname&protocol=http&host=172.25.1.205&port=8083&tries=1'] Namespace:pod-network-test-9781 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 31 13:52:32.625: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
May 31 13:52:32.626: INFO: ExecWithOptions: Clientset creation
May 31 13:52:32.626: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-9781/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.1.206%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.25.1.205%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
May 31 13:52:33.047: INFO: Waiting for responses: map[]
May 31 13:52:33.047: INFO: reached 172.25.1.205 after 0/1 tries
May 31 13:52:33.047: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:52:33.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9781" for this suite.

• [SLOW TEST:23.814 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":346,"completed":177,"skipped":3375,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:52:33.082: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod pod-subpath-test-downwardapi-2nj6
STEP: Creating a pod to test atomic-volume-subpath
May 31 13:52:33.236: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-2nj6" in namespace "subpath-844" to be "Succeeded or Failed"
May 31 13:52:33.258: INFO: Pod "pod-subpath-test-downwardapi-2nj6": Phase="Pending", Reason="", readiness=false. Elapsed: 21.890107ms
May 31 13:52:35.269: INFO: Pod "pod-subpath-test-downwardapi-2nj6": Phase="Running", Reason="", readiness=true. Elapsed: 2.033370196s
May 31 13:52:37.281: INFO: Pod "pod-subpath-test-downwardapi-2nj6": Phase="Running", Reason="", readiness=true. Elapsed: 4.045123717s
May 31 13:52:39.293: INFO: Pod "pod-subpath-test-downwardapi-2nj6": Phase="Running", Reason="", readiness=true. Elapsed: 6.057187152s
May 31 13:52:41.303: INFO: Pod "pod-subpath-test-downwardapi-2nj6": Phase="Running", Reason="", readiness=true. Elapsed: 8.066912979s
May 31 13:52:43.318: INFO: Pod "pod-subpath-test-downwardapi-2nj6": Phase="Running", Reason="", readiness=true. Elapsed: 10.082430517s
May 31 13:52:45.327: INFO: Pod "pod-subpath-test-downwardapi-2nj6": Phase="Running", Reason="", readiness=true. Elapsed: 12.091421341s
May 31 13:52:47.339: INFO: Pod "pod-subpath-test-downwardapi-2nj6": Phase="Running", Reason="", readiness=true. Elapsed: 14.10301166s
May 31 13:52:49.356: INFO: Pod "pod-subpath-test-downwardapi-2nj6": Phase="Running", Reason="", readiness=true. Elapsed: 16.119784169s
May 31 13:52:51.375: INFO: Pod "pod-subpath-test-downwardapi-2nj6": Phase="Running", Reason="", readiness=true. Elapsed: 18.138704729s
May 31 13:52:53.392: INFO: Pod "pod-subpath-test-downwardapi-2nj6": Phase="Running", Reason="", readiness=true. Elapsed: 20.156426674s
May 31 13:52:55.403: INFO: Pod "pod-subpath-test-downwardapi-2nj6": Phase="Running", Reason="", readiness=false. Elapsed: 22.166844443s
May 31 13:52:57.415: INFO: Pod "pod-subpath-test-downwardapi-2nj6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.179320861s
STEP: Saw pod success
May 31 13:52:57.416: INFO: Pod "pod-subpath-test-downwardapi-2nj6" satisfied condition "Succeeded or Failed"
May 31 13:52:57.423: INFO: Trying to get logs from node ip-172-31-3-160.eu-central-1.compute.internal pod pod-subpath-test-downwardapi-2nj6 container test-container-subpath-downwardapi-2nj6: <nil>
STEP: delete the pod
May 31 13:52:57.467: INFO: Waiting for pod pod-subpath-test-downwardapi-2nj6 to disappear
May 31 13:52:57.499: INFO: Pod pod-subpath-test-downwardapi-2nj6 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-2nj6
May 31 13:52:57.499: INFO: Deleting pod "pod-subpath-test-downwardapi-2nj6" in namespace "subpath-844"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:52:57.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-844" for this suite.

• [SLOW TEST:24.476 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":178,"skipped":3388,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:52:57.562: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 31 13:52:57.658: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ab319598-aa7a-4105-ab18-c1d9e08dd107" in namespace "projected-4212" to be "Succeeded or Failed"
May 31 13:52:57.668: INFO: Pod "downwardapi-volume-ab319598-aa7a-4105-ab18-c1d9e08dd107": Phase="Pending", Reason="", readiness=false. Elapsed: 9.678313ms
May 31 13:52:59.680: INFO: Pod "downwardapi-volume-ab319598-aa7a-4105-ab18-c1d9e08dd107": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021985404s
May 31 13:53:01.692: INFO: Pod "downwardapi-volume-ab319598-aa7a-4105-ab18-c1d9e08dd107": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033523676s
May 31 13:53:03.702: INFO: Pod "downwardapi-volume-ab319598-aa7a-4105-ab18-c1d9e08dd107": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.044203396s
STEP: Saw pod success
May 31 13:53:03.702: INFO: Pod "downwardapi-volume-ab319598-aa7a-4105-ab18-c1d9e08dd107" satisfied condition "Succeeded or Failed"
May 31 13:53:03.708: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod downwardapi-volume-ab319598-aa7a-4105-ab18-c1d9e08dd107 container client-container: <nil>
STEP: delete the pod
May 31 13:53:03.758: INFO: Waiting for pod downwardapi-volume-ab319598-aa7a-4105-ab18-c1d9e08dd107 to disappear
May 31 13:53:03.764: INFO: Pod downwardapi-volume-ab319598-aa7a-4105-ab18-c1d9e08dd107 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:53:03.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4212" for this suite.

• [SLOW TEST:6.225 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":179,"skipped":3391,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:53:03.803: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 13:53:03.873: INFO: Pod name sample-pod: Found 0 pods out of 1
May 31 13:53:08.884: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Scaling up "test-rs" replicaset 
May 31 13:53:08.898: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet
May 31 13:53:08.920: INFO: observed ReplicaSet test-rs in namespace replicaset-9720 with ReadyReplicas 1, AvailableReplicas 1
May 31 13:53:08.942: INFO: observed ReplicaSet test-rs in namespace replicaset-9720 with ReadyReplicas 1, AvailableReplicas 1
May 31 13:53:08.982: INFO: observed ReplicaSet test-rs in namespace replicaset-9720 with ReadyReplicas 1, AvailableReplicas 1
May 31 13:53:08.994: INFO: observed ReplicaSet test-rs in namespace replicaset-9720 with ReadyReplicas 1, AvailableReplicas 1
May 31 13:53:10.658: INFO: observed ReplicaSet test-rs in namespace replicaset-9720 with ReadyReplicas 2, AvailableReplicas 2
May 31 13:53:10.720: INFO: observed Replicaset test-rs in namespace replicaset-9720 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:53:10.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9720" for this suite.

• [SLOW TEST:6.937 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","total":346,"completed":180,"skipped":3438,"failed":0}
SS
------------------------------
[sig-node] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:53:10.740: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating secret secrets-633/secret-test-5c541c20-3b10-44f8-9bd5-712bd705104e
STEP: Creating a pod to test consume secrets
May 31 13:53:10.867: INFO: Waiting up to 5m0s for pod "pod-configmaps-ae146f79-0be4-4c5c-9c0d-a6be08efb73f" in namespace "secrets-633" to be "Succeeded or Failed"
May 31 13:53:10.879: INFO: Pod "pod-configmaps-ae146f79-0be4-4c5c-9c0d-a6be08efb73f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.167029ms
May 31 13:53:12.892: INFO: Pod "pod-configmaps-ae146f79-0be4-4c5c-9c0d-a6be08efb73f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024787906s
May 31 13:53:14.904: INFO: Pod "pod-configmaps-ae146f79-0be4-4c5c-9c0d-a6be08efb73f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036538241s
STEP: Saw pod success
May 31 13:53:14.904: INFO: Pod "pod-configmaps-ae146f79-0be4-4c5c-9c0d-a6be08efb73f" satisfied condition "Succeeded or Failed"
May 31 13:53:14.910: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod pod-configmaps-ae146f79-0be4-4c5c-9c0d-a6be08efb73f container env-test: <nil>
STEP: delete the pod
May 31 13:53:14.945: INFO: Waiting for pod pod-configmaps-ae146f79-0be4-4c5c-9c0d-a6be08efb73f to disappear
May 31 13:53:14.960: INFO: Pod pod-configmaps-ae146f79-0be4-4c5c-9c0d-a6be08efb73f no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:53:14.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-633" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":181,"skipped":3440,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:53:14.982: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a suspended cronjob
STEP: Ensuring no jobs are scheduled
STEP: Ensuring no job exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:58:15.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-9368" for this suite.

• [SLOW TEST:300.195 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","total":346,"completed":182,"skipped":3474,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:58:15.180: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 13:58:15.267: INFO: The status of Pod busybox-scheduling-3cb8ac6c-92cc-4ef5-bf18-6b3c6dc3d71a is Pending, waiting for it to be Running (with Ready = true)
May 31 13:58:17.279: INFO: The status of Pod busybox-scheduling-3cb8ac6c-92cc-4ef5-bf18-6b3c6dc3d71a is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:58:17.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8103" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":346,"completed":183,"skipped":3489,"failed":0}
SS
------------------------------
[sig-node] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:58:17.382: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 13:58:17.482: INFO: The status of Pod busybox-host-aliases713c17ce-f205-4cf9-a955-44c53b5dd54b is Pending, waiting for it to be Running (with Ready = true)
May 31 13:58:19.497: INFO: The status of Pod busybox-host-aliases713c17ce-f205-4cf9-a955-44c53b5dd54b is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:58:19.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2343" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":184,"skipped":3491,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:58:19.548: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
May 31 13:58:19.629: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
May 31 13:58:21.639: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the pod with lifecycle hook
May 31 13:58:21.661: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
May 31 13:58:23.671: INFO: The status of Pod pod-with-prestop-exec-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
May 31 13:58:23.692: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 31 13:58:23.701: INFO: Pod pod-with-prestop-exec-hook still exists
May 31 13:58:25.702: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 31 13:58:25.712: INFO: Pod pod-with-prestop-exec-hook still exists
May 31 13:58:27.703: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 31 13:58:27.715: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:58:27.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8570" for this suite.

• [SLOW TEST:8.241 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":346,"completed":185,"skipped":3518,"failed":0}
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:58:27.790: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating all guestbook components
May 31 13:58:27.850: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

May 31 13:58:27.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-4295 create -f -'
May 31 13:58:29.422: INFO: stderr: ""
May 31 13:58:29.422: INFO: stdout: "service/agnhost-replica created\n"
May 31 13:58:29.422: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

May 31 13:58:29.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-4295 create -f -'
May 31 13:58:29.771: INFO: stderr: ""
May 31 13:58:29.771: INFO: stdout: "service/agnhost-primary created\n"
May 31 13:58:29.771: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

May 31 13:58:29.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-4295 create -f -'
May 31 13:58:30.025: INFO: stderr: ""
May 31 13:58:30.025: INFO: stdout: "service/frontend created\n"
May 31 13:58:30.025: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.33
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

May 31 13:58:30.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-4295 create -f -'
May 31 13:58:30.268: INFO: stderr: ""
May 31 13:58:30.268: INFO: stdout: "deployment.apps/frontend created\n"
May 31 13:58:30.268: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.33
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May 31 13:58:30.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-4295 create -f -'
May 31 13:58:31.030: INFO: stderr: ""
May 31 13:58:31.030: INFO: stdout: "deployment.apps/agnhost-primary created\n"
May 31 13:58:31.030: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.33
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May 31 13:58:31.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-4295 create -f -'
May 31 13:58:31.347: INFO: stderr: ""
May 31 13:58:31.347: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
May 31 13:58:31.347: INFO: Waiting for all frontend pods to be Running.
May 31 13:58:36.399: INFO: Waiting for frontend to serve content.
May 31 13:58:36.463: INFO: Trying to add a new entry to the guestbook.
May 31 13:58:36.523: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
May 31 13:58:36.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-4295 delete --grace-period=0 --force -f -'
May 31 13:58:36.796: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 31 13:58:36.796: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
May 31 13:58:36.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-4295 delete --grace-period=0 --force -f -'
May 31 13:58:36.924: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 31 13:58:36.924: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
May 31 13:58:36.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-4295 delete --grace-period=0 --force -f -'
May 31 13:58:37.071: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 31 13:58:37.071: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 31 13:58:37.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-4295 delete --grace-period=0 --force -f -'
May 31 13:58:37.254: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 31 13:58:37.254: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 31 13:58:37.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-4295 delete --grace-period=0 --force -f -'
May 31 13:58:37.672: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 31 13:58:37.672: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
May 31 13:58:37.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-4295 delete --grace-period=0 --force -f -'
May 31 13:58:37.957: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 31 13:58:37.957: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:58:37.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4295" for this suite.

• [SLOW TEST:10.215 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:339
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":346,"completed":186,"skipped":3518,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:58:38.006: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting a starting resourceVersion
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:58:40.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8406" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":346,"completed":187,"skipped":3535,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:58:40.924: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 31 13:58:41.018: INFO: Waiting up to 5m0s for pod "downwardapi-volume-098c71b5-e6f8-4657-b445-0b7d70ebf4a9" in namespace "projected-8733" to be "Succeeded or Failed"
May 31 13:58:41.030: INFO: Pod "downwardapi-volume-098c71b5-e6f8-4657-b445-0b7d70ebf4a9": Phase="Pending", Reason="", readiness=false. Elapsed: 11.610228ms
May 31 13:58:43.043: INFO: Pod "downwardapi-volume-098c71b5-e6f8-4657-b445-0b7d70ebf4a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024558688s
May 31 13:58:45.057: INFO: Pod "downwardapi-volume-098c71b5-e6f8-4657-b445-0b7d70ebf4a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038358847s
STEP: Saw pod success
May 31 13:58:45.057: INFO: Pod "downwardapi-volume-098c71b5-e6f8-4657-b445-0b7d70ebf4a9" satisfied condition "Succeeded or Failed"
May 31 13:58:45.065: INFO: Trying to get logs from node ip-172-31-3-160.eu-central-1.compute.internal pod downwardapi-volume-098c71b5-e6f8-4657-b445-0b7d70ebf4a9 container client-container: <nil>
STEP: delete the pod
May 31 13:58:45.143: INFO: Waiting for pod downwardapi-volume-098c71b5-e6f8-4657-b445-0b7d70ebf4a9 to disappear
May 31 13:58:45.151: INFO: Pod downwardapi-volume-098c71b5-e6f8-4657-b445-0b7d70ebf4a9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:58:45.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8733" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":188,"skipped":3554,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:58:45.174: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-map-38ad0362-044e-47d7-b873-280a69f71927
STEP: Creating a pod to test consume configMaps
May 31 13:58:45.304: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e9448173-3a41-4e4f-8b31-d19a88c6af73" in namespace "projected-9943" to be "Succeeded or Failed"
May 31 13:58:45.312: INFO: Pod "pod-projected-configmaps-e9448173-3a41-4e4f-8b31-d19a88c6af73": Phase="Pending", Reason="", readiness=false. Elapsed: 8.277082ms
May 31 13:58:47.321: INFO: Pod "pod-projected-configmaps-e9448173-3a41-4e4f-8b31-d19a88c6af73": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017557043s
May 31 13:58:49.335: INFO: Pod "pod-projected-configmaps-e9448173-3a41-4e4f-8b31-d19a88c6af73": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031379958s
STEP: Saw pod success
May 31 13:58:49.335: INFO: Pod "pod-projected-configmaps-e9448173-3a41-4e4f-8b31-d19a88c6af73" satisfied condition "Succeeded or Failed"
May 31 13:58:49.345: INFO: Trying to get logs from node ip-172-31-3-160.eu-central-1.compute.internal pod pod-projected-configmaps-e9448173-3a41-4e4f-8b31-d19a88c6af73 container agnhost-container: <nil>
STEP: delete the pod
May 31 13:58:49.428: INFO: Waiting for pod pod-projected-configmaps-e9448173-3a41-4e4f-8b31-d19a88c6af73 to disappear
May 31 13:58:49.441: INFO: Pod pod-projected-configmaps-e9448173-3a41-4e4f-8b31-d19a88c6af73 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:58:49.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9943" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":189,"skipped":3584,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:58:49.483: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap configmap-8523/configmap-test-a7d38e38-d86e-47b9-a06f-80883d7c1d63
STEP: Creating a pod to test consume configMaps
May 31 13:58:49.584: INFO: Waiting up to 5m0s for pod "pod-configmaps-4b49b061-f1fb-40fe-b418-bd30e8a33c0e" in namespace "configmap-8523" to be "Succeeded or Failed"
May 31 13:58:49.602: INFO: Pod "pod-configmaps-4b49b061-f1fb-40fe-b418-bd30e8a33c0e": Phase="Pending", Reason="", readiness=false. Elapsed: 18.131474ms
May 31 13:58:51.614: INFO: Pod "pod-configmaps-4b49b061-f1fb-40fe-b418-bd30e8a33c0e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029881648s
May 31 13:58:53.628: INFO: Pod "pod-configmaps-4b49b061-f1fb-40fe-b418-bd30e8a33c0e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04317144s
STEP: Saw pod success
May 31 13:58:53.628: INFO: Pod "pod-configmaps-4b49b061-f1fb-40fe-b418-bd30e8a33c0e" satisfied condition "Succeeded or Failed"
May 31 13:58:53.635: INFO: Trying to get logs from node ip-172-31-3-160.eu-central-1.compute.internal pod pod-configmaps-4b49b061-f1fb-40fe-b418-bd30e8a33c0e container env-test: <nil>
STEP: delete the pod
May 31 13:58:53.680: INFO: Waiting for pod pod-configmaps-4b49b061-f1fb-40fe-b418-bd30e8a33c0e to disappear
May 31 13:58:53.686: INFO: Pod pod-configmaps-4b49b061-f1fb-40fe-b418-bd30e8a33c0e no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:58:53.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8523" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":190,"skipped":3596,"failed":0}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:58:53.714: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 31 13:58:54.234: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 31 13:58:57.311: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:58:57.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2203" for this suite.
STEP: Destroying namespace "webhook-2203-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":346,"completed":191,"skipped":3600,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:58:57.663: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-9660
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating stateful set ss in namespace statefulset-9660
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9660
May 31 13:58:57.788: INFO: Found 0 stateful pods, waiting for 1
May 31 13:59:07.809: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
May 31 13:59:07.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=statefulset-9660 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 31 13:59:08.742: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 31 13:59:08.742: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 31 13:59:08.742: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 31 13:59:08.751: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 31 13:59:18.766: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 31 13:59:18.766: INFO: Waiting for statefulset status.replicas updated to 0
May 31 13:59:18.804: INFO: POD   NODE                                           PHASE    GRACE  CONDITIONS
May 31 13:59:18.804: INFO: ss-0  ip-172-31-8-180.eu-central-1.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-31 13:58:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-05-31 13:59:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-05-31 13:59:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-31 13:58:57 +0000 UTC  }]
May 31 13:59:18.804: INFO: 
May 31 13:59:18.805: INFO: StatefulSet ss has not reached scale 3, at 1
May 31 13:59:19.817: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.990506108s
May 31 13:59:20.830: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.978295305s
May 31 13:59:21.845: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.965640717s
May 31 13:59:22.858: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.951244848s
May 31 13:59:23.875: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.936097913s
May 31 13:59:24.884: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.920981535s
May 31 13:59:25.896: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.912156435s
May 31 13:59:26.906: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.900432153s
May 31 13:59:27.950: INFO: Verifying statefulset ss doesn't scale past 3 for another 890.148151ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9660
May 31 13:59:28.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=statefulset-9660 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 31 13:59:29.293: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 31 13:59:29.293: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 31 13:59:29.293: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 31 13:59:29.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=statefulset-9660 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 31 13:59:29.760: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 31 13:59:29.760: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 31 13:59:29.760: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 31 13:59:29.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=statefulset-9660 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 31 13:59:30.335: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 31 13:59:30.335: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 31 13:59:30.335: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 31 13:59:30.349: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 31 13:59:30.349: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 31 13:59:30.349: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
May 31 13:59:30.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=statefulset-9660 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 31 13:59:30.667: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 31 13:59:30.667: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 31 13:59:30.667: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 31 13:59:30.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=statefulset-9660 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 31 13:59:31.017: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 31 13:59:31.017: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 31 13:59:31.017: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 31 13:59:31.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=statefulset-9660 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 31 13:59:31.339: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 31 13:59:31.339: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 31 13:59:31.339: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 31 13:59:31.339: INFO: Waiting for statefulset status.replicas updated to 0
May 31 13:59:31.347: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
May 31 13:59:41.368: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 31 13:59:41.368: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 31 13:59:41.368: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 31 13:59:41.411: INFO: POD   NODE                                            PHASE    GRACE  CONDITIONS
May 31 13:59:41.411: INFO: ss-0  ip-172-31-8-180.eu-central-1.compute.internal   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-31 13:58:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-05-31 13:59:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-05-31 13:59:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-31 13:58:57 +0000 UTC  }]
May 31 13:59:41.411: INFO: ss-1  ip-172-31-3-160.eu-central-1.compute.internal   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-31 13:59:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-05-31 13:59:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-05-31 13:59:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-31 13:59:18 +0000 UTC  }]
May 31 13:59:41.411: INFO: ss-2  ip-172-31-11-247.eu-central-1.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-31 13:59:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-05-31 13:59:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-05-31 13:59:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-31 13:59:18 +0000 UTC  }]
May 31 13:59:41.411: INFO: 
May 31 13:59:41.411: INFO: StatefulSet ss has not reached scale 0, at 3
May 31 13:59:42.423: INFO: POD   NODE                                            PHASE    GRACE  CONDITIONS
May 31 13:59:42.423: INFO: ss-0  ip-172-31-8-180.eu-central-1.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-31 13:58:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-05-31 13:59:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-05-31 13:59:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-31 13:58:57 +0000 UTC  }]
May 31 13:59:42.423: INFO: ss-1  ip-172-31-3-160.eu-central-1.compute.internal   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-31 13:59:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-05-31 13:59:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-05-31 13:59:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-31 13:59:18 +0000 UTC  }]
May 31 13:59:42.423: INFO: ss-2  ip-172-31-11-247.eu-central-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-05-31 13:59:18 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-05-31 13:59:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-05-31 13:59:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-05-31 13:59:18 +0000 UTC  }]
May 31 13:59:42.423: INFO: 
May 31 13:59:42.423: INFO: StatefulSet ss has not reached scale 0, at 3
May 31 13:59:43.432: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.970709672s
May 31 13:59:44.441: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.961236275s
May 31 13:59:45.453: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.952349564s
May 31 13:59:46.464: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.940312131s
May 31 13:59:47.480: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.929373169s
May 31 13:59:48.491: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.913670603s
May 31 13:59:49.501: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.902686145s
May 31 13:59:50.517: INFO: Verifying statefulset ss doesn't scale past 0 for another 893.111288ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9660
May 31 13:59:51.526: INFO: Scaling statefulset ss to 0
May 31 13:59:51.549: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
May 31 13:59:51.555: INFO: Deleting all statefulset in ns statefulset-9660
May 31 13:59:51.561: INFO: Scaling statefulset ss to 0
May 31 13:59:51.585: INFO: Waiting for statefulset status.replicas updated to 0
May 31 13:59:51.590: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 13:59:51.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9660" for this suite.

• [SLOW TEST:53.978 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":346,"completed":192,"skipped":3636,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 13:59:51.649: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service in namespace services-3829
STEP: creating service affinity-nodeport-transition in namespace services-3829
STEP: creating replication controller affinity-nodeport-transition in namespace services-3829
I0531 13:59:51.735143      19 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-3829, replica count: 3
I0531 13:59:54.788188      19 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 31 13:59:54.816: INFO: Creating new exec pod
May 31 13:59:57.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-3829 exec execpod-affinitydfz9d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
May 31 13:59:58.475: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
May 31 13:59:58.476: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 31 13:59:58.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-3829 exec execpod-affinitydfz9d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.21.202 80'
May 31 13:59:58.848: INFO: stderr: "+ nc -v -t -w 2 10.240.21.202 80\n+ echo hostName\nConnection to 10.240.21.202 80 port [tcp/http] succeeded!\n"
May 31 13:59:58.848: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 31 13:59:58.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-3829 exec execpod-affinitydfz9d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.8.180 32469'
May 31 13:59:59.368: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.8.180 32469\nConnection to 172.31.8.180 32469 port [tcp/*] succeeded!\n"
May 31 13:59:59.368: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 31 13:59:59.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-3829 exec execpod-affinitydfz9d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.3.160 32469'
May 31 13:59:59.846: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.3.160 32469\nConnection to 172.31.3.160 32469 port [tcp/*] succeeded!\n"
May 31 13:59:59.846: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 31 13:59:59.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-3829 exec execpod-affinitydfz9d -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.11.247:32469/ ; done'
May 31 14:00:00.604: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:32469/\n"
May 31 14:00:00.604: INFO: stdout: "\naffinity-nodeport-transition-shkmt\naffinity-nodeport-transition-mfbzd\naffinity-nodeport-transition-s8rsf\naffinity-nodeport-transition-shkmt\naffinity-nodeport-transition-mfbzd\naffinity-nodeport-transition-s8rsf\naffinity-nodeport-transition-shkmt\naffinity-nodeport-transition-mfbzd\naffinity-nodeport-transition-s8rsf\naffinity-nodeport-transition-shkmt\naffinity-nodeport-transition-mfbzd\naffinity-nodeport-transition-s8rsf\naffinity-nodeport-transition-shkmt\naffinity-nodeport-transition-mfbzd\naffinity-nodeport-transition-s8rsf\naffinity-nodeport-transition-shkmt"
May 31 14:00:00.604: INFO: Received response from host: affinity-nodeport-transition-shkmt
May 31 14:00:00.604: INFO: Received response from host: affinity-nodeport-transition-mfbzd
May 31 14:00:00.604: INFO: Received response from host: affinity-nodeport-transition-s8rsf
May 31 14:00:00.604: INFO: Received response from host: affinity-nodeport-transition-shkmt
May 31 14:00:00.604: INFO: Received response from host: affinity-nodeport-transition-mfbzd
May 31 14:00:00.604: INFO: Received response from host: affinity-nodeport-transition-s8rsf
May 31 14:00:00.604: INFO: Received response from host: affinity-nodeport-transition-shkmt
May 31 14:00:00.604: INFO: Received response from host: affinity-nodeport-transition-mfbzd
May 31 14:00:00.604: INFO: Received response from host: affinity-nodeport-transition-s8rsf
May 31 14:00:00.604: INFO: Received response from host: affinity-nodeport-transition-shkmt
May 31 14:00:00.604: INFO: Received response from host: affinity-nodeport-transition-mfbzd
May 31 14:00:00.604: INFO: Received response from host: affinity-nodeport-transition-s8rsf
May 31 14:00:00.604: INFO: Received response from host: affinity-nodeport-transition-shkmt
May 31 14:00:00.604: INFO: Received response from host: affinity-nodeport-transition-mfbzd
May 31 14:00:00.604: INFO: Received response from host: affinity-nodeport-transition-s8rsf
May 31 14:00:00.604: INFO: Received response from host: affinity-nodeport-transition-shkmt
May 31 14:00:00.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-3829 exec execpod-affinitydfz9d -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.11.247:32469/ ; done'
May 31 14:00:01.233: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:32469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:32469/\n"
May 31 14:00:01.234: INFO: stdout: "\naffinity-nodeport-transition-mfbzd\naffinity-nodeport-transition-mfbzd\naffinity-nodeport-transition-mfbzd\naffinity-nodeport-transition-mfbzd\naffinity-nodeport-transition-mfbzd\naffinity-nodeport-transition-mfbzd\naffinity-nodeport-transition-mfbzd\naffinity-nodeport-transition-mfbzd\naffinity-nodeport-transition-mfbzd\naffinity-nodeport-transition-mfbzd\naffinity-nodeport-transition-mfbzd\naffinity-nodeport-transition-mfbzd\naffinity-nodeport-transition-mfbzd\naffinity-nodeport-transition-mfbzd\naffinity-nodeport-transition-mfbzd\naffinity-nodeport-transition-mfbzd"
May 31 14:00:01.234: INFO: Received response from host: affinity-nodeport-transition-mfbzd
May 31 14:00:01.234: INFO: Received response from host: affinity-nodeport-transition-mfbzd
May 31 14:00:01.234: INFO: Received response from host: affinity-nodeport-transition-mfbzd
May 31 14:00:01.234: INFO: Received response from host: affinity-nodeport-transition-mfbzd
May 31 14:00:01.234: INFO: Received response from host: affinity-nodeport-transition-mfbzd
May 31 14:00:01.234: INFO: Received response from host: affinity-nodeport-transition-mfbzd
May 31 14:00:01.234: INFO: Received response from host: affinity-nodeport-transition-mfbzd
May 31 14:00:01.234: INFO: Received response from host: affinity-nodeport-transition-mfbzd
May 31 14:00:01.234: INFO: Received response from host: affinity-nodeport-transition-mfbzd
May 31 14:00:01.234: INFO: Received response from host: affinity-nodeport-transition-mfbzd
May 31 14:00:01.234: INFO: Received response from host: affinity-nodeport-transition-mfbzd
May 31 14:00:01.234: INFO: Received response from host: affinity-nodeport-transition-mfbzd
May 31 14:00:01.234: INFO: Received response from host: affinity-nodeport-transition-mfbzd
May 31 14:00:01.234: INFO: Received response from host: affinity-nodeport-transition-mfbzd
May 31 14:00:01.234: INFO: Received response from host: affinity-nodeport-transition-mfbzd
May 31 14:00:01.234: INFO: Received response from host: affinity-nodeport-transition-mfbzd
May 31 14:00:01.234: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-3829, will wait for the garbage collector to delete the pods
May 31 14:00:01.361: INFO: Deleting ReplicationController affinity-nodeport-transition took: 16.387902ms
May 31 14:00:01.483: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 122.114639ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:00:03.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3829" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:12.208 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":193,"skipped":3671,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:00:03.875: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-upd-54323628-04c8-4c8b-8f75-56aa7b2a127e
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:00:08.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5803" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":194,"skipped":3760,"failed":0}
SS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:00:08.187: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
May 31 14:00:08.319: INFO: Waiting up to 1m0s for all nodes to be ready
May 31 14:01:08.401: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:01:08.419: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 14:01:08.517: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
May 31 14:01:08.525: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:01:08.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-9472" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:01:08.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6237" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:60.540 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":346,"completed":195,"skipped":3762,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:01:08.727: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service nodeport-test with type=NodePort in namespace services-9589
STEP: creating replication controller nodeport-test in namespace services-9589
I0531 14:01:08.821084      19 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-9589, replica count: 2
May 31 14:01:11.874: INFO: Creating new exec pod
I0531 14:01:11.874147      19 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 31 14:01:14.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-9589 exec execpod8kxpq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 31 14:01:15.424: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 31 14:01:15.424: INFO: stdout: ""
May 31 14:01:16.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-9589 exec execpod8kxpq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 31 14:01:16.860: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 31 14:01:16.860: INFO: stdout: ""
May 31 14:01:17.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-9589 exec execpod8kxpq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 31 14:01:17.895: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 31 14:01:17.895: INFO: stdout: ""
May 31 14:01:18.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-9589 exec execpod8kxpq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 31 14:01:18.805: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 31 14:01:18.805: INFO: stdout: ""
May 31 14:01:19.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-9589 exec execpod8kxpq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 31 14:01:19.773: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 31 14:01:19.773: INFO: stdout: ""
May 31 14:01:20.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-9589 exec execpod8kxpq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 31 14:01:20.759: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 31 14:01:20.759: INFO: stdout: ""
May 31 14:01:21.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-9589 exec execpod8kxpq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 31 14:01:21.774: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 31 14:01:21.774: INFO: stdout: ""
May 31 14:01:22.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-9589 exec execpod8kxpq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 31 14:01:22.783: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 31 14:01:22.783: INFO: stdout: ""
May 31 14:01:23.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-9589 exec execpod8kxpq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
May 31 14:01:23.818: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
May 31 14:01:23.818: INFO: stdout: "nodeport-test-vvpf9"
May 31 14:01:23.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-9589 exec execpod8kxpq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.31.112 80'
May 31 14:01:24.271: INFO: stderr: "+ nc -v -t -w 2 10.240.31.112 80\n+ echoConnection to 10.240.31.112 80 port [tcp/http] succeeded!\n hostName\n"
May 31 14:01:24.271: INFO: stdout: "nodeport-test-gcz2q"
May 31 14:01:24.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-9589 exec execpod8kxpq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.11.247 31789'
May 31 14:01:24.835: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.11.247 31789\nConnection to 172.31.11.247 31789 port [tcp/*] succeeded!\n"
May 31 14:01:24.835: INFO: stdout: ""
May 31 14:01:25.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-9589 exec execpod8kxpq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.11.247 31789'
May 31 14:01:26.237: INFO: stderr: "+ + nc -v -t -w 2 172.31.11.247 31789\necho hostName\nConnection to 172.31.11.247 31789 port [tcp/*] succeeded!\n"
May 31 14:01:26.237: INFO: stdout: ""
May 31 14:01:26.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-9589 exec execpod8kxpq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.11.247 31789'
May 31 14:01:27.381: INFO: stderr: "+ nc -v -t -w 2 172.31.11.247 31789\n+ echo hostName\nConnection to 172.31.11.247 31789 port [tcp/*] succeeded!\n"
May 31 14:01:27.381: INFO: stdout: "nodeport-test-vvpf9"
May 31 14:01:27.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-9589 exec execpod8kxpq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.8.180 31789'
May 31 14:01:27.809: INFO: stderr: "+ nc -v -t -w 2 172.31.8.180 31789\nConnection to 172.31.8.180 31789 port [tcp/*] succeeded!\n+ echo hostName\n"
May 31 14:01:27.809: INFO: stdout: "nodeport-test-vvpf9"
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:01:27.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9589" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:19.110 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":346,"completed":196,"skipped":3773,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:01:27.838: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 14:01:27.884: INFO: Creating simple deployment test-new-deployment
May 31 14:01:27.917: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the deployment Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
May 31 14:01:30.025: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-3258  02040c2c-73dc-4aac-9b81-cb0d7f046b44 32267 3 2022-05-31 14:01:27 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2022-05-31 14:01:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-31 14:01:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0041fe758 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-5d9fdcc779" has successfully progressed.,LastUpdateTime:2022-05-31 14:01:29 +0000 UTC,LastTransitionTime:2022-05-31 14:01:27 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-05-31 14:01:29 +0000 UTC,LastTransitionTime:2022-05-31 14:01:29 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 31 14:01:30.035: INFO: New ReplicaSet "test-new-deployment-5d9fdcc779" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-5d9fdcc779  deployment-3258  99f5fb81-1a14-4644-aeaf-5561764ed90f 32269 2 2022-05-31 14:01:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 02040c2c-73dc-4aac-9b81-cb0d7f046b44 0xc0042fe657 0xc0042fe658}] []  [{kube-controller-manager Update apps/v1 2022-05-31 14:01:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"02040c2c-73dc-4aac-9b81-cb0d7f046b44\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-31 14:01:29 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 5d9fdcc779,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0042fe6e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 31 14:01:30.043: INFO: Pod "test-new-deployment-5d9fdcc779-4bbmx" is not available:
&Pod{ObjectMeta:{test-new-deployment-5d9fdcc779-4bbmx test-new-deployment-5d9fdcc779- deployment-3258  3654b736-e5a6-4bc3-8931-e88e5d3189c9 32270 0 2022-05-31 14:01:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet test-new-deployment-5d9fdcc779 99f5fb81-1a14-4644-aeaf-5561764ed90f 0xc0042feab7 0xc0042feab8}] []  [{kube-controller-manager Update v1 2022-05-31 14:01:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"99f5fb81-1a14-4644-aeaf-5561764ed90f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-31 14:01:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mlt55,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mlt55,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-160.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:01:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:01:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:01:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:01:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.160,PodIP:,StartTime:2022-05-31 14:01:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 31 14:01:30.043: INFO: Pod "test-new-deployment-5d9fdcc779-xlw4j" is available:
&Pod{ObjectMeta:{test-new-deployment-5d9fdcc779-xlw4j test-new-deployment-5d9fdcc779- deployment-3258  27874815-54b9-41e7-bc27-f7101065d803 32254 0 2022-05-31 14:01:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:bc1f46b5001416f0830dd7f88cd7b7558a422508add270bf6809b8d2425c9ffd cni.projectcalico.org/podIP:172.25.1.220/32 cni.projectcalico.org/podIPs:172.25.1.220/32] [{apps/v1 ReplicaSet test-new-deployment-5d9fdcc779 99f5fb81-1a14-4644-aeaf-5561764ed90f 0xc0042fec97 0xc0042fec98}] []  [{kube-controller-manager Update v1 2022-05-31 14:01:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"99f5fb81-1a14-4644-aeaf-5561764ed90f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-05-31 14:01:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-05-31 14:01:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.220\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dk7pv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dk7pv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-8-180.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:01:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:01:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:01:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:01:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.8.180,PodIP:172.25.1.220,StartTime:2022-05-31 14:01:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-31 14:01:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://dfece891192d2e9555c9a88f74926d7e3f4689fcfcc02235b88880d642fcaa8a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.220,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:01:30.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3258" for this suite.
•{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","total":346,"completed":197,"skipped":3795,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:01:30.100: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-8897
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a new StatefulSet
May 31 14:01:30.202: INFO: Found 0 stateful pods, waiting for 3
May 31 14:01:40.211: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 31 14:01:40.212: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 31 14:01:40.212: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
May 31 14:01:40.285: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
May 31 14:01:50.350: INFO: Updating stateful set ss2
May 31 14:01:50.378: INFO: Waiting for Pod statefulset-8897/ss2-2 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
STEP: Restoring Pods to the correct revision when they are deleted
May 31 14:02:00.481: INFO: Found 2 stateful pods, waiting for 3
May 31 14:02:10.494: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 31 14:02:10.494: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 31 14:02:10.494: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
May 31 14:02:10.551: INFO: Updating stateful set ss2
May 31 14:02:10.575: INFO: Waiting for Pod statefulset-8897/ss2-1 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
May 31 14:02:20.624: INFO: Updating stateful set ss2
May 31 14:02:20.639: INFO: Waiting for StatefulSet statefulset-8897/ss2 to complete update
May 31 14:02:20.639: INFO: Waiting for Pod statefulset-8897/ss2-0 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
May 31 14:02:30.663: INFO: Deleting all statefulset in ns statefulset-8897
May 31 14:02:30.686: INFO: Scaling statefulset ss2 to 0
May 31 14:02:40.726: INFO: Waiting for statefulset status.replicas updated to 0
May 31 14:02:40.732: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:02:40.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8897" for this suite.

• [SLOW TEST:70.714 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":346,"completed":198,"skipped":3818,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:02:40.815: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating the pod
May 31 14:02:40.951: INFO: The status of Pod labelsupdated800649e-8b2f-46db-b742-887a61e109c9 is Pending, waiting for it to be Running (with Ready = true)
May 31 14:02:42.963: INFO: The status of Pod labelsupdated800649e-8b2f-46db-b742-887a61e109c9 is Running (Ready = true)
May 31 14:02:43.553: INFO: Successfully updated pod "labelsupdated800649e-8b2f-46db-b742-887a61e109c9"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:02:47.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6855" for this suite.

• [SLOW TEST:6.809 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":199,"skipped":3830,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:02:47.624: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: validating cluster-info
May 31 14:02:47.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-6447 cluster-info'
May 31 14:02:47.772: INFO: stderr: ""
May 31 14:02:47.772: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.240.16.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:02:47.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6447" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":346,"completed":200,"skipped":3838,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:02:47.793: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
May 31 14:02:47.905: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
May 31 14:02:47.950: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:02:48.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-2424" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":346,"completed":201,"skipped":3893,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:02:48.050: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create set of pods
May 31 14:02:48.129: INFO: created test-pod-1
May 31 14:02:50.152: INFO: running and ready test-pod-1
May 31 14:02:50.172: INFO: created test-pod-2
May 31 14:02:52.201: INFO: running and ready test-pod-2
May 31 14:02:52.212: INFO: created test-pod-3
May 31 14:02:54.241: INFO: running and ready test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
May 31 14:02:54.304: INFO: Pod quantity 3 is different from expected quantity 0
May 31 14:02:55.314: INFO: Pod quantity 3 is different from expected quantity 0
May 31 14:02:56.318: INFO: Pod quantity 1 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:02:57.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7563" for this suite.

• [SLOW TEST:9.301 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","total":346,"completed":202,"skipped":3926,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:02:57.352: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 14:02:57.402: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 31 14:03:02.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=crd-publish-openapi-413 --namespace=crd-publish-openapi-413 create -f -'
May 31 14:03:03.374: INFO: stderr: ""
May 31 14:03:03.374: INFO: stdout: "e2e-test-crd-publish-openapi-8893-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May 31 14:03:03.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=crd-publish-openapi-413 --namespace=crd-publish-openapi-413 delete e2e-test-crd-publish-openapi-8893-crds test-cr'
May 31 14:03:03.535: INFO: stderr: ""
May 31 14:03:03.535: INFO: stdout: "e2e-test-crd-publish-openapi-8893-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
May 31 14:03:03.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=crd-publish-openapi-413 --namespace=crd-publish-openapi-413 apply -f -'
May 31 14:03:03.895: INFO: stderr: ""
May 31 14:03:03.895: INFO: stdout: "e2e-test-crd-publish-openapi-8893-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
May 31 14:03:03.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=crd-publish-openapi-413 --namespace=crd-publish-openapi-413 delete e2e-test-crd-publish-openapi-8893-crds test-cr'
May 31 14:03:03.990: INFO: stderr: ""
May 31 14:03:03.990: INFO: stdout: "e2e-test-crd-publish-openapi-8893-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
May 31 14:03:03.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=crd-publish-openapi-413 explain e2e-test-crd-publish-openapi-8893-crds'
May 31 14:03:04.298: INFO: stderr: ""
May 31 14:03:04.298: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8893-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:03:07.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-413" for this suite.

• [SLOW TEST:10.560 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":346,"completed":203,"skipped":3938,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:03:07.914: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5455 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5455;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5455 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5455;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5455.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5455.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5455.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5455.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5455.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5455.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5455.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5455.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5455.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5455.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5455.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5455.svc;check="$$(dig +notcp +noall +answer +search 125.17.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.17.125_udp@PTR;check="$$(dig +tcp +noall +answer +search 125.17.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.17.125_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5455 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5455;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5455 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5455;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5455.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5455.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5455.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5455.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5455.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5455.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5455.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5455.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5455.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5455.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5455.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5455.svc;check="$$(dig +notcp +noall +answer +search 125.17.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.17.125_udp@PTR;check="$$(dig +tcp +noall +answer +search 125.17.240.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.240.17.125_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 31 14:03:12.253: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5455/dns-test-bd265c19-5bc3-4bbe-9ecf-3a30a6e14cd0: the server could not find the requested resource (get pods dns-test-bd265c19-5bc3-4bbe-9ecf-3a30a6e14cd0)
May 31 14:03:12.264: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5455/dns-test-bd265c19-5bc3-4bbe-9ecf-3a30a6e14cd0: the server could not find the requested resource (get pods dns-test-bd265c19-5bc3-4bbe-9ecf-3a30a6e14cd0)
May 31 14:03:12.379: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5455 from pod dns-5455/dns-test-bd265c19-5bc3-4bbe-9ecf-3a30a6e14cd0: the server could not find the requested resource (get pods dns-test-bd265c19-5bc3-4bbe-9ecf-3a30a6e14cd0)
May 31 14:03:12.410: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5455.svc from pod dns-5455/dns-test-bd265c19-5bc3-4bbe-9ecf-3a30a6e14cd0: the server could not find the requested resource (get pods dns-test-bd265c19-5bc3-4bbe-9ecf-3a30a6e14cd0)
May 31 14:03:12.418: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5455.svc from pod dns-5455/dns-test-bd265c19-5bc3-4bbe-9ecf-3a30a6e14cd0: the server could not find the requested resource (get pods dns-test-bd265c19-5bc3-4bbe-9ecf-3a30a6e14cd0)
May 31 14:03:12.429: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5455.svc from pod dns-5455/dns-test-bd265c19-5bc3-4bbe-9ecf-3a30a6e14cd0: the server could not find the requested resource (get pods dns-test-bd265c19-5bc3-4bbe-9ecf-3a30a6e14cd0)
May 31 14:03:12.502: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5455/dns-test-bd265c19-5bc3-4bbe-9ecf-3a30a6e14cd0: the server could not find the requested resource (get pods dns-test-bd265c19-5bc3-4bbe-9ecf-3a30a6e14cd0)
May 31 14:03:12.538: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5455/dns-test-bd265c19-5bc3-4bbe-9ecf-3a30a6e14cd0: the server could not find the requested resource (get pods dns-test-bd265c19-5bc3-4bbe-9ecf-3a30a6e14cd0)
May 31 14:03:12.553: INFO: Unable to read jessie_udp@dns-test-service.dns-5455 from pod dns-5455/dns-test-bd265c19-5bc3-4bbe-9ecf-3a30a6e14cd0: the server could not find the requested resource (get pods dns-test-bd265c19-5bc3-4bbe-9ecf-3a30a6e14cd0)
May 31 14:03:12.562: INFO: Unable to read jessie_tcp@dns-test-service.dns-5455 from pod dns-5455/dns-test-bd265c19-5bc3-4bbe-9ecf-3a30a6e14cd0: the server could not find the requested resource (get pods dns-test-bd265c19-5bc3-4bbe-9ecf-3a30a6e14cd0)
May 31 14:03:12.577: INFO: Unable to read jessie_udp@dns-test-service.dns-5455.svc from pod dns-5455/dns-test-bd265c19-5bc3-4bbe-9ecf-3a30a6e14cd0: the server could not find the requested resource (get pods dns-test-bd265c19-5bc3-4bbe-9ecf-3a30a6e14cd0)
May 31 14:03:12.586: INFO: Unable to read jessie_tcp@dns-test-service.dns-5455.svc from pod dns-5455/dns-test-bd265c19-5bc3-4bbe-9ecf-3a30a6e14cd0: the server could not find the requested resource (get pods dns-test-bd265c19-5bc3-4bbe-9ecf-3a30a6e14cd0)
May 31 14:03:12.595: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5455.svc from pod dns-5455/dns-test-bd265c19-5bc3-4bbe-9ecf-3a30a6e14cd0: the server could not find the requested resource (get pods dns-test-bd265c19-5bc3-4bbe-9ecf-3a30a6e14cd0)
May 31 14:03:12.604: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5455.svc from pod dns-5455/dns-test-bd265c19-5bc3-4bbe-9ecf-3a30a6e14cd0: the server could not find the requested resource (get pods dns-test-bd265c19-5bc3-4bbe-9ecf-3a30a6e14cd0)
May 31 14:03:12.641: INFO: Lookups using dns-5455/dns-test-bd265c19-5bc3-4bbe-9ecf-3a30a6e14cd0 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_tcp@dns-test-service.dns-5455 wheezy_tcp@dns-test-service.dns-5455.svc wheezy_udp@_http._tcp.dns-test-service.dns-5455.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5455.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5455 jessie_tcp@dns-test-service.dns-5455 jessie_udp@dns-test-service.dns-5455.svc jessie_tcp@dns-test-service.dns-5455.svc jessie_udp@_http._tcp.dns-test-service.dns-5455.svc jessie_tcp@_http._tcp.dns-test-service.dns-5455.svc]

May 31 14:03:18.116: INFO: DNS probes using dns-5455/dns-test-bd265c19-5bc3-4bbe-9ecf-3a30a6e14cd0 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:03:18.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5455" for this suite.

• [SLOW TEST:10.316 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":346,"completed":204,"skipped":3968,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:03:18.239: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
May 31 14:03:18.335: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8584  6709c3a8-62be-4806-a507-92309f88dce7 33326 0 2022-05-31 14:03:18 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-05-31 14:03:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 31 14:03:18.336: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8584  6709c3a8-62be-4806-a507-92309f88dce7 33327 0 2022-05-31 14:03:18 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-05-31 14:03:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
May 31 14:03:18.336: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8584  6709c3a8-62be-4806-a507-92309f88dce7 33328 0 2022-05-31 14:03:18 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-05-31 14:03:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
May 31 14:03:28.406: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8584  6709c3a8-62be-4806-a507-92309f88dce7 33387 0 2022-05-31 14:03:18 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-05-31 14:03:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 31 14:03:28.406: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8584  6709c3a8-62be-4806-a507-92309f88dce7 33388 0 2022-05-31 14:03:18 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-05-31 14:03:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
May 31 14:03:28.406: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-8584  6709c3a8-62be-4806-a507-92309f88dce7 33389 0 2022-05-31 14:03:18 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-05-31 14:03:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:03:28.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8584" for this suite.

• [SLOW TEST:10.184 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":346,"completed":205,"skipped":3994,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:03:28.426: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 14:03:30.495: INFO: Deleting pod "var-expansion-d3d82b27-3a0a-49d0-b842-e4e221966c08" in namespace "var-expansion-2074"
May 31 14:03:30.517: INFO: Wait up to 5m0s for pod "var-expansion-d3d82b27-3a0a-49d0-b842-e4e221966c08" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:03:34.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2074" for this suite.

• [SLOW TEST:6.125 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","total":346,"completed":206,"skipped":4015,"failed":0}
S
------------------------------
[sig-network] EndpointSlice 
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:03:34.553: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 14:03:34.612: INFO: Endpoints addresses: [3.124.138.202] , ports: [31059]
May 31 14:03:34.613: INFO: EndpointSlices addresses: [3.124.138.202] , ports: [31059]
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:03:34.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-6774" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","total":346,"completed":207,"skipped":4016,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:03:34.644: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
May 31 14:03:34.711: INFO: Waiting up to 1m0s for all nodes to be ready
May 31 14:04:34.771: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create pods that use 4/5 of node resources.
May 31 14:04:34.834: INFO: Created pod: pod0-0-sched-preemption-low-priority
May 31 14:04:34.851: INFO: Created pod: pod0-1-sched-preemption-medium-priority
May 31 14:04:34.901: INFO: Created pod: pod1-0-sched-preemption-medium-priority
May 31 14:04:34.912: INFO: Created pod: pod1-1-sched-preemption-medium-priority
May 31 14:04:34.962: INFO: Created pod: pod2-0-sched-preemption-medium-priority
May 31 14:04:34.982: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:04:49.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6691" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:74.608 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":346,"completed":208,"skipped":4068,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:04:49.255: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Creating a NodePort Service
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota
STEP: Ensuring resource quota status captures service creation
STEP: Deleting Services
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:05:00.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2852" for this suite.

• [SLOW TEST:11.300 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":346,"completed":209,"skipped":4083,"failed":0}
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:05:00.559: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-6366, will wait for the garbage collector to delete the pods
May 31 14:05:04.769: INFO: Deleting Job.batch foo took: 20.337787ms
May 31 14:05:04.869: INFO: Terminating Job.batch foo pods took: 100.29503ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:05:36.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6366" for this suite.

• [SLOW TEST:35.772 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":346,"completed":210,"skipped":4083,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:05:36.330: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 31 14:05:40.523: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:05:40.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8759" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]","total":346,"completed":211,"skipped":4103,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:05:40.601: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create set of pod templates
May 31 14:05:40.706: INFO: created test-podtemplate-1
May 31 14:05:40.716: INFO: created test-podtemplate-2
May 31 14:05:40.731: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
May 31 14:05:40.740: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
May 31 14:05:40.787: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:05:40.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-1512" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":346,"completed":212,"skipped":4114,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:05:40.817: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-5257
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating statefulset ss in namespace statefulset-5257
May 31 14:05:40.905: INFO: Found 0 stateful pods, waiting for 1
May 31 14:05:50.914: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
STEP: Patch a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
May 31 14:05:50.999: INFO: Deleting all statefulset in ns statefulset-5257
May 31 14:05:51.012: INFO: Scaling statefulset ss to 0
May 31 14:06:01.077: INFO: Waiting for statefulset status.replicas updated to 0
May 31 14:06:01.092: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:06:01.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5257" for this suite.

• [SLOW TEST:20.329 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":346,"completed":213,"skipped":4129,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:06:01.146: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0777 on node default medium
May 31 14:06:01.225: INFO: Waiting up to 5m0s for pod "pod-353fefb7-1de9-47d6-a62a-ffdcf4a4142b" in namespace "emptydir-6089" to be "Succeeded or Failed"
May 31 14:06:01.244: INFO: Pod "pod-353fefb7-1de9-47d6-a62a-ffdcf4a4142b": Phase="Pending", Reason="", readiness=false. Elapsed: 18.839559ms
May 31 14:06:03.261: INFO: Pod "pod-353fefb7-1de9-47d6-a62a-ffdcf4a4142b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036060424s
May 31 14:06:05.272: INFO: Pod "pod-353fefb7-1de9-47d6-a62a-ffdcf4a4142b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04683487s
STEP: Saw pod success
May 31 14:06:05.272: INFO: Pod "pod-353fefb7-1de9-47d6-a62a-ffdcf4a4142b" satisfied condition "Succeeded or Failed"
May 31 14:06:05.279: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod pod-353fefb7-1de9-47d6-a62a-ffdcf4a4142b container test-container: <nil>
STEP: delete the pod
May 31 14:06:05.331: INFO: Waiting for pod pod-353fefb7-1de9-47d6-a62a-ffdcf4a4142b to disappear
May 31 14:06:05.337: INFO: Pod pod-353fefb7-1de9-47d6-a62a-ffdcf4a4142b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:06:05.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6089" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":214,"skipped":4169,"failed":0}
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:06:05.368: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
May 31 14:06:05.461: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 31 14:06:05.504: INFO: Waiting for terminating namespaces to be deleted...
May 31 14:06:05.518: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-11-247.eu-central-1.compute.internal before test
May 31 14:06:05.553: INFO: calico-kube-controllers-786b7976d6-qvmrx from kube-system started at 2022-05-31 12:59:25 +0000 UTC (1 container statuses recorded)
May 31 14:06:05.553: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May 31 14:06:05.553: INFO: canal-jkvwq from kube-system started at 2022-05-31 12:58:35 +0000 UTC (2 container statuses recorded)
May 31 14:06:05.553: INFO: 	Container calico-node ready: true, restart count 0
May 31 14:06:05.553: INFO: 	Container kube-flannel ready: true, restart count 0
May 31 14:06:05.553: INFO: coredns-767874cf84-bl7km from kube-system started at 2022-05-31 12:59:25 +0000 UTC (1 container statuses recorded)
May 31 14:06:05.553: INFO: 	Container coredns ready: true, restart count 0
May 31 14:06:05.553: INFO: coredns-767874cf84-wsllw from kube-system started at 2022-05-31 12:59:25 +0000 UTC (1 container statuses recorded)
May 31 14:06:05.553: INFO: 	Container coredns ready: true, restart count 0
May 31 14:06:05.553: INFO: kube-proxy-7tkx8 from kube-system started at 2022-05-31 12:58:35 +0000 UTC (1 container statuses recorded)
May 31 14:06:05.553: INFO: 	Container kube-proxy ready: true, restart count 0
May 31 14:06:05.553: INFO: node-local-dns-2f4s9 from kube-system started at 2022-05-31 12:58:35 +0000 UTC (1 container statuses recorded)
May 31 14:06:05.553: INFO: 	Container node-cache ready: true, restart count 0
May 31 14:06:05.553: INFO: openvpn-client-76b67b68f8-rllqf from kube-system started at 2022-05-31 12:59:25 +0000 UTC (2 container statuses recorded)
May 31 14:06:05.553: INFO: 	Container dnat-controller ready: true, restart count 0
May 31 14:06:05.553: INFO: 	Container openvpn-client ready: true, restart count 0
May 31 14:06:05.553: INFO: user-ssh-keys-agent-79f9n from kube-system started at 2022-05-31 12:58:35 +0000 UTC (1 container statuses recorded)
May 31 14:06:05.553: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
May 31 14:06:05.553: INFO: dashboard-metrics-scraper-75d68f84c9-78rk7 from kubernetes-dashboard started at 2022-05-31 12:59:25 +0000 UTC (1 container statuses recorded)
May 31 14:06:05.554: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
May 31 14:06:05.554: INFO: dashboard-metrics-scraper-75d68f84c9-d4cl8 from kubernetes-dashboard started at 2022-05-31 12:59:25 +0000 UTC (1 container statuses recorded)
May 31 14:06:05.554: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
May 31 14:06:05.554: INFO: sonobuoy-systemd-logs-daemon-set-fb59706c47b44a31-jzqd2 from sonobuoy started at 2022-05-31 13:07:09 +0000 UTC (2 container statuses recorded)
May 31 14:06:05.554: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 31 14:06:05.554: INFO: 	Container systemd-logs ready: true, restart count 0
May 31 14:06:05.554: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-3-160.eu-central-1.compute.internal before test
May 31 14:06:05.579: INFO: canal-ddb54 from kube-system started at 2022-05-31 12:59:30 +0000 UTC (2 container statuses recorded)
May 31 14:06:05.579: INFO: 	Container calico-node ready: true, restart count 0
May 31 14:06:05.579: INFO: 	Container kube-flannel ready: true, restart count 0
May 31 14:06:05.579: INFO: kube-proxy-5w228 from kube-system started at 2022-05-31 12:59:30 +0000 UTC (1 container statuses recorded)
May 31 14:06:05.579: INFO: 	Container kube-proxy ready: true, restart count 0
May 31 14:06:05.579: INFO: node-local-dns-bnwqt from kube-system started at 2022-05-31 12:59:30 +0000 UTC (1 container statuses recorded)
May 31 14:06:05.580: INFO: 	Container node-cache ready: true, restart count 0
May 31 14:06:05.580: INFO: user-ssh-keys-agent-msrlk from kube-system started at 2022-05-31 12:59:30 +0000 UTC (1 container statuses recorded)
May 31 14:06:05.580: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
May 31 14:06:05.580: INFO: sonobuoy-e2e-job-701195ac2dd44242 from sonobuoy started at 2022-05-31 13:07:09 +0000 UTC (2 container statuses recorded)
May 31 14:06:05.580: INFO: 	Container e2e ready: true, restart count 0
May 31 14:06:05.580: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 31 14:06:05.580: INFO: sonobuoy-systemd-logs-daemon-set-fb59706c47b44a31-6t75j from sonobuoy started at 2022-05-31 13:07:09 +0000 UTC (2 container statuses recorded)
May 31 14:06:05.580: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 31 14:06:05.580: INFO: 	Container systemd-logs ready: true, restart count 0
May 31 14:06:05.580: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-8-180.eu-central-1.compute.internal before test
May 31 14:06:05.595: INFO: canal-5bcsn from kube-system started at 2022-05-31 12:58:46 +0000 UTC (2 container statuses recorded)
May 31 14:06:05.595: INFO: 	Container calico-node ready: true, restart count 0
May 31 14:06:05.595: INFO: 	Container kube-flannel ready: true, restart count 0
May 31 14:06:05.595: INFO: kube-proxy-78w8t from kube-system started at 2022-05-31 12:58:46 +0000 UTC (1 container statuses recorded)
May 31 14:06:05.595: INFO: 	Container kube-proxy ready: true, restart count 0
May 31 14:06:05.595: INFO: node-local-dns-l65lm from kube-system started at 2022-05-31 12:58:46 +0000 UTC (1 container statuses recorded)
May 31 14:06:05.595: INFO: 	Container node-cache ready: true, restart count 0
May 31 14:06:05.595: INFO: user-ssh-keys-agent-dztdn from kube-system started at 2022-05-31 12:58:46 +0000 UTC (1 container statuses recorded)
May 31 14:06:05.595: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
May 31 14:06:05.595: INFO: sonobuoy from sonobuoy started at 2022-05-31 13:07:02 +0000 UTC (1 container statuses recorded)
May 31 14:06:05.595: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 31 14:06:05.595: INFO: sonobuoy-systemd-logs-daemon-set-fb59706c47b44a31-qx4ld from sonobuoy started at 2022-05-31 13:07:09 +0000 UTC (2 container statuses recorded)
May 31 14:06:05.595: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 31 14:06:05.595: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-c5ff0610-722d-4b98-a29c-16d81039d707 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-c5ff0610-722d-4b98-a29c-16d81039d707 off the node ip-172-31-8-180.eu-central-1.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-c5ff0610-722d-4b98-a29c-16d81039d707
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:06:11.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8659" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:6.447 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":346,"completed":215,"skipped":4174,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:06:11.815: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0644 on node default medium
May 31 14:06:11.893: INFO: Waiting up to 5m0s for pod "pod-14f2f336-3e78-48b6-b2be-b3d1ccf6d68c" in namespace "emptydir-7074" to be "Succeeded or Failed"
May 31 14:06:11.899: INFO: Pod "pod-14f2f336-3e78-48b6-b2be-b3d1ccf6d68c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.934927ms
May 31 14:06:13.912: INFO: Pod "pod-14f2f336-3e78-48b6-b2be-b3d1ccf6d68c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018918022s
May 31 14:06:15.922: INFO: Pod "pod-14f2f336-3e78-48b6-b2be-b3d1ccf6d68c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028972808s
STEP: Saw pod success
May 31 14:06:15.922: INFO: Pod "pod-14f2f336-3e78-48b6-b2be-b3d1ccf6d68c" satisfied condition "Succeeded or Failed"
May 31 14:06:15.927: INFO: Trying to get logs from node ip-172-31-3-160.eu-central-1.compute.internal pod pod-14f2f336-3e78-48b6-b2be-b3d1ccf6d68c container test-container: <nil>
STEP: delete the pod
May 31 14:06:16.006: INFO: Waiting for pod pod-14f2f336-3e78-48b6-b2be-b3d1ccf6d68c to disappear
May 31 14:06:16.014: INFO: Pod pod-14f2f336-3e78-48b6-b2be-b3d1ccf6d68c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:06:16.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7074" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":216,"skipped":4179,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:06:16.040: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-7f6b23e9-d6e8-4089-9e3c-d79ff0408bad
STEP: Creating a pod to test consume secrets
May 31 14:06:16.165: INFO: Waiting up to 5m0s for pod "pod-secrets-4434959d-520b-4a4c-9f04-c34f17794e8d" in namespace "secrets-3613" to be "Succeeded or Failed"
May 31 14:06:16.176: INFO: Pod "pod-secrets-4434959d-520b-4a4c-9f04-c34f17794e8d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.377664ms
May 31 14:06:18.190: INFO: Pod "pod-secrets-4434959d-520b-4a4c-9f04-c34f17794e8d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024222999s
May 31 14:06:20.201: INFO: Pod "pod-secrets-4434959d-520b-4a4c-9f04-c34f17794e8d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035513844s
May 31 14:06:22.221: INFO: Pod "pod-secrets-4434959d-520b-4a4c-9f04-c34f17794e8d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.055459249s
STEP: Saw pod success
May 31 14:06:22.221: INFO: Pod "pod-secrets-4434959d-520b-4a4c-9f04-c34f17794e8d" satisfied condition "Succeeded or Failed"
May 31 14:06:22.231: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod pod-secrets-4434959d-520b-4a4c-9f04-c34f17794e8d container secret-volume-test: <nil>
STEP: delete the pod
May 31 14:06:22.310: INFO: Waiting for pod pod-secrets-4434959d-520b-4a4c-9f04-c34f17794e8d to disappear
May 31 14:06:22.317: INFO: Pod pod-secrets-4434959d-520b-4a4c-9f04-c34f17794e8d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:06:22.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3613" for this suite.

• [SLOW TEST:6.303 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":217,"skipped":4211,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:06:22.343: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test service account token: 
May 31 14:06:22.416: INFO: Waiting up to 5m0s for pod "test-pod-aa2af9d8-2cc4-4d24-9134-a8ac59d610b9" in namespace "svcaccounts-8725" to be "Succeeded or Failed"
May 31 14:06:22.421: INFO: Pod "test-pod-aa2af9d8-2cc4-4d24-9134-a8ac59d610b9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.314522ms
May 31 14:06:24.436: INFO: Pod "test-pod-aa2af9d8-2cc4-4d24-9134-a8ac59d610b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020337179s
May 31 14:06:26.451: INFO: Pod "test-pod-aa2af9d8-2cc4-4d24-9134-a8ac59d610b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035116016s
STEP: Saw pod success
May 31 14:06:26.451: INFO: Pod "test-pod-aa2af9d8-2cc4-4d24-9134-a8ac59d610b9" satisfied condition "Succeeded or Failed"
May 31 14:06:26.462: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod test-pod-aa2af9d8-2cc4-4d24-9134-a8ac59d610b9 container agnhost-container: <nil>
STEP: delete the pod
May 31 14:06:26.524: INFO: Waiting for pod test-pod-aa2af9d8-2cc4-4d24-9134-a8ac59d610b9 to disappear
May 31 14:06:26.533: INFO: Pod test-pod-aa2af9d8-2cc4-4d24-9134-a8ac59d610b9 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:06:26.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8725" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":346,"completed":218,"skipped":4224,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:06:26.556: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 14:06:26.634: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-d67ddaee-6e6b-4dfa-b84d-83a1ae61324b" in namespace "security-context-test-8419" to be "Succeeded or Failed"
May 31 14:06:26.644: INFO: Pod "alpine-nnp-false-d67ddaee-6e6b-4dfa-b84d-83a1ae61324b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.86562ms
May 31 14:06:28.658: INFO: Pod "alpine-nnp-false-d67ddaee-6e6b-4dfa-b84d-83a1ae61324b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023960654s
May 31 14:06:30.669: INFO: Pod "alpine-nnp-false-d67ddaee-6e6b-4dfa-b84d-83a1ae61324b": Phase="Running", Reason="", readiness=true. Elapsed: 4.034942416s
May 31 14:06:32.681: INFO: Pod "alpine-nnp-false-d67ddaee-6e6b-4dfa-b84d-83a1ae61324b": Phase="Running", Reason="", readiness=false. Elapsed: 6.046394173s
May 31 14:06:34.694: INFO: Pod "alpine-nnp-false-d67ddaee-6e6b-4dfa-b84d-83a1ae61324b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.059915105s
May 31 14:06:34.694: INFO: Pod "alpine-nnp-false-d67ddaee-6e6b-4dfa-b84d-83a1ae61324b" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:06:34.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8419" for this suite.

• [SLOW TEST:8.228 seconds]
[sig-node] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:296
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":219,"skipped":4264,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:06:34.785: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create set of events
May 31 14:06:34.850: INFO: created test-event-1
May 31 14:06:34.867: INFO: created test-event-2
May 31 14:06:34.877: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
May 31 14:06:34.884: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
May 31 14:06:34.937: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:06:34.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4959" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","total":346,"completed":220,"skipped":4318,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:06:34.976: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:06:35.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7014" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":346,"completed":221,"skipped":4331,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:06:35.087: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 14:06:35.195: INFO: The status of Pod server-envvars-4760d7df-cfc0-4288-9018-f3d1b740a78a is Pending, waiting for it to be Running (with Ready = true)
May 31 14:06:37.206: INFO: The status of Pod server-envvars-4760d7df-cfc0-4288-9018-f3d1b740a78a is Pending, waiting for it to be Running (with Ready = true)
May 31 14:06:39.205: INFO: The status of Pod server-envvars-4760d7df-cfc0-4288-9018-f3d1b740a78a is Running (Ready = true)
May 31 14:06:39.242: INFO: Waiting up to 5m0s for pod "client-envvars-328bec93-00d3-45c3-974b-1ee60c2eb4da" in namespace "pods-6675" to be "Succeeded or Failed"
May 31 14:06:39.252: INFO: Pod "client-envvars-328bec93-00d3-45c3-974b-1ee60c2eb4da": Phase="Pending", Reason="", readiness=false. Elapsed: 10.104435ms
May 31 14:06:41.262: INFO: Pod "client-envvars-328bec93-00d3-45c3-974b-1ee60c2eb4da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020517543s
May 31 14:06:43.278: INFO: Pod "client-envvars-328bec93-00d3-45c3-974b-1ee60c2eb4da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035615258s
STEP: Saw pod success
May 31 14:06:43.278: INFO: Pod "client-envvars-328bec93-00d3-45c3-974b-1ee60c2eb4da" satisfied condition "Succeeded or Failed"
May 31 14:06:43.283: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod client-envvars-328bec93-00d3-45c3-974b-1ee60c2eb4da container env3cont: <nil>
STEP: delete the pod
May 31 14:06:43.317: INFO: Waiting for pod client-envvars-328bec93-00d3-45c3-974b-1ee60c2eb4da to disappear
May 31 14:06:43.325: INFO: Pod client-envvars-328bec93-00d3-45c3-974b-1ee60c2eb4da no longer exists
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:06:43.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6675" for this suite.

• [SLOW TEST:8.257 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":346,"completed":222,"skipped":4350,"failed":0}
SSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:06:43.346: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 14:06:43.400: INFO: The status of Pod busybox-readonly-fs9d3dbfde-ef65-4cb5-8afe-6c8c01c1d6f1 is Pending, waiting for it to be Running (with Ready = true)
May 31 14:06:45.408: INFO: The status of Pod busybox-readonly-fs9d3dbfde-ef65-4cb5-8afe-6c8c01c1d6f1 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:06:45.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4766" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":223,"skipped":4354,"failed":0}
SSSSSSSS
------------------------------
[sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:06:45.451: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating server pod server in namespace prestop-6371
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-6371
STEP: Deleting pre-stop pod
May 31 14:06:56.629: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:06:56.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-6371" for this suite.

• [SLOW TEST:11.244 seconds]
[sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":346,"completed":224,"skipped":4362,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:06:56.699: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:07:23.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8638" for this suite.

• [SLOW TEST:26.663 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":346,"completed":225,"skipped":4371,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:07:23.364: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 14:07:23.400: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
May 31 14:07:26.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=crd-publish-openapi-4916 --namespace=crd-publish-openapi-4916 create -f -'
May 31 14:07:27.560: INFO: stderr: ""
May 31 14:07:27.560: INFO: stdout: "e2e-test-crd-publish-openapi-3555-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May 31 14:07:27.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=crd-publish-openapi-4916 --namespace=crd-publish-openapi-4916 delete e2e-test-crd-publish-openapi-3555-crds test-cr'
May 31 14:07:27.667: INFO: stderr: ""
May 31 14:07:27.667: INFO: stdout: "e2e-test-crd-publish-openapi-3555-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
May 31 14:07:27.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=crd-publish-openapi-4916 --namespace=crd-publish-openapi-4916 apply -f -'
May 31 14:07:29.149: INFO: stderr: ""
May 31 14:07:29.150: INFO: stdout: "e2e-test-crd-publish-openapi-3555-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
May 31 14:07:29.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=crd-publish-openapi-4916 --namespace=crd-publish-openapi-4916 delete e2e-test-crd-publish-openapi-3555-crds test-cr'
May 31 14:07:29.257: INFO: stderr: ""
May 31 14:07:29.257: INFO: stdout: "e2e-test-crd-publish-openapi-3555-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
May 31 14:07:29.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=crd-publish-openapi-4916 explain e2e-test-crd-publish-openapi-3555-crds'
May 31 14:07:29.465: INFO: stderr: ""
May 31 14:07:29.465: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3555-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:07:34.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4916" for this suite.

• [SLOW TEST:10.683 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":346,"completed":226,"skipped":4404,"failed":0}
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:07:34.048: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-5753
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a new StatefulSet
May 31 14:07:34.140: INFO: Found 0 stateful pods, waiting for 3
May 31 14:07:44.153: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 31 14:07:44.153: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 31 14:07:44.153: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
May 31 14:07:44.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=statefulset-5753 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 31 14:07:44.590: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 31 14:07:44.590: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 31 14:07:44.590: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
May 31 14:07:54.659: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
May 31 14:08:04.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=statefulset-5753 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 31 14:08:05.169: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 31 14:08:05.169: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 31 14:08:05.169: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision
May 31 14:08:15.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=statefulset-5753 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 31 14:08:15.791: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 31 14:08:15.791: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 31 14:08:15.791: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 31 14:08:25.863: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
May 31 14:08:35.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=statefulset-5753 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 31 14:08:36.384: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 31 14:08:36.384: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 31 14:08:36.384: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
May 31 14:08:46.445: INFO: Deleting all statefulset in ns statefulset-5753
May 31 14:08:46.454: INFO: Scaling statefulset ss2 to 0
May 31 14:08:56.493: INFO: Waiting for statefulset status.replicas updated to 0
May 31 14:08:56.501: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:08:56.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5753" for this suite.

• [SLOW TEST:82.510 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":346,"completed":227,"skipped":4409,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:08:56.560: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
May 31 14:08:56.611: INFO: Waiting up to 1m0s for all nodes to be ready
May 31 14:09:56.677: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 14:09:56.685: INFO: Starting informer...
STEP: Starting pods...
May 31 14:09:56.936: INFO: Pod1 is running on ip-172-31-8-180.eu-central-1.compute.internal. Tainting Node
May 31 14:09:59.183: INFO: Pod2 is running on ip-172-31-8-180.eu-central-1.compute.internal. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
May 31 14:10:05.116: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
May 31 14:10:25.165: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:10:25.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-9484" for this suite.

• [SLOW TEST:88.689 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":346,"completed":228,"skipped":4419,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:10:25.256: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod pod-subpath-test-configmap-q2hv
STEP: Creating a pod to test atomic-volume-subpath
May 31 14:10:25.368: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-q2hv" in namespace "subpath-4685" to be "Succeeded or Failed"
May 31 14:10:25.387: INFO: Pod "pod-subpath-test-configmap-q2hv": Phase="Pending", Reason="", readiness=false. Elapsed: 18.387346ms
May 31 14:10:27.395: INFO: Pod "pod-subpath-test-configmap-q2hv": Phase="Running", Reason="", readiness=true. Elapsed: 2.026780599s
May 31 14:10:29.407: INFO: Pod "pod-subpath-test-configmap-q2hv": Phase="Running", Reason="", readiness=true. Elapsed: 4.038771422s
May 31 14:10:31.421: INFO: Pod "pod-subpath-test-configmap-q2hv": Phase="Running", Reason="", readiness=true. Elapsed: 6.051948314s
May 31 14:10:33.437: INFO: Pod "pod-subpath-test-configmap-q2hv": Phase="Running", Reason="", readiness=true. Elapsed: 8.067985841s
May 31 14:10:35.446: INFO: Pod "pod-subpath-test-configmap-q2hv": Phase="Running", Reason="", readiness=true. Elapsed: 10.077202564s
May 31 14:10:37.454: INFO: Pod "pod-subpath-test-configmap-q2hv": Phase="Running", Reason="", readiness=true. Elapsed: 12.085274074s
May 31 14:10:39.474: INFO: Pod "pod-subpath-test-configmap-q2hv": Phase="Running", Reason="", readiness=true. Elapsed: 14.105812044s
May 31 14:10:41.490: INFO: Pod "pod-subpath-test-configmap-q2hv": Phase="Running", Reason="", readiness=true. Elapsed: 16.121408306s
May 31 14:10:43.505: INFO: Pod "pod-subpath-test-configmap-q2hv": Phase="Running", Reason="", readiness=true. Elapsed: 18.136423712s
May 31 14:10:45.515: INFO: Pod "pod-subpath-test-configmap-q2hv": Phase="Running", Reason="", readiness=true. Elapsed: 20.146595174s
May 31 14:10:47.525: INFO: Pod "pod-subpath-test-configmap-q2hv": Phase="Running", Reason="", readiness=false. Elapsed: 22.156025264s
May 31 14:10:49.536: INFO: Pod "pod-subpath-test-configmap-q2hv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.167722805s
STEP: Saw pod success
May 31 14:10:49.536: INFO: Pod "pod-subpath-test-configmap-q2hv" satisfied condition "Succeeded or Failed"
May 31 14:10:49.549: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod pod-subpath-test-configmap-q2hv container test-container-subpath-configmap-q2hv: <nil>
STEP: delete the pod
May 31 14:10:49.624: INFO: Waiting for pod pod-subpath-test-configmap-q2hv to disappear
May 31 14:10:49.632: INFO: Pod pod-subpath-test-configmap-q2hv no longer exists
STEP: Deleting pod pod-subpath-test-configmap-q2hv
May 31 14:10:49.632: INFO: Deleting pod "pod-subpath-test-configmap-q2hv" in namespace "subpath-4685"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:10:49.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4685" for this suite.

• [SLOW TEST:24.412 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]","total":346,"completed":229,"skipped":4431,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:10:49.668: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating Pod
STEP: Reading file content from the nginx-container
May 31 14:10:51.858: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-888 PodName:pod-sharedvolume-571b6d17-1f7e-46e8-a7ec-b8c8b3f08492 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 31 14:10:51.858: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
May 31 14:10:51.858: INFO: ExecWithOptions: Clientset creation
May 31 14:10:51.859: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/emptydir-888/pods/pod-sharedvolume-571b6d17-1f7e-46e8-a7ec-b8c8b3f08492/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true %!s(MISSING))
May 31 14:10:52.169: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:10:52.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-888" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":346,"completed":230,"skipped":4442,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:10:52.194: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service in namespace services-5917
STEP: creating service affinity-nodeport in namespace services-5917
STEP: creating replication controller affinity-nodeport in namespace services-5917
I0531 14:10:52.291181      19 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-5917, replica count: 3
I0531 14:10:55.344150      19 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 31 14:10:55.370: INFO: Creating new exec pod
May 31 14:11:00.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-5917 exec execpod-affinityk9lwj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
May 31 14:11:00.981: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport 80\n+ echo hostName\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
May 31 14:11:00.981: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 31 14:11:00.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-5917 exec execpod-affinityk9lwj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.19.38 80'
May 31 14:11:01.356: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.19.38 80\nConnection to 10.240.19.38 80 port [tcp/http] succeeded!\n"
May 31 14:11:01.356: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 31 14:11:01.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-5917 exec execpod-affinityk9lwj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.8.180 32002'
May 31 14:11:01.704: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.8.180 32002\nConnection to 172.31.8.180 32002 port [tcp/*] succeeded!\n"
May 31 14:11:01.704: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 31 14:11:01.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-5917 exec execpod-affinityk9lwj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.3.160 32002'
May 31 14:11:02.193: INFO: stderr: "+ nc -v -t -w 2 172.31.3.160 32002\n+ echo hostName\nConnection to 172.31.3.160 32002 port [tcp/*] succeeded!\n"
May 31 14:11:02.193: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 31 14:11:02.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-5917 exec execpod-affinityk9lwj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.11.247:32002/ ; done'
May 31 14:11:03.231: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:32002/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:32002/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:32002/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:32002/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:32002/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:32002/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:32002/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:32002/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:32002/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:32002/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:32002/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:32002/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:32002/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:32002/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:32002/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:32002/\n"
May 31 14:11:03.232: INFO: stdout: "\naffinity-nodeport-7g2df\naffinity-nodeport-7g2df\naffinity-nodeport-7g2df\naffinity-nodeport-7g2df\naffinity-nodeport-7g2df\naffinity-nodeport-7g2df\naffinity-nodeport-7g2df\naffinity-nodeport-7g2df\naffinity-nodeport-7g2df\naffinity-nodeport-7g2df\naffinity-nodeport-7g2df\naffinity-nodeport-7g2df\naffinity-nodeport-7g2df\naffinity-nodeport-7g2df\naffinity-nodeport-7g2df\naffinity-nodeport-7g2df"
May 31 14:11:03.232: INFO: Received response from host: affinity-nodeport-7g2df
May 31 14:11:03.232: INFO: Received response from host: affinity-nodeport-7g2df
May 31 14:11:03.232: INFO: Received response from host: affinity-nodeport-7g2df
May 31 14:11:03.232: INFO: Received response from host: affinity-nodeport-7g2df
May 31 14:11:03.232: INFO: Received response from host: affinity-nodeport-7g2df
May 31 14:11:03.232: INFO: Received response from host: affinity-nodeport-7g2df
May 31 14:11:03.232: INFO: Received response from host: affinity-nodeport-7g2df
May 31 14:11:03.232: INFO: Received response from host: affinity-nodeport-7g2df
May 31 14:11:03.232: INFO: Received response from host: affinity-nodeport-7g2df
May 31 14:11:03.232: INFO: Received response from host: affinity-nodeport-7g2df
May 31 14:11:03.232: INFO: Received response from host: affinity-nodeport-7g2df
May 31 14:11:03.232: INFO: Received response from host: affinity-nodeport-7g2df
May 31 14:11:03.232: INFO: Received response from host: affinity-nodeport-7g2df
May 31 14:11:03.232: INFO: Received response from host: affinity-nodeport-7g2df
May 31 14:11:03.232: INFO: Received response from host: affinity-nodeport-7g2df
May 31 14:11:03.232: INFO: Received response from host: affinity-nodeport-7g2df
May 31 14:11:03.232: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-5917, will wait for the garbage collector to delete the pods
May 31 14:11:03.376: INFO: Deleting ReplicationController affinity-nodeport took: 38.787208ms
May 31 14:11:03.476: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.674562ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:11:06.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5917" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:14.071 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":231,"skipped":4459,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:11:06.278: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create a ReplicaSet
STEP: Verify that the required pods have come up
May 31 14:11:06.396: INFO: Pod name sample-pod: Found 0 pods out of 3
May 31 14:11:11.422: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running
May 31 14:11:11.444: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets
STEP: DeleteCollection of the ReplicaSets
STEP: After DeleteCollection verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:11:11.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3988" for this suite.

• [SLOW TEST:5.267 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","total":346,"completed":232,"skipped":4546,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:11:11.546: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with secret that has name projected-secret-test-map-67dbc3cb-c0d4-41ab-a624-ff26191c6b1c
STEP: Creating a pod to test consume secrets
May 31 14:11:11.653: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-237037b5-5767-45d1-b474-cc361f86e609" in namespace "projected-6393" to be "Succeeded or Failed"
May 31 14:11:11.668: INFO: Pod "pod-projected-secrets-237037b5-5767-45d1-b474-cc361f86e609": Phase="Pending", Reason="", readiness=false. Elapsed: 15.226943ms
May 31 14:11:13.684: INFO: Pod "pod-projected-secrets-237037b5-5767-45d1-b474-cc361f86e609": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030301263s
May 31 14:11:15.696: INFO: Pod "pod-projected-secrets-237037b5-5767-45d1-b474-cc361f86e609": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042989297s
STEP: Saw pod success
May 31 14:11:15.697: INFO: Pod "pod-projected-secrets-237037b5-5767-45d1-b474-cc361f86e609" satisfied condition "Succeeded or Failed"
May 31 14:11:15.704: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod pod-projected-secrets-237037b5-5767-45d1-b474-cc361f86e609 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 31 14:11:15.786: INFO: Waiting for pod pod-projected-secrets-237037b5-5767-45d1-b474-cc361f86e609 to disappear
May 31 14:11:15.794: INFO: Pod pod-projected-secrets-237037b5-5767-45d1-b474-cc361f86e609 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:11:15.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6393" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":233,"skipped":4571,"failed":0}
SSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:11:15.833: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service in namespace services-1663
STEP: creating service affinity-clusterip-transition in namespace services-1663
STEP: creating replication controller affinity-clusterip-transition in namespace services-1663
I0531 14:11:15.954160      19 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-1663, replica count: 3
I0531 14:11:19.009221      19 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 31 14:11:19.047: INFO: Creating new exec pod
May 31 14:11:22.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-1663 exec execpod-affinity9fvlj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
May 31 14:11:22.438: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip-transition 80\n+ echo hostName\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
May 31 14:11:22.438: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 31 14:11:22.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-1663 exec execpod-affinity9fvlj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.28.144 80'
May 31 14:11:22.940: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.28.144 80\nConnection to 10.240.28.144 80 port [tcp/http] succeeded!\n"
May 31 14:11:22.940: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 31 14:11:22.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-1663 exec execpod-affinity9fvlj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.28.144:80/ ; done'
May 31 14:11:23.621: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.28.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.28.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.28.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.28.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.28.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.28.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.28.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.28.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.28.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.28.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.28.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.28.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.28.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.28.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.28.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.28.144:80/\n"
May 31 14:11:23.622: INFO: stdout: "\naffinity-clusterip-transition-892lk\naffinity-clusterip-transition-sb9tx\naffinity-clusterip-transition-c95ch\naffinity-clusterip-transition-892lk\naffinity-clusterip-transition-sb9tx\naffinity-clusterip-transition-c95ch\naffinity-clusterip-transition-892lk\naffinity-clusterip-transition-sb9tx\naffinity-clusterip-transition-c95ch\naffinity-clusterip-transition-892lk\naffinity-clusterip-transition-sb9tx\naffinity-clusterip-transition-c95ch\naffinity-clusterip-transition-892lk\naffinity-clusterip-transition-sb9tx\naffinity-clusterip-transition-c95ch\naffinity-clusterip-transition-892lk"
May 31 14:11:23.622: INFO: Received response from host: affinity-clusterip-transition-892lk
May 31 14:11:23.622: INFO: Received response from host: affinity-clusterip-transition-sb9tx
May 31 14:11:23.622: INFO: Received response from host: affinity-clusterip-transition-c95ch
May 31 14:11:23.622: INFO: Received response from host: affinity-clusterip-transition-892lk
May 31 14:11:23.622: INFO: Received response from host: affinity-clusterip-transition-sb9tx
May 31 14:11:23.622: INFO: Received response from host: affinity-clusterip-transition-c95ch
May 31 14:11:23.622: INFO: Received response from host: affinity-clusterip-transition-892lk
May 31 14:11:23.622: INFO: Received response from host: affinity-clusterip-transition-sb9tx
May 31 14:11:23.622: INFO: Received response from host: affinity-clusterip-transition-c95ch
May 31 14:11:23.622: INFO: Received response from host: affinity-clusterip-transition-892lk
May 31 14:11:23.622: INFO: Received response from host: affinity-clusterip-transition-sb9tx
May 31 14:11:23.622: INFO: Received response from host: affinity-clusterip-transition-c95ch
May 31 14:11:23.622: INFO: Received response from host: affinity-clusterip-transition-892lk
May 31 14:11:23.622: INFO: Received response from host: affinity-clusterip-transition-sb9tx
May 31 14:11:23.622: INFO: Received response from host: affinity-clusterip-transition-c95ch
May 31 14:11:23.622: INFO: Received response from host: affinity-clusterip-transition-892lk
May 31 14:11:23.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-1663 exec execpod-affinity9fvlj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.28.144:80/ ; done'
May 31 14:11:24.328: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.28.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.28.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.28.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.28.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.28.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.28.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.28.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.28.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.28.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.28.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.28.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.28.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.28.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.28.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.28.144:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.28.144:80/\n"
May 31 14:11:24.328: INFO: stdout: "\naffinity-clusterip-transition-c95ch\naffinity-clusterip-transition-c95ch\naffinity-clusterip-transition-c95ch\naffinity-clusterip-transition-c95ch\naffinity-clusterip-transition-c95ch\naffinity-clusterip-transition-c95ch\naffinity-clusterip-transition-c95ch\naffinity-clusterip-transition-c95ch\naffinity-clusterip-transition-c95ch\naffinity-clusterip-transition-c95ch\naffinity-clusterip-transition-c95ch\naffinity-clusterip-transition-c95ch\naffinity-clusterip-transition-c95ch\naffinity-clusterip-transition-c95ch\naffinity-clusterip-transition-c95ch\naffinity-clusterip-transition-c95ch"
May 31 14:11:24.328: INFO: Received response from host: affinity-clusterip-transition-c95ch
May 31 14:11:24.328: INFO: Received response from host: affinity-clusterip-transition-c95ch
May 31 14:11:24.328: INFO: Received response from host: affinity-clusterip-transition-c95ch
May 31 14:11:24.328: INFO: Received response from host: affinity-clusterip-transition-c95ch
May 31 14:11:24.328: INFO: Received response from host: affinity-clusterip-transition-c95ch
May 31 14:11:24.328: INFO: Received response from host: affinity-clusterip-transition-c95ch
May 31 14:11:24.328: INFO: Received response from host: affinity-clusterip-transition-c95ch
May 31 14:11:24.328: INFO: Received response from host: affinity-clusterip-transition-c95ch
May 31 14:11:24.328: INFO: Received response from host: affinity-clusterip-transition-c95ch
May 31 14:11:24.328: INFO: Received response from host: affinity-clusterip-transition-c95ch
May 31 14:11:24.328: INFO: Received response from host: affinity-clusterip-transition-c95ch
May 31 14:11:24.328: INFO: Received response from host: affinity-clusterip-transition-c95ch
May 31 14:11:24.328: INFO: Received response from host: affinity-clusterip-transition-c95ch
May 31 14:11:24.328: INFO: Received response from host: affinity-clusterip-transition-c95ch
May 31 14:11:24.328: INFO: Received response from host: affinity-clusterip-transition-c95ch
May 31 14:11:24.328: INFO: Received response from host: affinity-clusterip-transition-c95ch
May 31 14:11:24.328: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-1663, will wait for the garbage collector to delete the pods
May 31 14:11:24.465: INFO: Deleting ReplicationController affinity-clusterip-transition took: 34.343657ms
May 31 14:11:24.566: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.701627ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:11:26.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1663" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:10.594 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":234,"skipped":4577,"failed":0}
SSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:11:26.430: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 31 14:11:27.569: INFO: starting watch
STEP: patching
STEP: updating
May 31 14:11:27.595: INFO: waiting for watch events with expected annotations
May 31 14:11:27.595: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:11:27.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-1569" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":346,"completed":235,"skipped":4586,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:11:27.735: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1333
STEP: creating the pod
May 31 14:11:27.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-5651 create -f -'
May 31 14:11:28.126: INFO: stderr: ""
May 31 14:11:28.126: INFO: stdout: "pod/pause created\n"
May 31 14:11:28.126: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
May 31 14:11:28.126: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-5651" to be "running and ready"
May 31 14:11:28.132: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.67146ms
May 31 14:11:30.145: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.019461567s
May 31 14:11:30.145: INFO: Pod "pause" satisfied condition "running and ready"
May 31 14:11:30.145: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: adding the label testing-label with value testing-label-value to a pod
May 31 14:11:30.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-5651 label pods pause testing-label=testing-label-value'
May 31 14:11:30.294: INFO: stderr: ""
May 31 14:11:30.295: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
May 31 14:11:30.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-5651 get pod pause -L testing-label'
May 31 14:11:30.455: INFO: stderr: ""
May 31 14:11:30.455: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
May 31 14:11:30.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-5651 label pods pause testing-label-'
May 31 14:11:30.637: INFO: stderr: ""
May 31 14:11:30.637: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label
May 31 14:11:30.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-5651 get pod pause -L testing-label'
May 31 14:11:30.775: INFO: stderr: ""
May 31 14:11:30.775: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1339
STEP: using delete to clean up resources
May 31 14:11:30.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-5651 delete --grace-period=0 --force -f -'
May 31 14:11:30.904: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 31 14:11:30.904: INFO: stdout: "pod \"pause\" force deleted\n"
May 31 14:11:30.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-5651 get rc,svc -l name=pause --no-headers'
May 31 14:11:31.015: INFO: stderr: "No resources found in kubectl-5651 namespace.\n"
May 31 14:11:31.015: INFO: stdout: ""
May 31 14:11:31.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-5651 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 31 14:11:31.097: INFO: stderr: ""
May 31 14:11:31.097: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:11:31.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5651" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":346,"completed":236,"skipped":4612,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:11:31.124: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
May 31 14:11:37.380: INFO: 80 pods remaining
May 31 14:11:37.380: INFO: 80 pods has nil DeletionTimestamp
May 31 14:11:37.380: INFO: 
May 31 14:11:38.382: INFO: 71 pods remaining
May 31 14:11:38.382: INFO: 70 pods has nil DeletionTimestamp
May 31 14:11:38.382: INFO: 
May 31 14:11:39.377: INFO: 60 pods remaining
May 31 14:11:39.377: INFO: 60 pods has nil DeletionTimestamp
May 31 14:11:39.377: INFO: 
May 31 14:11:40.375: INFO: 40 pods remaining
May 31 14:11:40.386: INFO: 40 pods has nil DeletionTimestamp
May 31 14:11:40.386: INFO: 
May 31 14:11:41.385: INFO: 30 pods remaining
May 31 14:11:41.385: INFO: 29 pods has nil DeletionTimestamp
May 31 14:11:41.385: INFO: 
May 31 14:11:42.399: INFO: 20 pods remaining
May 31 14:11:42.399: INFO: 20 pods has nil DeletionTimestamp
May 31 14:11:42.399: INFO: 
STEP: Gathering metrics
W0531 14:11:43.367745      19 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
May 31 14:11:43.367: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:11:43.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7047" for this suite.

• [SLOW TEST:12.263 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":346,"completed":237,"skipped":4623,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:11:43.387: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-ab4c5587-ba30-46e9-9b65-46db958f65b7
STEP: Creating a pod to test consume configMaps
May 31 14:11:43.477: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-489d23f3-b084-4c58-8e85-652dc80a0ab4" in namespace "projected-4097" to be "Succeeded or Failed"
May 31 14:11:43.486: INFO: Pod "pod-projected-configmaps-489d23f3-b084-4c58-8e85-652dc80a0ab4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.870421ms
May 31 14:11:45.494: INFO: Pod "pod-projected-configmaps-489d23f3-b084-4c58-8e85-652dc80a0ab4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01704613s
May 31 14:11:47.511: INFO: Pod "pod-projected-configmaps-489d23f3-b084-4c58-8e85-652dc80a0ab4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033691062s
May 31 14:11:49.526: INFO: Pod "pod-projected-configmaps-489d23f3-b084-4c58-8e85-652dc80a0ab4": Phase="Running", Reason="", readiness=true. Elapsed: 6.048837563s
May 31 14:11:51.535: INFO: Pod "pod-projected-configmaps-489d23f3-b084-4c58-8e85-652dc80a0ab4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.057635241s
STEP: Saw pod success
May 31 14:11:51.535: INFO: Pod "pod-projected-configmaps-489d23f3-b084-4c58-8e85-652dc80a0ab4" satisfied condition "Succeeded or Failed"
May 31 14:11:51.545: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod pod-projected-configmaps-489d23f3-b084-4c58-8e85-652dc80a0ab4 container agnhost-container: <nil>
STEP: delete the pod
May 31 14:11:51.580: INFO: Waiting for pod pod-projected-configmaps-489d23f3-b084-4c58-8e85-652dc80a0ab4 to disappear
May 31 14:11:51.590: INFO: Pod pod-projected-configmaps-489d23f3-b084-4c58-8e85-652dc80a0ab4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:11:51.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4097" for this suite.

• [SLOW TEST:8.223 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":238,"skipped":4637,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:11:51.611: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-06119347-f935-4b11-86cd-5a4775e8168e
STEP: Creating a pod to test consume secrets
May 31 14:11:51.720: INFO: Waiting up to 5m0s for pod "pod-secrets-3fae9acb-01f4-4673-8ba6-8414b28fefbf" in namespace "secrets-7179" to be "Succeeded or Failed"
May 31 14:11:51.729: INFO: Pod "pod-secrets-3fae9acb-01f4-4673-8ba6-8414b28fefbf": Phase="Pending", Reason="", readiness=false. Elapsed: 9.387721ms
May 31 14:11:53.742: INFO: Pod "pod-secrets-3fae9acb-01f4-4673-8ba6-8414b28fefbf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021785793s
May 31 14:11:55.759: INFO: Pod "pod-secrets-3fae9acb-01f4-4673-8ba6-8414b28fefbf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039010619s
May 31 14:11:57.768: INFO: Pod "pod-secrets-3fae9acb-01f4-4673-8ba6-8414b28fefbf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.048095072s
STEP: Saw pod success
May 31 14:11:57.768: INFO: Pod "pod-secrets-3fae9acb-01f4-4673-8ba6-8414b28fefbf" satisfied condition "Succeeded or Failed"
May 31 14:11:57.775: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod pod-secrets-3fae9acb-01f4-4673-8ba6-8414b28fefbf container secret-volume-test: <nil>
STEP: delete the pod
May 31 14:11:57.814: INFO: Waiting for pod pod-secrets-3fae9acb-01f4-4673-8ba6-8414b28fefbf to disappear
May 31 14:11:57.821: INFO: Pod pod-secrets-3fae9acb-01f4-4673-8ba6-8414b28fefbf no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:11:57.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7179" for this suite.

• [SLOW TEST:6.234 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":239,"skipped":4645,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:11:57.850: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating Agnhost RC
May 31 14:11:57.913: INFO: namespace kubectl-1280
May 31 14:11:57.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-1280 create -f -'
May 31 14:11:58.291: INFO: stderr: ""
May 31 14:11:58.291: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
May 31 14:11:59.311: INFO: Selector matched 1 pods for map[app:agnhost]
May 31 14:11:59.311: INFO: Found 0 / 1
May 31 14:12:00.302: INFO: Selector matched 1 pods for map[app:agnhost]
May 31 14:12:00.302: INFO: Found 1 / 1
May 31 14:12:00.302: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 31 14:12:00.312: INFO: Selector matched 1 pods for map[app:agnhost]
May 31 14:12:00.312: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 31 14:12:00.312: INFO: wait on agnhost-primary startup in kubectl-1280 
May 31 14:12:00.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-1280 logs agnhost-primary-r7k72 agnhost-primary'
May 31 14:12:00.452: INFO: stderr: ""
May 31 14:12:00.452: INFO: stdout: "Paused\n"
STEP: exposing RC
May 31 14:12:00.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-1280 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
May 31 14:12:00.618: INFO: stderr: ""
May 31 14:12:00.618: INFO: stdout: "service/rm2 exposed\n"
May 31 14:12:00.626: INFO: Service rm2 in namespace kubectl-1280 found.
STEP: exposing service
May 31 14:12:02.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-1280 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
May 31 14:12:02.806: INFO: stderr: ""
May 31 14:12:02.806: INFO: stdout: "service/rm3 exposed\n"
May 31 14:12:02.816: INFO: Service rm3 in namespace kubectl-1280 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:12:04.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1280" for this suite.

• [SLOW TEST:7.020 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1248
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":346,"completed":240,"skipped":4666,"failed":0}
SSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:12:04.870: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 14:12:04.927: INFO: Creating deployment "webserver-deployment"
May 31 14:12:04.937: INFO: Waiting for observed generation 1
May 31 14:12:06.963: INFO: Waiting for all required pods to come up
May 31 14:12:07.002: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
May 31 14:12:09.060: INFO: Waiting for deployment "webserver-deployment" to complete
May 31 14:12:09.080: INFO: Updating deployment "webserver-deployment" with a non-existent image
May 31 14:12:09.100: INFO: Updating deployment webserver-deployment
May 31 14:12:09.100: INFO: Waiting for observed generation 2
May 31 14:12:11.120: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
May 31 14:12:11.127: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
May 31 14:12:11.133: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May 31 14:12:11.153: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
May 31 14:12:11.153: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
May 31 14:12:11.160: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
May 31 14:12:11.175: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
May 31 14:12:11.175: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
May 31 14:12:11.202: INFO: Updating deployment webserver-deployment
May 31 14:12:11.202: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
May 31 14:12:11.222: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
May 31 14:12:11.229: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
May 31 14:12:13.274: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-3561  f6982c4f-2969-4aa5-80a9-d5cc486bb2fb 38941 3 2022-05-31 14:12:04 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-05-31 14:12:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-31 14:12:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0041ba3d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-05-31 14:12:11 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-566f96c878" is progressing.,LastUpdateTime:2022-05-31 14:12:11 +0000 UTC,LastTransitionTime:2022-05-31 14:12:04 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

May 31 14:12:13.283: INFO: New ReplicaSet "webserver-deployment-566f96c878" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-566f96c878  deployment-3561  666371aa-744c-42b6-9a6f-6c34e7a76d54 38940 3 2022-05-31 14:12:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment f6982c4f-2969-4aa5-80a9-d5cc486bb2fb 0xc0034693d7 0xc0034693d8}] []  [{kube-controller-manager Update apps/v1 2022-05-31 14:12:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f6982c4f-2969-4aa5-80a9-d5cc486bb2fb\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-31 14:12:09 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 566f96c878,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003469508 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 31 14:12:13.284: INFO: All old ReplicaSets of Deployment "webserver-deployment":
May 31 14:12:13.284: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-5d9fdcc779  deployment-3561  85ea46e2-b062-4237-bfb5-676079029428 38927 3 2022-05-31 14:12:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment f6982c4f-2969-4aa5-80a9-d5cc486bb2fb 0xc003469687 0xc003469688}] []  [{kube-controller-manager Update apps/v1 2022-05-31 14:12:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f6982c4f-2969-4aa5-80a9-d5cc486bb2fb\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-31 14:12:07 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 5d9fdcc779,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003469808 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
May 31 14:12:13.319: INFO: Pod "webserver-deployment-566f96c878-26qxd" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-26qxd webserver-deployment-566f96c878- deployment-3561  d0702757-1b7c-432e-b343-64056af2437a 38809 0 2022-05-31 14:12:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:6510545d1abc7858b71e54e51b52bc68f0a9cdc684a23ca8d2abbecaecfb9819 cni.projectcalico.org/podIP:172.25.0.125/32 cni.projectcalico.org/podIPs:172.25.0.125/32] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 666371aa-744c-42b6-9a6f-6c34e7a76d54 0xc003f25f77 0xc003f25f78}] []  [{calico Update v1 2022-05-31 14:12:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-05-31 14:12:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"666371aa-744c-42b6-9a6f-6c34e7a76d54\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-31 14:12:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n5hgp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n5hgp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-247.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.247,PodIP:,StartTime:2022-05-31 14:12:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 31 14:12:13.319: INFO: Pod "webserver-deployment-566f96c878-4vqlv" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-4vqlv webserver-deployment-566f96c878- deployment-3561  5470b58d-9686-4c4a-9f34-ca941fdcf835 38965 0 2022-05-31 14:12:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:966bc496d37cda10afec26f43fb771b6bbc67049b9756556b5e187d52cebb81b cni.projectcalico.org/podIP:172.25.1.37/32 cni.projectcalico.org/podIPs:172.25.1.37/32] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 666371aa-744c-42b6-9a6f-6c34e7a76d54 0xc0048b61d7 0xc0048b61d8}] []  [{kube-controller-manager Update v1 2022-05-31 14:12:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"666371aa-744c-42b6-9a6f-6c34e7a76d54\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-31 14:12:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-05-31 14:12:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lr6bw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lr6bw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-8-180.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.8.180,PodIP:,StartTime:2022-05-31 14:12:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 31 14:12:13.319: INFO: Pod "webserver-deployment-566f96c878-6brsr" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-6brsr webserver-deployment-566f96c878- deployment-3561  66195587-148d-42ad-9a31-16fb90434635 38939 0 2022-05-31 14:12:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 666371aa-744c-42b6-9a6f-6c34e7a76d54 0xc0048b63d7 0xc0048b63d8}] []  [{kube-controller-manager Update v1 2022-05-31 14:12:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"666371aa-744c-42b6-9a6f-6c34e7a76d54\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-31 14:12:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-grc6n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-grc6n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-247.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.247,PodIP:,StartTime:2022-05-31 14:12:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 31 14:12:13.320: INFO: Pod "webserver-deployment-566f96c878-88gcn" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-88gcn webserver-deployment-566f96c878- deployment-3561  d75d09a0-91de-4539-a4c5-62262b2b2a7b 39000 0 2022-05-31 14:12:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:9b32f0f89469fc715b65c0d74723fd559393e33f2a65e328211bcb3d8803fff8 cni.projectcalico.org/podIP:172.25.0.129/32 cni.projectcalico.org/podIPs:172.25.0.129/32] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 666371aa-744c-42b6-9a6f-6c34e7a76d54 0xc0048b65d7 0xc0048b65d8}] []  [{kube-controller-manager Update v1 2022-05-31 14:12:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"666371aa-744c-42b6-9a6f-6c34e7a76d54\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-31 14:12:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-05-31 14:12:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8tjxj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8tjxj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-247.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.247,PodIP:,StartTime:2022-05-31 14:12:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 31 14:12:13.320: INFO: Pod "webserver-deployment-566f96c878-cfb54" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-cfb54 webserver-deployment-566f96c878- deployment-3561  27c4be7e-10e7-4485-ab0f-a6bb711c88a1 38989 0 2022-05-31 14:12:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:49b4fbdae378eb454b14aa7a234a01a8b06e4924a730af19e8918ff648ee818d cni.projectcalico.org/podIP:172.25.2.206/32 cni.projectcalico.org/podIPs:172.25.2.206/32] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 666371aa-744c-42b6-9a6f-6c34e7a76d54 0xc0048b67f7 0xc0048b67f8}] []  [{kube-controller-manager Update v1 2022-05-31 14:12:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"666371aa-744c-42b6-9a6f-6c34e7a76d54\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-31 14:12:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-05-31 14:12:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xwwfr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xwwfr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-160.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.160,PodIP:,StartTime:2022-05-31 14:12:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 31 14:12:13.320: INFO: Pod "webserver-deployment-566f96c878-d994j" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-d994j webserver-deployment-566f96c878- deployment-3561  9a9fed58-9cc0-4403-a376-7c4713a6d495 38982 0 2022-05-31 14:12:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:454f9e6bef729f42325575278f08acbcb6cc7a63edb63ca78072ab0b3f6aee97 cni.projectcalico.org/podIP:172.25.2.204/32 cni.projectcalico.org/podIPs:172.25.2.204/32] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 666371aa-744c-42b6-9a6f-6c34e7a76d54 0xc0048b6a17 0xc0048b6a18}] []  [{kube-controller-manager Update v1 2022-05-31 14:12:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"666371aa-744c-42b6-9a6f-6c34e7a76d54\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-31 14:12:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-05-31 14:12:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hsxd9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hsxd9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-160.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.160,PodIP:,StartTime:2022-05-31 14:12:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 31 14:12:13.320: INFO: Pod "webserver-deployment-566f96c878-dvbx2" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-dvbx2 webserver-deployment-566f96c878- deployment-3561  2a830160-9f45-44e9-872d-85abbeac1d97 38812 0 2022-05-31 14:12:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:58152e47e0fc7774562ecd038730706db88ff30cfe48d76a865eaebe350f8365 cni.projectcalico.org/podIP:172.25.0.126/32 cni.projectcalico.org/podIPs:172.25.0.126/32] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 666371aa-744c-42b6-9a6f-6c34e7a76d54 0xc0048b6c37 0xc0048b6c38}] []  [{calico Update v1 2022-05-31 14:12:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-05-31 14:12:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"666371aa-744c-42b6-9a6f-6c34e7a76d54\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-31 14:12:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rdzkk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rdzkk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-247.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.247,PodIP:,StartTime:2022-05-31 14:12:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 31 14:12:13.320: INFO: Pod "webserver-deployment-566f96c878-ggl2j" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-ggl2j webserver-deployment-566f96c878- deployment-3561  765548c3-f74a-4c71-9b4b-0c22bbb9bc04 38920 0 2022-05-31 14:12:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 666371aa-744c-42b6-9a6f-6c34e7a76d54 0xc0048b6e27 0xc0048b6e28}] []  [{kube-controller-manager Update v1 2022-05-31 14:12:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"666371aa-744c-42b6-9a6f-6c34e7a76d54\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sj4lp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sj4lp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-160.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 31 14:12:13.321: INFO: Pod "webserver-deployment-566f96c878-lcxrf" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-lcxrf webserver-deployment-566f96c878- deployment-3561  2c9163ca-ff71-401a-86d1-ae43787f07e8 38813 0 2022-05-31 14:12:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:8e039ad18286b79f3278770685c8ff8bb77a7b1956bfd424ae50981ed1aba0c2 cni.projectcalico.org/podIP:172.25.1.35/32 cni.projectcalico.org/podIPs:172.25.1.35/32] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 666371aa-744c-42b6-9a6f-6c34e7a76d54 0xc0048b6fc0 0xc0048b6fc1}] []  [{calico Update v1 2022-05-31 14:12:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-05-31 14:12:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"666371aa-744c-42b6-9a6f-6c34e7a76d54\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-31 14:12:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-975n5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-975n5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-8-180.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.8.180,PodIP:,StartTime:2022-05-31 14:12:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 31 14:12:13.321: INFO: Pod "webserver-deployment-566f96c878-qqjl9" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-qqjl9 webserver-deployment-566f96c878- deployment-3561  c2ea34c4-ef73-42e3-a4df-d3c3c358481e 39002 0 2022-05-31 14:12:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 666371aa-744c-42b6-9a6f-6c34e7a76d54 0xc0048b71b7 0xc0048b71b8}] []  [{kube-controller-manager Update v1 2022-05-31 14:12:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"666371aa-744c-42b6-9a6f-6c34e7a76d54\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-31 14:12:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2jmrv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2jmrv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-8-180.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.8.180,PodIP:,StartTime:2022-05-31 14:12:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 31 14:12:13.321: INFO: Pod "webserver-deployment-566f96c878-r29n5" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-r29n5 webserver-deployment-566f96c878- deployment-3561  ae2791b8-8c91-44a7-994d-3e40932bff8a 38810 0 2022-05-31 14:12:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:74fab24b4301a408d45b9cab3f59756650ea3e59beb456b3991319d63639581d cni.projectcalico.org/podIP:172.25.1.34/32 cni.projectcalico.org/podIPs:172.25.1.34/32] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 666371aa-744c-42b6-9a6f-6c34e7a76d54 0xc0048b7687 0xc0048b7688}] []  [{calico Update v1 2022-05-31 14:12:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-05-31 14:12:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"666371aa-744c-42b6-9a6f-6c34e7a76d54\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-31 14:12:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7bt4h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7bt4h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-8-180.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.8.180,PodIP:,StartTime:2022-05-31 14:12:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 31 14:12:13.321: INFO: Pod "webserver-deployment-566f96c878-sghf7" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-sghf7 webserver-deployment-566f96c878- deployment-3561  9d6e797a-d5c0-4840-87c7-a3db5af4cdc0 38959 0 2022-05-31 14:12:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:d9c5619a28d57667bbdee07646b5d90efe0e30ca79b890f3483cac06ff691f23 cni.projectcalico.org/podIP:172.25.2.203/32 cni.projectcalico.org/podIPs:172.25.2.203/32] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 666371aa-744c-42b6-9a6f-6c34e7a76d54 0xc0048b79e7 0xc0048b79e8}] []  [{calico Update v1 2022-05-31 14:12:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-05-31 14:12:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"666371aa-744c-42b6-9a6f-6c34e7a76d54\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-31 14:12:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wm88l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wm88l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-160.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.160,PodIP:,StartTime:2022-05-31 14:12:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 31 14:12:13.322: INFO: Pod "webserver-deployment-566f96c878-vnbzb" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-vnbzb webserver-deployment-566f96c878- deployment-3561  ba6063a7-2a01-461c-b11a-f4883eec4c0f 38811 0 2022-05-31 14:12:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[cni.projectcalico.org/containerID:e33721abafbceef28ae0520307d089105c0c725f1e1ea562f5cd3bbeb738f135 cni.projectcalico.org/podIP:172.25.2.202/32 cni.projectcalico.org/podIPs:172.25.2.202/32] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 666371aa-744c-42b6-9a6f-6c34e7a76d54 0xc0048b7c27 0xc0048b7c28}] []  [{calico Update v1 2022-05-31 14:12:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-05-31 14:12:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"666371aa-744c-42b6-9a6f-6c34e7a76d54\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-31 14:12:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pdbnz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pdbnz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-160.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.160,PodIP:,StartTime:2022-05-31 14:12:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 31 14:12:13.322: INFO: Pod "webserver-deployment-5d9fdcc779-2hpb6" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-2hpb6 webserver-deployment-5d9fdcc779- deployment-3561  19b99776-ca68-435a-a923-26511d467783 38983 0 2022-05-31 14:12:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:fd756dd8370025febedb0f408f5c8d2a324985a09c625682760e3a7f28eb8b3f cni.projectcalico.org/podIP:172.25.2.205/32 cni.projectcalico.org/podIPs:172.25.2.205/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 85ea46e2-b062-4237-bfb5-676079029428 0xc0048b7e37 0xc0048b7e38}] []  [{kube-controller-manager Update v1 2022-05-31 14:12:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85ea46e2-b062-4237-bfb5-676079029428\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-31 14:12:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-05-31 14:12:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t6g2x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t6g2x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-160.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.160,PodIP:,StartTime:2022-05-31 14:12:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 31 14:12:13.322: INFO: Pod "webserver-deployment-5d9fdcc779-5tn4k" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-5tn4k webserver-deployment-5d9fdcc779- deployment-3561  bea37521-b07a-45e6-bc02-2a80ae120bcc 39005 0 2022-05-31 14:12:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:9a4610d2f0f239f31454e507d14f8944f08a4ba9260e7f526d92fa274e4b754d cni.projectcalico.org/podIP:172.25.1.40/32 cni.projectcalico.org/podIPs:172.25.1.40/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 85ea46e2-b062-4237-bfb5-676079029428 0xc004332037 0xc004332038}] []  [{kube-controller-manager Update v1 2022-05-31 14:12:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85ea46e2-b062-4237-bfb5-676079029428\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-05-31 14:12:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xkwx8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xkwx8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-8-180.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 31 14:12:13.322: INFO: Pod "webserver-deployment-5d9fdcc779-5z2k4" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-5z2k4 webserver-deployment-5d9fdcc779- deployment-3561  3f592c95-26c5-4ace-b662-40a87c30d9c2 38966 0 2022-05-31 14:12:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:5a3d96b76fe1be49fda9bb1dd4cb53aa5e6bfd34778e1bd694dee34d4e27e3a0 cni.projectcalico.org/podIP:172.25.1.38/32 cni.projectcalico.org/podIPs:172.25.1.38/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 85ea46e2-b062-4237-bfb5-676079029428 0xc0043321d0 0xc0043321d1}] []  [{kube-controller-manager Update v1 2022-05-31 14:12:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85ea46e2-b062-4237-bfb5-676079029428\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-31 14:12:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-05-31 14:12:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k8ftl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k8ftl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-8-180.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.8.180,PodIP:,StartTime:2022-05-31 14:12:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 31 14:12:13.323: INFO: Pod "webserver-deployment-5d9fdcc779-9k6tw" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-9k6tw webserver-deployment-5d9fdcc779- deployment-3561  3d6f1640-f296-4a31-87be-8e0796e5f270 38722 0 2022-05-31 14:12:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:fc1c457d2aa6d6ebd5db7105493f18ce6be416d8647a5dcee5c83b9446cc9d70 cni.projectcalico.org/podIP:172.25.0.124/32 cni.projectcalico.org/podIPs:172.25.0.124/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 85ea46e2-b062-4237-bfb5-676079029428 0xc0043323c7 0xc0043323c8}] []  [{kube-controller-manager Update v1 2022-05-31 14:12:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85ea46e2-b062-4237-bfb5-676079029428\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-05-31 14:12:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-05-31 14:12:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.0.124\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9nkt7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9nkt7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-247.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.247,PodIP:172.25.0.124,StartTime:2022-05-31 14:12:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-31 14:12:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://f5ff1ff4509d7743310c0de33ebcf56c3b8ce594fae68f705d09ee5b8ada8156,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.0.124,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 31 14:12:13.323: INFO: Pod "webserver-deployment-5d9fdcc779-bgd8h" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-bgd8h webserver-deployment-5d9fdcc779- deployment-3561  6222d4ed-4fac-45e7-b773-4297772ba9f4 38905 0 2022-05-31 14:12:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 85ea46e2-b062-4237-bfb5-676079029428 0xc0043325c7 0xc0043325c8}] []  [{kube-controller-manager Update v1 2022-05-31 14:12:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85ea46e2-b062-4237-bfb5-676079029428\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nkbvk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nkbvk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-8-180.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 31 14:12:13.323: INFO: Pod "webserver-deployment-5d9fdcc779-bmhk2" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-bmhk2 webserver-deployment-5d9fdcc779- deployment-3561  4ac8cb7e-487b-4460-a9c7-11a30f8f66ca 38741 0 2022-05-31 14:12:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:7fe285f09ede2a98a8dc5bcb5b6f2ed566f16e978eee790c1d850f04689b394b cni.projectcalico.org/podIP:172.25.2.198/32 cni.projectcalico.org/podIPs:172.25.2.198/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 85ea46e2-b062-4237-bfb5-676079029428 0xc004332740 0xc004332741}] []  [{kube-controller-manager Update v1 2022-05-31 14:12:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85ea46e2-b062-4237-bfb5-676079029428\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-05-31 14:12:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-05-31 14:12:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.198\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zxlpq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zxlpq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-160.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.160,PodIP:172.25.2.198,StartTime:2022-05-31 14:12:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-31 14:12:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://281f17d74aac42137d278777754facbbc1c7c1b52c44289a5531d3bd85a142ec,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.198,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 31 14:12:13.323: INFO: Pod "webserver-deployment-5d9fdcc779-c7x59" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-c7x59 webserver-deployment-5d9fdcc779- deployment-3561  b1c8c57a-ee1a-4db4-aefa-a3eeb7b3a615 38954 0 2022-05-31 14:12:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:ab6c086056ce23f66f337b6ed951e760f3de4354aa5acabe675a471094f45c5b cni.projectcalico.org/podIP:172.25.1.36/32 cni.projectcalico.org/podIPs:172.25.1.36/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 85ea46e2-b062-4237-bfb5-676079029428 0xc004332fb7 0xc004332fb8}] []  [{calico Update v1 2022-05-31 14:12:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-05-31 14:12:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85ea46e2-b062-4237-bfb5-676079029428\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-31 14:12:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nc6wc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nc6wc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-8-180.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.8.180,PodIP:,StartTime:2022-05-31 14:12:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 31 14:12:13.324: INFO: Pod "webserver-deployment-5d9fdcc779-hrhzb" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-hrhzb webserver-deployment-5d9fdcc779- deployment-3561  042b06e8-998c-45d9-91fc-ae376bcde98a 38725 0 2022-05-31 14:12:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:45d392ce208ba97a3435e276b141d81953adc37039e58666d3bf1ca557bfe9ee cni.projectcalico.org/podIP:172.25.0.123/32 cni.projectcalico.org/podIPs:172.25.0.123/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 85ea46e2-b062-4237-bfb5-676079029428 0xc004333297 0xc004333298}] []  [{kube-controller-manager Update v1 2022-05-31 14:12:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85ea46e2-b062-4237-bfb5-676079029428\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-05-31 14:12:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-05-31 14:12:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.0.123\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cx8qx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cx8qx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-247.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.247,PodIP:172.25.0.123,StartTime:2022-05-31 14:12:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-31 14:12:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://53a6f60923ef59282405cd7d39b047cd6f83edc5b57335ecb8ab06681c4ddce7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.0.123,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 31 14:12:13.324: INFO: Pod "webserver-deployment-5d9fdcc779-m49wf" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-m49wf webserver-deployment-5d9fdcc779- deployment-3561  2fc86a48-4bb9-400e-9797-3196472668ba 38747 0 2022-05-31 14:12:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:faeda7feb67651afd056bf4bd0a6b57b0571a775630d168b5b3ac0ed549b85b8 cni.projectcalico.org/podIP:172.25.2.201/32 cni.projectcalico.org/podIPs:172.25.2.201/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 85ea46e2-b062-4237-bfb5-676079029428 0xc004333517 0xc004333518}] []  [{kube-controller-manager Update v1 2022-05-31 14:12:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85ea46e2-b062-4237-bfb5-676079029428\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-05-31 14:12:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-05-31 14:12:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.201\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wtcdh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wtcdh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-160.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.160,PodIP:172.25.2.201,StartTime:2022-05-31 14:12:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-31 14:12:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://5f473e3b773b083dfdd644bcd363378cd79234850843de0e323eade7206ba886,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.201,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 31 14:12:13.324: INFO: Pod "webserver-deployment-5d9fdcc779-p9ck2" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-p9ck2 webserver-deployment-5d9fdcc779- deployment-3561  66903fab-fab4-4ee1-a158-0949bb51c0c3 38947 0 2022-05-31 14:12:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 85ea46e2-b062-4237-bfb5-676079029428 0xc004333f07 0xc004333f08}] []  [{kube-controller-manager Update v1 2022-05-31 14:12:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85ea46e2-b062-4237-bfb5-676079029428\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-31 14:12:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6z484,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6z484,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-247.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.247,PodIP:,StartTime:2022-05-31 14:12:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 31 14:12:13.324: INFO: Pod "webserver-deployment-5d9fdcc779-pc68d" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-pc68d webserver-deployment-5d9fdcc779- deployment-3561  6cd9a692-d8d8-4cc4-b5be-8e0c1ece3b92 38937 0 2022-05-31 14:12:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 85ea46e2-b062-4237-bfb5-676079029428 0xc00406a0d7 0xc00406a0d8}] []  [{kube-controller-manager Update v1 2022-05-31 14:12:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85ea46e2-b062-4237-bfb5-676079029428\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-31 14:12:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lgjlc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lgjlc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-160.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.160,PodIP:,StartTime:2022-05-31 14:12:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 31 14:12:13.324: INFO: Pod "webserver-deployment-5d9fdcc779-q2sbz" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-q2sbz webserver-deployment-5d9fdcc779- deployment-3561  2e50e96e-1754-4141-91a4-12af519ab3ef 38984 0 2022-05-31 14:12:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:e5ed8caf698e7a6059216860673ab722b0b01c4d436e3f671d5728324efa3747 cni.projectcalico.org/podIP:172.25.0.128/32 cni.projectcalico.org/podIPs:172.25.0.128/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 85ea46e2-b062-4237-bfb5-676079029428 0xc00406a597 0xc00406a598}] []  [{kube-controller-manager Update v1 2022-05-31 14:12:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85ea46e2-b062-4237-bfb5-676079029428\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-31 14:12:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-05-31 14:12:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-km4fg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-km4fg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-247.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.247,PodIP:,StartTime:2022-05-31 14:12:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 31 14:12:13.325: INFO: Pod "webserver-deployment-5d9fdcc779-qrcqx" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-qrcqx webserver-deployment-5d9fdcc779- deployment-3561  32550a89-d405-4584-80d8-555bec2bec27 38727 0 2022-05-31 14:12:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:2f6341ffeb6c0c80780808e21563f800be9f1f2ba3354d67a7aa0a9aca84367f cni.projectcalico.org/podIP:172.25.1.32/32 cni.projectcalico.org/podIPs:172.25.1.32/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 85ea46e2-b062-4237-bfb5-676079029428 0xc00406a797 0xc00406a798}] []  [{kube-controller-manager Update v1 2022-05-31 14:12:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85ea46e2-b062-4237-bfb5-676079029428\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-05-31 14:12:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-05-31 14:12:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.32\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kb65t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kb65t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-8-180.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.8.180,PodIP:172.25.1.32,StartTime:2022-05-31 14:12:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-31 14:12:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://3e83c7e2d61a84442a1cbaf07f957690a770da4d5f145c607b4cfb751df5c6e4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.32,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 31 14:12:13.325: INFO: Pod "webserver-deployment-5d9fdcc779-rcw2r" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-rcw2r webserver-deployment-5d9fdcc779- deployment-3561  74300263-28b8-455e-9611-05f867212ac9 38964 0 2022-05-31 14:12:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:d1ff8924127abf29bafd702b978316dfadb97389b0e2133b4232e2cbb28ac692 cni.projectcalico.org/podIP:172.25.0.127/32 cni.projectcalico.org/podIPs:172.25.0.127/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 85ea46e2-b062-4237-bfb5-676079029428 0xc00406a9d0 0xc00406a9d1}] []  [{kube-controller-manager Update v1 2022-05-31 14:12:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85ea46e2-b062-4237-bfb5-676079029428\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-31 14:12:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-05-31 14:12:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sxkgj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sxkgj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-247.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.247,PodIP:,StartTime:2022-05-31 14:12:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 31 14:12:13.325: INFO: Pod "webserver-deployment-5d9fdcc779-sqhpq" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-sqhpq webserver-deployment-5d9fdcc779- deployment-3561  5fa7c32a-a8bd-4d8a-8ff2-dd2f468413ef 38750 0 2022-05-31 14:12:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:4b804cbf592637b0bcf14dec24f84ef91d77c7f98280b4c4a45cb75da78a488e cni.projectcalico.org/podIP:172.25.2.200/32 cni.projectcalico.org/podIPs:172.25.2.200/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 85ea46e2-b062-4237-bfb5-676079029428 0xc00406abc7 0xc00406abc8}] []  [{kube-controller-manager Update v1 2022-05-31 14:12:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85ea46e2-b062-4237-bfb5-676079029428\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-05-31 14:12:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-05-31 14:12:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.200\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-75qpc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-75qpc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-160.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.160,PodIP:172.25.2.200,StartTime:2022-05-31 14:12:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-31 14:12:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://94ddeff8e188fc9e295161c0582b301fbb282fdb3dd831da29d6df6d0499ba38,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.200,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 31 14:12:13.327: INFO: Pod "webserver-deployment-5d9fdcc779-v67jq" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-v67jq webserver-deployment-5d9fdcc779- deployment-3561  40b27505-ba76-4579-9c91-c994c3376cfe 39001 0 2022-05-31 14:12:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:aca18d8b7be2d7ee80c61c6cfbab04b7d03d252c26af12faa99316630cece338 cni.projectcalico.org/podIP:172.25.0.130/32 cni.projectcalico.org/podIPs:172.25.0.130/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 85ea46e2-b062-4237-bfb5-676079029428 0xc00406ae07 0xc00406ae08}] []  [{kube-controller-manager Update v1 2022-05-31 14:12:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85ea46e2-b062-4237-bfb5-676079029428\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-31 14:12:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2022-05-31 14:12:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f2zkj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f2zkj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-11-247.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.11.247,PodIP:,StartTime:2022-05-31 14:12:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 31 14:12:13.327: INFO: Pod "webserver-deployment-5d9fdcc779-vg5dt" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-vg5dt webserver-deployment-5d9fdcc779- deployment-3561  b7a11eb2-81f8-41c8-83a2-2816bf8adea7 39009 0 2022-05-31 14:12:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:f30ca50c191b689131efec094a75238b408bbd1f14ed30e91f53fb656e7da23e cni.projectcalico.org/podIP:172.25.2.207/32 cni.projectcalico.org/podIPs:172.25.2.207/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 85ea46e2-b062-4237-bfb5-676079029428 0xc00406b167 0xc00406b168}] []  [{kube-controller-manager Update v1 2022-05-31 14:12:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85ea46e2-b062-4237-bfb5-676079029428\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-05-31 14:12:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gxl2z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gxl2z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-160.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 31 14:12:13.328: INFO: Pod "webserver-deployment-5d9fdcc779-xjj26" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-xjj26 webserver-deployment-5d9fdcc779- deployment-3561  b748aaf4-4349-4f47-8115-eaccfa7acaaa 38734 0 2022-05-31 14:12:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:faed6475ca98f217ad422e2259e8a39f86fee0dd896a46fed620cf4de40927fd cni.projectcalico.org/podIP:172.25.1.33/32 cni.projectcalico.org/podIPs:172.25.1.33/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 85ea46e2-b062-4237-bfb5-676079029428 0xc00406b300 0xc00406b301}] []  [{kube-controller-manager Update v1 2022-05-31 14:12:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85ea46e2-b062-4237-bfb5-676079029428\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-05-31 14:12:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-05-31 14:12:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.33\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tsrvr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tsrvr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-8-180.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.8.180,PodIP:172.25.1.33,StartTime:2022-05-31 14:12:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-31 14:12:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://efdc3283bcfdd06a88eb3c53766339a5ae8c0ad70051a26925830a8b31d89217,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.33,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 31 14:12:13.328: INFO: Pod "webserver-deployment-5d9fdcc779-zn5sv" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-zn5sv webserver-deployment-5d9fdcc779- deployment-3561  a4627a2b-df5f-48e9-8859-acc47299fb9d 38744 0 2022-05-31 14:12:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:520fa776271333479aad2be28d752642b9316e811ab8f3375ea06eee9517e813 cni.projectcalico.org/podIP:172.25.2.199/32 cni.projectcalico.org/podIPs:172.25.2.199/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 85ea46e2-b062-4237-bfb5-676079029428 0xc00406b510 0xc00406b511}] []  [{kube-controller-manager Update v1 2022-05-31 14:12:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85ea46e2-b062-4237-bfb5-676079029428\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-05-31 14:12:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2022-05-31 14:12:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.199\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jgqbc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jgqbc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-160.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.160,PodIP:172.25.2.199,StartTime:2022-05-31 14:12:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-31 14:12:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://1ef003dc4f0ef5840e04415cb542c164b2415780ce2da9d95899aa4861532a93,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.199,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 31 14:12:13.328: INFO: Pod "webserver-deployment-5d9fdcc779-zvrg2" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-zvrg2 webserver-deployment-5d9fdcc779- deployment-3561  7e659ba2-a497-4c59-b694-6c36ad11e665 38970 0 2022-05-31 14:12:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[cni.projectcalico.org/containerID:4e78e9c838618348c33ab0119f9ea1067aadb7b48127616e208391e384194b22 cni.projectcalico.org/podIP:172.25.1.39/32 cni.projectcalico.org/podIPs:172.25.1.39/32] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 85ea46e2-b062-4237-bfb5-676079029428 0xc00406b727 0xc00406b728}] []  [{kube-controller-manager Update v1 2022-05-31 14:12:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85ea46e2-b062-4237-bfb5-676079029428\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2022-05-31 14:12:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xc2v7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xc2v7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-8-180.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:12:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:12:13.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3561" for this suite.

• [SLOW TEST:8.483 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":346,"completed":241,"skipped":4672,"failed":0}
S
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:12:13.354: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 14:12:13.405: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Creating first CR 
May 31 14:12:16.058: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-05-31T14:12:16Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-05-31T14:12:16Z]] name:name1 resourceVersion:39078 uid:56c40e79-83f4-44a6-9cca-f219fb755d56] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
May 31 14:12:26.071: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-05-31T14:12:26Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-05-31T14:12:26Z]] name:name2 resourceVersion:39385 uid:16cc5d8f-561e-4981-9e35-71241037cf9e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
May 31 14:12:36.091: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-05-31T14:12:16Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-05-31T14:12:36Z]] name:name1 resourceVersion:39421 uid:56c40e79-83f4-44a6-9cca-f219fb755d56] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
May 31 14:12:46.112: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-05-31T14:12:26Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-05-31T14:12:46Z]] name:name2 resourceVersion:39457 uid:16cc5d8f-561e-4981-9e35-71241037cf9e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
May 31 14:12:56.133: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-05-31T14:12:16Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-05-31T14:12:36Z]] name:name1 resourceVersion:39493 uid:56c40e79-83f4-44a6-9cca-f219fb755d56] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
May 31 14:13:06.160: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-05-31T14:12:26Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-05-31T14:12:46Z]] name:name2 resourceVersion:39529 uid:16cc5d8f-561e-4981-9e35-71241037cf9e] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:13:16.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-4487" for this suite.

• [SLOW TEST:63.356 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":346,"completed":242,"skipped":4673,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:13:16.711: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4781
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-4781
I0531 14:13:16.863005      19 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4781, replica count: 2
I0531 14:13:19.914040      19 runners.go:193] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0531 14:13:22.915425      19 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 31 14:13:22.915: INFO: Creating new exec pod
May 31 14:13:25.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-4781 exec execpodpv6qr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 31 14:13:26.386: INFO: stderr: "+ + echo hostName\nnc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 31 14:13:26.386: INFO: stdout: ""
May 31 14:13:27.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-4781 exec execpodpv6qr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 31 14:13:27.763: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 31 14:13:27.763: INFO: stdout: ""
May 31 14:13:28.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-4781 exec execpodpv6qr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
May 31 14:13:28.817: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
May 31 14:13:28.818: INFO: stdout: "externalname-service-5p5wr"
May 31 14:13:28.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-4781 exec execpodpv6qr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.18.101 80'
May 31 14:13:29.220: INFO: stderr: "+ nc -v -t -w 2 10.240.18.101 80\nConnection to 10.240.18.101 80 port [tcp/http] succeeded!\n+ echo hostName\n"
May 31 14:13:29.220: INFO: stdout: "externalname-service-9l8d8"
May 31 14:13:29.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-4781 exec execpodpv6qr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.11.247 31641'
May 31 14:13:29.629: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.11.247 31641\nConnection to 172.31.11.247 31641 port [tcp/*] succeeded!\n"
May 31 14:13:29.629: INFO: stdout: "externalname-service-5p5wr"
May 31 14:13:29.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-4781 exec execpodpv6qr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.3.160 31641'
May 31 14:13:30.103: INFO: stderr: "+ nc -v -t -w 2 172.31.3.160 31641\n+ echoConnection to 172.31.3.160 31641 port [tcp/*] succeeded!\n hostName\n"
May 31 14:13:30.103: INFO: stdout: "externalname-service-5p5wr"
May 31 14:13:30.103: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:13:30.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4781" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:13.467 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":346,"completed":243,"skipped":4681,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:13:30.178: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 14:13:30.309: INFO: Pod name cleanup-pod: Found 0 pods out of 1
May 31 14:13:35.345: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 31 14:13:35.345: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
May 31 14:13:35.395: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-8425  a8c01fa1-54a0-46c6-a2b3-7b10df616362 39739 1 2022-05-31 14:13:35 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2022-05-31 14:13:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0044a0b28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

May 31 14:13:35.404: INFO: New ReplicaSet "test-cleanup-deployment-56cd759769" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-56cd759769  deployment-8425  9fafe85b-c0e5-4d7e-80ca-21772d1746b7 39743 1 2022-05-31 14:13:35 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:56cd759769] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment a8c01fa1-54a0-46c6-a2b3-7b10df616362 0xc0044a1037 0xc0044a1038}] []  [{kube-controller-manager Update apps/v1 2022-05-31 14:13:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a8c01fa1-54a0-46c6-a2b3-7b10df616362\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 56cd759769,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:56cd759769] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.33 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0044a10c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
May 31 14:13:35.404: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
May 31 14:13:35.404: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-8425  9db7df88-5a07-49d8-ab3b-854ceb41bfaa 39741 1 2022-05-31 14:13:30 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment a8c01fa1-54a0-46c6-a2b3-7b10df616362 0xc0044a0f07 0xc0044a0f08}] []  [{e2e.test Update apps/v1 2022-05-31 14:13:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-31 14:13:31 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2022-05-31 14:13:35 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"a8c01fa1-54a0-46c6-a2b3-7b10df616362\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0044a0fc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 31 14:13:35.413: INFO: Pod "test-cleanup-controller-qh7c9" is available:
&Pod{ObjectMeta:{test-cleanup-controller-qh7c9 test-cleanup-controller- deployment-8425  f8b64501-f3c6-4886-8b2e-592180f102fd 39714 0 2022-05-31 14:13:30 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:2be0ba376a445d677010491873b2e9b303011aaabaad5e4c20db83d47221c8d9 cni.projectcalico.org/podIP:172.25.1.45/32 cni.projectcalico.org/podIPs:172.25.1.45/32] [{apps/v1 ReplicaSet test-cleanup-controller 9db7df88-5a07-49d8-ab3b-854ceb41bfaa 0xc0044a16b7 0xc0044a16b8}] []  [{calico Update v1 2022-05-31 14:13:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-05-31 14:13:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9db7df88-5a07-49d8-ab3b-854ceb41bfaa\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-31 14:13:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.1.45\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vkc5t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vkc5t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-8-180.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:13:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:13:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:13:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:13:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.8.180,PodIP:172.25.1.45,StartTime:2022-05-31 14:13:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-31 14:13:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://e5a8337026cbcb0fa04a3678f39f02bf153f1bac0f6bce552c7e060006f5916d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.1.45,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
May 31 14:13:35.414: INFO: Pod "test-cleanup-deployment-56cd759769-mw2ps" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-56cd759769-mw2ps test-cleanup-deployment-56cd759769- deployment-8425  27d35305-4547-40d7-a7cb-1c2570b68f2d 39745 0 2022-05-31 14:13:35 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:56cd759769] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-56cd759769 9fafe85b-c0e5-4d7e-80ca-21772d1746b7 0xc0044a18a7 0xc0044a18a8}] []  [{kube-controller-manager Update v1 2022-05-31 14:13:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9fafe85b-c0e5-4d7e-80ca-21772d1746b7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jxbvz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.33,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jxbvz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:13:35.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8425" for this suite.

• [SLOW TEST:5.276 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":346,"completed":244,"skipped":4689,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:13:35.502: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 14:13:35.571: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: creating the pod
STEP: submitting the pod to kubernetes
May 31 14:13:35.591: INFO: The status of Pod pod-exec-websocket-004628ac-30e0-4eb5-8841-929584f72af9 is Pending, waiting for it to be Running (with Ready = true)
May 31 14:13:37.608: INFO: The status of Pod pod-exec-websocket-004628ac-30e0-4eb5-8841-929584f72af9 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:13:37.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4087" for this suite.
•{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":346,"completed":245,"skipped":4741,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:13:37.791: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:13:37.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7658" for this suite.
•{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":346,"completed":246,"skipped":4766,"failed":0}

------------------------------
[sig-storage] Secrets 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:13:37.901: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:13:38.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-589" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","total":346,"completed":247,"skipped":4766,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:13:38.088: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: validating api versions
May 31 14:13:38.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-944 api-versions'
May 31 14:13:38.359: INFO: stderr: ""
May 31 14:13:38.359: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncluster.k8s.io/v1alpha1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\npolicy/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:13:38.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-944" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":346,"completed":248,"skipped":4837,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:13:38.380: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 14:13:38.500: INFO: The status of Pod pod-secrets-6d286e2e-3ac2-4080-b760-f3c3e99d99d1 is Pending, waiting for it to be Running (with Ready = true)
May 31 14:13:40.514: INFO: The status of Pod pod-secrets-6d286e2e-3ac2-4080-b760-f3c3e99d99d1 is Running (Ready = true)
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:13:40.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4561" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":346,"completed":249,"skipped":4839,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:13:40.629: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-map-11fe6b5e-4701-4209-962c-1fe62ce90558
STEP: Creating a pod to test consume configMaps
May 31 14:13:40.761: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b9b229b8-d80a-4133-a848-1f5d0b8e8fd3" in namespace "projected-7571" to be "Succeeded or Failed"
May 31 14:13:40.783: INFO: Pod "pod-projected-configmaps-b9b229b8-d80a-4133-a848-1f5d0b8e8fd3": Phase="Pending", Reason="", readiness=false. Elapsed: 22.398942ms
May 31 14:13:42.795: INFO: Pod "pod-projected-configmaps-b9b229b8-d80a-4133-a848-1f5d0b8e8fd3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034009015s
May 31 14:13:44.806: INFO: Pod "pod-projected-configmaps-b9b229b8-d80a-4133-a848-1f5d0b8e8fd3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044724085s
STEP: Saw pod success
May 31 14:13:44.806: INFO: Pod "pod-projected-configmaps-b9b229b8-d80a-4133-a848-1f5d0b8e8fd3" satisfied condition "Succeeded or Failed"
May 31 14:13:44.812: INFO: Trying to get logs from node ip-172-31-3-160.eu-central-1.compute.internal pod pod-projected-configmaps-b9b229b8-d80a-4133-a848-1f5d0b8e8fd3 container agnhost-container: <nil>
STEP: delete the pod
May 31 14:13:44.887: INFO: Waiting for pod pod-projected-configmaps-b9b229b8-d80a-4133-a848-1f5d0b8e8fd3 to disappear
May 31 14:13:44.894: INFO: Pod pod-projected-configmaps-b9b229b8-d80a-4133-a848-1f5d0b8e8fd3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:13:44.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7571" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":250,"skipped":4851,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:13:44.919: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 31 14:13:49.071: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:13:49.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-247" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]","total":346,"completed":251,"skipped":4861,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:13:49.130: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
May 31 14:13:49.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-844 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
May 31 14:13:49.357: INFO: stderr: ""
May 31 14:13:49.357: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
May 31 14:13:49.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-844 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "k8s.gcr.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
May 31 14:13:51.525: INFO: stderr: ""
May 31 14:13:51.525: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
May 31 14:13:51.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-844 delete pods e2e-test-httpd-pod'
May 31 14:13:54.389: INFO: stderr: ""
May 31 14:13:54.389: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:13:54.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-844" for this suite.

• [SLOW TEST:5.290 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:926
    should check if kubectl can dry-run update Pods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":346,"completed":252,"skipped":4889,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:13:54.419: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service in namespace services-5999
May 31 14:13:54.541: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
May 31 14:13:56.558: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
May 31 14:13:56.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-5999 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
May 31 14:13:57.325: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
May 31 14:13:57.325: INFO: stdout: "ipvs"
May 31 14:13:57.325: INFO: proxyMode: ipvs
May 31 14:13:57.345: INFO: Waiting for pod kube-proxy-mode-detector to disappear
May 31 14:13:57.351: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-5999
STEP: creating replication controller affinity-nodeport-timeout in namespace services-5999
I0531 14:13:57.388300      19 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-5999, replica count: 3
I0531 14:14:00.446835      19 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 31 14:14:00.497: INFO: Creating new exec pod
May 31 14:14:03.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-5999 exec execpod-affinityzsjzv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
May 31 14:14:04.104: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
May 31 14:14:04.104: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 31 14:14:04.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-5999 exec execpod-affinityzsjzv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.30.97 80'
May 31 14:14:04.521: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.30.97 80\nConnection to 10.240.30.97 80 port [tcp/http] succeeded!\n"
May 31 14:14:04.521: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 31 14:14:04.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-5999 exec execpod-affinityzsjzv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.3.160 30043'
May 31 14:14:04.940: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.3.160 30043\nConnection to 172.31.3.160 30043 port [tcp/*] succeeded!\n"
May 31 14:14:04.941: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 31 14:14:04.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-5999 exec execpod-affinityzsjzv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.31.11.247 30043'
May 31 14:14:05.477: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.31.11.247 30043\nConnection to 172.31.11.247 30043 port [tcp/*] succeeded!\n"
May 31 14:14:05.477: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 31 14:14:05.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-5999 exec execpod-affinityzsjzv -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.31.11.247:30043/ ; done'
May 31 14:14:06.043: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:30043/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:30043/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:30043/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:30043/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:30043/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:30043/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:30043/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:30043/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:30043/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:30043/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:30043/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:30043/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:30043/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:30043/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:30043/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.31.11.247:30043/\n"
May 31 14:14:06.043: INFO: stdout: "\naffinity-nodeport-timeout-w678f\naffinity-nodeport-timeout-w678f\naffinity-nodeport-timeout-w678f\naffinity-nodeport-timeout-w678f\naffinity-nodeport-timeout-w678f\naffinity-nodeport-timeout-w678f\naffinity-nodeport-timeout-w678f\naffinity-nodeport-timeout-w678f\naffinity-nodeport-timeout-w678f\naffinity-nodeport-timeout-w678f\naffinity-nodeport-timeout-w678f\naffinity-nodeport-timeout-w678f\naffinity-nodeport-timeout-w678f\naffinity-nodeport-timeout-w678f\naffinity-nodeport-timeout-w678f\naffinity-nodeport-timeout-w678f"
May 31 14:14:06.043: INFO: Received response from host: affinity-nodeport-timeout-w678f
May 31 14:14:06.043: INFO: Received response from host: affinity-nodeport-timeout-w678f
May 31 14:14:06.043: INFO: Received response from host: affinity-nodeport-timeout-w678f
May 31 14:14:06.043: INFO: Received response from host: affinity-nodeport-timeout-w678f
May 31 14:14:06.043: INFO: Received response from host: affinity-nodeport-timeout-w678f
May 31 14:14:06.043: INFO: Received response from host: affinity-nodeport-timeout-w678f
May 31 14:14:06.043: INFO: Received response from host: affinity-nodeport-timeout-w678f
May 31 14:14:06.043: INFO: Received response from host: affinity-nodeport-timeout-w678f
May 31 14:14:06.043: INFO: Received response from host: affinity-nodeport-timeout-w678f
May 31 14:14:06.043: INFO: Received response from host: affinity-nodeport-timeout-w678f
May 31 14:14:06.043: INFO: Received response from host: affinity-nodeport-timeout-w678f
May 31 14:14:06.043: INFO: Received response from host: affinity-nodeport-timeout-w678f
May 31 14:14:06.043: INFO: Received response from host: affinity-nodeport-timeout-w678f
May 31 14:14:06.043: INFO: Received response from host: affinity-nodeport-timeout-w678f
May 31 14:14:06.043: INFO: Received response from host: affinity-nodeport-timeout-w678f
May 31 14:14:06.043: INFO: Received response from host: affinity-nodeport-timeout-w678f
May 31 14:14:06.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-5999 exec execpod-affinityzsjzv -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.31.11.247:30043/'
May 31 14:14:06.448: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.31.11.247:30043/\n"
May 31 14:14:06.448: INFO: stdout: "affinity-nodeport-timeout-w678f"
May 31 14:16:16.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-5999 exec execpod-affinityzsjzv -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.31.11.247:30043/'
May 31 14:16:17.024: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.31.11.247:30043/\n"
May 31 14:16:17.024: INFO: stdout: "affinity-nodeport-timeout-qs7c2"
May 31 14:16:17.024: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-5999, will wait for the garbage collector to delete the pods
May 31 14:16:17.206: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 18.33521ms
May 31 14:16:17.408: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 201.386861ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:16:19.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5999" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:145.248 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":253,"skipped":4913,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:16:19.673: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1025.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1025.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1025.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1025.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 31 14:16:21.981: INFO: DNS probes using dns-test-dbc3e5bd-8a2f-4ab9-a9a4-6925e062f6dc succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1025.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1025.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1025.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1025.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 31 14:16:26.145: INFO: File jessie_udp@dns-test-service-3.dns-1025.svc.cluster.local from pod  dns-1025/dns-test-30b7ca60-7da1-4a22-9ca4-62b1549fcc3f contains 'foo.example.com.
' instead of 'bar.example.com.'
May 31 14:16:26.145: INFO: Lookups using dns-1025/dns-test-30b7ca60-7da1-4a22-9ca4-62b1549fcc3f failed for: [jessie_udp@dns-test-service-3.dns-1025.svc.cluster.local]

May 31 14:16:31.257: INFO: DNS probes using dns-test-30b7ca60-7da1-4a22-9ca4-62b1549fcc3f succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1025.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-1025.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1025.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-1025.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 31 14:16:35.516: INFO: DNS probes using dns-test-697a6450-659c-4b18-a4df-0c2be055b063 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:16:35.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1025" for this suite.

• [SLOW TEST:15.909 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":346,"completed":254,"skipped":4931,"failed":0}
SS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:16:35.586: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a service nodeport-service with the type=NodePort in namespace services-3191
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-3191
STEP: creating replication controller externalsvc in namespace services-3191
I0531 14:16:35.770769      19 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3191, replica count: 2
I0531 14:16:38.821470      19 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
May 31 14:16:38.872: INFO: Creating new exec pod
May 31 14:16:40.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-3191 exec execpodfq9n7 -- /bin/sh -x -c nslookup nodeport-service.services-3191.svc.cluster.local'
May 31 14:16:42.003: INFO: stderr: "+ nslookup nodeport-service.services-3191.svc.cluster.local\n"
May 31 14:16:42.003: INFO: stdout: "Server:\t\t169.254.20.10\nAddress:\t169.254.20.10#53\n\nnodeport-service.services-3191.svc.cluster.local\tcanonical name = externalsvc.services-3191.svc.cluster.local.\nName:\texternalsvc.services-3191.svc.cluster.local\nAddress: 10.240.29.25\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3191, will wait for the garbage collector to delete the pods
May 31 14:16:42.073: INFO: Deleting ReplicationController externalsvc took: 11.531638ms
May 31 14:16:42.175: INFO: Terminating ReplicationController externalsvc pods took: 101.972107ms
May 31 14:16:44.036: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:16:44.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3191" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:8.526 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":346,"completed":255,"skipped":4933,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:16:44.112: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting the auto-created API token
May 31 14:16:44.707: INFO: created pod pod-service-account-defaultsa
May 31 14:16:44.707: INFO: pod pod-service-account-defaultsa service account token volume mount: true
May 31 14:16:44.718: INFO: created pod pod-service-account-mountsa
May 31 14:16:44.718: INFO: pod pod-service-account-mountsa service account token volume mount: true
May 31 14:16:44.739: INFO: created pod pod-service-account-nomountsa
May 31 14:16:44.739: INFO: pod pod-service-account-nomountsa service account token volume mount: false
May 31 14:16:44.758: INFO: created pod pod-service-account-defaultsa-mountspec
May 31 14:16:44.758: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
May 31 14:16:44.767: INFO: created pod pod-service-account-mountsa-mountspec
May 31 14:16:44.768: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
May 31 14:16:44.781: INFO: created pod pod-service-account-nomountsa-mountspec
May 31 14:16:44.781: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
May 31 14:16:44.789: INFO: created pod pod-service-account-defaultsa-nomountspec
May 31 14:16:44.789: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
May 31 14:16:44.808: INFO: created pod pod-service-account-mountsa-nomountspec
May 31 14:16:44.808: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
May 31 14:16:44.824: INFO: created pod pod-service-account-nomountsa-nomountspec
May 31 14:16:44.824: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:16:44.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3349" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":346,"completed":256,"skipped":4946,"failed":0}
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:16:44.853: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-map-123e8ba6-eb67-4de9-a59e-5c3c8a8940e8
STEP: Creating a pod to test consume secrets
May 31 14:16:44.924: INFO: Waiting up to 5m0s for pod "pod-secrets-43dedf35-d2f5-4911-8e4f-9c2cdbee4601" in namespace "secrets-5569" to be "Succeeded or Failed"
May 31 14:16:44.936: INFO: Pod "pod-secrets-43dedf35-d2f5-4911-8e4f-9c2cdbee4601": Phase="Pending", Reason="", readiness=false. Elapsed: 11.973031ms
May 31 14:16:46.958: INFO: Pod "pod-secrets-43dedf35-d2f5-4911-8e4f-9c2cdbee4601": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0338535s
May 31 14:16:48.974: INFO: Pod "pod-secrets-43dedf35-d2f5-4911-8e4f-9c2cdbee4601": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049639846s
STEP: Saw pod success
May 31 14:16:48.974: INFO: Pod "pod-secrets-43dedf35-d2f5-4911-8e4f-9c2cdbee4601" satisfied condition "Succeeded or Failed"
May 31 14:16:48.997: INFO: Trying to get logs from node ip-172-31-3-160.eu-central-1.compute.internal pod pod-secrets-43dedf35-d2f5-4911-8e4f-9c2cdbee4601 container secret-volume-test: <nil>
STEP: delete the pod
May 31 14:16:49.045: INFO: Waiting for pod pod-secrets-43dedf35-d2f5-4911-8e4f-9c2cdbee4601 to disappear
May 31 14:16:49.061: INFO: Pod pod-secrets-43dedf35-d2f5-4911-8e4f-9c2cdbee4601 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:16:49.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5569" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":257,"skipped":4954,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:16:49.087: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: set up a multi version CRD
May 31 14:16:49.166: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:17:18.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3854" for this suite.

• [SLOW TEST:29.601 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":346,"completed":258,"skipped":4984,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:17:18.688: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:17:29.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1590" for this suite.

• [SLOW TEST:11.186 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":346,"completed":259,"skipped":4991,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:17:29.875: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-1caabe44-c580-4089-9450-da3d44a2d079
STEP: Creating a pod to test consume configMaps
May 31 14:17:29.962: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-eaf27cb3-5ef3-4651-b5e7-71f927d49d9a" in namespace "projected-9029" to be "Succeeded or Failed"
May 31 14:17:29.970: INFO: Pod "pod-projected-configmaps-eaf27cb3-5ef3-4651-b5e7-71f927d49d9a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.69918ms
May 31 14:17:31.983: INFO: Pod "pod-projected-configmaps-eaf27cb3-5ef3-4651-b5e7-71f927d49d9a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020897134s
May 31 14:17:34.000: INFO: Pod "pod-projected-configmaps-eaf27cb3-5ef3-4651-b5e7-71f927d49d9a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037488605s
May 31 14:17:36.016: INFO: Pod "pod-projected-configmaps-eaf27cb3-5ef3-4651-b5e7-71f927d49d9a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.053904888s
STEP: Saw pod success
May 31 14:17:36.017: INFO: Pod "pod-projected-configmaps-eaf27cb3-5ef3-4651-b5e7-71f927d49d9a" satisfied condition "Succeeded or Failed"
May 31 14:17:36.024: INFO: Trying to get logs from node ip-172-31-3-160.eu-central-1.compute.internal pod pod-projected-configmaps-eaf27cb3-5ef3-4651-b5e7-71f927d49d9a container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 31 14:17:36.100: INFO: Waiting for pod pod-projected-configmaps-eaf27cb3-5ef3-4651-b5e7-71f927d49d9a to disappear
May 31 14:17:36.108: INFO: Pod pod-projected-configmaps-eaf27cb3-5ef3-4651-b5e7-71f927d49d9a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:17:36.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9029" for this suite.

• [SLOW TEST:6.255 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":260,"skipped":5059,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:17:36.136: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 14:17:36.187: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
May 31 14:17:39.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=crd-publish-openapi-6847 --namespace=crd-publish-openapi-6847 create -f -'
May 31 14:17:41.717: INFO: stderr: ""
May 31 14:17:41.717: INFO: stdout: "e2e-test-crd-publish-openapi-6660-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May 31 14:17:41.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=crd-publish-openapi-6847 --namespace=crd-publish-openapi-6847 delete e2e-test-crd-publish-openapi-6660-crds test-foo'
May 31 14:17:41.861: INFO: stderr: ""
May 31 14:17:41.861: INFO: stdout: "e2e-test-crd-publish-openapi-6660-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
May 31 14:17:41.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=crd-publish-openapi-6847 --namespace=crd-publish-openapi-6847 apply -f -'
May 31 14:17:42.494: INFO: stderr: ""
May 31 14:17:42.494: INFO: stdout: "e2e-test-crd-publish-openapi-6660-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
May 31 14:17:42.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=crd-publish-openapi-6847 --namespace=crd-publish-openapi-6847 delete e2e-test-crd-publish-openapi-6660-crds test-foo'
May 31 14:17:42.616: INFO: stderr: ""
May 31 14:17:42.616: INFO: stdout: "e2e-test-crd-publish-openapi-6660-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with value outside defined enum values
May 31 14:17:42.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=crd-publish-openapi-6847 --namespace=crd-publish-openapi-6847 create -f -'
May 31 14:17:43.484: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
May 31 14:17:43.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=crd-publish-openapi-6847 --namespace=crd-publish-openapi-6847 create -f -'
May 31 14:17:44.451: INFO: rc: 1
May 31 14:17:44.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=crd-publish-openapi-6847 --namespace=crd-publish-openapi-6847 apply -f -'
May 31 14:17:44.883: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
May 31 14:17:44.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=crd-publish-openapi-6847 --namespace=crd-publish-openapi-6847 create -f -'
May 31 14:17:45.190: INFO: rc: 1
May 31 14:17:45.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=crd-publish-openapi-6847 --namespace=crd-publish-openapi-6847 apply -f -'
May 31 14:17:45.399: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
May 31 14:17:45.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=crd-publish-openapi-6847 explain e2e-test-crd-publish-openapi-6660-crds'
May 31 14:17:45.601: INFO: stderr: ""
May 31 14:17:45.601: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6660-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
May 31 14:17:45.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=crd-publish-openapi-6847 explain e2e-test-crd-publish-openapi-6660-crds.metadata'
May 31 14:17:45.831: INFO: stderr: ""
May 31 14:17:45.831: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6660-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
May 31 14:17:45.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=crd-publish-openapi-6847 explain e2e-test-crd-publish-openapi-6660-crds.spec'
May 31 14:17:46.052: INFO: stderr: ""
May 31 14:17:46.052: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6660-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
May 31 14:17:46.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=crd-publish-openapi-6847 explain e2e-test-crd-publish-openapi-6660-crds.spec.bars'
May 31 14:17:46.271: INFO: stderr: ""
May 31 14:17:46.271: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6660-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
May 31 14:17:46.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=crd-publish-openapi-6847 explain e2e-test-crd-publish-openapi-6660-crds.spec.bars2'
May 31 14:17:46.486: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:17:50.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6847" for this suite.

• [SLOW TEST:14.435 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":346,"completed":261,"skipped":5140,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:17:50.572: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create a Replicaset
STEP: Verify that the required pods have come up.
May 31 14:17:50.719: INFO: Pod name sample-pod: Found 0 pods out of 1
May 31 14:17:55.735: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Getting /status
May 31 14:17:55.744: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status
May 31 14:17:55.766: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated
May 31 14:17:55.772: INFO: Observed &ReplicaSet event: ADDED
May 31 14:17:55.772: INFO: Observed &ReplicaSet event: MODIFIED
May 31 14:17:55.772: INFO: Observed &ReplicaSet event: MODIFIED
May 31 14:17:55.773: INFO: Observed &ReplicaSet event: MODIFIED
May 31 14:17:55.773: INFO: Found replicaset test-rs in namespace replicaset-2373 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
May 31 14:17:55.773: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status
May 31 14:17:55.773: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
May 31 14:17:55.785: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched
May 31 14:17:55.790: INFO: Observed &ReplicaSet event: ADDED
May 31 14:17:55.790: INFO: Observed &ReplicaSet event: MODIFIED
May 31 14:17:55.791: INFO: Observed &ReplicaSet event: MODIFIED
May 31 14:17:55.791: INFO: Observed &ReplicaSet event: MODIFIED
May 31 14:17:55.791: INFO: Observed replicaset test-rs in namespace replicaset-2373 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May 31 14:17:55.791: INFO: Observed &ReplicaSet event: MODIFIED
May 31 14:17:55.791: INFO: Found replicaset test-rs in namespace replicaset-2373 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
May 31 14:17:55.792: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:17:55.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2373" for this suite.

• [SLOW TEST:5.257 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","total":346,"completed":262,"skipped":5159,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:17:55.832: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0777 on node default medium
May 31 14:17:55.921: INFO: Waiting up to 5m0s for pod "pod-a23776f1-92ac-4d81-a754-56983051dbed" in namespace "emptydir-6345" to be "Succeeded or Failed"
May 31 14:17:55.929: INFO: Pod "pod-a23776f1-92ac-4d81-a754-56983051dbed": Phase="Pending", Reason="", readiness=false. Elapsed: 7.628384ms
May 31 14:17:57.940: INFO: Pod "pod-a23776f1-92ac-4d81-a754-56983051dbed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018947324s
May 31 14:17:59.955: INFO: Pod "pod-a23776f1-92ac-4d81-a754-56983051dbed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033291061s
STEP: Saw pod success
May 31 14:17:59.955: INFO: Pod "pod-a23776f1-92ac-4d81-a754-56983051dbed" satisfied condition "Succeeded or Failed"
May 31 14:17:59.962: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod pod-a23776f1-92ac-4d81-a754-56983051dbed container test-container: <nil>
STEP: delete the pod
May 31 14:18:00.054: INFO: Waiting for pod pod-a23776f1-92ac-4d81-a754-56983051dbed to disappear
May 31 14:18:00.067: INFO: Pod pod-a23776f1-92ac-4d81-a754-56983051dbed no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:18:00.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6345" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":263,"skipped":5184,"failed":0}

------------------------------
[sig-apps] Deployment 
  should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:18:00.094: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a Deployment
May 31 14:18:00.175: INFO: Creating simple deployment test-deployment-2qk4n
May 31 14:18:00.208: INFO: deployment "test-deployment-2qk4n" doesn't have the required revision set
STEP: Getting /status
May 31 14:18:02.254: INFO: Deployment test-deployment-2qk4n has Conditions: [{Available True 2022-05-31 14:18:02 +0000 UTC 2022-05-31 14:18:02 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2022-05-31 14:18:02 +0000 UTC 2022-05-31 14:18:00 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-2qk4n-764bc7c4b7" has successfully progressed.}]
STEP: updating Deployment Status
May 31 14:18:02.294: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.May, 31, 14, 18, 2, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 31, 14, 18, 2, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 31, 14, 18, 2, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 31, 14, 18, 0, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-2qk4n-764bc7c4b7\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated
May 31 14:18:02.299: INFO: Observed &Deployment event: ADDED
May 31 14:18:02.299: INFO: Observed Deployment test-deployment-2qk4n in namespace deployment-2182 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-05-31 14:18:00 +0000 UTC 2022-05-31 14:18:00 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-2qk4n-764bc7c4b7"}
May 31 14:18:02.300: INFO: Observed &Deployment event: MODIFIED
May 31 14:18:02.300: INFO: Observed Deployment test-deployment-2qk4n in namespace deployment-2182 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-05-31 14:18:00 +0000 UTC 2022-05-31 14:18:00 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-2qk4n-764bc7c4b7"}
May 31 14:18:02.300: INFO: Observed Deployment test-deployment-2qk4n in namespace deployment-2182 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-05-31 14:18:00 +0000 UTC 2022-05-31 14:18:00 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
May 31 14:18:02.300: INFO: Observed &Deployment event: MODIFIED
May 31 14:18:02.300: INFO: Observed Deployment test-deployment-2qk4n in namespace deployment-2182 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-05-31 14:18:00 +0000 UTC 2022-05-31 14:18:00 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
May 31 14:18:02.300: INFO: Observed Deployment test-deployment-2qk4n in namespace deployment-2182 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-05-31 14:18:00 +0000 UTC 2022-05-31 14:18:00 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-2qk4n-764bc7c4b7" is progressing.}
May 31 14:18:02.301: INFO: Observed &Deployment event: MODIFIED
May 31 14:18:02.301: INFO: Observed Deployment test-deployment-2qk4n in namespace deployment-2182 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-05-31 14:18:02 +0000 UTC 2022-05-31 14:18:02 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
May 31 14:18:02.301: INFO: Observed Deployment test-deployment-2qk4n in namespace deployment-2182 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-05-31 14:18:02 +0000 UTC 2022-05-31 14:18:00 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-2qk4n-764bc7c4b7" has successfully progressed.}
May 31 14:18:02.301: INFO: Observed &Deployment event: MODIFIED
May 31 14:18:02.301: INFO: Observed Deployment test-deployment-2qk4n in namespace deployment-2182 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-05-31 14:18:02 +0000 UTC 2022-05-31 14:18:02 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
May 31 14:18:02.302: INFO: Observed Deployment test-deployment-2qk4n in namespace deployment-2182 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-05-31 14:18:02 +0000 UTC 2022-05-31 14:18:00 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-2qk4n-764bc7c4b7" has successfully progressed.}
May 31 14:18:02.302: INFO: Found Deployment test-deployment-2qk4n in namespace deployment-2182 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May 31 14:18:02.302: INFO: Deployment test-deployment-2qk4n has an updated status
STEP: patching the Statefulset Status
May 31 14:18:02.302: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
May 31 14:18:02.329: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched
May 31 14:18:02.343: INFO: Observed &Deployment event: ADDED
May 31 14:18:02.343: INFO: Observed deployment test-deployment-2qk4n in namespace deployment-2182 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-05-31 14:18:00 +0000 UTC 2022-05-31 14:18:00 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-2qk4n-764bc7c4b7"}
May 31 14:18:02.343: INFO: Observed &Deployment event: MODIFIED
May 31 14:18:02.343: INFO: Observed deployment test-deployment-2qk4n in namespace deployment-2182 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-05-31 14:18:00 +0000 UTC 2022-05-31 14:18:00 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-2qk4n-764bc7c4b7"}
May 31 14:18:02.343: INFO: Observed deployment test-deployment-2qk4n in namespace deployment-2182 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-05-31 14:18:00 +0000 UTC 2022-05-31 14:18:00 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
May 31 14:18:02.343: INFO: Observed &Deployment event: MODIFIED
May 31 14:18:02.343: INFO: Observed deployment test-deployment-2qk4n in namespace deployment-2182 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-05-31 14:18:00 +0000 UTC 2022-05-31 14:18:00 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
May 31 14:18:02.343: INFO: Observed deployment test-deployment-2qk4n in namespace deployment-2182 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-05-31 14:18:00 +0000 UTC 2022-05-31 14:18:00 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-2qk4n-764bc7c4b7" is progressing.}
May 31 14:18:02.343: INFO: Observed &Deployment event: MODIFIED
May 31 14:18:02.344: INFO: Observed deployment test-deployment-2qk4n in namespace deployment-2182 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-05-31 14:18:02 +0000 UTC 2022-05-31 14:18:02 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
May 31 14:18:02.344: INFO: Observed deployment test-deployment-2qk4n in namespace deployment-2182 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-05-31 14:18:02 +0000 UTC 2022-05-31 14:18:00 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-2qk4n-764bc7c4b7" has successfully progressed.}
May 31 14:18:02.344: INFO: Observed &Deployment event: MODIFIED
May 31 14:18:02.344: INFO: Observed deployment test-deployment-2qk4n in namespace deployment-2182 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-05-31 14:18:02 +0000 UTC 2022-05-31 14:18:02 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
May 31 14:18:02.344: INFO: Observed deployment test-deployment-2qk4n in namespace deployment-2182 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-05-31 14:18:02 +0000 UTC 2022-05-31 14:18:00 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-2qk4n-764bc7c4b7" has successfully progressed.}
May 31 14:18:02.344: INFO: Observed deployment test-deployment-2qk4n in namespace deployment-2182 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
May 31 14:18:02.344: INFO: Observed &Deployment event: MODIFIED
May 31 14:18:02.344: INFO: Found deployment test-deployment-2qk4n in namespace deployment-2182 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
May 31 14:18:02.344: INFO: Deployment test-deployment-2qk4n has a patched status
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
May 31 14:18:02.371: INFO: Deployment "test-deployment-2qk4n":
&Deployment{ObjectMeta:{test-deployment-2qk4n  deployment-2182  2fc29e78-d5d1-4b19-afce-3ec05e1c8e50 41758 1 2022-05-31 14:18:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2022-05-31 14:18:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2022-05-31 14:18:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2022-05-31 14:18:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004bcde58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-2qk4n-764bc7c4b7",LastUpdateTime:2022-05-31 14:18:02 +0000 UTC,LastTransitionTime:2022-05-31 14:18:02 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

May 31 14:18:02.410: INFO: New ReplicaSet "test-deployment-2qk4n-764bc7c4b7" of Deployment "test-deployment-2qk4n":
&ReplicaSet{ObjectMeta:{test-deployment-2qk4n-764bc7c4b7  deployment-2182  2ec3f2f0-45a0-42b7-ab91-1dbbf7075005 41753 1 2022-05-31 14:18:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:764bc7c4b7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-2qk4n 2fc29e78-d5d1-4b19-afce-3ec05e1c8e50 0xc004c442d0 0xc004c442d1}] []  [{kube-controller-manager Update apps/v1 2022-05-31 14:18:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2fc29e78-d5d1-4b19-afce-3ec05e1c8e50\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-05-31 14:18:02 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 764bc7c4b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:764bc7c4b7] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c44398 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
May 31 14:18:02.424: INFO: Pod "test-deployment-2qk4n-764bc7c4b7-7g978" is available:
&Pod{ObjectMeta:{test-deployment-2qk4n-764bc7c4b7-7g978 test-deployment-2qk4n-764bc7c4b7- deployment-2182  24b4e9e5-c396-4dae-9e91-f15af669975f 41752 0 2022-05-31 14:18:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:764bc7c4b7] map[cni.projectcalico.org/containerID:0114474cb954ce54a406f2cfc0118d5fdba3b5141467f4aa2358b7c2c2e28b1f cni.projectcalico.org/podIP:172.25.2.223/32 cni.projectcalico.org/podIPs:172.25.2.223/32] [{apps/v1 ReplicaSet test-deployment-2qk4n-764bc7c4b7 2ec3f2f0-45a0-42b7-ab91-1dbbf7075005 0xc004c0ae20 0xc004c0ae21}] []  [{calico Update v1 2022-05-31 14:18:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2022-05-31 14:18:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2ec3f2f0-45a0-42b7-ab91-1dbbf7075005\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-05-31 14:18:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.25.2.223\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qbpdl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qbpdl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-3-160.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:18:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:18:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:18:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-05-31 14:18:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.31.3.160,PodIP:172.25.2.223,StartTime:2022-05-31 14:18:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-05-31 14:18:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://34cb53dbf74272c4b1161e9d1cf31a52679a4fe5cb10c21cc56f8196f72f1e21,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.25.2.223,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:18:02.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2182" for this suite.
•{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","total":346,"completed":264,"skipped":5184,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:18:02.453: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:18:19.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1569" for this suite.

• [SLOW TEST:17.232 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":346,"completed":265,"skipped":5205,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:18:19.687: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 14:18:19.788: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
May 31 14:18:19.814: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 31 14:18:19.814: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched.
May 31 14:18:19.877: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 31 14:18:19.877: INFO: Node ip-172-31-3-160.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 31 14:18:20.888: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 31 14:18:20.889: INFO: Node ip-172-31-3-160.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 31 14:18:21.894: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May 31 14:18:21.894: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled
May 31 14:18:21.940: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May 31 14:18:21.940: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
May 31 14:18:22.950: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 31 14:18:22.950: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
May 31 14:18:22.974: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 31 14:18:22.974: INFO: Node ip-172-31-3-160.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 31 14:18:23.992: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 31 14:18:23.992: INFO: Node ip-172-31-3-160.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 31 14:18:24.990: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 31 14:18:24.990: INFO: Node ip-172-31-3-160.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 31 14:18:25.986: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 31 14:18:25.986: INFO: Node ip-172-31-3-160.eu-central-1.compute.internal is running 0 daemon pod, expected 1
May 31 14:18:26.987: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
May 31 14:18:26.987: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9825, will wait for the garbage collector to delete the pods
May 31 14:18:27.071: INFO: Deleting DaemonSet.extensions daemon-set took: 12.226386ms
May 31 14:18:27.172: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.19803ms
May 31 14:18:29.283: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
May 31 14:18:29.283: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
May 31 14:18:29.290: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"41973"},"items":null}

May 31 14:18:29.297: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"41973"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:18:29.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9825" for this suite.

• [SLOW TEST:9.681 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":346,"completed":266,"skipped":5251,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:18:29.369: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod busybox-afb890fa-e69c-4671-bac9-517696f9dce1 in namespace container-probe-5059
May 31 14:18:31.437: INFO: Started pod busybox-afb890fa-e69c-4671-bac9-517696f9dce1 in namespace container-probe-5059
STEP: checking the pod's current state and verifying that restartCount is present
May 31 14:18:31.444: INFO: Initial restart count of pod busybox-afb890fa-e69c-4671-bac9-517696f9dce1 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:22:33.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5059" for this suite.

• [SLOW TEST:243.919 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":267,"skipped":5266,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:22:33.289: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:22:33.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2185" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":268,"skipped":5295,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:22:33.635: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:22:37.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3412" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":346,"completed":269,"skipped":5307,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:22:37.805: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service endpoint-test2 in namespace services-6401
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6401 to expose endpoints map[]
May 31 14:22:37.905: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
May 31 14:22:38.935: INFO: successfully validated that service endpoint-test2 in namespace services-6401 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-6401
May 31 14:22:38.980: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
May 31 14:22:40.997: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6401 to expose endpoints map[pod1:[80]]
May 31 14:22:41.027: INFO: successfully validated that service endpoint-test2 in namespace services-6401 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1
May 31 14:22:41.028: INFO: Creating new exec pod
May 31 14:22:44.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-6401 exec execpodprv6h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
May 31 14:22:44.483: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
May 31 14:22:44.483: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 31 14:22:44.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-6401 exec execpodprv6h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.17.240 80'
May 31 14:22:44.881: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.17.240 80\nConnection to 10.240.17.240 80 port [tcp/http] succeeded!\n"
May 31 14:22:44.881: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-6401
May 31 14:22:44.898: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
May 31 14:22:46.923: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6401 to expose endpoints map[pod1:[80] pod2:[80]]
May 31 14:22:46.961: INFO: successfully validated that service endpoint-test2 in namespace services-6401 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2
May 31 14:22:47.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-6401 exec execpodprv6h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
May 31 14:22:48.362: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
May 31 14:22:48.362: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 31 14:22:48.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-6401 exec execpodprv6h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.17.240 80'
May 31 14:22:48.746: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.17.240 80\nConnection to 10.240.17.240 80 port [tcp/http] succeeded!\n"
May 31 14:22:48.746: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-6401
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6401 to expose endpoints map[pod2:[80]]
May 31 14:22:49.867: INFO: successfully validated that service endpoint-test2 in namespace services-6401 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2
May 31 14:22:50.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-6401 exec execpodprv6h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
May 31 14:22:51.284: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
May 31 14:22:51.284: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
May 31 14:22:51.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=services-6401 exec execpodprv6h -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.17.240 80'
May 31 14:22:51.707: INFO: stderr: "+ nc -v -t -w 2 10.240.17.240 80\n+ echo hostNameConnection to 10.240.17.240 80 port [tcp/http] succeeded!\n\n"
May 31 14:22:51.707: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-6401
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6401 to expose endpoints map[]
May 31 14:22:51.770: INFO: successfully validated that service endpoint-test2 in namespace services-6401 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:22:51.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6401" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753

• [SLOW TEST:14.039 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":346,"completed":270,"skipped":5312,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:22:51.844: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward api env vars
May 31 14:22:51.960: INFO: Waiting up to 5m0s for pod "downward-api-debefd8a-ac75-4672-93f0-6783a6222244" in namespace "downward-api-7068" to be "Succeeded or Failed"
May 31 14:22:51.971: INFO: Pod "downward-api-debefd8a-ac75-4672-93f0-6783a6222244": Phase="Pending", Reason="", readiness=false. Elapsed: 11.000299ms
May 31 14:22:53.985: INFO: Pod "downward-api-debefd8a-ac75-4672-93f0-6783a6222244": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024998192s
May 31 14:22:56.009: INFO: Pod "downward-api-debefd8a-ac75-4672-93f0-6783a6222244": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048972559s
STEP: Saw pod success
May 31 14:22:56.009: INFO: Pod "downward-api-debefd8a-ac75-4672-93f0-6783a6222244" satisfied condition "Succeeded or Failed"
May 31 14:22:56.016: INFO: Trying to get logs from node ip-172-31-3-160.eu-central-1.compute.internal pod downward-api-debefd8a-ac75-4672-93f0-6783a6222244 container dapi-container: <nil>
STEP: delete the pod
May 31 14:22:56.063: INFO: Waiting for pod downward-api-debefd8a-ac75-4672-93f0-6783a6222244 to disappear
May 31 14:22:56.071: INFO: Pod downward-api-debefd8a-ac75-4672-93f0-6783a6222244 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:22:56.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7068" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":346,"completed":271,"skipped":5334,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:22:56.093: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 pods, got 1 pods
STEP: expected 0 rs, got 1 rs
STEP: Gathering metrics
W0531 14:22:56.765118      19 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
May 31 14:22:56.765: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:22:56.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5485" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":346,"completed":272,"skipped":5390,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:22:56.796: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:22:56.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-812" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":346,"completed":273,"skipped":5407,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should delete a collection of services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:22:56.928: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should delete a collection of services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a collection of services
May 31 14:22:56.970: INFO: Creating e2e-svc-a-dpx9m
May 31 14:22:56.992: INFO: Creating e2e-svc-b-wplxn
May 31 14:22:57.012: INFO: Creating e2e-svc-c-j6wfz
STEP: deleting service collection
May 31 14:22:57.094: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:22:57.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4905" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","total":346,"completed":274,"skipped":5427,"failed":0}
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:22:57.118: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 14:22:57.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-1458 create -f -'
May 31 14:22:58.073: INFO: stderr: ""
May 31 14:22:58.073: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
May 31 14:22:58.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-1458 create -f -'
May 31 14:22:58.450: INFO: stderr: ""
May 31 14:22:58.450: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
May 31 14:22:59.464: INFO: Selector matched 1 pods for map[app:agnhost]
May 31 14:22:59.464: INFO: Found 1 / 1
May 31 14:22:59.464: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 31 14:22:59.470: INFO: Selector matched 1 pods for map[app:agnhost]
May 31 14:22:59.470: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 31 14:22:59.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-1458 describe pod agnhost-primary-2r2ps'
May 31 14:22:59.568: INFO: stderr: ""
May 31 14:22:59.568: INFO: stdout: "Name:         agnhost-primary-2r2ps\nNamespace:    kubectl-1458\nPriority:     0\nNode:         ip-172-31-8-180.eu-central-1.compute.internal/172.31.8.180\nStart Time:   Tue, 31 May 2022 14:22:58 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  cni.projectcalico.org/containerID: 2c79268de7d84563b17c32f8ad61dd9c93d531e20fe38472792010abf53cf215\n              cni.projectcalico.org/podIP: 172.25.1.57/32\n              cni.projectcalico.org/podIPs: 172.25.1.57/32\nStatus:       Running\nIP:           172.25.1.57\nIPs:\n  IP:           172.25.1.57\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://4290cbe4576c406081daaafc3a3b695b66c5da66bfbed3a0e2af4a1acebb397d\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.33\n    Image ID:       k8s.gcr.io/e2e-test-images/agnhost@sha256:5b3a9f1c71c09c00649d8374224642ff7029ce91a721ec9132e6ed45fa73fd43\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 31 May 2022 14:22:59 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-knx5j (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-knx5j:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-1458/agnhost-primary-2r2ps to ip-172-31-8-180.eu-central-1.compute.internal\n  Normal  Pulled     1s    kubelet            Container image \"k8s.gcr.io/e2e-test-images/agnhost:2.33\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    0s    kubelet            Started container agnhost-primary\n"
May 31 14:22:59.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-1458 describe rc agnhost-primary'
May 31 14:22:59.684: INFO: stderr: ""
May 31 14:22:59.684: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-1458\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.33\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  1s    replication-controller  Created pod: agnhost-primary-2r2ps\n"
May 31 14:22:59.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-1458 describe service agnhost-primary'
May 31 14:22:59.803: INFO: stderr: ""
May 31 14:22:59.803: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-1458\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.240.31.131\nIPs:               10.240.31.131\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.25.1.57:6379\nSession Affinity:  None\nEvents:            <none>\n"
May 31 14:22:59.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-1458 describe node ip-172-31-11-247.eu-central-1.compute.internal'
May 31 14:22:59.944: INFO: stderr: ""
May 31 14:22:59.944: INFO: stdout: "Name:               ip-172-31-11-247.eu-central-1.compute.internal\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=t3a.medium\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=eu-central-1\n                    failure-domain.beta.kubernetes.io/zone=eu-central-1a\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-172-31-11-247\n                    kubernetes.io/os=linux\n                    machine-controller/owned-by=860073dc-eb36-4440-b09a-204b72718b47\n                    node.kubernetes.io/instance-type=t3a.medium\n                    system/cluster=kz7h9l58lv\n                    system/project=9ncg6rpkgp\n                    topology.kubernetes.io/region=eu-central-1\n                    topology.kubernetes.io/zone=eu-central-1a\n                    v1.machine-controller.kubermatic.io/operating-system=ubuntu\n                    x-kubernetes.io/distribution=ubuntu\nAnnotations:        cluster.k8s.io/machine: kube-system/kkp-2.20.3-conformance-worker-rgbpmr-fbdd65d45-knkxm\n                    flannel.alpha.coreos.com/backend-data: {\"VNI\":1,\"VtepMAC\":\"b6:c4:b7:4a:53:69\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 172.31.11.247\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 172.31.11.247/20\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.25.0.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 31 May 2022 12:58:34 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-172-31-11-247.eu-central-1.compute.internal\n  AcquireTime:     <unset>\n  RenewTime:       Tue, 31 May 2022 14:22:59 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Tue, 31 May 2022 12:59:46 +0000   Tue, 31 May 2022 12:59:46 +0000   FlannelIsUp                  Flannel is running on this node\n  MemoryPressure       False   Tue, 31 May 2022 14:19:13 +0000   Tue, 31 May 2022 12:58:34 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Tue, 31 May 2022 14:19:13 +0000   Tue, 31 May 2022 12:58:34 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Tue, 31 May 2022 14:19:13 +0000   Tue, 31 May 2022 12:58:34 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Tue, 31 May 2022 14:19:13 +0000   Tue, 31 May 2022 12:59:25 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:   172.31.11.247\n  ExternalIP:   35.159.22.4\n  Hostname:     ip-172-31-11-247.eu-central-1.compute.internal\n  InternalDNS:  ip-172-31-11-247.eu-central-1.compute.internal\n  ExternalDNS:  ec2-35-159-22-4.eu-central-1.compute.amazonaws.com\nCapacity:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         2\n  ephemeral-storage:           25346000Ki\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      3970988Ki\n  pods:                        110\n  scheduling.k8s.io/foo:       5\nAllocatable:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         1600m\n  ephemeral-storage:           21211389914\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      3458988Ki\n  pods:                        110\n  scheduling.k8s.io/foo:       5\nSystem Info:\n  Machine ID:                 ec29d94cbcbf38f95c11c2472d40d3b8\n  System UUID:                ec29d94c-bcbf-38f9-5c11-c2472d40d3b8\n  Boot ID:                    f49bf9f4-1231-4e2c-bc4c-c6b61474c230\n  Kernel Version:             5.13.0-1025-aws\n  OS Image:                   Ubuntu 20.04.4 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.4.13\n  Kubelet Version:            v1.23.6\n  Kube-Proxy Version:         v1.23.6\nPodCIDR:                      172.25.0.0/24\nPodCIDRs:                     172.25.0.0/24\nProviderID:                   aws:///eu-central-1a/i-032e16f733fc4e19d\nNon-terminated Pods:          (11 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits   Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------   ---------------  -------------  ---\n  kube-system                 calico-kube-controllers-786b7976d6-qvmrx                   0 (0%)        0 (0%)       0 (0%)           0 (0%)         87m\n  kube-system                 canal-jkvwq                                                250m (15%)    0 (0%)       0 (0%)           0 (0%)         84m\n  kube-system                 coredns-767874cf84-bl7km                                   50m (3%)      100m (6%)    32Mi (0%)        64Mi (1%)      87m\n  kube-system                 coredns-767874cf84-wsllw                                   50m (3%)      100m (6%)    32Mi (0%)        64Mi (1%)      87m\n  kube-system                 kube-proxy-7tkx8                                           75m (4%)      250m (15%)   50Mi (1%)        250Mi (7%)     84m\n  kube-system                 node-local-dns-2f4s9                                       0 (0%)        0 (0%)       0 (0%)           0 (0%)         84m\n  kube-system                 openvpn-client-76b67b68f8-rllqf                            30m (1%)      1100m (68%)  30Mi (0%)        82Mi (2%)      87m\n  kube-system                 user-ssh-keys-agent-79f9n                                  0 (0%)        0 (0%)       0 (0%)           0 (0%)         84m\n  kubernetes-dashboard        dashboard-metrics-scraper-75d68f84c9-78rk7                 50m (3%)      100m (6%)    32Mi (0%)        64Mi (1%)      87m\n  kubernetes-dashboard        dashboard-metrics-scraper-75d68f84c9-d4cl8                 50m (3%)      100m (6%)    32Mi (0%)        64Mi (1%)      87m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-fb59706c47b44a31-jzqd2    0 (0%)        0 (0%)       0 (0%)           0 (0%)         75m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests    Limits\n  --------                    --------    ------\n  cpu                         555m (34%)  1750m (109%)\n  memory                      208Mi (6%)  588Mi (17%)\n  ephemeral-storage           0 (0%)      0 (0%)\n  hugepages-1Gi               0 (0%)      0 (0%)\n  hugepages-2Mi               0 (0%)      0 (0%)\n  attachable-volumes-aws-ebs  0           0\n  scheduling.k8s.io/foo       0           0\nEvents:                       <none>\n"
May 31 14:22:59.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-1458 describe namespace kubectl-1458'
May 31 14:23:00.063: INFO: stderr: ""
May 31 14:23:00.063: INFO: stdout: "Name:         kubectl-1458\nLabels:       e2e-framework=kubectl\n              e2e-run=77d1639e-6015-4707-8ed5-d2f0bab378c6\n              kubernetes.io/metadata.name=kubectl-1458\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:23:00.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1458" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":346,"completed":275,"skipped":5432,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:23:00.088: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Given a Pod with a 'name' label pod-adoption is created
May 31 14:23:00.200: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
May 31 14:23:02.214: INFO: The status of Pod pod-adoption is Running (Ready = true)
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:23:03.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2896" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":346,"completed":276,"skipped":5446,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:23:03.298: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:23:07.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-4469" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":277,"skipped":5460,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:23:07.546: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
May 31 14:23:07.824: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:23:07.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-611" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":346,"completed":278,"skipped":5485,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:23:07.993: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:23:08.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8281" for this suite.
STEP: Destroying namespace "nspatchtest-4a2a3b2d-bdae-447f-8570-03078afa236b-3225" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":346,"completed":279,"skipped":5513,"failed":0}
SS
------------------------------
[sig-apps] ReplicaSet 
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:23:08.260: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota
May 31 14:23:08.379: INFO: Pod name sample-pod: Found 0 pods out of 1
May 31 14:23:13.399: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the replicaset Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:23:13.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9559" for this suite.

• [SLOW TEST:5.269 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","total":346,"completed":280,"skipped":5515,"failed":0}
SSSS
------------------------------
[sig-apps] CronJob 
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:23:13.530: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a ReplaceConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring the job is replaced with a new one
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:25:01.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-2551" for this suite.

• [SLOW TEST:108.163 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","total":346,"completed":281,"skipped":5519,"failed":0}
SS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:25:01.694: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 14:25:02.711: INFO: Checking APIGroup: apiregistration.k8s.io
May 31 14:25:02.715: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
May 31 14:25:02.715: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
May 31 14:25:02.715: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
May 31 14:25:02.715: INFO: Checking APIGroup: apps
May 31 14:25:02.727: INFO: PreferredVersion.GroupVersion: apps/v1
May 31 14:25:02.727: INFO: Versions found [{apps/v1 v1}]
May 31 14:25:02.727: INFO: apps/v1 matches apps/v1
May 31 14:25:02.727: INFO: Checking APIGroup: events.k8s.io
May 31 14:25:02.732: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
May 31 14:25:02.732: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
May 31 14:25:02.732: INFO: events.k8s.io/v1 matches events.k8s.io/v1
May 31 14:25:02.732: INFO: Checking APIGroup: authentication.k8s.io
May 31 14:25:02.736: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
May 31 14:25:02.736: INFO: Versions found [{authentication.k8s.io/v1 v1}]
May 31 14:25:02.736: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
May 31 14:25:02.736: INFO: Checking APIGroup: authorization.k8s.io
May 31 14:25:02.755: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
May 31 14:25:02.755: INFO: Versions found [{authorization.k8s.io/v1 v1}]
May 31 14:25:02.755: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
May 31 14:25:02.755: INFO: Checking APIGroup: autoscaling
May 31 14:25:02.759: INFO: PreferredVersion.GroupVersion: autoscaling/v2
May 31 14:25:02.759: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
May 31 14:25:02.759: INFO: autoscaling/v2 matches autoscaling/v2
May 31 14:25:02.759: INFO: Checking APIGroup: batch
May 31 14:25:02.763: INFO: PreferredVersion.GroupVersion: batch/v1
May 31 14:25:02.763: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
May 31 14:25:02.763: INFO: batch/v1 matches batch/v1
May 31 14:25:02.763: INFO: Checking APIGroup: certificates.k8s.io
May 31 14:25:02.766: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
May 31 14:25:02.766: INFO: Versions found [{certificates.k8s.io/v1 v1}]
May 31 14:25:02.766: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
May 31 14:25:02.766: INFO: Checking APIGroup: networking.k8s.io
May 31 14:25:02.770: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
May 31 14:25:02.770: INFO: Versions found [{networking.k8s.io/v1 v1}]
May 31 14:25:02.770: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
May 31 14:25:02.770: INFO: Checking APIGroup: policy
May 31 14:25:02.773: INFO: PreferredVersion.GroupVersion: policy/v1
May 31 14:25:02.773: INFO: Versions found [{policy/v1 v1} {policy/v1beta1 v1beta1}]
May 31 14:25:02.773: INFO: policy/v1 matches policy/v1
May 31 14:25:02.773: INFO: Checking APIGroup: rbac.authorization.k8s.io
May 31 14:25:02.776: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
May 31 14:25:02.777: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
May 31 14:25:02.777: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
May 31 14:25:02.777: INFO: Checking APIGroup: storage.k8s.io
May 31 14:25:02.784: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
May 31 14:25:02.784: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
May 31 14:25:02.784: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
May 31 14:25:02.784: INFO: Checking APIGroup: admissionregistration.k8s.io
May 31 14:25:02.788: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
May 31 14:25:02.788: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
May 31 14:25:02.788: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
May 31 14:25:02.788: INFO: Checking APIGroup: apiextensions.k8s.io
May 31 14:25:02.792: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
May 31 14:25:02.793: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
May 31 14:25:02.793: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
May 31 14:25:02.793: INFO: Checking APIGroup: scheduling.k8s.io
May 31 14:25:02.798: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
May 31 14:25:02.798: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
May 31 14:25:02.798: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
May 31 14:25:02.798: INFO: Checking APIGroup: coordination.k8s.io
May 31 14:25:02.811: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
May 31 14:25:02.811: INFO: Versions found [{coordination.k8s.io/v1 v1}]
May 31 14:25:02.811: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
May 31 14:25:02.811: INFO: Checking APIGroup: node.k8s.io
May 31 14:25:02.818: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
May 31 14:25:02.819: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
May 31 14:25:02.819: INFO: node.k8s.io/v1 matches node.k8s.io/v1
May 31 14:25:02.819: INFO: Checking APIGroup: discovery.k8s.io
May 31 14:25:02.822: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
May 31 14:25:02.822: INFO: Versions found [{discovery.k8s.io/v1 v1} {discovery.k8s.io/v1beta1 v1beta1}]
May 31 14:25:02.823: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
May 31 14:25:02.823: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
May 31 14:25:02.828: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
May 31 14:25:02.828: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
May 31 14:25:02.828: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
May 31 14:25:02.828: INFO: Checking APIGroup: crd.projectcalico.org
May 31 14:25:02.842: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
May 31 14:25:02.842: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
May 31 14:25:02.842: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
May 31 14:25:02.842: INFO: Checking APIGroup: cluster.k8s.io
May 31 14:25:02.845: INFO: PreferredVersion.GroupVersion: cluster.k8s.io/v1alpha1
May 31 14:25:02.845: INFO: Versions found [{cluster.k8s.io/v1alpha1 v1alpha1}]
May 31 14:25:02.845: INFO: cluster.k8s.io/v1alpha1 matches cluster.k8s.io/v1alpha1
May 31 14:25:02.845: INFO: Checking APIGroup: metrics.k8s.io
May 31 14:25:02.854: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
May 31 14:25:02.854: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
May 31 14:25:02.854: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:25:02.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-5354" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":346,"completed":282,"skipped":5521,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:25:02.882: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward api env vars
May 31 14:25:02.977: INFO: Waiting up to 5m0s for pod "downward-api-2053083f-04d3-470e-8b96-4cb14fb84016" in namespace "downward-api-2605" to be "Succeeded or Failed"
May 31 14:25:02.993: INFO: Pod "downward-api-2053083f-04d3-470e-8b96-4cb14fb84016": Phase="Pending", Reason="", readiness=false. Elapsed: 15.803436ms
May 31 14:25:05.008: INFO: Pod "downward-api-2053083f-04d3-470e-8b96-4cb14fb84016": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031061358s
May 31 14:25:07.024: INFO: Pod "downward-api-2053083f-04d3-470e-8b96-4cb14fb84016": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047063272s
STEP: Saw pod success
May 31 14:25:07.024: INFO: Pod "downward-api-2053083f-04d3-470e-8b96-4cb14fb84016" satisfied condition "Succeeded or Failed"
May 31 14:25:07.031: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod downward-api-2053083f-04d3-470e-8b96-4cb14fb84016 container dapi-container: <nil>
STEP: delete the pod
May 31 14:25:07.088: INFO: Waiting for pod downward-api-2053083f-04d3-470e-8b96-4cb14fb84016 to disappear
May 31 14:25:07.096: INFO: Pod downward-api-2053083f-04d3-470e-8b96-4cb14fb84016 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:25:07.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2605" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":346,"completed":283,"skipped":5541,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:25:07.122: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 14:25:07.175: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:25:07.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6369" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":346,"completed":284,"skipped":5557,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:25:07.820: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-map-e19fd0c4-7efe-4e12-aa94-7daead3369f7
STEP: Creating a pod to test consume configMaps
May 31 14:25:07.975: INFO: Waiting up to 5m0s for pod "pod-configmaps-e3f4dca8-f765-4df9-84f2-4c8801f6768d" in namespace "configmap-7195" to be "Succeeded or Failed"
May 31 14:25:07.990: INFO: Pod "pod-configmaps-e3f4dca8-f765-4df9-84f2-4c8801f6768d": Phase="Pending", Reason="", readiness=false. Elapsed: 14.763371ms
May 31 14:25:09.999: INFO: Pod "pod-configmaps-e3f4dca8-f765-4df9-84f2-4c8801f6768d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023742312s
May 31 14:25:12.016: INFO: Pod "pod-configmaps-e3f4dca8-f765-4df9-84f2-4c8801f6768d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040782169s
STEP: Saw pod success
May 31 14:25:12.016: INFO: Pod "pod-configmaps-e3f4dca8-f765-4df9-84f2-4c8801f6768d" satisfied condition "Succeeded or Failed"
May 31 14:25:12.023: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod pod-configmaps-e3f4dca8-f765-4df9-84f2-4c8801f6768d container agnhost-container: <nil>
STEP: delete the pod
May 31 14:25:12.064: INFO: Waiting for pod pod-configmaps-e3f4dca8-f765-4df9-84f2-4c8801f6768d to disappear
May 31 14:25:12.072: INFO: Pod pod-configmaps-e3f4dca8-f765-4df9-84f2-4c8801f6768d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:25:12.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7195" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":285,"skipped":5558,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:25:12.115: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 31 14:25:12.200: INFO: Waiting up to 5m0s for pod "pod-072e98b6-0625-4c5b-9ddb-07ef18028c05" in namespace "emptydir-6421" to be "Succeeded or Failed"
May 31 14:25:12.214: INFO: Pod "pod-072e98b6-0625-4c5b-9ddb-07ef18028c05": Phase="Pending", Reason="", readiness=false. Elapsed: 14.519829ms
May 31 14:25:14.227: INFO: Pod "pod-072e98b6-0625-4c5b-9ddb-07ef18028c05": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027050881s
May 31 14:25:16.242: INFO: Pod "pod-072e98b6-0625-4c5b-9ddb-07ef18028c05": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041781557s
STEP: Saw pod success
May 31 14:25:16.242: INFO: Pod "pod-072e98b6-0625-4c5b-9ddb-07ef18028c05" satisfied condition "Succeeded or Failed"
May 31 14:25:16.249: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod pod-072e98b6-0625-4c5b-9ddb-07ef18028c05 container test-container: <nil>
STEP: delete the pod
May 31 14:25:16.321: INFO: Waiting for pod pod-072e98b6-0625-4c5b-9ddb-07ef18028c05 to disappear
May 31 14:25:16.328: INFO: Pod pod-072e98b6-0625-4c5b-9ddb-07ef18028c05 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:25:16.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6421" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":286,"skipped":5577,"failed":0}
SS
------------------------------
[sig-network] EndpointSlice 
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:25:16.376: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:25:16.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-1983" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","total":346,"completed":287,"skipped":5579,"failed":0}
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:25:16.654: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with secret that has name projected-secret-test-20b9f391-fdde-4bb6-b956-153628acd424
STEP: Creating a pod to test consume secrets
May 31 14:25:16.764: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-01d7f95d-fe57-42e6-83b8-1952de8c206b" in namespace "projected-6828" to be "Succeeded or Failed"
May 31 14:25:16.786: INFO: Pod "pod-projected-secrets-01d7f95d-fe57-42e6-83b8-1952de8c206b": Phase="Pending", Reason="", readiness=false. Elapsed: 21.414969ms
May 31 14:25:18.809: INFO: Pod "pod-projected-secrets-01d7f95d-fe57-42e6-83b8-1952de8c206b": Phase="Running", Reason="", readiness=false. Elapsed: 2.044618959s
May 31 14:25:20.820: INFO: Pod "pod-projected-secrets-01d7f95d-fe57-42e6-83b8-1952de8c206b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.056223654s
STEP: Saw pod success
May 31 14:25:20.820: INFO: Pod "pod-projected-secrets-01d7f95d-fe57-42e6-83b8-1952de8c206b" satisfied condition "Succeeded or Failed"
May 31 14:25:20.830: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod pod-projected-secrets-01d7f95d-fe57-42e6-83b8-1952de8c206b container projected-secret-volume-test: <nil>
STEP: delete the pod
May 31 14:25:20.869: INFO: Waiting for pod pod-projected-secrets-01d7f95d-fe57-42e6-83b8-1952de8c206b to disappear
May 31 14:25:20.879: INFO: Pod pod-projected-secrets-01d7f95d-fe57-42e6-83b8-1952de8c206b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:25:20.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6828" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":288,"skipped":5580,"failed":0}

------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:25:20.917: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:25:27.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6458" for this suite.
STEP: Destroying namespace "nsdeletetest-3301" for this suite.
May 31 14:25:27.206: INFO: Namespace nsdeletetest-3301 was already deleted
STEP: Destroying namespace "nsdeletetest-8655" for this suite.

• [SLOW TEST:6.299 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":346,"completed":289,"skipped":5580,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:25:27.216: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod with one valid and two invalid sysctls
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:25:27.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-3338" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":290,"skipped":5593,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:25:27.378: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod liveness-c38cfd22-6f15-4d46-9144-366edf3fc5cc in namespace container-probe-9303
May 31 14:25:29.482: INFO: Started pod liveness-c38cfd22-6f15-4d46-9144-366edf3fc5cc in namespace container-probe-9303
STEP: checking the pod's current state and verifying that restartCount is present
May 31 14:25:29.490: INFO: Initial restart count of pod liveness-c38cfd22-6f15-4d46-9144-366edf3fc5cc is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:29:31.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9303" for this suite.

• [SLOW TEST:244.004 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":346,"completed":291,"skipped":5609,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:29:31.383: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: set up a multi version CRD
May 31 14:29:31.427: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:29:54.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8289" for this suite.

• [SLOW TEST:23.029 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":346,"completed":292,"skipped":5616,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:29:54.416: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:30:54.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7596" for this suite.

• [SLOW TEST:60.122 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":346,"completed":293,"skipped":5625,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:30:54.538: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
May 31 14:30:56.745: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:30:58.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-4485" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","total":346,"completed":294,"skipped":5685,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:30:58.870: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-f061e188-7cc1-4579-a931-1e90811b3a0c
STEP: Creating a pod to test consume secrets
May 31 14:30:58.956: INFO: Waiting up to 5m0s for pod "pod-secrets-0d95093c-745d-428c-88fb-bbc3ee863dcd" in namespace "secrets-7133" to be "Succeeded or Failed"
May 31 14:30:58.965: INFO: Pod "pod-secrets-0d95093c-745d-428c-88fb-bbc3ee863dcd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.973613ms
May 31 14:31:00.974: INFO: Pod "pod-secrets-0d95093c-745d-428c-88fb-bbc3ee863dcd": Phase="Running", Reason="", readiness=true. Elapsed: 2.017882354s
May 31 14:31:02.984: INFO: Pod "pod-secrets-0d95093c-745d-428c-88fb-bbc3ee863dcd": Phase="Running", Reason="", readiness=false. Elapsed: 4.028045089s
May 31 14:31:04.997: INFO: Pod "pod-secrets-0d95093c-745d-428c-88fb-bbc3ee863dcd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041077855s
STEP: Saw pod success
May 31 14:31:04.997: INFO: Pod "pod-secrets-0d95093c-745d-428c-88fb-bbc3ee863dcd" satisfied condition "Succeeded or Failed"
May 31 14:31:05.003: INFO: Trying to get logs from node ip-172-31-3-160.eu-central-1.compute.internal pod pod-secrets-0d95093c-745d-428c-88fb-bbc3ee863dcd container secret-volume-test: <nil>
STEP: delete the pod
May 31 14:31:05.053: INFO: Waiting for pod pod-secrets-0d95093c-745d-428c-88fb-bbc3ee863dcd to disappear
May 31 14:31:05.062: INFO: Pod pod-secrets-0d95093c-745d-428c-88fb-bbc3ee863dcd no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:31:05.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7133" for this suite.

• [SLOW TEST:6.221 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":295,"skipped":5708,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:31:05.124: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Performing setup for networking test in namespace pod-network-test-9137
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 31 14:31:05.180: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 31 14:31:05.259: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 31 14:31:07.272: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 31 14:31:09.275: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 31 14:31:11.273: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 31 14:31:13.270: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 31 14:31:15.273: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 31 14:31:17.271: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 31 14:31:19.280: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 31 14:31:21.275: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 31 14:31:23.272: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 31 14:31:25.280: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 31 14:31:27.276: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 31 14:31:27.294: INFO: The status of Pod netserver-1 is Running (Ready = true)
May 31 14:31:27.309: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
May 31 14:31:29.410: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May 31 14:31:29.410: INFO: Going to poll 172.25.0.136 on port 8081 at least 0 times, with a maximum of 39 tries before failing
May 31 14:31:29.417: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.0.136 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9137 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 31 14:31:29.417: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
May 31 14:31:29.418: INFO: ExecWithOptions: Clientset creation
May 31 14:31:29.418: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-9137/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.25.0.136+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
May 31 14:31:30.686: INFO: Found all 1 expected endpoints: [netserver-0]
May 31 14:31:30.686: INFO: Going to poll 172.25.2.233 on port 8081 at least 0 times, with a maximum of 39 tries before failing
May 31 14:31:30.697: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.2.233 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9137 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 31 14:31:30.698: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
May 31 14:31:30.701: INFO: ExecWithOptions: Clientset creation
May 31 14:31:30.701: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-9137/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.25.2.233+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
May 31 14:31:32.086: INFO: Found all 1 expected endpoints: [netserver-1]
May 31 14:31:32.086: INFO: Going to poll 172.25.1.69 on port 8081 at least 0 times, with a maximum of 39 tries before failing
May 31 14:31:32.100: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.1.69 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9137 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 31 14:31:32.100: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
May 31 14:31:32.101: INFO: ExecWithOptions: Clientset creation
May 31 14:31:32.101: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-9137/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.25.1.69+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
May 31 14:31:33.450: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:31:33.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9137" for this suite.

• [SLOW TEST:28.378 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":296,"skipped":5848,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:31:33.510: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:31:33.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5078" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","total":346,"completed":297,"skipped":5860,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:31:33.733: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:31:44.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7694" for this suite.

• [SLOW TEST:11.208 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":346,"completed":298,"skipped":5881,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:31:44.943: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 31 14:31:45.942: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 31 14:31:48.982: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:31:59.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9410" for this suite.
STEP: Destroying namespace "webhook-9410-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:14.768 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":346,"completed":299,"skipped":5892,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:31:59.712: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting /apis
STEP: getting /apis/discovery.k8s.io
STEP: getting /apis/discovery.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 31 14:31:59.860: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
May 31 14:31:59.879: INFO: starting watch
STEP: patching
STEP: updating
May 31 14:31:59.914: INFO: waiting for watch events with expected annotations
May 31 14:31:59.914: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:31:59.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-37" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","total":346,"completed":300,"skipped":5918,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:32:00.019: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod pod-subpath-test-projected-bnk4
STEP: Creating a pod to test atomic-volume-subpath
May 31 14:32:00.143: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-bnk4" in namespace "subpath-703" to be "Succeeded or Failed"
May 31 14:32:00.171: INFO: Pod "pod-subpath-test-projected-bnk4": Phase="Pending", Reason="", readiness=false. Elapsed: 27.286364ms
May 31 14:32:02.196: INFO: Pod "pod-subpath-test-projected-bnk4": Phase="Running", Reason="", readiness=true. Elapsed: 2.052712229s
May 31 14:32:04.208: INFO: Pod "pod-subpath-test-projected-bnk4": Phase="Running", Reason="", readiness=true. Elapsed: 4.064267033s
May 31 14:32:06.222: INFO: Pod "pod-subpath-test-projected-bnk4": Phase="Running", Reason="", readiness=true. Elapsed: 6.078068446s
May 31 14:32:08.233: INFO: Pod "pod-subpath-test-projected-bnk4": Phase="Running", Reason="", readiness=true. Elapsed: 8.088972508s
May 31 14:32:10.244: INFO: Pod "pod-subpath-test-projected-bnk4": Phase="Running", Reason="", readiness=true. Elapsed: 10.100054611s
May 31 14:32:12.258: INFO: Pod "pod-subpath-test-projected-bnk4": Phase="Running", Reason="", readiness=true. Elapsed: 12.113978267s
May 31 14:32:14.269: INFO: Pod "pod-subpath-test-projected-bnk4": Phase="Running", Reason="", readiness=true. Elapsed: 14.12518925s
May 31 14:32:16.284: INFO: Pod "pod-subpath-test-projected-bnk4": Phase="Running", Reason="", readiness=true. Elapsed: 16.140166109s
May 31 14:32:18.298: INFO: Pod "pod-subpath-test-projected-bnk4": Phase="Running", Reason="", readiness=true. Elapsed: 18.154190905s
May 31 14:32:20.311: INFO: Pod "pod-subpath-test-projected-bnk4": Phase="Running", Reason="", readiness=true. Elapsed: 20.167525808s
May 31 14:32:22.327: INFO: Pod "pod-subpath-test-projected-bnk4": Phase="Running", Reason="", readiness=false. Elapsed: 22.182914984s
May 31 14:32:24.338: INFO: Pod "pod-subpath-test-projected-bnk4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.194417284s
STEP: Saw pod success
May 31 14:32:24.338: INFO: Pod "pod-subpath-test-projected-bnk4" satisfied condition "Succeeded or Failed"
May 31 14:32:24.346: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod pod-subpath-test-projected-bnk4 container test-container-subpath-projected-bnk4: <nil>
STEP: delete the pod
May 31 14:32:24.394: INFO: Waiting for pod pod-subpath-test-projected-bnk4 to disappear
May 31 14:32:24.401: INFO: Pod pod-subpath-test-projected-bnk4 no longer exists
STEP: Deleting pod pod-subpath-test-projected-bnk4
May 31 14:32:24.402: INFO: Deleting pod "pod-subpath-test-projected-bnk4" in namespace "subpath-703"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:32:24.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-703" for this suite.

• [SLOW TEST:24.420 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":301,"skipped":5933,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:32:24.439: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
May 31 14:32:24.542: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-916  4d629b9a-7103-4eee-afc4-864e1ed44533 46406 0 2022-05-31 14:32:24 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-05-31 14:32:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
May 31 14:32:24.542: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-916  4d629b9a-7103-4eee-afc4-864e1ed44533 46407 0 2022-05-31 14:32:24 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-05-31 14:32:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
May 31 14:32:24.584: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-916  4d629b9a-7103-4eee-afc4-864e1ed44533 46409 0 2022-05-31 14:32:24 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-05-31 14:32:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
May 31 14:32:24.584: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-916  4d629b9a-7103-4eee-afc4-864e1ed44533 46410 0 2022-05-31 14:32:24 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-05-31 14:32:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:32:24.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-916" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":346,"completed":302,"skipped":5955,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:32:24.606: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 31 14:32:24.681: INFO: Waiting up to 5m0s for pod "pod-c1234fec-980f-4143-9587-9ca38436c02a" in namespace "emptydir-3928" to be "Succeeded or Failed"
May 31 14:32:24.690: INFO: Pod "pod-c1234fec-980f-4143-9587-9ca38436c02a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.14652ms
May 31 14:32:26.701: INFO: Pod "pod-c1234fec-980f-4143-9587-9ca38436c02a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020316967s
May 31 14:32:28.710: INFO: Pod "pod-c1234fec-980f-4143-9587-9ca38436c02a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029051461s
STEP: Saw pod success
May 31 14:32:28.710: INFO: Pod "pod-c1234fec-980f-4143-9587-9ca38436c02a" satisfied condition "Succeeded or Failed"
May 31 14:32:28.718: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod pod-c1234fec-980f-4143-9587-9ca38436c02a container test-container: <nil>
STEP: delete the pod
May 31 14:32:28.757: INFO: Waiting for pod pod-c1234fec-980f-4143-9587-9ca38436c02a to disappear
May 31 14:32:28.764: INFO: Pod pod-c1234fec-980f-4143-9587-9ca38436c02a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:32:28.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3928" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":303,"skipped":5983,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:32:28.804: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test override arguments
May 31 14:32:28.899: INFO: Waiting up to 5m0s for pod "client-containers-8d32ec85-baea-496f-8b4e-7b12a6586340" in namespace "containers-7929" to be "Succeeded or Failed"
May 31 14:32:28.922: INFO: Pod "client-containers-8d32ec85-baea-496f-8b4e-7b12a6586340": Phase="Pending", Reason="", readiness=false. Elapsed: 22.699518ms
May 31 14:32:30.932: INFO: Pod "client-containers-8d32ec85-baea-496f-8b4e-7b12a6586340": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032687803s
May 31 14:32:32.948: INFO: Pod "client-containers-8d32ec85-baea-496f-8b4e-7b12a6586340": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048373849s
STEP: Saw pod success
May 31 14:32:32.948: INFO: Pod "client-containers-8d32ec85-baea-496f-8b4e-7b12a6586340" satisfied condition "Succeeded or Failed"
May 31 14:32:32.954: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod client-containers-8d32ec85-baea-496f-8b4e-7b12a6586340 container agnhost-container: <nil>
STEP: delete the pod
May 31 14:32:33.039: INFO: Waiting for pod client-containers-8d32ec85-baea-496f-8b4e-7b12a6586340 to disappear
May 31 14:32:33.056: INFO: Pod client-containers-8d32ec85-baea-496f-8b4e-7b12a6586340 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:32:33.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7929" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":346,"completed":304,"skipped":6010,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:32:33.085: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:32:40.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1040" for this suite.

• [SLOW TEST:7.117 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":346,"completed":305,"skipped":6018,"failed":0}
SSSSS
------------------------------
[sig-apps] CronJob 
  should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:32:40.203: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a cronjob
STEP: creating
STEP: getting
STEP: listing
STEP: watching
May 31 14:32:40.313: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
May 31 14:32:40.337: INFO: starting watch
STEP: patching
STEP: updating
May 31 14:32:40.395: INFO: waiting for watch events with expected annotations
May 31 14:32:40.396: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:32:40.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-6948" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","total":346,"completed":306,"skipped":6023,"failed":0}
S
------------------------------
[sig-node] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:32:40.586: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 14:32:40.649: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-655fa34c-e617-44e1-a181-f76f5d24b636" in namespace "security-context-test-1304" to be "Succeeded or Failed"
May 31 14:32:40.657: INFO: Pod "busybox-privileged-false-655fa34c-e617-44e1-a181-f76f5d24b636": Phase="Pending", Reason="", readiness=false. Elapsed: 8.275236ms
May 31 14:32:42.672: INFO: Pod "busybox-privileged-false-655fa34c-e617-44e1-a181-f76f5d24b636": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022938353s
May 31 14:32:44.684: INFO: Pod "busybox-privileged-false-655fa34c-e617-44e1-a181-f76f5d24b636": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03468503s
May 31 14:32:44.684: INFO: Pod "busybox-privileged-false-655fa34c-e617-44e1-a181-f76f5d24b636" satisfied condition "Succeeded or Failed"
May 31 14:32:44.759: INFO: Got logs for pod "busybox-privileged-false-655fa34c-e617-44e1-a181-f76f5d24b636": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:32:44.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1304" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":307,"skipped":6024,"failed":0}

------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:32:44.788: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 31 14:32:44.877: INFO: Waiting up to 5m0s for pod "downwardapi-volume-19f8c53c-aa17-4096-b06a-53ac5d45c9ea" in namespace "projected-887" to be "Succeeded or Failed"
May 31 14:32:44.888: INFO: Pod "downwardapi-volume-19f8c53c-aa17-4096-b06a-53ac5d45c9ea": Phase="Pending", Reason="", readiness=false. Elapsed: 10.693878ms
May 31 14:32:46.900: INFO: Pod "downwardapi-volume-19f8c53c-aa17-4096-b06a-53ac5d45c9ea": Phase="Running", Reason="", readiness=false. Elapsed: 2.022793746s
May 31 14:32:48.911: INFO: Pod "downwardapi-volume-19f8c53c-aa17-4096-b06a-53ac5d45c9ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033638895s
STEP: Saw pod success
May 31 14:32:48.911: INFO: Pod "downwardapi-volume-19f8c53c-aa17-4096-b06a-53ac5d45c9ea" satisfied condition "Succeeded or Failed"
May 31 14:32:48.918: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod downwardapi-volume-19f8c53c-aa17-4096-b06a-53ac5d45c9ea container client-container: <nil>
STEP: delete the pod
May 31 14:32:48.953: INFO: Waiting for pod downwardapi-volume-19f8c53c-aa17-4096-b06a-53ac5d45c9ea to disappear
May 31 14:32:48.960: INFO: Pod downwardapi-volume-19f8c53c-aa17-4096-b06a-53ac5d45c9ea no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:32:48.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-887" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":308,"skipped":6024,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:32:48.984: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:32:49.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-8201" for this suite.
•{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","total":346,"completed":309,"skipped":6045,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:32:49.197: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Performing setup for networking test in namespace pod-network-test-3097
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 31 14:32:49.248: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
May 31 14:32:49.317: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
May 31 14:32:51.327: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 31 14:32:53.331: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 31 14:32:55.332: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 31 14:32:57.329: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 31 14:32:59.330: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 31 14:33:01.327: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 31 14:33:03.330: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 31 14:33:05.327: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 31 14:33:07.328: INFO: The status of Pod netserver-0 is Running (Ready = false)
May 31 14:33:09.329: INFO: The status of Pod netserver-0 is Running (Ready = true)
May 31 14:33:09.348: INFO: The status of Pod netserver-1 is Running (Ready = true)
May 31 14:33:09.376: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
May 31 14:33:11.419: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
May 31 14:33:11.419: INFO: Breadth first check of 172.25.0.137 on host 172.31.11.247...
May 31 14:33:11.425: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.78:9080/dial?request=hostname&protocol=udp&host=172.25.0.137&port=8081&tries=1'] Namespace:pod-network-test-3097 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 31 14:33:11.426: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
May 31 14:33:11.426: INFO: ExecWithOptions: Clientset creation
May 31 14:33:11.427: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-3097/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.1.78%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.25.0.137%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
May 31 14:33:11.751: INFO: Waiting for responses: map[]
May 31 14:33:11.751: INFO: reached 172.25.0.137 after 0/1 tries
May 31 14:33:11.751: INFO: Breadth first check of 172.25.2.234 on host 172.31.3.160...
May 31 14:33:11.760: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.78:9080/dial?request=hostname&protocol=udp&host=172.25.2.234&port=8081&tries=1'] Namespace:pod-network-test-3097 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 31 14:33:11.760: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
May 31 14:33:11.761: INFO: ExecWithOptions: Clientset creation
May 31 14:33:11.761: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-3097/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.1.78%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.25.2.234%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
May 31 14:33:12.015: INFO: Waiting for responses: map[]
May 31 14:33:12.016: INFO: reached 172.25.2.234 after 0/1 tries
May 31 14:33:12.016: INFO: Breadth first check of 172.25.1.77 on host 172.31.8.180...
May 31 14:33:12.028: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.78:9080/dial?request=hostname&protocol=udp&host=172.25.1.77&port=8081&tries=1'] Namespace:pod-network-test-3097 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
May 31 14:33:12.028: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
May 31 14:33:12.029: INFO: ExecWithOptions: Clientset creation
May 31 14:33:12.029: INFO: ExecWithOptions: execute(POST https://10.240.16.1:443/api/v1/namespaces/pod-network-test-3097/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.25.1.78%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.25.1.77%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
May 31 14:33:12.382: INFO: Waiting for responses: map[]
May 31 14:33:12.382: INFO: reached 172.25.1.77 after 0/1 tries
May 31 14:33:12.382: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:33:12.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3097" for this suite.

• [SLOW TEST:23.216 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":346,"completed":310,"skipped":6072,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:33:12.419: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 31 14:33:12.515: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ea7ed873-a63b-47be-bc79-8e80b49c8c2a" in namespace "downward-api-1813" to be "Succeeded or Failed"
May 31 14:33:12.528: INFO: Pod "downwardapi-volume-ea7ed873-a63b-47be-bc79-8e80b49c8c2a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.713085ms
May 31 14:33:14.552: INFO: Pod "downwardapi-volume-ea7ed873-a63b-47be-bc79-8e80b49c8c2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036405747s
May 31 14:33:16.566: INFO: Pod "downwardapi-volume-ea7ed873-a63b-47be-bc79-8e80b49c8c2a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049969685s
STEP: Saw pod success
May 31 14:33:16.566: INFO: Pod "downwardapi-volume-ea7ed873-a63b-47be-bc79-8e80b49c8c2a" satisfied condition "Succeeded or Failed"
May 31 14:33:16.574: INFO: Trying to get logs from node ip-172-31-3-160.eu-central-1.compute.internal pod downwardapi-volume-ea7ed873-a63b-47be-bc79-8e80b49c8c2a container client-container: <nil>
STEP: delete the pod
May 31 14:33:16.671: INFO: Waiting for pod downwardapi-volume-ea7ed873-a63b-47be-bc79-8e80b49c8c2a to disappear
May 31 14:33:16.679: INFO: Pod downwardapi-volume-ea7ed873-a63b-47be-bc79-8e80b49c8c2a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:33:16.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1813" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":311,"skipped":6118,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:33:16.706: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0666 on node default medium
May 31 14:33:16.770: INFO: Waiting up to 5m0s for pod "pod-4967af5c-7a64-46e2-8b03-3a82b52ac805" in namespace "emptydir-6870" to be "Succeeded or Failed"
May 31 14:33:16.777: INFO: Pod "pod-4967af5c-7a64-46e2-8b03-3a82b52ac805": Phase="Pending", Reason="", readiness=false. Elapsed: 6.931173ms
May 31 14:33:18.786: INFO: Pod "pod-4967af5c-7a64-46e2-8b03-3a82b52ac805": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015897175s
May 31 14:33:20.800: INFO: Pod "pod-4967af5c-7a64-46e2-8b03-3a82b52ac805": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0300772s
STEP: Saw pod success
May 31 14:33:20.800: INFO: Pod "pod-4967af5c-7a64-46e2-8b03-3a82b52ac805" satisfied condition "Succeeded or Failed"
May 31 14:33:20.808: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod pod-4967af5c-7a64-46e2-8b03-3a82b52ac805 container test-container: <nil>
STEP: delete the pod
May 31 14:33:20.857: INFO: Waiting for pod pod-4967af5c-7a64-46e2-8b03-3a82b52ac805 to disappear
May 31 14:33:20.863: INFO: Pod pod-4967af5c-7a64-46e2-8b03-3a82b52ac805 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:33:20.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6870" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":312,"skipped":6142,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:33:20.897: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-143c6f98-26db-41e7-a85e-f81ea7278d04
STEP: Creating a pod to test consume configMaps
May 31 14:33:21.013: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d62cfb4b-de37-4b6a-8314-d51a1fb0375c" in namespace "projected-5265" to be "Succeeded or Failed"
May 31 14:33:21.040: INFO: Pod "pod-projected-configmaps-d62cfb4b-de37-4b6a-8314-d51a1fb0375c": Phase="Pending", Reason="", readiness=false. Elapsed: 26.916887ms
May 31 14:33:23.052: INFO: Pod "pod-projected-configmaps-d62cfb4b-de37-4b6a-8314-d51a1fb0375c": Phase="Running", Reason="", readiness=false. Elapsed: 2.038736247s
May 31 14:33:25.064: INFO: Pod "pod-projected-configmaps-d62cfb4b-de37-4b6a-8314-d51a1fb0375c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050430589s
STEP: Saw pod success
May 31 14:33:25.064: INFO: Pod "pod-projected-configmaps-d62cfb4b-de37-4b6a-8314-d51a1fb0375c" satisfied condition "Succeeded or Failed"
May 31 14:33:25.071: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod pod-projected-configmaps-d62cfb4b-de37-4b6a-8314-d51a1fb0375c container agnhost-container: <nil>
STEP: delete the pod
May 31 14:33:25.111: INFO: Waiting for pod pod-projected-configmaps-d62cfb4b-de37-4b6a-8314-d51a1fb0375c to disappear
May 31 14:33:25.117: INFO: Pod pod-projected-configmaps-d62cfb4b-de37-4b6a-8314-d51a1fb0375c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:33:25.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5265" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":313,"skipped":6148,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:33:25.141: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
May 31 14:33:25.202: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering the sample API server.
May 31 14:33:26.182: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
May 31 14:33:28.273: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 31, 14, 33, 26, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 31, 14, 33, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 31, 14, 33, 26, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 31, 14, 33, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 31 14:33:30.282: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 31, 14, 33, 26, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 31, 14, 33, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 31, 14, 33, 26, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 31, 14, 33, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 31 14:33:32.286: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 31, 14, 33, 26, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 31, 14, 33, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 31, 14, 33, 26, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 31, 14, 33, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 31 14:33:34.291: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 31, 14, 33, 26, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 31, 14, 33, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 31, 14, 33, 26, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 31, 14, 33, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 31 14:33:36.283: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 31, 14, 33, 26, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 31, 14, 33, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 31, 14, 33, 26, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 31, 14, 33, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 31 14:33:38.286: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 31, 14, 33, 26, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 31, 14, 33, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 31, 14, 33, 26, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 31, 14, 33, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 31 14:33:40.283: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 31, 14, 33, 26, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 31, 14, 33, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 31, 14, 33, 26, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 31, 14, 33, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 31 14:33:42.288: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 31, 14, 33, 26, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 31, 14, 33, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 31, 14, 33, 26, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 31, 14, 33, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 31 14:33:44.282: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 31, 14, 33, 26, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 31, 14, 33, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 31, 14, 33, 26, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 31, 14, 33, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 31 14:33:48.622: INFO: Waited 2.31755811s for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}'
STEP: List APIServices
May 31 14:33:49.069: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:33:49.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-1329" for this suite.

• [SLOW TEST:24.344 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":346,"completed":314,"skipped":6156,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:33:49.486: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-map-bca8fd13-dddd-4e13-b477-20caf49b06d6
STEP: Creating a pod to test consume secrets
May 31 14:33:49.567: INFO: Waiting up to 5m0s for pod "pod-secrets-17a3b615-ca10-4f8e-ab43-129915e9de0f" in namespace "secrets-4903" to be "Succeeded or Failed"
May 31 14:33:49.578: INFO: Pod "pod-secrets-17a3b615-ca10-4f8e-ab43-129915e9de0f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.06303ms
May 31 14:33:51.588: INFO: Pod "pod-secrets-17a3b615-ca10-4f8e-ab43-129915e9de0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020054824s
May 31 14:33:53.608: INFO: Pod "pod-secrets-17a3b615-ca10-4f8e-ab43-129915e9de0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040314141s
STEP: Saw pod success
May 31 14:33:53.608: INFO: Pod "pod-secrets-17a3b615-ca10-4f8e-ab43-129915e9de0f" satisfied condition "Succeeded or Failed"
May 31 14:33:53.620: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod pod-secrets-17a3b615-ca10-4f8e-ab43-129915e9de0f container secret-volume-test: <nil>
STEP: delete the pod
May 31 14:33:53.703: INFO: Waiting for pod pod-secrets-17a3b615-ca10-4f8e-ab43-129915e9de0f to disappear
May 31 14:33:53.711: INFO: Pod pod-secrets-17a3b615-ca10-4f8e-ab43-129915e9de0f no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:33:53.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4903" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":315,"skipped":6170,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:33:53.734: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:33:53.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8021" for this suite.
•{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","total":346,"completed":316,"skipped":6190,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:33:53.881: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 31 14:33:54.772: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 31 14:33:57.816: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:33:58.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9011" for this suite.
STEP: Destroying namespace "webhook-9011-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":346,"completed":317,"skipped":6196,"failed":0}
SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:33:58.505: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
May 31 14:33:58.601: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:33:58.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6862" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":346,"completed":318,"skipped":6203,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:33:58.682: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
May 31 14:33:58.751: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
May 31 14:34:01.788: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:34:18.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1557" for this suite.

• [SLOW TEST:19.884 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":346,"completed":319,"skipped":6227,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:34:18.570: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
May 31 14:34:19.942: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 31 14:34:22.990: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 14:34:23.001: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:34:26.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-453" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:8.045 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":346,"completed":320,"skipped":6265,"failed":0}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:34:26.616: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:34:42.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1446" for this suite.

• [SLOW TEST:16.300 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":346,"completed":321,"skipped":6269,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:34:42.918: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
May 31 14:34:42.978: INFO: PodSpec: initContainers in spec.initContainers
May 31 14:35:23.298: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-22625306-c961-4d43-b92b-ca880cdc3afa", GenerateName:"", Namespace:"init-container-9078", SelfLink:"", UID:"45a3d86a-9ea6-4bde-98da-3ddce80cffe7", ResourceVersion:"47958", Generation:0, CreationTimestamp:time.Date(2022, time.May, 31, 14, 34, 42, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"978769981"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"467764df4cbe74f4cea291b1e420c23d99a6371d563ea6395d9fa76ff4c10b3d", "cni.projectcalico.org/podIP":"172.25.1.85/32", "cni.projectcalico.org/podIPs":"172.25.1.85/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.May, 31, 14, 34, 42, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc006b153b0), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.May, 31, 14, 34, 43, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc006b153e0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.May, 31, 14, 34, 44, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc006b15410), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-64dn7", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0031eb560), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-64dn7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-64dn7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.6", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-64dn7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004374720), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-172-31-8-180.eu-central-1.compute.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002db9e30), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0043747b0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0043747d0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0043747d8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0043747dc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc004e16260), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.May, 31, 14, 34, 43, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.May, 31, 14, 34, 43, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.May, 31, 14, 34, 43, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.May, 31, 14, 34, 42, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.31.8.180", PodIP:"172.25.1.85", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.25.1.85"}}, StartTime:time.Date(2022, time.May, 31, 14, 34, 43, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002db9f10)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002db9f80)}, Ready:false, RestartCount:3, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"k8s.gcr.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://b74e206ae6b7aa38223f7c84a8c68ba40b0b90aa718d9565962a86237ed75909", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0031eb6c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0031eb640), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.6", ImageID:"", ContainerID:"", Started:(*bool)(0xc00437485f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:35:23.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9078" for this suite.

• [SLOW TEST:40.417 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":346,"completed":322,"skipped":6282,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:35:23.335: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test substitution in container's args
May 31 14:35:23.510: INFO: Waiting up to 5m0s for pod "var-expansion-1ae427dc-ff91-47f7-a29f-7d6490900879" in namespace "var-expansion-7623" to be "Succeeded or Failed"
May 31 14:35:23.537: INFO: Pod "var-expansion-1ae427dc-ff91-47f7-a29f-7d6490900879": Phase="Pending", Reason="", readiness=false. Elapsed: 27.593172ms
May 31 14:35:25.548: INFO: Pod "var-expansion-1ae427dc-ff91-47f7-a29f-7d6490900879": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038814399s
May 31 14:35:27.562: INFO: Pod "var-expansion-1ae427dc-ff91-47f7-a29f-7d6490900879": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052280569s
STEP: Saw pod success
May 31 14:35:27.562: INFO: Pod "var-expansion-1ae427dc-ff91-47f7-a29f-7d6490900879" satisfied condition "Succeeded or Failed"
May 31 14:35:27.572: INFO: Trying to get logs from node ip-172-31-3-160.eu-central-1.compute.internal pod var-expansion-1ae427dc-ff91-47f7-a29f-7d6490900879 container dapi-container: <nil>
STEP: delete the pod
May 31 14:35:27.656: INFO: Waiting for pod var-expansion-1ae427dc-ff91-47f7-a29f-7d6490900879 to disappear
May 31 14:35:27.664: INFO: Pod var-expansion-1ae427dc-ff91-47f7-a29f-7d6490900879 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:35:27.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7623" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":346,"completed":323,"skipped":6310,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:35:27.715: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1539
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
May 31 14:35:27.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-2992 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2'
May 31 14:35:28.861: INFO: stderr: ""
May 31 14:35:28.861: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1543
May 31 14:35:28.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=kubectl-2992 delete pods e2e-test-httpd-pod'
May 31 14:35:31.777: INFO: stderr: ""
May 31 14:35:31.777: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:35:31.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2992" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":346,"completed":324,"skipped":6368,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:35:31.802: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
May 31 14:35:32.222: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
May 31 14:35:34.250: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.May, 31, 14, 35, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 31, 14, 35, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.May, 31, 14, 35, 32, 0, time.Local), LastTransitionTime:time.Date(2022, time.May, 31, 14, 35, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-78948c58f6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
May 31 14:35:37.290: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 14:35:37.312: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4196-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:35:40.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1716" for this suite.
STEP: Destroying namespace "webhook-1716-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.422 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":346,"completed":325,"skipped":6377,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:35:41.224: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 31 14:35:41.323: INFO: Waiting up to 5m0s for pod "pod-fc761bff-de4c-428f-9a6f-1f7f4b7aa2f6" in namespace "emptydir-4116" to be "Succeeded or Failed"
May 31 14:35:41.336: INFO: Pod "pod-fc761bff-de4c-428f-9a6f-1f7f4b7aa2f6": Phase="Pending", Reason="", readiness=false. Elapsed: 12.972517ms
May 31 14:35:43.355: INFO: Pod "pod-fc761bff-de4c-428f-9a6f-1f7f4b7aa2f6": Phase="Running", Reason="", readiness=true. Elapsed: 2.031333305s
May 31 14:35:45.368: INFO: Pod "pod-fc761bff-de4c-428f-9a6f-1f7f4b7aa2f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045198856s
STEP: Saw pod success
May 31 14:35:45.368: INFO: Pod "pod-fc761bff-de4c-428f-9a6f-1f7f4b7aa2f6" satisfied condition "Succeeded or Failed"
May 31 14:35:45.376: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod pod-fc761bff-de4c-428f-9a6f-1f7f4b7aa2f6 container test-container: <nil>
STEP: delete the pod
May 31 14:35:45.466: INFO: Waiting for pod pod-fc761bff-de4c-428f-9a6f-1f7f4b7aa2f6 to disappear
May 31 14:35:45.479: INFO: Pod pod-fc761bff-de4c-428f-9a6f-1f7f4b7aa2f6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:35:45.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4116" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":326,"skipped":6384,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:35:45.511: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
May 31 14:35:45.624: INFO: Waiting up to 1m0s for all nodes to be ready
May 31 14:36:45.698: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create pods that use 4/5 of node resources.
May 31 14:36:45.744: INFO: Created pod: pod0-0-sched-preemption-low-priority
May 31 14:36:45.755: INFO: Created pod: pod0-1-sched-preemption-medium-priority
May 31 14:36:45.784: INFO: Created pod: pod1-0-sched-preemption-medium-priority
May 31 14:36:45.801: INFO: Created pod: pod1-1-sched-preemption-medium-priority
May 31 14:36:45.851: INFO: Created pod: pod2-0-sched-preemption-medium-priority
May 31 14:36:45.863: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:36:54.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-242" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:68.713 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":346,"completed":327,"skipped":6402,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:36:54.228: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:37:06.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-38" for this suite.

• [SLOW TEST:12.123 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":346,"completed":328,"skipped":6424,"failed":0}
SS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:37:06.353: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test override all
May 31 14:37:06.431: INFO: Waiting up to 5m0s for pod "client-containers-0b3166ff-5a19-4c14-b37f-6791392a93d8" in namespace "containers-5016" to be "Succeeded or Failed"
May 31 14:37:06.454: INFO: Pod "client-containers-0b3166ff-5a19-4c14-b37f-6791392a93d8": Phase="Pending", Reason="", readiness=false. Elapsed: 22.953594ms
May 31 14:37:08.463: INFO: Pod "client-containers-0b3166ff-5a19-4c14-b37f-6791392a93d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031716111s
May 31 14:37:10.475: INFO: Pod "client-containers-0b3166ff-5a19-4c14-b37f-6791392a93d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043726029s
STEP: Saw pod success
May 31 14:37:10.475: INFO: Pod "client-containers-0b3166ff-5a19-4c14-b37f-6791392a93d8" satisfied condition "Succeeded or Failed"
May 31 14:37:10.506: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod client-containers-0b3166ff-5a19-4c14-b37f-6791392a93d8 container agnhost-container: <nil>
STEP: delete the pod
May 31 14:37:10.599: INFO: Waiting for pod client-containers-0b3166ff-5a19-4c14-b37f-6791392a93d8 to disappear
May 31 14:37:10.606: INFO: Pod client-containers-0b3166ff-5a19-4c14-b37f-6791392a93d8 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:37:10.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5016" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":346,"completed":329,"skipped":6426,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:37:10.640: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-2498
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-2498
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2498
May 31 14:37:10.738: INFO: Found 0 stateful pods, waiting for 1
May 31 14:37:20.751: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
May 31 14:37:20.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=statefulset-2498 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 31 14:37:21.198: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 31 14:37:21.198: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 31 14:37:21.198: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 31 14:37:21.206: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 31 14:37:31.218: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 31 14:37:31.218: INFO: Waiting for statefulset status.replicas updated to 0
May 31 14:37:31.260: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999964s
May 31 14:37:32.272: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.992783488s
May 31 14:37:33.287: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.97958569s
May 31 14:37:34.301: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.96518356s
May 31 14:37:35.313: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.950567723s
May 31 14:37:36.323: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.939348888s
May 31 14:37:37.338: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.928379599s
May 31 14:37:38.350: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.914671954s
May 31 14:37:39.360: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.901138634s
May 31 14:37:40.371: INFO: Verifying statefulset ss doesn't scale past 1 for another 892.075598ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2498
May 31 14:37:41.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=statefulset-2498 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 31 14:37:41.861: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 31 14:37:41.861: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 31 14:37:41.861: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 31 14:37:41.871: INFO: Found 1 stateful pods, waiting for 3
May 31 14:37:51.890: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 31 14:37:51.890: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 31 14:37:51.890: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
May 31 14:37:51.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=statefulset-2498 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 31 14:37:52.324: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 31 14:37:52.324: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 31 14:37:52.324: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 31 14:37:52.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=statefulset-2498 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 31 14:37:52.732: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 31 14:37:52.732: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 31 14:37:52.732: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 31 14:37:52.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=statefulset-2498 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
May 31 14:37:53.224: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
May 31 14:37:53.224: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
May 31 14:37:53.224: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

May 31 14:37:53.224: INFO: Waiting for statefulset status.replicas updated to 0
May 31 14:37:53.233: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
May 31 14:38:03.266: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 31 14:38:03.266: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 31 14:38:03.266: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 31 14:38:03.300: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999947s
May 31 14:38:04.312: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.985733957s
May 31 14:38:05.322: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.973273032s
May 31 14:38:06.337: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.963113976s
May 31 14:38:07.353: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.948595182s
May 31 14:38:08.370: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.93249107s
May 31 14:38:09.380: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.914686644s
May 31 14:38:10.402: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.905579276s
May 31 14:38:11.414: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.882552968s
May 31 14:38:12.426: INFO: Verifying statefulset ss doesn't scale past 3 for another 871.272991ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2498
May 31 14:38:13.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=statefulset-2498 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 31 14:38:13.958: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 31 14:38:13.958: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 31 14:38:13.958: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 31 14:38:13.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=statefulset-2498 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 31 14:38:14.424: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 31 14:38:14.424: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 31 14:38:14.424: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 31 14:38:14.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-937481794 --namespace=statefulset-2498 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
May 31 14:38:14.808: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
May 31 14:38:14.808: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
May 31 14:38:14.808: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

May 31 14:38:14.808: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
May 31 14:38:24.863: INFO: Deleting all statefulset in ns statefulset-2498
May 31 14:38:24.870: INFO: Scaling statefulset ss to 0
May 31 14:38:24.895: INFO: Waiting for statefulset status.replicas updated to 0
May 31 14:38:24.904: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:38:24.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2498" for this suite.

• [SLOW TEST:74.325 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":346,"completed":330,"skipped":6499,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:38:24.982: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod pod-subpath-test-configmap-9xgj
STEP: Creating a pod to test atomic-volume-subpath
May 31 14:38:25.066: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-9xgj" in namespace "subpath-4199" to be "Succeeded or Failed"
May 31 14:38:25.075: INFO: Pod "pod-subpath-test-configmap-9xgj": Phase="Pending", Reason="", readiness=false. Elapsed: 8.864122ms
May 31 14:38:27.088: INFO: Pod "pod-subpath-test-configmap-9xgj": Phase="Running", Reason="", readiness=true. Elapsed: 2.022033271s
May 31 14:38:29.108: INFO: Pod "pod-subpath-test-configmap-9xgj": Phase="Running", Reason="", readiness=true. Elapsed: 4.04187533s
May 31 14:38:31.120: INFO: Pod "pod-subpath-test-configmap-9xgj": Phase="Running", Reason="", readiness=true. Elapsed: 6.054272611s
May 31 14:38:33.138: INFO: Pod "pod-subpath-test-configmap-9xgj": Phase="Running", Reason="", readiness=true. Elapsed: 8.071844648s
May 31 14:38:35.151: INFO: Pod "pod-subpath-test-configmap-9xgj": Phase="Running", Reason="", readiness=true. Elapsed: 10.085183142s
May 31 14:38:37.173: INFO: Pod "pod-subpath-test-configmap-9xgj": Phase="Running", Reason="", readiness=true. Elapsed: 12.106655849s
May 31 14:38:39.184: INFO: Pod "pod-subpath-test-configmap-9xgj": Phase="Running", Reason="", readiness=true. Elapsed: 14.118287332s
May 31 14:38:41.195: INFO: Pod "pod-subpath-test-configmap-9xgj": Phase="Running", Reason="", readiness=true. Elapsed: 16.128940107s
May 31 14:38:43.213: INFO: Pod "pod-subpath-test-configmap-9xgj": Phase="Running", Reason="", readiness=true. Elapsed: 18.147379582s
May 31 14:38:45.231: INFO: Pod "pod-subpath-test-configmap-9xgj": Phase="Running", Reason="", readiness=true. Elapsed: 20.165455899s
May 31 14:38:47.247: INFO: Pod "pod-subpath-test-configmap-9xgj": Phase="Running", Reason="", readiness=false. Elapsed: 22.181082s
May 31 14:38:49.261: INFO: Pod "pod-subpath-test-configmap-9xgj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.195142913s
STEP: Saw pod success
May 31 14:38:49.261: INFO: Pod "pod-subpath-test-configmap-9xgj" satisfied condition "Succeeded or Failed"
May 31 14:38:49.274: INFO: Trying to get logs from node ip-172-31-8-180.eu-central-1.compute.internal pod pod-subpath-test-configmap-9xgj container test-container-subpath-configmap-9xgj: <nil>
STEP: delete the pod
May 31 14:38:49.327: INFO: Waiting for pod pod-subpath-test-configmap-9xgj to disappear
May 31 14:38:49.336: INFO: Pod pod-subpath-test-configmap-9xgj no longer exists
STEP: Deleting pod pod-subpath-test-configmap-9xgj
May 31 14:38:49.336: INFO: Deleting pod "pod-subpath-test-configmap-9xgj" in namespace "subpath-4199"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:38:49.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4199" for this suite.

• [SLOW TEST:24.386 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":331,"skipped":6514,"failed":0}
SS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:38:49.368: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:38:49.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-753" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:753
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":346,"completed":332,"skipped":6516,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:38:49.469: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
May 31 14:38:49.519: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 31 14:38:49.536: INFO: Waiting for terminating namespaces to be deleted...
May 31 14:38:49.544: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-11-247.eu-central-1.compute.internal before test
May 31 14:38:49.558: INFO: calico-kube-controllers-786b7976d6-qvmrx from kube-system started at 2022-05-31 12:59:25 +0000 UTC (1 container statuses recorded)
May 31 14:38:49.558: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May 31 14:38:49.558: INFO: canal-jkvwq from kube-system started at 2022-05-31 12:58:35 +0000 UTC (2 container statuses recorded)
May 31 14:38:49.559: INFO: 	Container calico-node ready: true, restart count 0
May 31 14:38:49.559: INFO: 	Container kube-flannel ready: true, restart count 0
May 31 14:38:49.559: INFO: coredns-767874cf84-bl7km from kube-system started at 2022-05-31 12:59:25 +0000 UTC (1 container statuses recorded)
May 31 14:38:49.559: INFO: 	Container coredns ready: true, restart count 0
May 31 14:38:49.559: INFO: coredns-767874cf84-wsllw from kube-system started at 2022-05-31 12:59:25 +0000 UTC (1 container statuses recorded)
May 31 14:38:49.559: INFO: 	Container coredns ready: true, restart count 0
May 31 14:38:49.559: INFO: kube-proxy-7tkx8 from kube-system started at 2022-05-31 12:58:35 +0000 UTC (1 container statuses recorded)
May 31 14:38:49.559: INFO: 	Container kube-proxy ready: true, restart count 0
May 31 14:38:49.559: INFO: node-local-dns-2f4s9 from kube-system started at 2022-05-31 12:58:35 +0000 UTC (1 container statuses recorded)
May 31 14:38:49.559: INFO: 	Container node-cache ready: true, restart count 0
May 31 14:38:49.559: INFO: openvpn-client-76b67b68f8-rllqf from kube-system started at 2022-05-31 12:59:25 +0000 UTC (2 container statuses recorded)
May 31 14:38:49.559: INFO: 	Container dnat-controller ready: true, restart count 0
May 31 14:38:49.559: INFO: 	Container openvpn-client ready: true, restart count 0
May 31 14:38:49.559: INFO: user-ssh-keys-agent-79f9n from kube-system started at 2022-05-31 12:58:35 +0000 UTC (1 container statuses recorded)
May 31 14:38:49.559: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
May 31 14:38:49.559: INFO: dashboard-metrics-scraper-75d68f84c9-78rk7 from kubernetes-dashboard started at 2022-05-31 12:59:25 +0000 UTC (1 container statuses recorded)
May 31 14:38:49.559: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
May 31 14:38:49.559: INFO: dashboard-metrics-scraper-75d68f84c9-d4cl8 from kubernetes-dashboard started at 2022-05-31 12:59:25 +0000 UTC (1 container statuses recorded)
May 31 14:38:49.560: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
May 31 14:38:49.560: INFO: sonobuoy-systemd-logs-daemon-set-fb59706c47b44a31-jzqd2 from sonobuoy started at 2022-05-31 13:07:09 +0000 UTC (2 container statuses recorded)
May 31 14:38:49.560: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 31 14:38:49.560: INFO: 	Container systemd-logs ready: true, restart count 0
May 31 14:38:49.560: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-3-160.eu-central-1.compute.internal before test
May 31 14:38:49.579: INFO: canal-ddb54 from kube-system started at 2022-05-31 12:59:30 +0000 UTC (2 container statuses recorded)
May 31 14:38:49.579: INFO: 	Container calico-node ready: true, restart count 0
May 31 14:38:49.580: INFO: 	Container kube-flannel ready: true, restart count 0
May 31 14:38:49.580: INFO: kube-proxy-5w228 from kube-system started at 2022-05-31 12:59:30 +0000 UTC (1 container statuses recorded)
May 31 14:38:49.580: INFO: 	Container kube-proxy ready: true, restart count 0
May 31 14:38:49.580: INFO: node-local-dns-bnwqt from kube-system started at 2022-05-31 12:59:30 +0000 UTC (1 container statuses recorded)
May 31 14:38:49.580: INFO: 	Container node-cache ready: true, restart count 0
May 31 14:38:49.580: INFO: user-ssh-keys-agent-msrlk from kube-system started at 2022-05-31 12:59:30 +0000 UTC (1 container statuses recorded)
May 31 14:38:49.580: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
May 31 14:38:49.580: INFO: sonobuoy-e2e-job-701195ac2dd44242 from sonobuoy started at 2022-05-31 13:07:09 +0000 UTC (2 container statuses recorded)
May 31 14:38:49.580: INFO: 	Container e2e ready: true, restart count 0
May 31 14:38:49.580: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 31 14:38:49.580: INFO: sonobuoy-systemd-logs-daemon-set-fb59706c47b44a31-6t75j from sonobuoy started at 2022-05-31 13:07:09 +0000 UTC (2 container statuses recorded)
May 31 14:38:49.580: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 31 14:38:49.580: INFO: 	Container systemd-logs ready: true, restart count 0
May 31 14:38:49.580: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-8-180.eu-central-1.compute.internal before test
May 31 14:38:49.594: INFO: canal-5bcsn from kube-system started at 2022-05-31 12:58:46 +0000 UTC (2 container statuses recorded)
May 31 14:38:49.594: INFO: 	Container calico-node ready: true, restart count 0
May 31 14:38:49.594: INFO: 	Container kube-flannel ready: true, restart count 0
May 31 14:38:49.594: INFO: kube-proxy-78w8t from kube-system started at 2022-05-31 12:58:46 +0000 UTC (1 container statuses recorded)
May 31 14:38:49.594: INFO: 	Container kube-proxy ready: true, restart count 0
May 31 14:38:49.594: INFO: node-local-dns-l65lm from kube-system started at 2022-05-31 12:58:46 +0000 UTC (1 container statuses recorded)
May 31 14:38:49.594: INFO: 	Container node-cache ready: true, restart count 0
May 31 14:38:49.594: INFO: user-ssh-keys-agent-dztdn from kube-system started at 2022-05-31 12:58:46 +0000 UTC (1 container statuses recorded)
May 31 14:38:49.594: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
May 31 14:38:49.594: INFO: sonobuoy from sonobuoy started at 2022-05-31 13:07:02 +0000 UTC (1 container statuses recorded)
May 31 14:38:49.594: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 31 14:38:49.594: INFO: sonobuoy-systemd-logs-daemon-set-fb59706c47b44a31-qx4ld from sonobuoy started at 2022-05-31 13:07:09 +0000 UTC (2 container statuses recorded)
May 31 14:38:49.594: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 31 14:38:49.594: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-6ac54777-dbe6-4aa1-bb59-b7388597c20f 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.31.8.180 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-6ac54777-dbe6-4aa1-bb59-b7388597c20f off the node ip-172-31-8-180.eu-central-1.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-6ac54777-dbe6-4aa1-bb59-b7388597c20f
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:43:53.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5444" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:304.387 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":346,"completed":333,"skipped":6533,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:43:53.858: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating the pod
May 31 14:43:53.991: INFO: The status of Pod annotationupdatebb856c13-5f58-4937-85cb-1613584e1adf is Pending, waiting for it to be Running (with Ready = true)
May 31 14:43:56.002: INFO: The status of Pod annotationupdatebb856c13-5f58-4937-85cb-1613584e1adf is Running (Ready = true)
May 31 14:43:56.600: INFO: Successfully updated pod "annotationupdatebb856c13-5f58-4937-85cb-1613584e1adf"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:44:00.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3283" for this suite.

• [SLOW TEST:6.821 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":334,"skipped":6539,"failed":0}
SSSSS
------------------------------
[sig-node] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:44:00.680: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-282f898c-306b-4be9-9d3b-37a0c7498e10
STEP: Creating a pod to test consume secrets
May 31 14:44:00.753: INFO: Waiting up to 5m0s for pod "pod-secrets-09b86e6f-d552-4f38-a57a-fd40bbd3899a" in namespace "secrets-2485" to be "Succeeded or Failed"
May 31 14:44:00.762: INFO: Pod "pod-secrets-09b86e6f-d552-4f38-a57a-fd40bbd3899a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.571498ms
May 31 14:44:02.777: INFO: Pod "pod-secrets-09b86e6f-d552-4f38-a57a-fd40bbd3899a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023819148s
May 31 14:44:04.790: INFO: Pod "pod-secrets-09b86e6f-d552-4f38-a57a-fd40bbd3899a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037075146s
STEP: Saw pod success
May 31 14:44:04.790: INFO: Pod "pod-secrets-09b86e6f-d552-4f38-a57a-fd40bbd3899a" satisfied condition "Succeeded or Failed"
May 31 14:44:04.803: INFO: Trying to get logs from node ip-172-31-3-160.eu-central-1.compute.internal pod pod-secrets-09b86e6f-d552-4f38-a57a-fd40bbd3899a container secret-env-test: <nil>
STEP: delete the pod
May 31 14:44:04.854: INFO: Waiting for pod pod-secrets-09b86e6f-d552-4f38-a57a-fd40bbd3899a to disappear
May 31 14:44:04.876: INFO: Pod pod-secrets-09b86e6f-d552-4f38-a57a-fd40bbd3899a no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:44:04.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2485" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":346,"completed":335,"skipped":6544,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:44:04.928: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 31 14:44:05.017: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f6dd085d-7e13-444e-bd7f-89777beba3a8" in namespace "downward-api-1258" to be "Succeeded or Failed"
May 31 14:44:05.027: INFO: Pod "downwardapi-volume-f6dd085d-7e13-444e-bd7f-89777beba3a8": Phase="Pending", Reason="", readiness=false. Elapsed: 9.864992ms
May 31 14:44:07.043: INFO: Pod "downwardapi-volume-f6dd085d-7e13-444e-bd7f-89777beba3a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024959142s
May 31 14:44:09.053: INFO: Pod "downwardapi-volume-f6dd085d-7e13-444e-bd7f-89777beba3a8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035440955s
May 31 14:44:11.065: INFO: Pod "downwardapi-volume-f6dd085d-7e13-444e-bd7f-89777beba3a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.047222151s
STEP: Saw pod success
May 31 14:44:11.065: INFO: Pod "downwardapi-volume-f6dd085d-7e13-444e-bd7f-89777beba3a8" satisfied condition "Succeeded or Failed"
May 31 14:44:11.071: INFO: Trying to get logs from node ip-172-31-3-160.eu-central-1.compute.internal pod downwardapi-volume-f6dd085d-7e13-444e-bd7f-89777beba3a8 container client-container: <nil>
STEP: delete the pod
May 31 14:44:11.108: INFO: Waiting for pod downwardapi-volume-f6dd085d-7e13-444e-bd7f-89777beba3a8 to disappear
May 31 14:44:11.117: INFO: Pod downwardapi-volume-f6dd085d-7e13-444e-bd7f-89777beba3a8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:44:11.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1258" for this suite.

• [SLOW TEST:6.209 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":336,"skipped":6557,"failed":0}
SSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:44:11.140: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:56
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod busybox-8a5eb6b5-f731-42fa-a796-ef66e738b182 in namespace container-probe-254
May 31 14:44:13.217: INFO: Started pod busybox-8a5eb6b5-f731-42fa-a796-ef66e738b182 in namespace container-probe-254
STEP: checking the pod's current state and verifying that restartCount is present
May 31 14:44:13.223: INFO: Initial restart count of pod busybox-8a5eb6b5-f731-42fa-a796-ef66e738b182 is 0
May 31 14:45:03.602: INFO: Restart count of pod container-probe-254/busybox-8a5eb6b5-f731-42fa-a796-ef66e738b182 is now 1 (50.378530904s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:45:03.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-254" for this suite.

• [SLOW TEST:52.512 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":337,"skipped":6564,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:45:03.655: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
May 31 14:45:06.785: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:45:06.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8893" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":346,"completed":338,"skipped":6581,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:45:06.862: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
May 31 14:45:06.921: INFO: Waiting up to 1m0s for all nodes to be ready
May 31 14:46:06.992: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 14:46:06.999: INFO: Starting informer...
STEP: Starting pod...
May 31 14:46:07.228: INFO: Pod is running on ip-172-31-8-180.eu-central-1.compute.internal. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
May 31 14:46:07.284: INFO: Pod wasn't evicted. Proceeding
May 31 14:46:07.284: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
May 31 14:47:22.320: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:47:22.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-7827" for this suite.

• [SLOW TEST:135.488 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":346,"completed":339,"skipped":6594,"failed":0}
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:47:22.354: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name s-test-opt-del-d9f41044-2d1d-4f86-96af-19da21596e4c
STEP: Creating secret with name s-test-opt-upd-130aaeb1-33eb-4e9b-a102-d7fea2b18226
STEP: Creating the pod
May 31 14:47:22.460: INFO: The status of Pod pod-projected-secrets-2101c3dd-ea45-433d-92eb-953350b9363b is Pending, waiting for it to be Running (with Ready = true)
May 31 14:47:24.471: INFO: The status of Pod pod-projected-secrets-2101c3dd-ea45-433d-92eb-953350b9363b is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-d9f41044-2d1d-4f86-96af-19da21596e4c
STEP: Updating secret s-test-opt-upd-130aaeb1-33eb-4e9b-a102-d7fea2b18226
STEP: Creating secret with name s-test-opt-create-83b5a3ce-58f3-4e63-97aa-3dedc276db81
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:47:26.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5209" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":340,"skipped":6594,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:47:26.829: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:47:26.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-7319" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":346,"completed":341,"skipped":6619,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:47:26.989: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
May 31 14:47:27.094: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 31 14:47:27.110: INFO: Waiting for terminating namespaces to be deleted...
May 31 14:47:27.118: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-11-247.eu-central-1.compute.internal before test
May 31 14:47:27.134: INFO: calico-kube-controllers-786b7976d6-qvmrx from kube-system started at 2022-05-31 12:59:25 +0000 UTC (1 container statuses recorded)
May 31 14:47:27.134: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May 31 14:47:27.134: INFO: canal-jkvwq from kube-system started at 2022-05-31 12:58:35 +0000 UTC (2 container statuses recorded)
May 31 14:47:27.134: INFO: 	Container calico-node ready: true, restart count 0
May 31 14:47:27.134: INFO: 	Container kube-flannel ready: true, restart count 0
May 31 14:47:27.134: INFO: coredns-767874cf84-bl7km from kube-system started at 2022-05-31 12:59:25 +0000 UTC (1 container statuses recorded)
May 31 14:47:27.134: INFO: 	Container coredns ready: true, restart count 0
May 31 14:47:27.134: INFO: coredns-767874cf84-wsllw from kube-system started at 2022-05-31 12:59:25 +0000 UTC (1 container statuses recorded)
May 31 14:47:27.134: INFO: 	Container coredns ready: true, restart count 0
May 31 14:47:27.134: INFO: kube-proxy-7tkx8 from kube-system started at 2022-05-31 12:58:35 +0000 UTC (1 container statuses recorded)
May 31 14:47:27.134: INFO: 	Container kube-proxy ready: true, restart count 0
May 31 14:47:27.134: INFO: node-local-dns-2f4s9 from kube-system started at 2022-05-31 12:58:35 +0000 UTC (1 container statuses recorded)
May 31 14:47:27.134: INFO: 	Container node-cache ready: true, restart count 0
May 31 14:47:27.134: INFO: openvpn-client-76b67b68f8-rllqf from kube-system started at 2022-05-31 12:59:25 +0000 UTC (2 container statuses recorded)
May 31 14:47:27.134: INFO: 	Container dnat-controller ready: true, restart count 0
May 31 14:47:27.134: INFO: 	Container openvpn-client ready: true, restart count 0
May 31 14:47:27.134: INFO: user-ssh-keys-agent-79f9n from kube-system started at 2022-05-31 12:58:35 +0000 UTC (1 container statuses recorded)
May 31 14:47:27.134: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
May 31 14:47:27.134: INFO: dashboard-metrics-scraper-75d68f84c9-78rk7 from kubernetes-dashboard started at 2022-05-31 12:59:25 +0000 UTC (1 container statuses recorded)
May 31 14:47:27.134: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
May 31 14:47:27.134: INFO: dashboard-metrics-scraper-75d68f84c9-d4cl8 from kubernetes-dashboard started at 2022-05-31 12:59:25 +0000 UTC (1 container statuses recorded)
May 31 14:47:27.134: INFO: 	Container dashboard-metrics-scraper ready: true, restart count 0
May 31 14:47:27.134: INFO: sonobuoy-systemd-logs-daemon-set-fb59706c47b44a31-jzqd2 from sonobuoy started at 2022-05-31 13:07:09 +0000 UTC (2 container statuses recorded)
May 31 14:47:27.134: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 31 14:47:27.134: INFO: 	Container systemd-logs ready: true, restart count 0
May 31 14:47:27.134: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-3-160.eu-central-1.compute.internal before test
May 31 14:47:27.145: INFO: canal-ddb54 from kube-system started at 2022-05-31 12:59:30 +0000 UTC (2 container statuses recorded)
May 31 14:47:27.145: INFO: 	Container calico-node ready: true, restart count 0
May 31 14:47:27.145: INFO: 	Container kube-flannel ready: true, restart count 0
May 31 14:47:27.145: INFO: kube-proxy-5w228 from kube-system started at 2022-05-31 12:59:30 +0000 UTC (1 container statuses recorded)
May 31 14:47:27.145: INFO: 	Container kube-proxy ready: true, restart count 0
May 31 14:47:27.145: INFO: node-local-dns-bnwqt from kube-system started at 2022-05-31 12:59:30 +0000 UTC (1 container statuses recorded)
May 31 14:47:27.145: INFO: 	Container node-cache ready: true, restart count 0
May 31 14:47:27.145: INFO: user-ssh-keys-agent-msrlk from kube-system started at 2022-05-31 12:59:30 +0000 UTC (1 container statuses recorded)
May 31 14:47:27.145: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
May 31 14:47:27.145: INFO: sonobuoy-e2e-job-701195ac2dd44242 from sonobuoy started at 2022-05-31 13:07:09 +0000 UTC (2 container statuses recorded)
May 31 14:47:27.146: INFO: 	Container e2e ready: true, restart count 0
May 31 14:47:27.146: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 31 14:47:27.146: INFO: sonobuoy-systemd-logs-daemon-set-fb59706c47b44a31-6t75j from sonobuoy started at 2022-05-31 13:07:09 +0000 UTC (2 container statuses recorded)
May 31 14:47:27.146: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 31 14:47:27.146: INFO: 	Container systemd-logs ready: true, restart count 0
May 31 14:47:27.146: INFO: 
Logging pods the apiserver thinks is on node ip-172-31-8-180.eu-central-1.compute.internal before test
May 31 14:47:27.173: INFO: canal-5bcsn from kube-system started at 2022-05-31 12:58:46 +0000 UTC (2 container statuses recorded)
May 31 14:47:27.173: INFO: 	Container calico-node ready: true, restart count 0
May 31 14:47:27.174: INFO: 	Container kube-flannel ready: true, restart count 0
May 31 14:47:27.174: INFO: kube-proxy-78w8t from kube-system started at 2022-05-31 12:58:46 +0000 UTC (1 container statuses recorded)
May 31 14:47:27.174: INFO: 	Container kube-proxy ready: true, restart count 0
May 31 14:47:27.174: INFO: node-local-dns-l65lm from kube-system started at 2022-05-31 12:58:46 +0000 UTC (1 container statuses recorded)
May 31 14:47:27.174: INFO: 	Container node-cache ready: true, restart count 0
May 31 14:47:27.174: INFO: user-ssh-keys-agent-dztdn from kube-system started at 2022-05-31 12:58:46 +0000 UTC (1 container statuses recorded)
May 31 14:47:27.174: INFO: 	Container user-ssh-keys-agent ready: true, restart count 0
May 31 14:47:27.174: INFO: pod-projected-secrets-2101c3dd-ea45-433d-92eb-953350b9363b from projected-5209 started at 2022-05-31 14:47:22 +0000 UTC (3 container statuses recorded)
May 31 14:47:27.174: INFO: 	Container creates-volume-test ready: true, restart count 0
May 31 14:47:27.174: INFO: 	Container dels-volume-test ready: true, restart count 0
May 31 14:47:27.174: INFO: 	Container upds-volume-test ready: true, restart count 0
May 31 14:47:27.174: INFO: sonobuoy from sonobuoy started at 2022-05-31 13:07:02 +0000 UTC (1 container statuses recorded)
May 31 14:47:27.174: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 31 14:47:27.174: INFO: sonobuoy-systemd-logs-daemon-set-fb59706c47b44a31-qx4ld from sonobuoy started at 2022-05-31 13:07:09 +0000 UTC (2 container statuses recorded)
May 31 14:47:27.174: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 31 14:47:27.174: INFO: 	Container systemd-logs ready: true, restart count 0
May 31 14:47:27.174: INFO: taint-eviction-4 from taint-single-pod-7827 started at 2022-05-31 14:46:07 +0000 UTC (1 container statuses recorded)
May 31 14:47:27.174: INFO: 	Container pause ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: verifying the node has the label node ip-172-31-11-247.eu-central-1.compute.internal
STEP: verifying the node has the label node ip-172-31-3-160.eu-central-1.compute.internal
STEP: verifying the node has the label node ip-172-31-8-180.eu-central-1.compute.internal
May 31 14:47:27.324: INFO: Pod calico-kube-controllers-786b7976d6-qvmrx requesting resource cpu=0m on Node ip-172-31-11-247.eu-central-1.compute.internal
May 31 14:47:27.324: INFO: Pod canal-5bcsn requesting resource cpu=250m on Node ip-172-31-8-180.eu-central-1.compute.internal
May 31 14:47:27.324: INFO: Pod canal-ddb54 requesting resource cpu=250m on Node ip-172-31-3-160.eu-central-1.compute.internal
May 31 14:47:27.324: INFO: Pod canal-jkvwq requesting resource cpu=250m on Node ip-172-31-11-247.eu-central-1.compute.internal
May 31 14:47:27.324: INFO: Pod coredns-767874cf84-bl7km requesting resource cpu=50m on Node ip-172-31-11-247.eu-central-1.compute.internal
May 31 14:47:27.324: INFO: Pod coredns-767874cf84-wsllw requesting resource cpu=50m on Node ip-172-31-11-247.eu-central-1.compute.internal
May 31 14:47:27.324: INFO: Pod kube-proxy-5w228 requesting resource cpu=75m on Node ip-172-31-3-160.eu-central-1.compute.internal
May 31 14:47:27.324: INFO: Pod kube-proxy-78w8t requesting resource cpu=75m on Node ip-172-31-8-180.eu-central-1.compute.internal
May 31 14:47:27.324: INFO: Pod kube-proxy-7tkx8 requesting resource cpu=75m on Node ip-172-31-11-247.eu-central-1.compute.internal
May 31 14:47:27.324: INFO: Pod node-local-dns-2f4s9 requesting resource cpu=0m on Node ip-172-31-11-247.eu-central-1.compute.internal
May 31 14:47:27.324: INFO: Pod node-local-dns-bnwqt requesting resource cpu=0m on Node ip-172-31-3-160.eu-central-1.compute.internal
May 31 14:47:27.324: INFO: Pod node-local-dns-l65lm requesting resource cpu=0m on Node ip-172-31-8-180.eu-central-1.compute.internal
May 31 14:47:27.324: INFO: Pod openvpn-client-76b67b68f8-rllqf requesting resource cpu=30m on Node ip-172-31-11-247.eu-central-1.compute.internal
May 31 14:47:27.324: INFO: Pod user-ssh-keys-agent-79f9n requesting resource cpu=0m on Node ip-172-31-11-247.eu-central-1.compute.internal
May 31 14:47:27.324: INFO: Pod user-ssh-keys-agent-dztdn requesting resource cpu=0m on Node ip-172-31-8-180.eu-central-1.compute.internal
May 31 14:47:27.324: INFO: Pod user-ssh-keys-agent-msrlk requesting resource cpu=0m on Node ip-172-31-3-160.eu-central-1.compute.internal
May 31 14:47:27.324: INFO: Pod dashboard-metrics-scraper-75d68f84c9-78rk7 requesting resource cpu=50m on Node ip-172-31-11-247.eu-central-1.compute.internal
May 31 14:47:27.324: INFO: Pod dashboard-metrics-scraper-75d68f84c9-d4cl8 requesting resource cpu=50m on Node ip-172-31-11-247.eu-central-1.compute.internal
May 31 14:47:27.324: INFO: Pod pod-projected-secrets-2101c3dd-ea45-433d-92eb-953350b9363b requesting resource cpu=0m on Node ip-172-31-8-180.eu-central-1.compute.internal
May 31 14:47:27.324: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-172-31-8-180.eu-central-1.compute.internal
May 31 14:47:27.324: INFO: Pod sonobuoy-e2e-job-701195ac2dd44242 requesting resource cpu=0m on Node ip-172-31-3-160.eu-central-1.compute.internal
May 31 14:47:27.324: INFO: Pod sonobuoy-systemd-logs-daemon-set-fb59706c47b44a31-6t75j requesting resource cpu=0m on Node ip-172-31-3-160.eu-central-1.compute.internal
May 31 14:47:27.324: INFO: Pod sonobuoy-systemd-logs-daemon-set-fb59706c47b44a31-jzqd2 requesting resource cpu=0m on Node ip-172-31-11-247.eu-central-1.compute.internal
May 31 14:47:27.324: INFO: Pod sonobuoy-systemd-logs-daemon-set-fb59706c47b44a31-qx4ld requesting resource cpu=0m on Node ip-172-31-8-180.eu-central-1.compute.internal
May 31 14:47:27.324: INFO: Pod taint-eviction-4 requesting resource cpu=0m on Node ip-172-31-8-180.eu-central-1.compute.internal
STEP: Starting Pods to consume most of the cluster CPU.
May 31 14:47:27.324: INFO: Creating a pod which consumes cpu=731m on Node ip-172-31-11-247.eu-central-1.compute.internal
May 31 14:47:27.339: INFO: Creating a pod which consumes cpu=892m on Node ip-172-31-3-160.eu-central-1.compute.internal
May 31 14:47:27.349: INFO: Creating a pod which consumes cpu=892m on Node ip-172-31-8-180.eu-central-1.compute.internal
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5d8a4969-ed2c-4a43-a3ae-46cd1b8ae5f5.16f437eb69335844], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9321/filler-pod-5d8a4969-ed2c-4a43-a3ae-46cd1b8ae5f5 to ip-172-31-8-180.eu-central-1.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5d8a4969-ed2c-4a43-a3ae-46cd1b8ae5f5.16f437eb9a9f415f], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.6" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5d8a4969-ed2c-4a43-a3ae-46cd1b8ae5f5.16f437eb9f486298], Reason = [Created], Message = [Created container filler-pod-5d8a4969-ed2c-4a43-a3ae-46cd1b8ae5f5]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5d8a4969-ed2c-4a43-a3ae-46cd1b8ae5f5.16f437eba4deceaf], Reason = [Started], Message = [Started container filler-pod-5d8a4969-ed2c-4a43-a3ae-46cd1b8ae5f5]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5e95f1bc-09c0-4297-9b3b-3893f3b4622b.16f437eb6730afa8], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9321/filler-pod-5e95f1bc-09c0-4297-9b3b-3893f3b4622b to ip-172-31-11-247.eu-central-1.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5e95f1bc-09c0-4297-9b3b-3893f3b4622b.16f437eb8ff61cf4], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.6" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5e95f1bc-09c0-4297-9b3b-3893f3b4622b.16f437eb941183e1], Reason = [Created], Message = [Created container filler-pod-5e95f1bc-09c0-4297-9b3b-3893f3b4622b]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5e95f1bc-09c0-4297-9b3b-3893f3b4622b.16f437eb9a94b445], Reason = [Started], Message = [Started container filler-pod-5e95f1bc-09c0-4297-9b3b-3893f3b4622b]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c81f24c0-2a71-4f29-b918-a4374758f733.16f437eb6839632c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9321/filler-pod-c81f24c0-2a71-4f29-b918-a4374758f733 to ip-172-31-3-160.eu-central-1.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c81f24c0-2a71-4f29-b918-a4374758f733.16f437eb9c33489e], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.6" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c81f24c0-2a71-4f29-b918-a4374758f733.16f437eba1300233], Reason = [Created], Message = [Created container filler-pod-c81f24c0-2a71-4f29-b918-a4374758f733]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c81f24c0-2a71-4f29-b918-a4374758f733.16f437eba9262c23], Reason = [Started], Message = [Started container filler-pod-c81f24c0-2a71-4f29-b918-a4374758f733]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16f437ebe3d1a7e5], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node ip-172-31-11-247.eu-central-1.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-172-31-3-160.eu-central-1.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-172-31-8-180.eu-central-1.compute.internal
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:47:30.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9321" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":346,"completed":342,"skipped":6640,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:47:30.586: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 14:47:30.643: INFO: Got root ca configmap in namespace "svcaccounts-9625"
May 31 14:47:30.656: INFO: Deleted root ca configmap in namespace "svcaccounts-9625"
STEP: waiting for a new root ca configmap created
May 31 14:47:31.165: INFO: Recreated root ca configmap in namespace "svcaccounts-9625"
May 31 14:47:31.173: INFO: Updated root ca configmap in namespace "svcaccounts-9625"
STEP: waiting for the root ca configmap reconciled
May 31 14:47:31.684: INFO: Reconciled root ca configmap in namespace "svcaccounts-9625"
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:47:31.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9625" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","total":346,"completed":343,"skipped":6660,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:47:31.707: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
May 31 14:47:31.774: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a8ffdd4f-6fd0-4eb9-8b4c-759c873e1aef" in namespace "downward-api-8952" to be "Succeeded or Failed"
May 31 14:47:31.791: INFO: Pod "downwardapi-volume-a8ffdd4f-6fd0-4eb9-8b4c-759c873e1aef": Phase="Pending", Reason="", readiness=false. Elapsed: 16.771711ms
May 31 14:47:33.800: INFO: Pod "downwardapi-volume-a8ffdd4f-6fd0-4eb9-8b4c-759c873e1aef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026140435s
May 31 14:47:35.810: INFO: Pod "downwardapi-volume-a8ffdd4f-6fd0-4eb9-8b4c-759c873e1aef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035660351s
STEP: Saw pod success
May 31 14:47:35.810: INFO: Pod "downwardapi-volume-a8ffdd4f-6fd0-4eb9-8b4c-759c873e1aef" satisfied condition "Succeeded or Failed"
May 31 14:47:35.816: INFO: Trying to get logs from node ip-172-31-3-160.eu-central-1.compute.internal pod downwardapi-volume-a8ffdd4f-6fd0-4eb9-8b4c-759c873e1aef container client-container: <nil>
STEP: delete the pod
May 31 14:47:35.902: INFO: Waiting for pod downwardapi-volume-a8ffdd4f-6fd0-4eb9-8b4c-759c873e1aef to disappear
May 31 14:47:35.908: INFO: Pod downwardapi-volume-a8ffdd4f-6fd0-4eb9-8b4c-759c873e1aef no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:47:35.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8952" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":344,"skipped":6669,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:47:35.948: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
May 31 14:47:36.059: INFO: Waiting up to 1m0s for all nodes to be ready
May 31 14:48:36.124: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:48:36.134: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
May 31 14:48:38.287: INFO: found a healthy node: ip-172-31-8-180.eu-central-1.compute.internal
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 14:48:52.605: INFO: pods created so far: [1 1 1]
May 31 14:48:52.605: INFO: length of pods created so far: 3
May 31 14:48:54.635: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:49:01.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-8113" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:49:01.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-1852" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:85.909 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":346,"completed":345,"skipped":6687,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
May 31 14:49:01.858: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
May 31 14:49:01.913: INFO: >>> kubeConfig: /tmp/kubeconfig-937481794
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
May 31 14:49:05.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-3595" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":346,"completed":346,"skipped":6690,"failed":0}
SSSSSSSSMay 31 14:49:05.347: INFO: Running AfterSuite actions on all nodes
May 31 14:49:05.347: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func18.2
May 31 14:49:05.347: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func8.2
May 31 14:49:05.347: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func7.2
May 31 14:49:05.347: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
May 31 14:49:05.347: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
May 31 14:49:05.347: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
May 31 14:49:05.347: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
May 31 14:49:05.348: INFO: Running AfterSuite actions on node 1
May 31 14:49:05.348: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/sonobuoy/results/junit_01.xml
{"msg":"Test Suite completed","total":346,"completed":346,"skipped":6698,"failed":0}

Ran 346 of 7044 Specs in 6071.437 seconds
SUCCESS! -- 346 Passed | 0 Failed | 0 Pending | 6698 Skipped
PASS

Ginkgo ran 1 suite in 1h41m15.429222169s
Test Suite Passed
