I0731 10:31:15.852414      23 e2e.go:132] Starting e2e run "abf6ed35-6c60-4022-ac82-39dc496d486f" on Ginkgo node 1
{"msg":"Test Suite starting","total":346,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1659263475 - Will randomize all specs
Will run 346 of 7050 specs

Jul 31 10:31:18.648: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
Jul 31 10:31:18.652: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jul 31 10:31:18.691: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jul 31 10:31:18.756: INFO: 51 / 51 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jul 31 10:31:18.756: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Jul 31 10:31:18.756: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jul 31 10:31:18.766: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'cloud-controller-manager' (0 seconds elapsed)
Jul 31 10:31:18.766: INFO: 8 / 8 pods ready in namespace 'kube-system' in daemonset 'csi-ridge-node' (0 seconds elapsed)
Jul 31 10:31:18.766: INFO: 8 / 8 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jul 31 10:31:18.766: INFO: 8 / 8 pods ready in namespace 'kube-system' in daemonset 'meta' (0 seconds elapsed)
Jul 31 10:31:18.766: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ridge-auth' (0 seconds elapsed)
Jul 31 10:31:18.766: INFO: 8 / 8 pods ready in namespace 'kube-system' in daemonset 'weave-net' (0 seconds elapsed)
Jul 31 10:31:18.766: INFO: e2e test version: v1.23.9
Jul 31 10:31:18.768: INFO: kube-apiserver version: v1.23.9
Jul 31 10:31:18.768: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
Jul 31 10:31:18.775: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:31:18.775: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename gc
W0731 10:31:18.847758      23 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
Jul 31 10:31:18.848: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
Jul 31 10:31:31.301: INFO: 69 pods remaining
Jul 31 10:31:31.301: INFO: 69 pods has nil DeletionTimestamp
Jul 31 10:31:31.301: INFO: 
STEP: Gathering metrics
Jul 31 10:31:36.349: INFO: The status of Pod kube-controller-manager-master-um8faxf3d4468jbdm49zh4o57y is Running (Ready = true)
Jul 31 10:31:36.781: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jul 31 10:31:36.781: INFO: Deleting pod "simpletest-rc-to-be-deleted-22gxt" in namespace "gc-1070"
Jul 31 10:31:36.809: INFO: Deleting pod "simpletest-rc-to-be-deleted-25xlb" in namespace "gc-1070"
Jul 31 10:31:36.864: INFO: Deleting pod "simpletest-rc-to-be-deleted-2bplk" in namespace "gc-1070"
Jul 31 10:31:36.903: INFO: Deleting pod "simpletest-rc-to-be-deleted-2mkz6" in namespace "gc-1070"
Jul 31 10:31:36.973: INFO: Deleting pod "simpletest-rc-to-be-deleted-2qx2t" in namespace "gc-1070"
Jul 31 10:31:37.033: INFO: Deleting pod "simpletest-rc-to-be-deleted-2vx8t" in namespace "gc-1070"
Jul 31 10:31:37.083: INFO: Deleting pod "simpletest-rc-to-be-deleted-47fxz" in namespace "gc-1070"
Jul 31 10:31:37.164: INFO: Deleting pod "simpletest-rc-to-be-deleted-49vwj" in namespace "gc-1070"
Jul 31 10:31:37.212: INFO: Deleting pod "simpletest-rc-to-be-deleted-4phdw" in namespace "gc-1070"
Jul 31 10:31:37.280: INFO: Deleting pod "simpletest-rc-to-be-deleted-4xtqx" in namespace "gc-1070"
Jul 31 10:31:37.360: INFO: Deleting pod "simpletest-rc-to-be-deleted-57f9x" in namespace "gc-1070"
Jul 31 10:31:37.431: INFO: Deleting pod "simpletest-rc-to-be-deleted-5bz9m" in namespace "gc-1070"
Jul 31 10:31:37.536: INFO: Deleting pod "simpletest-rc-to-be-deleted-5p8gh" in namespace "gc-1070"
Jul 31 10:31:37.803: INFO: Deleting pod "simpletest-rc-to-be-deleted-5s7ww" in namespace "gc-1070"
Jul 31 10:31:37.848: INFO: Deleting pod "simpletest-rc-to-be-deleted-676xv" in namespace "gc-1070"
Jul 31 10:31:37.943: INFO: Deleting pod "simpletest-rc-to-be-deleted-67kpc" in namespace "gc-1070"
Jul 31 10:31:38.003: INFO: Deleting pod "simpletest-rc-to-be-deleted-6f6cl" in namespace "gc-1070"
Jul 31 10:31:38.124: INFO: Deleting pod "simpletest-rc-to-be-deleted-6gkkw" in namespace "gc-1070"
Jul 31 10:31:38.186: INFO: Deleting pod "simpletest-rc-to-be-deleted-6t74c" in namespace "gc-1070"
Jul 31 10:31:38.278: INFO: Deleting pod "simpletest-rc-to-be-deleted-74vfp" in namespace "gc-1070"
Jul 31 10:31:38.304: INFO: Deleting pod "simpletest-rc-to-be-deleted-78bpv" in namespace "gc-1070"
Jul 31 10:31:38.392: INFO: Deleting pod "simpletest-rc-to-be-deleted-7gvv5" in namespace "gc-1070"
Jul 31 10:31:38.496: INFO: Deleting pod "simpletest-rc-to-be-deleted-7nhwn" in namespace "gc-1070"
Jul 31 10:31:38.556: INFO: Deleting pod "simpletest-rc-to-be-deleted-7q2lg" in namespace "gc-1070"
Jul 31 10:31:38.649: INFO: Deleting pod "simpletest-rc-to-be-deleted-7szlq" in namespace "gc-1070"
Jul 31 10:31:38.711: INFO: Deleting pod "simpletest-rc-to-be-deleted-9j72f" in namespace "gc-1070"
Jul 31 10:31:38.783: INFO: Deleting pod "simpletest-rc-to-be-deleted-9qwj5" in namespace "gc-1070"
Jul 31 10:31:38.845: INFO: Deleting pod "simpletest-rc-to-be-deleted-bgznt" in namespace "gc-1070"
Jul 31 10:31:38.882: INFO: Deleting pod "simpletest-rc-to-be-deleted-bkw89" in namespace "gc-1070"
Jul 31 10:31:38.985: INFO: Deleting pod "simpletest-rc-to-be-deleted-bsjzd" in namespace "gc-1070"
Jul 31 10:31:39.040: INFO: Deleting pod "simpletest-rc-to-be-deleted-bvjxt" in namespace "gc-1070"
Jul 31 10:31:39.075: INFO: Deleting pod "simpletest-rc-to-be-deleted-c7ql7" in namespace "gc-1070"
Jul 31 10:31:39.120: INFO: Deleting pod "simpletest-rc-to-be-deleted-d8fj4" in namespace "gc-1070"
Jul 31 10:31:39.194: INFO: Deleting pod "simpletest-rc-to-be-deleted-d9f76" in namespace "gc-1070"
Jul 31 10:31:39.241: INFO: Deleting pod "simpletest-rc-to-be-deleted-dbxmd" in namespace "gc-1070"
Jul 31 10:31:39.399: INFO: Deleting pod "simpletest-rc-to-be-deleted-dcnhp" in namespace "gc-1070"
Jul 31 10:31:39.460: INFO: Deleting pod "simpletest-rc-to-be-deleted-dffc5" in namespace "gc-1070"
Jul 31 10:31:39.587: INFO: Deleting pod "simpletest-rc-to-be-deleted-djl4d" in namespace "gc-1070"
Jul 31 10:31:39.638: INFO: Deleting pod "simpletest-rc-to-be-deleted-dnjnc" in namespace "gc-1070"
Jul 31 10:31:39.737: INFO: Deleting pod "simpletest-rc-to-be-deleted-f2cwz" in namespace "gc-1070"
Jul 31 10:31:39.841: INFO: Deleting pod "simpletest-rc-to-be-deleted-f6jln" in namespace "gc-1070"
Jul 31 10:31:40.016: INFO: Deleting pod "simpletest-rc-to-be-deleted-ffxkl" in namespace "gc-1070"
Jul 31 10:31:40.070: INFO: Deleting pod "simpletest-rc-to-be-deleted-ftpd7" in namespace "gc-1070"
Jul 31 10:31:40.128: INFO: Deleting pod "simpletest-rc-to-be-deleted-fxn49" in namespace "gc-1070"
Jul 31 10:31:40.214: INFO: Deleting pod "simpletest-rc-to-be-deleted-g45vn" in namespace "gc-1070"
Jul 31 10:31:40.286: INFO: Deleting pod "simpletest-rc-to-be-deleted-g9c6l" in namespace "gc-1070"
Jul 31 10:31:40.347: INFO: Deleting pod "simpletest-rc-to-be-deleted-g9spk" in namespace "gc-1070"
Jul 31 10:31:40.436: INFO: Deleting pod "simpletest-rc-to-be-deleted-ggpxm" in namespace "gc-1070"
Jul 31 10:31:40.544: INFO: Deleting pod "simpletest-rc-to-be-deleted-gwnrk" in namespace "gc-1070"
Jul 31 10:31:40.588: INFO: Deleting pod "simpletest-rc-to-be-deleted-gzkm7" in namespace "gc-1070"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:31:40.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1070" for this suite.

• [SLOW TEST:21.899 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":346,"completed":1,"skipped":13,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:31:40.675: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-7d4fef84-ad84-4a0f-9017-6c05fb7610d2
STEP: Creating a pod to test consume secrets
Jul 31 10:31:40.828: INFO: Waiting up to 5m0s for pod "pod-secrets-fb3395a2-92e2-4c8c-9919-81216df2ae76" in namespace "secrets-1341" to be "Succeeded or Failed"
Jul 31 10:31:40.855: INFO: Pod "pod-secrets-fb3395a2-92e2-4c8c-9919-81216df2ae76": Phase="Pending", Reason="", readiness=false. Elapsed: 27.021002ms
Jul 31 10:31:42.867: INFO: Pod "pod-secrets-fb3395a2-92e2-4c8c-9919-81216df2ae76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039062641s
Jul 31 10:31:44.880: INFO: Pod "pod-secrets-fb3395a2-92e2-4c8c-9919-81216df2ae76": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051854844s
Jul 31 10:31:46.908: INFO: Pod "pod-secrets-fb3395a2-92e2-4c8c-9919-81216df2ae76": Phase="Pending", Reason="", readiness=false. Elapsed: 6.079605467s
Jul 31 10:31:48.922: INFO: Pod "pod-secrets-fb3395a2-92e2-4c8c-9919-81216df2ae76": Phase="Pending", Reason="", readiness=false. Elapsed: 8.093600323s
Jul 31 10:31:50.938: INFO: Pod "pod-secrets-fb3395a2-92e2-4c8c-9919-81216df2ae76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.110014366s
STEP: Saw pod success
Jul 31 10:31:50.938: INFO: Pod "pod-secrets-fb3395a2-92e2-4c8c-9919-81216df2ae76" satisfied condition "Succeeded or Failed"
Jul 31 10:31:50.943: INFO: Trying to get logs from node p1-eipeq63rfgo6ooocmu3kqk3tcc pod pod-secrets-fb3395a2-92e2-4c8c-9919-81216df2ae76 container secret-volume-test: <nil>
STEP: delete the pod
Jul 31 10:31:51.019: INFO: Waiting for pod pod-secrets-fb3395a2-92e2-4c8c-9919-81216df2ae76 to disappear
Jul 31 10:31:51.035: INFO: Pod pod-secrets-fb3395a2-92e2-4c8c-9919-81216df2ae76 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:31:51.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1341" for this suite.

• [SLOW TEST:10.420 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":2,"skipped":39,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:31:51.095: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Jul 31 10:31:51.200: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 31 10:32:51.256: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 10:32:51.261: INFO: Starting informer...
STEP: Starting pod...
Jul 31 10:32:51.483: INFO: Pod is running on p1-eipeq63rfgo6ooocmu3kqk3tcc. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Jul 31 10:32:51.519: INFO: Pod wasn't evicted. Proceeding
Jul 31 10:32:51.519: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Jul 31 10:34:06.553: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:34:06.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-8063" for this suite.

• [SLOW TEST:135.502 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":346,"completed":3,"skipped":56,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:34:06.598: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5350.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5350.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5350.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5350.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5350.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5350.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5350.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5350.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5350.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5350.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5350.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5350.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 0.28.102.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.102.28.0_udp@PTR;check="$$(dig +tcp +noall +answer +search 0.28.102.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.102.28.0_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5350.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5350.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5350.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5350.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5350.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5350.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5350.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5350.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5350.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5350.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5350.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5350.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 0.28.102.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.102.28.0_udp@PTR;check="$$(dig +tcp +noall +answer +search 0.28.102.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.102.28.0_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 31 10:34:24.813: INFO: Unable to read wheezy_udp@dns-test-service.dns-5350.svc.cluster.local from pod dns-5350/dns-test-972e608e-5147-42ac-9f64-bd887845d2ee: the server could not find the requested resource (get pods dns-test-972e608e-5147-42ac-9f64-bd887845d2ee)
Jul 31 10:34:24.820: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5350.svc.cluster.local from pod dns-5350/dns-test-972e608e-5147-42ac-9f64-bd887845d2ee: the server could not find the requested resource (get pods dns-test-972e608e-5147-42ac-9f64-bd887845d2ee)
Jul 31 10:34:24.828: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5350.svc.cluster.local from pod dns-5350/dns-test-972e608e-5147-42ac-9f64-bd887845d2ee: the server could not find the requested resource (get pods dns-test-972e608e-5147-42ac-9f64-bd887845d2ee)
Jul 31 10:34:24.834: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5350.svc.cluster.local from pod dns-5350/dns-test-972e608e-5147-42ac-9f64-bd887845d2ee: the server could not find the requested resource (get pods dns-test-972e608e-5147-42ac-9f64-bd887845d2ee)
Jul 31 10:34:24.864: INFO: Unable to read jessie_udp@dns-test-service.dns-5350.svc.cluster.local from pod dns-5350/dns-test-972e608e-5147-42ac-9f64-bd887845d2ee: the server could not find the requested resource (get pods dns-test-972e608e-5147-42ac-9f64-bd887845d2ee)
Jul 31 10:34:24.870: INFO: Unable to read jessie_tcp@dns-test-service.dns-5350.svc.cluster.local from pod dns-5350/dns-test-972e608e-5147-42ac-9f64-bd887845d2ee: the server could not find the requested resource (get pods dns-test-972e608e-5147-42ac-9f64-bd887845d2ee)
Jul 31 10:34:24.877: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5350.svc.cluster.local from pod dns-5350/dns-test-972e608e-5147-42ac-9f64-bd887845d2ee: the server could not find the requested resource (get pods dns-test-972e608e-5147-42ac-9f64-bd887845d2ee)
Jul 31 10:34:24.883: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5350.svc.cluster.local from pod dns-5350/dns-test-972e608e-5147-42ac-9f64-bd887845d2ee: the server could not find the requested resource (get pods dns-test-972e608e-5147-42ac-9f64-bd887845d2ee)
Jul 31 10:34:24.908: INFO: Lookups using dns-5350/dns-test-972e608e-5147-42ac-9f64-bd887845d2ee failed for: [wheezy_udp@dns-test-service.dns-5350.svc.cluster.local wheezy_tcp@dns-test-service.dns-5350.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5350.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5350.svc.cluster.local jessie_udp@dns-test-service.dns-5350.svc.cluster.local jessie_tcp@dns-test-service.dns-5350.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5350.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5350.svc.cluster.local]

Jul 31 10:34:29.920: INFO: Unable to read wheezy_udp@dns-test-service.dns-5350.svc.cluster.local from pod dns-5350/dns-test-972e608e-5147-42ac-9f64-bd887845d2ee: the server could not find the requested resource (get pods dns-test-972e608e-5147-42ac-9f64-bd887845d2ee)
Jul 31 10:34:29.924: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5350.svc.cluster.local from pod dns-5350/dns-test-972e608e-5147-42ac-9f64-bd887845d2ee: the server could not find the requested resource (get pods dns-test-972e608e-5147-42ac-9f64-bd887845d2ee)
Jul 31 10:34:29.930: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5350.svc.cluster.local from pod dns-5350/dns-test-972e608e-5147-42ac-9f64-bd887845d2ee: the server could not find the requested resource (get pods dns-test-972e608e-5147-42ac-9f64-bd887845d2ee)
Jul 31 10:34:29.935: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5350.svc.cluster.local from pod dns-5350/dns-test-972e608e-5147-42ac-9f64-bd887845d2ee: the server could not find the requested resource (get pods dns-test-972e608e-5147-42ac-9f64-bd887845d2ee)
Jul 31 10:34:29.967: INFO: Unable to read jessie_udp@dns-test-service.dns-5350.svc.cluster.local from pod dns-5350/dns-test-972e608e-5147-42ac-9f64-bd887845d2ee: the server could not find the requested resource (get pods dns-test-972e608e-5147-42ac-9f64-bd887845d2ee)
Jul 31 10:34:29.971: INFO: Unable to read jessie_tcp@dns-test-service.dns-5350.svc.cluster.local from pod dns-5350/dns-test-972e608e-5147-42ac-9f64-bd887845d2ee: the server could not find the requested resource (get pods dns-test-972e608e-5147-42ac-9f64-bd887845d2ee)
Jul 31 10:34:29.977: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5350.svc.cluster.local from pod dns-5350/dns-test-972e608e-5147-42ac-9f64-bd887845d2ee: the server could not find the requested resource (get pods dns-test-972e608e-5147-42ac-9f64-bd887845d2ee)
Jul 31 10:34:29.983: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5350.svc.cluster.local from pod dns-5350/dns-test-972e608e-5147-42ac-9f64-bd887845d2ee: the server could not find the requested resource (get pods dns-test-972e608e-5147-42ac-9f64-bd887845d2ee)
Jul 31 10:34:30.008: INFO: Lookups using dns-5350/dns-test-972e608e-5147-42ac-9f64-bd887845d2ee failed for: [wheezy_udp@dns-test-service.dns-5350.svc.cluster.local wheezy_tcp@dns-test-service.dns-5350.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5350.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5350.svc.cluster.local jessie_udp@dns-test-service.dns-5350.svc.cluster.local jessie_tcp@dns-test-service.dns-5350.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5350.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5350.svc.cluster.local]

Jul 31 10:34:34.920: INFO: Unable to read wheezy_udp@dns-test-service.dns-5350.svc.cluster.local from pod dns-5350/dns-test-972e608e-5147-42ac-9f64-bd887845d2ee: the server could not find the requested resource (get pods dns-test-972e608e-5147-42ac-9f64-bd887845d2ee)
Jul 31 10:34:34.926: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5350.svc.cluster.local from pod dns-5350/dns-test-972e608e-5147-42ac-9f64-bd887845d2ee: the server could not find the requested resource (get pods dns-test-972e608e-5147-42ac-9f64-bd887845d2ee)
Jul 31 10:34:34.933: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5350.svc.cluster.local from pod dns-5350/dns-test-972e608e-5147-42ac-9f64-bd887845d2ee: the server could not find the requested resource (get pods dns-test-972e608e-5147-42ac-9f64-bd887845d2ee)
Jul 31 10:34:34.940: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5350.svc.cluster.local from pod dns-5350/dns-test-972e608e-5147-42ac-9f64-bd887845d2ee: the server could not find the requested resource (get pods dns-test-972e608e-5147-42ac-9f64-bd887845d2ee)
Jul 31 10:34:34.973: INFO: Unable to read jessie_udp@dns-test-service.dns-5350.svc.cluster.local from pod dns-5350/dns-test-972e608e-5147-42ac-9f64-bd887845d2ee: the server could not find the requested resource (get pods dns-test-972e608e-5147-42ac-9f64-bd887845d2ee)
Jul 31 10:34:34.979: INFO: Unable to read jessie_tcp@dns-test-service.dns-5350.svc.cluster.local from pod dns-5350/dns-test-972e608e-5147-42ac-9f64-bd887845d2ee: the server could not find the requested resource (get pods dns-test-972e608e-5147-42ac-9f64-bd887845d2ee)
Jul 31 10:34:34.985: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5350.svc.cluster.local from pod dns-5350/dns-test-972e608e-5147-42ac-9f64-bd887845d2ee: the server could not find the requested resource (get pods dns-test-972e608e-5147-42ac-9f64-bd887845d2ee)
Jul 31 10:34:34.992: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5350.svc.cluster.local from pod dns-5350/dns-test-972e608e-5147-42ac-9f64-bd887845d2ee: the server could not find the requested resource (get pods dns-test-972e608e-5147-42ac-9f64-bd887845d2ee)
Jul 31 10:34:35.021: INFO: Lookups using dns-5350/dns-test-972e608e-5147-42ac-9f64-bd887845d2ee failed for: [wheezy_udp@dns-test-service.dns-5350.svc.cluster.local wheezy_tcp@dns-test-service.dns-5350.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5350.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5350.svc.cluster.local jessie_udp@dns-test-service.dns-5350.svc.cluster.local jessie_tcp@dns-test-service.dns-5350.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5350.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5350.svc.cluster.local]

Jul 31 10:34:39.918: INFO: Unable to read wheezy_udp@dns-test-service.dns-5350.svc.cluster.local from pod dns-5350/dns-test-972e608e-5147-42ac-9f64-bd887845d2ee: the server could not find the requested resource (get pods dns-test-972e608e-5147-42ac-9f64-bd887845d2ee)
Jul 31 10:34:39.924: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5350.svc.cluster.local from pod dns-5350/dns-test-972e608e-5147-42ac-9f64-bd887845d2ee: the server could not find the requested resource (get pods dns-test-972e608e-5147-42ac-9f64-bd887845d2ee)
Jul 31 10:34:39.929: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5350.svc.cluster.local from pod dns-5350/dns-test-972e608e-5147-42ac-9f64-bd887845d2ee: the server could not find the requested resource (get pods dns-test-972e608e-5147-42ac-9f64-bd887845d2ee)
Jul 31 10:34:39.934: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5350.svc.cluster.local from pod dns-5350/dns-test-972e608e-5147-42ac-9f64-bd887845d2ee: the server could not find the requested resource (get pods dns-test-972e608e-5147-42ac-9f64-bd887845d2ee)
Jul 31 10:34:39.964: INFO: Unable to read jessie_udp@dns-test-service.dns-5350.svc.cluster.local from pod dns-5350/dns-test-972e608e-5147-42ac-9f64-bd887845d2ee: the server could not find the requested resource (get pods dns-test-972e608e-5147-42ac-9f64-bd887845d2ee)
Jul 31 10:34:39.971: INFO: Unable to read jessie_tcp@dns-test-service.dns-5350.svc.cluster.local from pod dns-5350/dns-test-972e608e-5147-42ac-9f64-bd887845d2ee: the server could not find the requested resource (get pods dns-test-972e608e-5147-42ac-9f64-bd887845d2ee)
Jul 31 10:34:39.977: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5350.svc.cluster.local from pod dns-5350/dns-test-972e608e-5147-42ac-9f64-bd887845d2ee: the server could not find the requested resource (get pods dns-test-972e608e-5147-42ac-9f64-bd887845d2ee)
Jul 31 10:34:39.981: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5350.svc.cluster.local from pod dns-5350/dns-test-972e608e-5147-42ac-9f64-bd887845d2ee: the server could not find the requested resource (get pods dns-test-972e608e-5147-42ac-9f64-bd887845d2ee)
Jul 31 10:34:40.002: INFO: Lookups using dns-5350/dns-test-972e608e-5147-42ac-9f64-bd887845d2ee failed for: [wheezy_udp@dns-test-service.dns-5350.svc.cluster.local wheezy_tcp@dns-test-service.dns-5350.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5350.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5350.svc.cluster.local jessie_udp@dns-test-service.dns-5350.svc.cluster.local jessie_tcp@dns-test-service.dns-5350.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5350.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5350.svc.cluster.local]

Jul 31 10:34:44.919: INFO: Unable to read wheezy_udp@dns-test-service.dns-5350.svc.cluster.local from pod dns-5350/dns-test-972e608e-5147-42ac-9f64-bd887845d2ee: the server could not find the requested resource (get pods dns-test-972e608e-5147-42ac-9f64-bd887845d2ee)
Jul 31 10:34:44.925: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5350.svc.cluster.local from pod dns-5350/dns-test-972e608e-5147-42ac-9f64-bd887845d2ee: the server could not find the requested resource (get pods dns-test-972e608e-5147-42ac-9f64-bd887845d2ee)
Jul 31 10:34:44.931: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5350.svc.cluster.local from pod dns-5350/dns-test-972e608e-5147-42ac-9f64-bd887845d2ee: the server could not find the requested resource (get pods dns-test-972e608e-5147-42ac-9f64-bd887845d2ee)
Jul 31 10:34:44.938: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5350.svc.cluster.local from pod dns-5350/dns-test-972e608e-5147-42ac-9f64-bd887845d2ee: the server could not find the requested resource (get pods dns-test-972e608e-5147-42ac-9f64-bd887845d2ee)
Jul 31 10:34:44.971: INFO: Unable to read jessie_udp@dns-test-service.dns-5350.svc.cluster.local from pod dns-5350/dns-test-972e608e-5147-42ac-9f64-bd887845d2ee: the server could not find the requested resource (get pods dns-test-972e608e-5147-42ac-9f64-bd887845d2ee)
Jul 31 10:34:44.979: INFO: Unable to read jessie_tcp@dns-test-service.dns-5350.svc.cluster.local from pod dns-5350/dns-test-972e608e-5147-42ac-9f64-bd887845d2ee: the server could not find the requested resource (get pods dns-test-972e608e-5147-42ac-9f64-bd887845d2ee)
Jul 31 10:34:44.987: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5350.svc.cluster.local from pod dns-5350/dns-test-972e608e-5147-42ac-9f64-bd887845d2ee: the server could not find the requested resource (get pods dns-test-972e608e-5147-42ac-9f64-bd887845d2ee)
Jul 31 10:34:44.993: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5350.svc.cluster.local from pod dns-5350/dns-test-972e608e-5147-42ac-9f64-bd887845d2ee: the server could not find the requested resource (get pods dns-test-972e608e-5147-42ac-9f64-bd887845d2ee)
Jul 31 10:34:45.024: INFO: Lookups using dns-5350/dns-test-972e608e-5147-42ac-9f64-bd887845d2ee failed for: [wheezy_udp@dns-test-service.dns-5350.svc.cluster.local wheezy_tcp@dns-test-service.dns-5350.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5350.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5350.svc.cluster.local jessie_udp@dns-test-service.dns-5350.svc.cluster.local jessie_tcp@dns-test-service.dns-5350.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5350.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5350.svc.cluster.local]

Jul 31 10:34:50.008: INFO: DNS probes using dns-5350/dns-test-972e608e-5147-42ac-9f64-bd887845d2ee succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:34:50.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5350" for this suite.

• [SLOW TEST:43.600 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":346,"completed":4,"skipped":127,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:34:50.203: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Jul 31 10:34:50.335: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ff1df542-f339-409e-b442-730607201a3f" in namespace "projected-1258" to be "Succeeded or Failed"
Jul 31 10:34:50.352: INFO: Pod "downwardapi-volume-ff1df542-f339-409e-b442-730607201a3f": Phase="Pending", Reason="", readiness=false. Elapsed: 17.103061ms
Jul 31 10:34:52.364: INFO: Pod "downwardapi-volume-ff1df542-f339-409e-b442-730607201a3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028973627s
Jul 31 10:34:54.373: INFO: Pod "downwardapi-volume-ff1df542-f339-409e-b442-730607201a3f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038033585s
Jul 31 10:34:56.384: INFO: Pod "downwardapi-volume-ff1df542-f339-409e-b442-730607201a3f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.049240592s
Jul 31 10:34:58.398: INFO: Pod "downwardapi-volume-ff1df542-f339-409e-b442-730607201a3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.063355647s
STEP: Saw pod success
Jul 31 10:34:58.398: INFO: Pod "downwardapi-volume-ff1df542-f339-409e-b442-730607201a3f" satisfied condition "Succeeded or Failed"
Jul 31 10:34:58.404: INFO: Trying to get logs from node p1-ashfcfj4pzo6aemt1j3a9ah7ew pod downwardapi-volume-ff1df542-f339-409e-b442-730607201a3f container client-container: <nil>
STEP: delete the pod
Jul 31 10:34:58.451: INFO: Waiting for pod downwardapi-volume-ff1df542-f339-409e-b442-730607201a3f to disappear
Jul 31 10:34:58.455: INFO: Pod downwardapi-volume-ff1df542-f339-409e-b442-730607201a3f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:34:58.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1258" for this suite.

• [SLOW TEST:8.275 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":5,"skipped":151,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:34:58.478: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir volume type on node default medium
Jul 31 10:34:58.559: INFO: Waiting up to 5m0s for pod "pod-d81cbdae-b9da-4281-b8be-43ff74c3d8ae" in namespace "emptydir-5977" to be "Succeeded or Failed"
Jul 31 10:34:58.574: INFO: Pod "pod-d81cbdae-b9da-4281-b8be-43ff74c3d8ae": Phase="Pending", Reason="", readiness=false. Elapsed: 14.928683ms
Jul 31 10:35:00.583: INFO: Pod "pod-d81cbdae-b9da-4281-b8be-43ff74c3d8ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024565641s
Jul 31 10:35:02.597: INFO: Pod "pod-d81cbdae-b9da-4281-b8be-43ff74c3d8ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038758113s
STEP: Saw pod success
Jul 31 10:35:02.598: INFO: Pod "pod-d81cbdae-b9da-4281-b8be-43ff74c3d8ae" satisfied condition "Succeeded or Failed"
Jul 31 10:35:02.602: INFO: Trying to get logs from node p1-eipeq63rfgo6ooocmu3kqk3tcc pod pod-d81cbdae-b9da-4281-b8be-43ff74c3d8ae container test-container: <nil>
STEP: delete the pod
Jul 31 10:35:02.660: INFO: Waiting for pod pod-d81cbdae-b9da-4281-b8be-43ff74c3d8ae to disappear
Jul 31 10:35:02.667: INFO: Pod pod-d81cbdae-b9da-4281-b8be-43ff74c3d8ae no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:35:02.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5977" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":6,"skipped":156,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:35:02.693: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jul 31 10:35:02.790: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 31 10:36:02.854: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:36:02.859: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 10:36:02.963: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Jul 31 10:36:02.969: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:36:03.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-8532" for this suite.
[AfterEach] PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:36:03.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-2089" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:60.478 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":346,"completed":7,"skipped":188,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:36:03.174: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with secret that has name projected-secret-test-map-a2219dc1-e7a8-451d-98c5-1ef9dcd97fe2
STEP: Creating a pod to test consume secrets
Jul 31 10:36:03.256: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-23c3c6d6-6fc7-4947-8bc4-c8c42f73b0f1" in namespace "projected-4313" to be "Succeeded or Failed"
Jul 31 10:36:03.275: INFO: Pod "pod-projected-secrets-23c3c6d6-6fc7-4947-8bc4-c8c42f73b0f1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.655877ms
Jul 31 10:36:05.284: INFO: Pod "pod-projected-secrets-23c3c6d6-6fc7-4947-8bc4-c8c42f73b0f1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027486105s
Jul 31 10:36:07.345: INFO: Pod "pod-projected-secrets-23c3c6d6-6fc7-4947-8bc4-c8c42f73b0f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.088770535s
STEP: Saw pod success
Jul 31 10:36:07.345: INFO: Pod "pod-projected-secrets-23c3c6d6-6fc7-4947-8bc4-c8c42f73b0f1" satisfied condition "Succeeded or Failed"
Jul 31 10:36:07.566: INFO: Trying to get logs from node p1-ashfcfj4pzo6aemt1j3a9ah7ew pod pod-projected-secrets-23c3c6d6-6fc7-4947-8bc4-c8c42f73b0f1 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 31 10:36:07.620: INFO: Waiting for pod pod-projected-secrets-23c3c6d6-6fc7-4947-8bc4-c8c42f73b0f1 to disappear
Jul 31 10:36:07.632: INFO: Pod pod-projected-secrets-23c3c6d6-6fc7-4947-8bc4-c8c42f73b0f1 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:36:07.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4313" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":8,"skipped":205,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:36:07.656: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 10:36:07.720: INFO: Creating simple deployment test-new-deployment
Jul 31 10:36:07.744: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
Jul 31 10:36:09.766: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.July, 31, 10, 36, 7, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 10, 36, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.July, 31, 10, 36, 7, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 10, 36, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-5d9fdcc779\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 31 10:36:11.779: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.July, 31, 10, 36, 7, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 10, 36, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.July, 31, 10, 36, 7, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 10, 36, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-5d9fdcc779\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the deployment Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Jul 31 10:36:13.922: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-9336  c9a36add-adf0-45db-a6fb-cce52bd57cdc 6725 3 2022-07-31 10:36:07 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2022-07-31 10:36:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-07-31 10:36:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00265fdd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-5d9fdcc779" has successfully progressed.,LastUpdateTime:2022-07-31 10:36:13 +0000 UTC,LastTransitionTime:2022-07-31 10:36:07 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-07-31 10:36:13 +0000 UTC,LastTransitionTime:2022-07-31 10:36:13 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jul 31 10:36:13.929: INFO: New ReplicaSet "test-new-deployment-5d9fdcc779" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-5d9fdcc779  deployment-9336  456081df-5240-42df-acbe-fc0a6d2ba8ae 6727 2 2022-07-31 10:36:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment c9a36add-adf0-45db-a6fb-cce52bd57cdc 0xc0020581f0 0xc0020581f1}] []  [{kube-controller-manager Update apps/v1 2022-07-31 10:36:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9a36add-adf0-45db-a6fb-cce52bd57cdc\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-07-31 10:36:13 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 5d9fdcc779,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002058278 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jul 31 10:36:13.941: INFO: Pod "test-new-deployment-5d9fdcc779-9c4cl" is available:
&Pod{ObjectMeta:{test-new-deployment-5d9fdcc779-9c4cl test-new-deployment-5d9fdcc779- deployment-9336  d89eef8d-3b8d-4342-b24f-dd5f41d202fe 6713 0 2022-07-31 10:36:07 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet test-new-deployment-5d9fdcc779 456081df-5240-42df-acbe-fc0a6d2ba8ae 0xc002058640 0xc002058641}] []  [{kube-controller-manager Update v1 2022-07-31 10:36:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"456081df-5240-42df-acbe-fc0a6d2ba8ae\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-07-31 10:36:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.28.0.2\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m72pp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m72pp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:p1-eipeq63rfgo6ooocmu3kqk3tcc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 10:36:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 10:36:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 10:36:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 10:36:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.27.21.69,PodIP:172.28.0.2,StartTime:2022-07-31 10:36:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-07-31 10:36:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:docker://f3aea2923a6b63d0241d49e675724e0fdb28474b0fe628be5ce2800e54c04c7f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.28.0.2,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 10:36:13.942: INFO: Pod "test-new-deployment-5d9fdcc779-sntdl" is not available:
&Pod{ObjectMeta:{test-new-deployment-5d9fdcc779-sntdl test-new-deployment-5d9fdcc779- deployment-9336  49aa5b50-03f9-4d5a-8e0b-a5c9eef5f7ef 6726 0 2022-07-31 10:36:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet test-new-deployment-5d9fdcc779 456081df-5240-42df-acbe-fc0a6d2ba8ae 0xc002058810 0xc002058811}] []  [{kube-controller-manager Update v1 2022-07-31 10:36:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"456081df-5240-42df-acbe-fc0a6d2ba8ae\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-07-31 10:36:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nh2vh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nh2vh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:p1-nc3cz44465xd41a6poxnmrpeqo,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 10:36:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 10:36:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 10:36:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 10:36:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.27.21.79,PodIP:,StartTime:2022-07-31 10:36:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:36:13.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9336" for this suite.

• [SLOW TEST:6.319 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","total":346,"completed":9,"skipped":237,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:36:13.975: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 10:36:14.127: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-e1126afe-432c-49cf-9765-ec3e6cab89eb" in namespace "security-context-test-8790" to be "Succeeded or Failed"
Jul 31 10:36:14.148: INFO: Pod "busybox-privileged-false-e1126afe-432c-49cf-9765-ec3e6cab89eb": Phase="Pending", Reason="", readiness=false. Elapsed: 20.648276ms
Jul 31 10:36:16.159: INFO: Pod "busybox-privileged-false-e1126afe-432c-49cf-9765-ec3e6cab89eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031803393s
Jul 31 10:36:18.170: INFO: Pod "busybox-privileged-false-e1126afe-432c-49cf-9765-ec3e6cab89eb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043242348s
Jul 31 10:36:20.189: INFO: Pod "busybox-privileged-false-e1126afe-432c-49cf-9765-ec3e6cab89eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.062127064s
Jul 31 10:36:20.189: INFO: Pod "busybox-privileged-false-e1126afe-432c-49cf-9765-ec3e6cab89eb" satisfied condition "Succeeded or Failed"
Jul 31 10:36:20.202: INFO: Got logs for pod "busybox-privileged-false-e1126afe-432c-49cf-9765-ec3e6cab89eb": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:36:20.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8790" for this suite.

• [SLOW TEST:6.270 seconds]
[sig-node] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:232
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":10,"skipped":246,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:36:20.246: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should create and stop a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a replication controller
Jul 31 10:36:20.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-3628 create -f -'
Jul 31 10:36:21.627: INFO: stderr: ""
Jul 31 10:36:21.627: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 31 10:36:21.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-3628 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 31 10:36:21.722: INFO: stderr: ""
Jul 31 10:36:21.722: INFO: stdout: "update-demo-nautilus-2v8x7 update-demo-nautilus-4rgrz "
Jul 31 10:36:21.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-3628 get pods update-demo-nautilus-2v8x7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 31 10:36:21.803: INFO: stderr: ""
Jul 31 10:36:21.803: INFO: stdout: ""
Jul 31 10:36:21.803: INFO: update-demo-nautilus-2v8x7 is created but not running
Jul 31 10:36:26.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-3628 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 31 10:36:26.902: INFO: stderr: ""
Jul 31 10:36:26.902: INFO: stdout: "update-demo-nautilus-2v8x7 update-demo-nautilus-4rgrz "
Jul 31 10:36:26.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-3628 get pods update-demo-nautilus-2v8x7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 31 10:36:26.982: INFO: stderr: ""
Jul 31 10:36:26.982: INFO: stdout: ""
Jul 31 10:36:26.982: INFO: update-demo-nautilus-2v8x7 is created but not running
Jul 31 10:36:31.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-3628 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 31 10:36:32.073: INFO: stderr: ""
Jul 31 10:36:32.073: INFO: stdout: "update-demo-nautilus-2v8x7 update-demo-nautilus-4rgrz "
Jul 31 10:36:32.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-3628 get pods update-demo-nautilus-2v8x7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 31 10:36:32.138: INFO: stderr: ""
Jul 31 10:36:32.138: INFO: stdout: "true"
Jul 31 10:36:32.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-3628 get pods update-demo-nautilus-2v8x7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 31 10:36:32.211: INFO: stderr: ""
Jul 31 10:36:32.211: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jul 31 10:36:32.211: INFO: validating pod update-demo-nautilus-2v8x7
Jul 31 10:36:32.220: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 31 10:36:32.220: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 31 10:36:32.220: INFO: update-demo-nautilus-2v8x7 is verified up and running
Jul 31 10:36:32.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-3628 get pods update-demo-nautilus-4rgrz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 31 10:36:32.300: INFO: stderr: ""
Jul 31 10:36:32.300: INFO: stdout: "true"
Jul 31 10:36:32.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-3628 get pods update-demo-nautilus-4rgrz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 31 10:36:32.372: INFO: stderr: ""
Jul 31 10:36:32.372: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jul 31 10:36:32.372: INFO: validating pod update-demo-nautilus-4rgrz
Jul 31 10:36:32.384: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 31 10:36:32.384: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 31 10:36:32.384: INFO: update-demo-nautilus-4rgrz is verified up and running
STEP: using delete to clean up resources
Jul 31 10:36:32.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-3628 delete --grace-period=0 --force -f -'
Jul 31 10:36:32.456: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 31 10:36:32.456: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul 31 10:36:32.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-3628 get rc,svc -l name=update-demo --no-headers'
Jul 31 10:36:32.532: INFO: stderr: "No resources found in kubectl-3628 namespace.\n"
Jul 31 10:36:32.532: INFO: stdout: ""
Jul 31 10:36:32.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-3628 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 31 10:36:32.602: INFO: stderr: ""
Jul 31 10:36:32.602: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:36:32.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3628" for this suite.

• [SLOW TEST:12.383 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should create and stop a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":346,"completed":11,"skipped":292,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:36:32.630: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-5326
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a new StatefulSet
Jul 31 10:36:32.753: INFO: Found 0 stateful pods, waiting for 3
Jul 31 10:36:42.773: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 31 10:36:42.774: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 31 10:36:42.774: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Jul 31 10:36:52.774: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 31 10:36:52.774: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 31 10:36:52.774: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
Jul 31 10:36:52.823: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jul 31 10:37:02.897: INFO: Updating stateful set ss2
Jul 31 10:37:02.906: INFO: Waiting for Pod statefulset-5326/ss2-2 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
STEP: Restoring Pods to the correct revision when they are deleted
Jul 31 10:37:13.062: INFO: Found 2 stateful pods, waiting for 3
Jul 31 10:37:23.087: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 31 10:37:23.087: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 31 10:37:23.087: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jul 31 10:37:23.135: INFO: Updating stateful set ss2
Jul 31 10:37:23.146: INFO: Waiting for Pod statefulset-5326/ss2-1 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
Jul 31 10:37:33.215: INFO: Updating stateful set ss2
Jul 31 10:37:33.225: INFO: Waiting for StatefulSet statefulset-5326/ss2 to complete update
Jul 31 10:37:33.226: INFO: Waiting for Pod statefulset-5326/ss2-0 to have revision ss2-5f8764d585 update revision ss2-57bbdd95cb
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Jul 31 10:37:43.268: INFO: Deleting all statefulset in ns statefulset-5326
Jul 31 10:37:43.274: INFO: Scaling statefulset ss2 to 0
Jul 31 10:37:53.319: INFO: Waiting for statefulset status.replicas updated to 0
Jul 31 10:37:53.324: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:37:53.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5326" for this suite.

• [SLOW TEST:80.747 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":346,"completed":12,"skipped":328,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:37:53.377: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 31 10:37:54.234: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 31 10:37:57.304: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:37:57.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4464" for this suite.
STEP: Destroying namespace "webhook-4464-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":346,"completed":13,"skipped":371,"failed":0}
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:37:57.921: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 10:37:57.972: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:37:59.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7057" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":346,"completed":14,"skipped":374,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:37:59.042: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul 31 10:37:59.234: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:37:59.234: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:37:59.235: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:37:59.242: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 31 10:37:59.242: INFO: Node p1-ashfcfj4pzo6aemt1j3a9ah7ew is running 0 daemon pod, expected 1
Jul 31 10:38:00.253: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:00.253: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:00.253: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:00.258: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 31 10:38:00.258: INFO: Node p1-ashfcfj4pzo6aemt1j3a9ah7ew is running 0 daemon pod, expected 1
Jul 31 10:38:01.263: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:01.263: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:01.264: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:01.268: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jul 31 10:38:01.268: INFO: Node p1-eipeq63rfgo6ooocmu3kqk3tcc is running 0 daemon pod, expected 1
Jul 31 10:38:02.257: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:02.257: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:02.257: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:02.263: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jul 31 10:38:02.263: INFO: Node p1-esqoe7n9eqq76k8ojd9tajh51e is running 0 daemon pod, expected 1
Jul 31 10:38:03.256: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:03.256: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:03.256: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:03.262: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jul 31 10:38:03.262: INFO: Node p1-esqoe7n9eqq76k8ojd9tajh51e is running 0 daemon pod, expected 1
Jul 31 10:38:04.255: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:04.255: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:04.255: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:04.260: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jul 31 10:38:04.260: INFO: Node p1-esqoe7n9eqq76k8ojd9tajh51e is running 0 daemon pod, expected 1
Jul 31 10:38:05.260: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:05.260: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:05.260: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:05.265: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jul 31 10:38:05.265: INFO: Node p1-esqoe7n9eqq76k8ojd9tajh51e is running 0 daemon pod, expected 1
Jul 31 10:38:06.255: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:06.255: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:06.255: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:06.260: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jul 31 10:38:06.260: INFO: Node p1-esqoe7n9eqq76k8ojd9tajh51e is running 0 daemon pod, expected 1
Jul 31 10:38:07.285: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:07.285: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:07.285: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:07.290: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Jul 31 10:38:07.290: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jul 31 10:38:07.349: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:07.349: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:07.349: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:07.366: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jul 31 10:38:07.366: INFO: Node p1-mz5rqwt5oar4y4n97cfumgzrxe is running 0 daemon pod, expected 1
Jul 31 10:38:08.384: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:08.384: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:08.384: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:08.391: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jul 31 10:38:08.391: INFO: Node p1-mz5rqwt5oar4y4n97cfumgzrxe is running 0 daemon pod, expected 1
Jul 31 10:38:09.381: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:09.381: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:09.381: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:09.387: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Jul 31 10:38:09.387: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6264, will wait for the garbage collector to delete the pods
Jul 31 10:38:09.481: INFO: Deleting DaemonSet.extensions daemon-set took: 28.96694ms
Jul 31 10:38:09.582: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.96526ms
Jul 31 10:38:12.496: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 31 10:38:12.497: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jul 31 10:38:12.503: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"7732"},"items":null}

Jul 31 10:38:12.508: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"7732"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:38:12.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6264" for this suite.

• [SLOW TEST:13.526 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":346,"completed":15,"skipped":392,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:38:12.569: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 10:38:12.701: INFO: Create a RollingUpdate DaemonSet
Jul 31 10:38:12.719: INFO: Check that daemon pods launch on every node of the cluster
Jul 31 10:38:12.729: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:12.729: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:12.729: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:12.733: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 31 10:38:12.733: INFO: Node p1-ashfcfj4pzo6aemt1j3a9ah7ew is running 0 daemon pod, expected 1
Jul 31 10:38:13.744: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:13.744: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:13.744: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:13.749: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 31 10:38:13.749: INFO: Node p1-ashfcfj4pzo6aemt1j3a9ah7ew is running 0 daemon pod, expected 1
Jul 31 10:38:14.754: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:14.754: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:14.754: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:14.761: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jul 31 10:38:14.761: INFO: Node p1-esqoe7n9eqq76k8ojd9tajh51e is running 0 daemon pod, expected 1
Jul 31 10:38:15.742: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:15.742: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:15.743: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:15.748: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jul 31 10:38:15.748: INFO: Node p1-esqoe7n9eqq76k8ojd9tajh51e is running 0 daemon pod, expected 1
Jul 31 10:38:16.744: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:16.744: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:16.744: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:16.750: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Jul 31 10:38:16.750: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
Jul 31 10:38:16.750: INFO: Update the DaemonSet to trigger a rollout
Jul 31 10:38:16.779: INFO: Updating DaemonSet daemon-set
Jul 31 10:38:18.813: INFO: Roll back the DaemonSet before rollout is complete
Jul 31 10:38:18.832: INFO: Updating DaemonSet daemon-set
Jul 31 10:38:18.832: INFO: Make sure DaemonSet rollback is complete
Jul 31 10:38:18.837: INFO: Wrong image for pod: daemon-set-ltbwt. Expected: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Jul 31 10:38:18.837: INFO: Pod daemon-set-ltbwt is not available
Jul 31 10:38:18.844: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:18.845: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:18.845: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:19.865: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:19.865: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:19.865: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:20.877: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:20.877: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:20.877: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:21.860: INFO: Pod daemon-set-gzx7h is not available
Jul 31 10:38:21.870: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:21.870: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 10:38:21.870: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1422, will wait for the garbage collector to delete the pods
Jul 31 10:38:21.963: INFO: Deleting DaemonSet.extensions daemon-set took: 15.074846ms
Jul 31 10:38:22.064: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.05496ms
Jul 31 10:38:24.075: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 31 10:38:24.075: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jul 31 10:38:24.080: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"7950"},"items":null}

Jul 31 10:38:24.085: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"7950"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:38:24.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1422" for this suite.

• [SLOW TEST:11.568 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":346,"completed":16,"skipped":407,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:38:24.137: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod pod-subpath-test-projected-9clr
STEP: Creating a pod to test atomic-volume-subpath
Jul 31 10:38:24.284: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-9clr" in namespace "subpath-8590" to be "Succeeded or Failed"
Jul 31 10:38:24.303: INFO: Pod "pod-subpath-test-projected-9clr": Phase="Pending", Reason="", readiness=false. Elapsed: 19.082636ms
Jul 31 10:38:26.313: INFO: Pod "pod-subpath-test-projected-9clr": Phase="Running", Reason="", readiness=true. Elapsed: 2.028526054s
Jul 31 10:38:28.324: INFO: Pod "pod-subpath-test-projected-9clr": Phase="Running", Reason="", readiness=true. Elapsed: 4.039637771s
Jul 31 10:38:30.340: INFO: Pod "pod-subpath-test-projected-9clr": Phase="Running", Reason="", readiness=true. Elapsed: 6.055534584s
Jul 31 10:38:32.358: INFO: Pod "pod-subpath-test-projected-9clr": Phase="Running", Reason="", readiness=true. Elapsed: 8.073851569s
Jul 31 10:38:34.376: INFO: Pod "pod-subpath-test-projected-9clr": Phase="Running", Reason="", readiness=true. Elapsed: 10.091494336s
Jul 31 10:38:36.384: INFO: Pod "pod-subpath-test-projected-9clr": Phase="Running", Reason="", readiness=true. Elapsed: 12.099332585s
Jul 31 10:38:38.399: INFO: Pod "pod-subpath-test-projected-9clr": Phase="Running", Reason="", readiness=true. Elapsed: 14.11492335s
Jul 31 10:38:40.412: INFO: Pod "pod-subpath-test-projected-9clr": Phase="Running", Reason="", readiness=true. Elapsed: 16.128131229s
Jul 31 10:38:42.427: INFO: Pod "pod-subpath-test-projected-9clr": Phase="Running", Reason="", readiness=true. Elapsed: 18.142765095s
Jul 31 10:38:44.440: INFO: Pod "pod-subpath-test-projected-9clr": Phase="Running", Reason="", readiness=true. Elapsed: 20.155371693s
Jul 31 10:38:46.448: INFO: Pod "pod-subpath-test-projected-9clr": Phase="Running", Reason="", readiness=false. Elapsed: 22.163835668s
Jul 31 10:38:48.464: INFO: Pod "pod-subpath-test-projected-9clr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.179803264s
STEP: Saw pod success
Jul 31 10:38:48.464: INFO: Pod "pod-subpath-test-projected-9clr" satisfied condition "Succeeded or Failed"
Jul 31 10:38:48.468: INFO: Trying to get logs from node p1-nc3cz44465xd41a6poxnmrpeqo pod pod-subpath-test-projected-9clr container test-container-subpath-projected-9clr: <nil>
STEP: delete the pod
Jul 31 10:38:48.530: INFO: Waiting for pod pod-subpath-test-projected-9clr to disappear
Jul 31 10:38:48.537: INFO: Pod pod-subpath-test-projected-9clr no longer exists
STEP: Deleting pod pod-subpath-test-projected-9clr
Jul 31 10:38:48.537: INFO: Deleting pod "pod-subpath-test-projected-9clr" in namespace "subpath-8590"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:38:48.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8590" for this suite.

• [SLOW TEST:24.418 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":17,"skipped":419,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:38:48.556: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod pod-subpath-test-configmap-xq88
STEP: Creating a pod to test atomic-volume-subpath
Jul 31 10:38:48.677: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-xq88" in namespace "subpath-9648" to be "Succeeded or Failed"
Jul 31 10:38:48.701: INFO: Pod "pod-subpath-test-configmap-xq88": Phase="Pending", Reason="", readiness=false. Elapsed: 24.535921ms
Jul 31 10:38:50.718: INFO: Pod "pod-subpath-test-configmap-xq88": Phase="Running", Reason="", readiness=true. Elapsed: 2.041321234s
Jul 31 10:38:52.731: INFO: Pod "pod-subpath-test-configmap-xq88": Phase="Running", Reason="", readiness=true. Elapsed: 4.054138152s
Jul 31 10:38:54.739: INFO: Pod "pod-subpath-test-configmap-xq88": Phase="Running", Reason="", readiness=true. Elapsed: 6.062259708s
Jul 31 10:38:56.749: INFO: Pod "pod-subpath-test-configmap-xq88": Phase="Running", Reason="", readiness=true. Elapsed: 8.072684135s
Jul 31 10:38:58.761: INFO: Pod "pod-subpath-test-configmap-xq88": Phase="Running", Reason="", readiness=true. Elapsed: 10.084661201s
Jul 31 10:39:00.778: INFO: Pod "pod-subpath-test-configmap-xq88": Phase="Running", Reason="", readiness=true. Elapsed: 12.101638439s
Jul 31 10:39:02.793: INFO: Pod "pod-subpath-test-configmap-xq88": Phase="Running", Reason="", readiness=true. Elapsed: 14.116452703s
Jul 31 10:39:04.806: INFO: Pod "pod-subpath-test-configmap-xq88": Phase="Running", Reason="", readiness=true. Elapsed: 16.129132727s
Jul 31 10:39:06.819: INFO: Pod "pod-subpath-test-configmap-xq88": Phase="Running", Reason="", readiness=true. Elapsed: 18.141938364s
Jul 31 10:39:08.831: INFO: Pod "pod-subpath-test-configmap-xq88": Phase="Running", Reason="", readiness=true. Elapsed: 20.154854079s
Jul 31 10:39:11.065: INFO: Pod "pod-subpath-test-configmap-xq88": Phase="Running", Reason="", readiness=false. Elapsed: 22.388434469s
Jul 31 10:39:13.082: INFO: Pod "pod-subpath-test-configmap-xq88": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.405198166s
STEP: Saw pod success
Jul 31 10:39:13.082: INFO: Pod "pod-subpath-test-configmap-xq88" satisfied condition "Succeeded or Failed"
Jul 31 10:39:13.087: INFO: Trying to get logs from node p1-ashfcfj4pzo6aemt1j3a9ah7ew pod pod-subpath-test-configmap-xq88 container test-container-subpath-configmap-xq88: <nil>
STEP: delete the pod
Jul 31 10:39:13.135: INFO: Waiting for pod pod-subpath-test-configmap-xq88 to disappear
Jul 31 10:39:13.139: INFO: Pod pod-subpath-test-configmap-xq88 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-xq88
Jul 31 10:39:13.139: INFO: Deleting pod "pod-subpath-test-configmap-xq88" in namespace "subpath-9648"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:39:13.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9648" for this suite.

• [SLOW TEST:24.608 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":18,"skipped":432,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:39:13.164: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-a3e3b25b-e259-4d9f-8e18-83eeede64eab
STEP: Creating a pod to test consume secrets
Jul 31 10:39:13.366: INFO: Waiting up to 5m0s for pod "pod-secrets-a6ef6e1b-c72c-4b38-9509-6ad935b6db1d" in namespace "secrets-920" to be "Succeeded or Failed"
Jul 31 10:39:13.371: INFO: Pod "pod-secrets-a6ef6e1b-c72c-4b38-9509-6ad935b6db1d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.539695ms
Jul 31 10:39:15.384: INFO: Pod "pod-secrets-a6ef6e1b-c72c-4b38-9509-6ad935b6db1d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01783317s
Jul 31 10:39:17.400: INFO: Pod "pod-secrets-a6ef6e1b-c72c-4b38-9509-6ad935b6db1d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033760688s
Jul 31 10:39:19.414: INFO: Pod "pod-secrets-a6ef6e1b-c72c-4b38-9509-6ad935b6db1d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.047353545s
STEP: Saw pod success
Jul 31 10:39:19.414: INFO: Pod "pod-secrets-a6ef6e1b-c72c-4b38-9509-6ad935b6db1d" satisfied condition "Succeeded or Failed"
Jul 31 10:39:19.419: INFO: Trying to get logs from node p1-eipeq63rfgo6ooocmu3kqk3tcc pod pod-secrets-a6ef6e1b-c72c-4b38-9509-6ad935b6db1d container secret-volume-test: <nil>
STEP: delete the pod
Jul 31 10:39:19.483: INFO: Waiting for pod pod-secrets-a6ef6e1b-c72c-4b38-9509-6ad935b6db1d to disappear
Jul 31 10:39:19.491: INFO: Pod pod-secrets-a6ef6e1b-c72c-4b38-9509-6ad935b6db1d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:39:19.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-920" for this suite.
STEP: Destroying namespace "secret-namespace-9546" for this suite.

• [SLOW TEST:6.359 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":346,"completed":19,"skipped":482,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:39:19.527: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-110.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-110.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-110.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-110.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 31 10:39:23.738: INFO: DNS probes using dns-test-7b99fdff-c67d-40b8-a842-8c66c2e68cc3 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-110.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-110.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-110.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-110.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 31 10:39:27.917: INFO: File wheezy_udp@dns-test-service-3.dns-110.svc.cluster.local from pod  dns-110/dns-test-3e722853-8484-4c1f-91d2-83066c9bbd5e contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 31 10:39:27.923: INFO: File jessie_udp@dns-test-service-3.dns-110.svc.cluster.local from pod  dns-110/dns-test-3e722853-8484-4c1f-91d2-83066c9bbd5e contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 31 10:39:27.923: INFO: Lookups using dns-110/dns-test-3e722853-8484-4c1f-91d2-83066c9bbd5e failed for: [wheezy_udp@dns-test-service-3.dns-110.svc.cluster.local jessie_udp@dns-test-service-3.dns-110.svc.cluster.local]

Jul 31 10:39:32.936: INFO: File wheezy_udp@dns-test-service-3.dns-110.svc.cluster.local from pod  dns-110/dns-test-3e722853-8484-4c1f-91d2-83066c9bbd5e contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 31 10:39:32.942: INFO: File jessie_udp@dns-test-service-3.dns-110.svc.cluster.local from pod  dns-110/dns-test-3e722853-8484-4c1f-91d2-83066c9bbd5e contains '' instead of 'bar.example.com.'
Jul 31 10:39:32.942: INFO: Lookups using dns-110/dns-test-3e722853-8484-4c1f-91d2-83066c9bbd5e failed for: [wheezy_udp@dns-test-service-3.dns-110.svc.cluster.local jessie_udp@dns-test-service-3.dns-110.svc.cluster.local]

Jul 31 10:39:37.936: INFO: File wheezy_udp@dns-test-service-3.dns-110.svc.cluster.local from pod  dns-110/dns-test-3e722853-8484-4c1f-91d2-83066c9bbd5e contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 31 10:39:37.941: INFO: File jessie_udp@dns-test-service-3.dns-110.svc.cluster.local from pod  dns-110/dns-test-3e722853-8484-4c1f-91d2-83066c9bbd5e contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 31 10:39:37.942: INFO: Lookups using dns-110/dns-test-3e722853-8484-4c1f-91d2-83066c9bbd5e failed for: [wheezy_udp@dns-test-service-3.dns-110.svc.cluster.local jessie_udp@dns-test-service-3.dns-110.svc.cluster.local]

Jul 31 10:39:42.931: INFO: File wheezy_udp@dns-test-service-3.dns-110.svc.cluster.local from pod  dns-110/dns-test-3e722853-8484-4c1f-91d2-83066c9bbd5e contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 31 10:39:42.938: INFO: File jessie_udp@dns-test-service-3.dns-110.svc.cluster.local from pod  dns-110/dns-test-3e722853-8484-4c1f-91d2-83066c9bbd5e contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 31 10:39:42.938: INFO: Lookups using dns-110/dns-test-3e722853-8484-4c1f-91d2-83066c9bbd5e failed for: [wheezy_udp@dns-test-service-3.dns-110.svc.cluster.local jessie_udp@dns-test-service-3.dns-110.svc.cluster.local]

Jul 31 10:39:47.932: INFO: File wheezy_udp@dns-test-service-3.dns-110.svc.cluster.local from pod  dns-110/dns-test-3e722853-8484-4c1f-91d2-83066c9bbd5e contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 31 10:39:47.940: INFO: File jessie_udp@dns-test-service-3.dns-110.svc.cluster.local from pod  dns-110/dns-test-3e722853-8484-4c1f-91d2-83066c9bbd5e contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 31 10:39:47.940: INFO: Lookups using dns-110/dns-test-3e722853-8484-4c1f-91d2-83066c9bbd5e failed for: [wheezy_udp@dns-test-service-3.dns-110.svc.cluster.local jessie_udp@dns-test-service-3.dns-110.svc.cluster.local]

Jul 31 10:39:52.937: INFO: DNS probes using dns-test-3e722853-8484-4c1f-91d2-83066c9bbd5e succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-110.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-110.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-110.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-110.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 31 10:39:57.165: INFO: DNS probes using dns-test-5294b7c0-f7f7-431d-9257-f57275b10c35 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:39:57.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-110" for this suite.

• [SLOW TEST:38.016 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":346,"completed":20,"skipped":499,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:39:57.544: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should support creating EndpointSlice API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting /apis
STEP: getting /apis/discovery.k8s.io
STEP: getting /apis/discovery.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jul 31 10:39:57.671: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jul 31 10:39:57.678: INFO: starting watch
STEP: patching
STEP: updating
Jul 31 10:39:57.706: INFO: waiting for watch events with expected annotations
Jul 31 10:39:57.706: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:39:57.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-4825" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","total":346,"completed":21,"skipped":517,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:39:57.786: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1411
STEP: creating an pod
Jul 31 10:39:57.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-4403 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.39 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Jul 31 10:39:57.979: INFO: stderr: ""
Jul 31 10:39:57.979: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Waiting for log generator to start.
Jul 31 10:39:57.979: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jul 31 10:39:57.979: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-4403" to be "running and ready, or succeeded"
Jul 31 10:39:57.997: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 17.648739ms
Jul 31 10:40:00.008: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.029240067s
Jul 31 10:40:00.008: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jul 31 10:40:00.008: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Jul 31 10:40:00.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-4403 logs logs-generator logs-generator'
Jul 31 10:40:00.094: INFO: stderr: ""
Jul 31 10:40:00.094: INFO: stdout: "I0731 10:39:59.196166       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/4zp 310\nI0731 10:39:59.396650       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/mvqn 338\nI0731 10:39:59.597278       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/fclb 413\nI0731 10:39:59.796660       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/f5c9 279\nI0731 10:39:59.996967       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/8m2 479\n"
STEP: limiting log lines
Jul 31 10:40:00.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-4403 logs logs-generator logs-generator --tail=1'
Jul 31 10:40:00.179: INFO: stderr: ""
Jul 31 10:40:00.179: INFO: stdout: "I0731 10:39:59.996967       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/8m2 479\n"
Jul 31 10:40:00.179: INFO: got output "I0731 10:39:59.996967       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/8m2 479\n"
STEP: limiting log bytes
Jul 31 10:40:00.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-4403 logs logs-generator logs-generator --limit-bytes=1'
Jul 31 10:40:00.277: INFO: stderr: ""
Jul 31 10:40:00.277: INFO: stdout: "I"
Jul 31 10:40:00.277: INFO: got output "I"
STEP: exposing timestamps
Jul 31 10:40:00.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-4403 logs logs-generator logs-generator --tail=1 --timestamps'
Jul 31 10:40:00.373: INFO: stderr: ""
Jul 31 10:40:00.373: INFO: stdout: "2022-07-31T10:40:00.196578516Z I0731 10:40:00.196328       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/2wf 464\n"
Jul 31 10:40:00.373: INFO: got output "2022-07-31T10:40:00.196578516Z I0731 10:40:00.196328       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/2wf 464\n"
STEP: restricting to a time range
Jul 31 10:40:02.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-4403 logs logs-generator logs-generator --since=1s'
Jul 31 10:40:02.982: INFO: stderr: ""
Jul 31 10:40:02.982: INFO: stdout: "I0731 10:40:01.996472       1 logs_generator.go:76] 14 GET /api/v1/namespaces/ns/pods/fzvf 476\nI0731 10:40:02.196974       1 logs_generator.go:76] 15 GET /api/v1/namespaces/kube-system/pods/xqxd 381\nI0731 10:40:02.396342       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/default/pods/296 571\nI0731 10:40:02.596821       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/vhmv 342\nI0731 10:40:02.797269       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/hcd 582\n"
Jul 31 10:40:02.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-4403 logs logs-generator logs-generator --since=24h'
Jul 31 10:40:03.078: INFO: stderr: ""
Jul 31 10:40:03.078: INFO: stdout: "I0731 10:39:59.196166       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/4zp 310\nI0731 10:39:59.396650       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/mvqn 338\nI0731 10:39:59.597278       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/fclb 413\nI0731 10:39:59.796660       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/f5c9 279\nI0731 10:39:59.996967       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/8m2 479\nI0731 10:40:00.196328       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/2wf 464\nI0731 10:40:00.396774       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/wjnq 564\nI0731 10:40:00.596216       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/n8rh 339\nI0731 10:40:00.796760       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/kjk 212\nI0731 10:40:00.997207       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/p4q 520\nI0731 10:40:01.196588       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/8gp 360\nI0731 10:40:01.397074       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/zdf 248\nI0731 10:40:01.596519       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/default/pods/8hgb 501\nI0731 10:40:01.796987       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/zsb2 224\nI0731 10:40:01.996472       1 logs_generator.go:76] 14 GET /api/v1/namespaces/ns/pods/fzvf 476\nI0731 10:40:02.196974       1 logs_generator.go:76] 15 GET /api/v1/namespaces/kube-system/pods/xqxd 381\nI0731 10:40:02.396342       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/default/pods/296 571\nI0731 10:40:02.596821       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/vhmv 342\nI0731 10:40:02.797269       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/hcd 582\nI0731 10:40:02.996670       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/x68v 350\n"
[AfterEach] Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1416
Jul 31 10:40:03.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-4403 delete pod logs-generator'
Jul 31 10:40:03.830: INFO: stderr: ""
Jul 31 10:40:03.830: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:40:03.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4403" for this suite.

• [SLOW TEST:6.068 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1408
    should be able to retrieve and filter logs  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":346,"completed":22,"skipped":535,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:40:03.854: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:40:20.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5786" for this suite.

• [SLOW TEST:16.362 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":346,"completed":23,"skipped":562,"failed":0}
SSSSSS
------------------------------
[sig-node] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:40:20.217: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 10:40:20.295: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jul 31 10:40:20.340: INFO: The status of Pod pod-exec-websocket-2b17bbcc-1642-4814-b598-9c1d8f20853d is Pending, waiting for it to be Running (with Ready = true)
Jul 31 10:40:22.354: INFO: The status of Pod pod-exec-websocket-2b17bbcc-1642-4814-b598-9c1d8f20853d is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:40:22.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7197" for this suite.
•{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":346,"completed":24,"skipped":568,"failed":0}
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:40:22.744: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-map-9a515e3f-cf7a-40bb-9352-678184aab8da
STEP: Creating a pod to test consume configMaps
Jul 31 10:40:22.850: INFO: Waiting up to 5m0s for pod "pod-configmaps-2c7c6cae-aa6f-4249-b5c4-e3d37cf576d0" in namespace "configmap-6190" to be "Succeeded or Failed"
Jul 31 10:40:22.869: INFO: Pod "pod-configmaps-2c7c6cae-aa6f-4249-b5c4-e3d37cf576d0": Phase="Pending", Reason="", readiness=false. Elapsed: 19.354988ms
Jul 31 10:40:24.882: INFO: Pod "pod-configmaps-2c7c6cae-aa6f-4249-b5c4-e3d37cf576d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032596708s
Jul 31 10:40:26.893: INFO: Pod "pod-configmaps-2c7c6cae-aa6f-4249-b5c4-e3d37cf576d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043603292s
STEP: Saw pod success
Jul 31 10:40:26.894: INFO: Pod "pod-configmaps-2c7c6cae-aa6f-4249-b5c4-e3d37cf576d0" satisfied condition "Succeeded or Failed"
Jul 31 10:40:26.898: INFO: Trying to get logs from node p1-ashfcfj4pzo6aemt1j3a9ah7ew pod pod-configmaps-2c7c6cae-aa6f-4249-b5c4-e3d37cf576d0 container agnhost-container: <nil>
STEP: delete the pod
Jul 31 10:40:26.961: INFO: Waiting for pod pod-configmaps-2c7c6cae-aa6f-4249-b5c4-e3d37cf576d0 to disappear
Jul 31 10:40:26.966: INFO: Pod pod-configmaps-2c7c6cae-aa6f-4249-b5c4-e3d37cf576d0 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:40:26.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6190" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":25,"skipped":569,"failed":0}
SSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:40:26.992: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jul 31 10:40:27.069: INFO: Pod name pod-release: Found 0 pods out of 1
Jul 31 10:40:32.088: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:40:33.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2231" for this suite.

• [SLOW TEST:6.154 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":346,"completed":26,"skipped":577,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:40:33.149: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 10:40:33.249: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jul 31 10:40:36.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=crd-publish-openapi-1420 --namespace=crd-publish-openapi-1420 create -f -'
Jul 31 10:40:37.655: INFO: stderr: ""
Jul 31 10:40:37.655: INFO: stdout: "e2e-test-crd-publish-openapi-7719-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jul 31 10:40:37.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=crd-publish-openapi-1420 --namespace=crd-publish-openapi-1420 delete e2e-test-crd-publish-openapi-7719-crds test-cr'
Jul 31 10:40:37.766: INFO: stderr: ""
Jul 31 10:40:37.766: INFO: stdout: "e2e-test-crd-publish-openapi-7719-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jul 31 10:40:37.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=crd-publish-openapi-1420 --namespace=crd-publish-openapi-1420 apply -f -'
Jul 31 10:40:37.965: INFO: stderr: ""
Jul 31 10:40:37.965: INFO: stdout: "e2e-test-crd-publish-openapi-7719-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jul 31 10:40:37.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=crd-publish-openapi-1420 --namespace=crd-publish-openapi-1420 delete e2e-test-crd-publish-openapi-7719-crds test-cr'
Jul 31 10:40:38.095: INFO: stderr: ""
Jul 31 10:40:38.095: INFO: stdout: "e2e-test-crd-publish-openapi-7719-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jul 31 10:40:38.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=crd-publish-openapi-1420 explain e2e-test-crd-publish-openapi-7719-crds'
Jul 31 10:40:38.286: INFO: stderr: ""
Jul 31 10:40:38.286: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7719-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:40:41.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1420" for this suite.

• [SLOW TEST:8.643 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":346,"completed":27,"skipped":599,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:40:41.792: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 10:40:41.923: INFO: Waiting up to 5m0s for pod "busybox-user-65534-ff781e6c-4b21-486b-a62b-cb58ee7ebf25" in namespace "security-context-test-9250" to be "Succeeded or Failed"
Jul 31 10:40:41.942: INFO: Pod "busybox-user-65534-ff781e6c-4b21-486b-a62b-cb58ee7ebf25": Phase="Pending", Reason="", readiness=false. Elapsed: 18.146297ms
Jul 31 10:40:43.954: INFO: Pod "busybox-user-65534-ff781e6c-4b21-486b-a62b-cb58ee7ebf25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030222683s
Jul 31 10:40:45.967: INFO: Pod "busybox-user-65534-ff781e6c-4b21-486b-a62b-cb58ee7ebf25": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043943259s
Jul 31 10:40:45.967: INFO: Pod "busybox-user-65534-ff781e6c-4b21-486b-a62b-cb58ee7ebf25" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:40:45.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9250" for this suite.
•{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":28,"skipped":642,"failed":0}
SS
------------------------------
[sig-network] Services 
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:40:45.996: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service in namespace services-4783
STEP: creating service affinity-nodeport in namespace services-4783
STEP: creating replication controller affinity-nodeport in namespace services-4783
I0731 10:40:46.155501      23 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-4783, replica count: 3
I0731 10:40:49.206667      23 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 31 10:40:49.229: INFO: Creating new exec pod
Jul 31 10:40:52.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-4783 exec execpod-affinityklcfm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Jul 31 10:40:52.509: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jul 31 10:40:52.509: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 31 10:40:52.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-4783 exec execpod-affinityklcfm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.29.187 80'
Jul 31 10:40:52.715: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.102.29.187 80\nConnection to 10.102.29.187 80 port [tcp/http] succeeded!\n"
Jul 31 10:40:52.715: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 31 10:40:52.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-4783 exec execpod-affinityklcfm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.27.21.69 31924'
Jul 31 10:40:52.919: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.27.21.69 31924\nConnection to 172.27.21.69 31924 port [tcp/*] succeeded!\n"
Jul 31 10:40:52.919: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 31 10:40:52.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-4783 exec execpod-affinityklcfm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.27.21.79 31924'
Jul 31 10:40:53.123: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.27.21.79 31924\nConnection to 172.27.21.79 31924 port [tcp/*] succeeded!\n"
Jul 31 10:40:53.123: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 31 10:40:53.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-4783 exec execpod-affinityklcfm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.27.21.73:31924/ ; done'
Jul 31 10:40:53.438: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:31924/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:31924/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:31924/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:31924/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:31924/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:31924/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:31924/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:31924/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:31924/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:31924/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:31924/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:31924/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:31924/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:31924/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:31924/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:31924/\n"
Jul 31 10:40:53.438: INFO: stdout: "\naffinity-nodeport-g92nt\naffinity-nodeport-g92nt\naffinity-nodeport-g92nt\naffinity-nodeport-g92nt\naffinity-nodeport-g92nt\naffinity-nodeport-g92nt\naffinity-nodeport-g92nt\naffinity-nodeport-g92nt\naffinity-nodeport-g92nt\naffinity-nodeport-g92nt\naffinity-nodeport-g92nt\naffinity-nodeport-g92nt\naffinity-nodeport-g92nt\naffinity-nodeport-g92nt\naffinity-nodeport-g92nt\naffinity-nodeport-g92nt"
Jul 31 10:40:53.438: INFO: Received response from host: affinity-nodeport-g92nt
Jul 31 10:40:53.438: INFO: Received response from host: affinity-nodeport-g92nt
Jul 31 10:40:53.438: INFO: Received response from host: affinity-nodeport-g92nt
Jul 31 10:40:53.438: INFO: Received response from host: affinity-nodeport-g92nt
Jul 31 10:40:53.438: INFO: Received response from host: affinity-nodeport-g92nt
Jul 31 10:40:53.438: INFO: Received response from host: affinity-nodeport-g92nt
Jul 31 10:40:53.438: INFO: Received response from host: affinity-nodeport-g92nt
Jul 31 10:40:53.438: INFO: Received response from host: affinity-nodeport-g92nt
Jul 31 10:40:53.438: INFO: Received response from host: affinity-nodeport-g92nt
Jul 31 10:40:53.438: INFO: Received response from host: affinity-nodeport-g92nt
Jul 31 10:40:53.438: INFO: Received response from host: affinity-nodeport-g92nt
Jul 31 10:40:53.438: INFO: Received response from host: affinity-nodeport-g92nt
Jul 31 10:40:53.438: INFO: Received response from host: affinity-nodeport-g92nt
Jul 31 10:40:53.438: INFO: Received response from host: affinity-nodeport-g92nt
Jul 31 10:40:53.438: INFO: Received response from host: affinity-nodeport-g92nt
Jul 31 10:40:53.438: INFO: Received response from host: affinity-nodeport-g92nt
Jul 31 10:40:53.438: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-4783, will wait for the garbage collector to delete the pods
Jul 31 10:40:53.571: INFO: Deleting ReplicationController affinity-nodeport took: 10.797863ms
Jul 31 10:40:53.672: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.372481ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:40:55.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4783" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:9.864 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":29,"skipped":644,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:40:55.861: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Creating a NodePort Service
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota
STEP: Ensuring resource quota status captures service creation
STEP: Deleting Services
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:41:07.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9835" for this suite.

• [SLOW TEST:11.498 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":346,"completed":30,"skipped":655,"failed":0}
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:41:07.359: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-map-703ce080-63fe-42c6-adc9-e64acf8a86e6
STEP: Creating a pod to test consume secrets
Jul 31 10:41:07.455: INFO: Waiting up to 5m0s for pod "pod-secrets-5ac2c82e-3a7e-427b-a289-69ecb2d7134e" in namespace "secrets-4948" to be "Succeeded or Failed"
Jul 31 10:41:07.484: INFO: Pod "pod-secrets-5ac2c82e-3a7e-427b-a289-69ecb2d7134e": Phase="Pending", Reason="", readiness=false. Elapsed: 28.063121ms
Jul 31 10:41:09.496: INFO: Pod "pod-secrets-5ac2c82e-3a7e-427b-a289-69ecb2d7134e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040898595s
Jul 31 10:41:11.511: INFO: Pod "pod-secrets-5ac2c82e-3a7e-427b-a289-69ecb2d7134e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055229555s
STEP: Saw pod success
Jul 31 10:41:11.511: INFO: Pod "pod-secrets-5ac2c82e-3a7e-427b-a289-69ecb2d7134e" satisfied condition "Succeeded or Failed"
Jul 31 10:41:11.516: INFO: Trying to get logs from node p1-nc3cz44465xd41a6poxnmrpeqo pod pod-secrets-5ac2c82e-3a7e-427b-a289-69ecb2d7134e container secret-volume-test: <nil>
STEP: delete the pod
Jul 31 10:41:11.576: INFO: Waiting for pod pod-secrets-5ac2c82e-3a7e-427b-a289-69ecb2d7134e to disappear
Jul 31 10:41:11.583: INFO: Pod pod-secrets-5ac2c82e-3a7e-427b-a289-69ecb2d7134e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:41:11.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4948" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":31,"skipped":661,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:41:11.606: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Jul 31 10:41:11.712: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ec67b937-bb91-4a12-884f-2cd269d79fb8" in namespace "downward-api-5056" to be "Succeeded or Failed"
Jul 31 10:41:11.738: INFO: Pod "downwardapi-volume-ec67b937-bb91-4a12-884f-2cd269d79fb8": Phase="Pending", Reason="", readiness=false. Elapsed: 25.355235ms
Jul 31 10:41:13.833: INFO: Pod "downwardapi-volume-ec67b937-bb91-4a12-884f-2cd269d79fb8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.120741628s
Jul 31 10:41:15.850: INFO: Pod "downwardapi-volume-ec67b937-bb91-4a12-884f-2cd269d79fb8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.137139935s
Jul 31 10:41:17.862: INFO: Pod "downwardapi-volume-ec67b937-bb91-4a12-884f-2cd269d79fb8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.149790665s
STEP: Saw pod success
Jul 31 10:41:17.862: INFO: Pod "downwardapi-volume-ec67b937-bb91-4a12-884f-2cd269d79fb8" satisfied condition "Succeeded or Failed"
Jul 31 10:41:17.867: INFO: Trying to get logs from node p1-nc3cz44465xd41a6poxnmrpeqo pod downwardapi-volume-ec67b937-bb91-4a12-884f-2cd269d79fb8 container client-container: <nil>
STEP: delete the pod
Jul 31 10:41:17.923: INFO: Waiting for pod downwardapi-volume-ec67b937-bb91-4a12-884f-2cd269d79fb8 to disappear
Jul 31 10:41:17.928: INFO: Pod downwardapi-volume-ec67b937-bb91-4a12-884f-2cd269d79fb8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:41:17.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5056" for this suite.

• [SLOW TEST:6.338 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":32,"skipped":673,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:41:17.945: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Jul 31 10:41:18.062: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4235d492-be87-48e1-90ba-b0591518d44c" in namespace "projected-4042" to be "Succeeded or Failed"
Jul 31 10:41:18.084: INFO: Pod "downwardapi-volume-4235d492-be87-48e1-90ba-b0591518d44c": Phase="Pending", Reason="", readiness=false. Elapsed: 22.372921ms
Jul 31 10:41:20.109: INFO: Pod "downwardapi-volume-4235d492-be87-48e1-90ba-b0591518d44c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047459872s
Jul 31 10:41:22.120: INFO: Pod "downwardapi-volume-4235d492-be87-48e1-90ba-b0591518d44c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.057797244s
STEP: Saw pod success
Jul 31 10:41:22.120: INFO: Pod "downwardapi-volume-4235d492-be87-48e1-90ba-b0591518d44c" satisfied condition "Succeeded or Failed"
Jul 31 10:41:22.124: INFO: Trying to get logs from node p1-eipeq63rfgo6ooocmu3kqk3tcc pod downwardapi-volume-4235d492-be87-48e1-90ba-b0591518d44c container client-container: <nil>
STEP: delete the pod
Jul 31 10:41:22.191: INFO: Waiting for pod downwardapi-volume-4235d492-be87-48e1-90ba-b0591518d44c to disappear
Jul 31 10:41:22.196: INFO: Pod downwardapi-volume-4235d492-be87-48e1-90ba-b0591518d44c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:41:22.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4042" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":33,"skipped":713,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:41:22.211: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating pod
Jul 31 10:41:22.319: INFO: The status of Pod pod-hostip-1438f226-94f3-4fc0-ba2c-9cea4c9a071f is Pending, waiting for it to be Running (with Ready = true)
Jul 31 10:41:24.335: INFO: The status of Pod pod-hostip-1438f226-94f3-4fc0-ba2c-9cea4c9a071f is Running (Ready = true)
Jul 31 10:41:24.346: INFO: Pod pod-hostip-1438f226-94f3-4fc0-ba2c-9cea4c9a071f has hostIP: 172.27.21.79
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:41:24.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5049" for this suite.
•{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","total":346,"completed":34,"skipped":753,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:41:24.378: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jul 31 10:41:24.493: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-319  64dbd73c-c14a-44a2-b0a2-e2fb2e1a57c1 9244 0 2022-07-31 10:41:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-07-31 10:41:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 31 10:41:24.494: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-319  64dbd73c-c14a-44a2-b0a2-e2fb2e1a57c1 9244 0 2022-07-31 10:41:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-07-31 10:41:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jul 31 10:41:24.507: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-319  64dbd73c-c14a-44a2-b0a2-e2fb2e1a57c1 9245 0 2022-07-31 10:41:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-07-31 10:41:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 31 10:41:24.507: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-319  64dbd73c-c14a-44a2-b0a2-e2fb2e1a57c1 9245 0 2022-07-31 10:41:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-07-31 10:41:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jul 31 10:41:24.519: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-319  64dbd73c-c14a-44a2-b0a2-e2fb2e1a57c1 9246 0 2022-07-31 10:41:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-07-31 10:41:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 31 10:41:24.519: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-319  64dbd73c-c14a-44a2-b0a2-e2fb2e1a57c1 9246 0 2022-07-31 10:41:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-07-31 10:41:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jul 31 10:41:24.528: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-319  64dbd73c-c14a-44a2-b0a2-e2fb2e1a57c1 9247 0 2022-07-31 10:41:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-07-31 10:41:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 31 10:41:24.529: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-319  64dbd73c-c14a-44a2-b0a2-e2fb2e1a57c1 9247 0 2022-07-31 10:41:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2022-07-31 10:41:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jul 31 10:41:24.544: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-319  99bcd60a-5e92-49bd-a14b-bac5ede7b5b3 9248 0 2022-07-31 10:41:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-07-31 10:41:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 31 10:41:24.544: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-319  99bcd60a-5e92-49bd-a14b-bac5ede7b5b3 9248 0 2022-07-31 10:41:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-07-31 10:41:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jul 31 10:41:34.579: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-319  99bcd60a-5e92-49bd-a14b-bac5ede7b5b3 9299 0 2022-07-31 10:41:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-07-31 10:41:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 31 10:41:34.579: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-319  99bcd60a-5e92-49bd-a14b-bac5ede7b5b3 9299 0 2022-07-31 10:41:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2022-07-31 10:41:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:41:44.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-319" for this suite.

• [SLOW TEST:20.260 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":346,"completed":35,"skipped":768,"failed":0}
SSSSSSS
------------------------------
[sig-network] HostPort 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:41:44.639: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename hostport
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/hostport.go:47
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled
Jul 31 10:41:44.776: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jul 31 10:41:46.793: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jul 31 10:41:48.795: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.27.21.69 on the node which pod1 resides and expect scheduled
Jul 31 10:41:48.815: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jul 31 10:41:50.823: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.27.21.69 but use UDP protocol on the node which pod2 resides
Jul 31 10:41:50.839: INFO: The status of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jul 31 10:41:52.855: INFO: The status of Pod pod3 is Running (Ready = false)
Jul 31 10:41:54.859: INFO: The status of Pod pod3 is Running (Ready = true)
Jul 31 10:41:54.881: INFO: The status of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jul 31 10:41:56.892: INFO: The status of Pod e2e-host-exec is Running (Ready = true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323
Jul 31 10:41:56.896: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.27.21.69 http://127.0.0.1:54323/hostname] Namespace:hostport-3102 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 31 10:41:56.896: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
Jul 31 10:41:56.896: INFO: ExecWithOptions: Clientset creation
Jul 31 10:41:56.896: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-3102/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+172.27.21.69+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true %!s(MISSING))
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.27.21.69, port: 54323
Jul 31 10:41:57.033: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.27.21.69:54323/hostname] Namespace:hostport-3102 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 31 10:41:57.033: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
Jul 31 10:41:57.034: INFO: ExecWithOptions: Clientset creation
Jul 31 10:41:57.034: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-3102/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F172.27.21.69%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true %!s(MISSING))
STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.27.21.69, port: 54323 UDP
Jul 31 10:41:57.171: INFO: ExecWithOptions {Command:[/bin/sh -c nc -vuz -w 5 172.27.21.69 54323] Namespace:hostport-3102 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 31 10:41:57.171: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
Jul 31 10:41:57.172: INFO: ExecWithOptions: Clientset creation
Jul 31 10:41:57.172: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-3102/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=nc+-vuz+-w+5+172.27.21.69+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true %!s(MISSING))
[AfterEach] [sig-network] HostPort
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:42:02.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-3102" for this suite.

• [SLOW TEST:17.701 seconds]
[sig-network] HostPort
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","total":346,"completed":36,"skipped":775,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:42:02.340: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl can dry-run update Pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Jul 31 10:42:02.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-8005 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jul 31 10:42:02.517: INFO: stderr: ""
Jul 31 10:42:02.517: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Jul 31 10:42:02.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-8005 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "k8s.gcr.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Jul 31 10:42:03.268: INFO: stderr: ""
Jul 31 10:42:03.268: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Jul 31 10:42:03.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-8005 delete pods e2e-test-httpd-pod'
Jul 31 10:42:05.844: INFO: stderr: ""
Jul 31 10:42:05.844: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:42:05.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8005" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":346,"completed":37,"skipped":777,"failed":0}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:42:05.885: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 31 10:42:06.421: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 31 10:42:09.476: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:42:09.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5119" for this suite.
STEP: Destroying namespace "webhook-5119-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":346,"completed":38,"skipped":778,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring 
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:42:09.792: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename endpointslicemirroring
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslicemirroring.go:39
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: mirroring a new custom Endpoint
Jul 31 10:42:09.960: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint
STEP: mirroring deletion of a custom Endpoint
[AfterEach] [sig-network] EndpointSliceMirroring
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:42:12.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-2002" for this suite.
•{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","total":346,"completed":39,"skipped":809,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:42:12.041: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 10:42:12.127: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-f97b0fe7-ea17-4d3f-a4f9-6c88b6bcdb08" in namespace "security-context-test-6456" to be "Succeeded or Failed"
Jul 31 10:42:12.144: INFO: Pod "alpine-nnp-false-f97b0fe7-ea17-4d3f-a4f9-6c88b6bcdb08": Phase="Pending", Reason="", readiness=false. Elapsed: 17.093058ms
Jul 31 10:42:14.154: INFO: Pod "alpine-nnp-false-f97b0fe7-ea17-4d3f-a4f9-6c88b6bcdb08": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026851683s
Jul 31 10:42:16.172: INFO: Pod "alpine-nnp-false-f97b0fe7-ea17-4d3f-a4f9-6c88b6bcdb08": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045396832s
Jul 31 10:42:18.187: INFO: Pod "alpine-nnp-false-f97b0fe7-ea17-4d3f-a4f9-6c88b6bcdb08": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.060516845s
Jul 31 10:42:18.187: INFO: Pod "alpine-nnp-false-f97b0fe7-ea17-4d3f-a4f9-6c88b6bcdb08" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:42:18.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6456" for this suite.

• [SLOW TEST:6.181 seconds]
[sig-node] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:296
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":40,"skipped":855,"failed":0}
SS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:42:18.223: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename ingressclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:186
[It]  should support creating IngressClass API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jul 31 10:42:18.347: INFO: starting watch
STEP: patching
STEP: updating
Jul 31 10:42:18.400: INFO: waiting for watch events with expected annotations
Jul 31 10:42:18.400: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:42:18.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-215" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":346,"completed":41,"skipped":857,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:42:18.473: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Waiting for the pdb to be processed
STEP: Updating PodDisruptionBudget status
STEP: Waiting for all pods to be running
Jul 31 10:42:20.595: INFO: running pods: 0 < 1
Jul 31 10:42:22.607: INFO: running pods: 0 < 1
STEP: locating a running pod
STEP: Waiting for the pdb to be processed
STEP: Patching PodDisruptionBudget status
STEP: Waiting for the pdb to be processed
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:42:24.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-5571" for this suite.

• [SLOW TEST:6.221 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","total":346,"completed":42,"skipped":932,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:42:24.696: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting the auto-created API token
STEP: reading a file in the container
Jul 31 10:42:27.309: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8625 pod-service-account-4b8cd037-3ba8-47f7-b7f0-3300ea3b3d0a -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jul 31 10:42:27.762: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8625 pod-service-account-4b8cd037-3ba8-47f7-b7f0-3300ea3b3d0a -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jul 31 10:42:27.974: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8625 pod-service-account-4b8cd037-3ba8-47f7-b7f0-3300ea3b3d0a -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:42:28.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8625" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":346,"completed":43,"skipped":943,"failed":0}
SSS
------------------------------
[sig-node] Secrets 
  should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:42:28.234: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should patch a secret [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:42:28.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4534" for this suite.
•{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","total":346,"completed":44,"skipped":946,"failed":0}
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:42:28.381: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name cm-test-opt-del-c8ffcfef-eb80-40ec-9156-919f5b7dc15d
STEP: Creating configMap with name cm-test-opt-upd-91a7a787-4065-4051-a737-7893bf25b812
STEP: Creating the pod
Jul 31 10:42:28.485: INFO: The status of Pod pod-projected-configmaps-58e86a15-03c7-4b39-a19c-53abddf2b5a0 is Pending, waiting for it to be Running (with Ready = true)
Jul 31 10:42:30.498: INFO: The status of Pod pod-projected-configmaps-58e86a15-03c7-4b39-a19c-53abddf2b5a0 is Pending, waiting for it to be Running (with Ready = true)
Jul 31 10:42:32.500: INFO: The status of Pod pod-projected-configmaps-58e86a15-03c7-4b39-a19c-53abddf2b5a0 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-c8ffcfef-eb80-40ec-9156-919f5b7dc15d
STEP: Updating configmap cm-test-opt-upd-91a7a787-4065-4051-a737-7893bf25b812
STEP: Creating configMap with name cm-test-opt-create-db5f8c35-7aed-4e00-bbb3-c654bdd2edbf
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:42:34.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3279" for this suite.

• [SLOW TEST:6.281 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":45,"skipped":952,"failed":0}
SSS
------------------------------
[sig-node] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:42:34.663: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 10:42:34.722: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jul 31 10:42:34.780: INFO: The status of Pod pod-logs-websocket-314f9b1d-f7f1-4172-b519-f195c7bc8306 is Pending, waiting for it to be Running (with Ready = true)
Jul 31 10:42:36.797: INFO: The status of Pod pod-logs-websocket-314f9b1d-f7f1-4172-b519-f195c7bc8306 is Running (Ready = true)
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:42:36.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6662" for this suite.
•{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":346,"completed":46,"skipped":955,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:42:36.853: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Jul 31 10:42:36.937: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4e55e66b-6e96-403b-8e6d-e7ff7501ab04" in namespace "downward-api-3698" to be "Succeeded or Failed"
Jul 31 10:42:36.962: INFO: Pod "downwardapi-volume-4e55e66b-6e96-403b-8e6d-e7ff7501ab04": Phase="Pending", Reason="", readiness=false. Elapsed: 25.237539ms
Jul 31 10:42:38.979: INFO: Pod "downwardapi-volume-4e55e66b-6e96-403b-8e6d-e7ff7501ab04": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042184763s
Jul 31 10:42:40.991: INFO: Pod "downwardapi-volume-4e55e66b-6e96-403b-8e6d-e7ff7501ab04": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053512839s
STEP: Saw pod success
Jul 31 10:42:40.991: INFO: Pod "downwardapi-volume-4e55e66b-6e96-403b-8e6d-e7ff7501ab04" satisfied condition "Succeeded or Failed"
Jul 31 10:42:40.995: INFO: Trying to get logs from node p1-nc3cz44465xd41a6poxnmrpeqo pod downwardapi-volume-4e55e66b-6e96-403b-8e6d-e7ff7501ab04 container client-container: <nil>
STEP: delete the pod
Jul 31 10:42:41.049: INFO: Waiting for pod downwardapi-volume-4e55e66b-6e96-403b-8e6d-e7ff7501ab04 to disappear
Jul 31 10:42:41.054: INFO: Pod downwardapi-volume-4e55e66b-6e96-403b-8e6d-e7ff7501ab04 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:42:41.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3698" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":47,"skipped":981,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:42:41.070: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:296
[It] should scale a replication controller  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a replication controller
Jul 31 10:42:41.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-8533 create -f -'
Jul 31 10:42:41.822: INFO: stderr: ""
Jul 31 10:42:41.822: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 31 10:42:41.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-8533 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 31 10:42:41.927: INFO: stderr: ""
Jul 31 10:42:41.927: INFO: stdout: "update-demo-nautilus-bnpmn update-demo-nautilus-xz9tk "
Jul 31 10:42:41.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-8533 get pods update-demo-nautilus-bnpmn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 31 10:42:42.004: INFO: stderr: ""
Jul 31 10:42:42.005: INFO: stdout: ""
Jul 31 10:42:42.005: INFO: update-demo-nautilus-bnpmn is created but not running
Jul 31 10:42:47.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-8533 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 31 10:42:47.084: INFO: stderr: ""
Jul 31 10:42:47.084: INFO: stdout: "update-demo-nautilus-bnpmn update-demo-nautilus-xz9tk "
Jul 31 10:42:47.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-8533 get pods update-demo-nautilus-bnpmn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 31 10:42:47.154: INFO: stderr: ""
Jul 31 10:42:47.154: INFO: stdout: "true"
Jul 31 10:42:47.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-8533 get pods update-demo-nautilus-bnpmn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 31 10:42:47.228: INFO: stderr: ""
Jul 31 10:42:47.228: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jul 31 10:42:47.228: INFO: validating pod update-demo-nautilus-bnpmn
Jul 31 10:42:47.237: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 31 10:42:47.238: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 31 10:42:47.238: INFO: update-demo-nautilus-bnpmn is verified up and running
Jul 31 10:42:47.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-8533 get pods update-demo-nautilus-xz9tk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 31 10:42:47.308: INFO: stderr: ""
Jul 31 10:42:47.309: INFO: stdout: ""
Jul 31 10:42:47.309: INFO: update-demo-nautilus-xz9tk is created but not running
Jul 31 10:42:52.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-8533 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 31 10:42:52.420: INFO: stderr: ""
Jul 31 10:42:52.420: INFO: stdout: "update-demo-nautilus-bnpmn update-demo-nautilus-xz9tk "
Jul 31 10:42:52.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-8533 get pods update-demo-nautilus-bnpmn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 31 10:42:52.505: INFO: stderr: ""
Jul 31 10:42:52.505: INFO: stdout: "true"
Jul 31 10:42:52.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-8533 get pods update-demo-nautilus-bnpmn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 31 10:42:52.569: INFO: stderr: ""
Jul 31 10:42:52.569: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jul 31 10:42:52.569: INFO: validating pod update-demo-nautilus-bnpmn
Jul 31 10:42:52.581: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 31 10:42:52.581: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 31 10:42:52.581: INFO: update-demo-nautilus-bnpmn is verified up and running
Jul 31 10:42:52.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-8533 get pods update-demo-nautilus-xz9tk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 31 10:42:52.653: INFO: stderr: ""
Jul 31 10:42:52.653: INFO: stdout: "true"
Jul 31 10:42:52.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-8533 get pods update-demo-nautilus-xz9tk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 31 10:42:52.723: INFO: stderr: ""
Jul 31 10:42:52.723: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jul 31 10:42:52.724: INFO: validating pod update-demo-nautilus-xz9tk
Jul 31 10:42:52.733: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 31 10:42:52.733: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 31 10:42:52.733: INFO: update-demo-nautilus-xz9tk is verified up and running
STEP: scaling down the replication controller
Jul 31 10:42:52.735: INFO: scanned /root for discovery docs: <nil>
Jul 31 10:42:52.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-8533 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jul 31 10:42:53.837: INFO: stderr: ""
Jul 31 10:42:53.837: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 31 10:42:53.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-8533 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 31 10:42:53.919: INFO: stderr: ""
Jul 31 10:42:53.919: INFO: stdout: "update-demo-nautilus-bnpmn update-demo-nautilus-xz9tk "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jul 31 10:42:58.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-8533 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 31 10:42:58.995: INFO: stderr: ""
Jul 31 10:42:58.995: INFO: stdout: "update-demo-nautilus-bnpmn "
Jul 31 10:42:58.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-8533 get pods update-demo-nautilus-bnpmn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 31 10:42:59.081: INFO: stderr: ""
Jul 31 10:42:59.081: INFO: stdout: "true"
Jul 31 10:42:59.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-8533 get pods update-demo-nautilus-bnpmn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 31 10:42:59.149: INFO: stderr: ""
Jul 31 10:42:59.149: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jul 31 10:42:59.149: INFO: validating pod update-demo-nautilus-bnpmn
Jul 31 10:42:59.156: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 31 10:42:59.156: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 31 10:42:59.156: INFO: update-demo-nautilus-bnpmn is verified up and running
STEP: scaling up the replication controller
Jul 31 10:42:59.159: INFO: scanned /root for discovery docs: <nil>
Jul 31 10:42:59.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-8533 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jul 31 10:43:00.274: INFO: stderr: ""
Jul 31 10:43:00.274: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 31 10:43:00.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-8533 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 31 10:43:00.349: INFO: stderr: ""
Jul 31 10:43:00.349: INFO: stdout: "update-demo-nautilus-bnpmn update-demo-nautilus-jl94f "
Jul 31 10:43:00.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-8533 get pods update-demo-nautilus-bnpmn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 31 10:43:00.415: INFO: stderr: ""
Jul 31 10:43:00.415: INFO: stdout: "true"
Jul 31 10:43:00.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-8533 get pods update-demo-nautilus-bnpmn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 31 10:43:00.486: INFO: stderr: ""
Jul 31 10:43:00.486: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jul 31 10:43:00.486: INFO: validating pod update-demo-nautilus-bnpmn
Jul 31 10:43:00.493: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 31 10:43:00.493: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 31 10:43:00.493: INFO: update-demo-nautilus-bnpmn is verified up and running
Jul 31 10:43:00.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-8533 get pods update-demo-nautilus-jl94f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 31 10:43:00.563: INFO: stderr: ""
Jul 31 10:43:00.563: INFO: stdout: ""
Jul 31 10:43:00.563: INFO: update-demo-nautilus-jl94f is created but not running
Jul 31 10:43:05.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-8533 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jul 31 10:43:05.688: INFO: stderr: ""
Jul 31 10:43:05.688: INFO: stdout: "update-demo-nautilus-bnpmn update-demo-nautilus-jl94f "
Jul 31 10:43:05.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-8533 get pods update-demo-nautilus-bnpmn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 31 10:43:05.762: INFO: stderr: ""
Jul 31 10:43:05.762: INFO: stdout: "true"
Jul 31 10:43:05.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-8533 get pods update-demo-nautilus-bnpmn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 31 10:43:05.827: INFO: stderr: ""
Jul 31 10:43:05.827: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jul 31 10:43:05.828: INFO: validating pod update-demo-nautilus-bnpmn
Jul 31 10:43:05.834: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 31 10:43:05.834: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 31 10:43:05.835: INFO: update-demo-nautilus-bnpmn is verified up and running
Jul 31 10:43:05.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-8533 get pods update-demo-nautilus-jl94f -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jul 31 10:43:05.914: INFO: stderr: ""
Jul 31 10:43:05.914: INFO: stdout: "true"
Jul 31 10:43:05.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-8533 get pods update-demo-nautilus-jl94f -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jul 31 10:43:05.986: INFO: stderr: ""
Jul 31 10:43:05.986: INFO: stdout: "k8s.gcr.io/e2e-test-images/nautilus:1.5"
Jul 31 10:43:05.986: INFO: validating pod update-demo-nautilus-jl94f
Jul 31 10:43:05.995: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 31 10:43:05.995: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 31 10:43:05.995: INFO: update-demo-nautilus-jl94f is verified up and running
STEP: using delete to clean up resources
Jul 31 10:43:05.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-8533 delete --grace-period=0 --force -f -'
Jul 31 10:43:06.095: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 31 10:43:06.095: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul 31 10:43:06.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-8533 get rc,svc -l name=update-demo --no-headers'
Jul 31 10:43:06.204: INFO: stderr: "No resources found in kubectl-8533 namespace.\n"
Jul 31 10:43:06.204: INFO: stdout: ""
Jul 31 10:43:06.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-8533 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 31 10:43:06.306: INFO: stderr: ""
Jul 31 10:43:06.306: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:43:06.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8533" for this suite.

• [SLOW TEST:25.257 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:294
    should scale a replication controller  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":346,"completed":48,"skipped":987,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:43:06.327: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
Jul 31 10:43:06.428: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jul 31 10:43:08.443: INFO: The status of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jul 31 10:43:10.441: INFO: The status of Pod test-pod is Running (Ready = true)
STEP: Creating hostNetwork=true pod
Jul 31 10:43:10.483: INFO: The status of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jul 31 10:43:12.495: INFO: The status of Pod test-host-network-pod is Running (Ready = true)
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jul 31 10:43:12.500: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7674 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 31 10:43:12.500: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
Jul 31 10:43:12.501: INFO: ExecWithOptions: Clientset creation
Jul 31 10:43:12.501: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7674/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
Jul 31 10:43:12.652: INFO: Exec stderr: ""
Jul 31 10:43:12.652: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7674 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 31 10:43:12.652: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
Jul 31 10:43:12.653: INFO: ExecWithOptions: Clientset creation
Jul 31 10:43:12.653: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7674/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
Jul 31 10:43:12.792: INFO: Exec stderr: ""
Jul 31 10:43:12.792: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7674 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 31 10:43:12.792: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
Jul 31 10:43:12.793: INFO: ExecWithOptions: Clientset creation
Jul 31 10:43:12.793: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7674/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
Jul 31 10:43:12.935: INFO: Exec stderr: ""
Jul 31 10:43:12.935: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7674 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 31 10:43:12.935: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
Jul 31 10:43:12.936: INFO: ExecWithOptions: Clientset creation
Jul 31 10:43:12.936: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7674/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
Jul 31 10:43:13.078: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jul 31 10:43:13.078: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7674 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 31 10:43:13.078: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
Jul 31 10:43:13.079: INFO: ExecWithOptions: Clientset creation
Jul 31 10:43:13.079: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7674/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true %!s(MISSING))
Jul 31 10:43:13.213: INFO: Exec stderr: ""
Jul 31 10:43:13.213: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7674 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 31 10:43:13.213: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
Jul 31 10:43:13.215: INFO: ExecWithOptions: Clientset creation
Jul 31 10:43:13.215: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7674/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true %!s(MISSING))
Jul 31 10:43:13.360: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jul 31 10:43:13.360: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7674 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 31 10:43:13.360: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
Jul 31 10:43:13.361: INFO: ExecWithOptions: Clientset creation
Jul 31 10:43:13.361: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7674/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
Jul 31 10:43:13.499: INFO: Exec stderr: ""
Jul 31 10:43:13.499: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7674 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 31 10:43:13.499: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
Jul 31 10:43:13.500: INFO: ExecWithOptions: Clientset creation
Jul 31 10:43:13.500: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7674/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true %!s(MISSING))
Jul 31 10:43:13.665: INFO: Exec stderr: ""
Jul 31 10:43:13.665: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7674 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 31 10:43:13.665: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
Jul 31 10:43:13.666: INFO: ExecWithOptions: Clientset creation
Jul 31 10:43:13.666: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7674/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
Jul 31 10:43:13.801: INFO: Exec stderr: ""
Jul 31 10:43:13.801: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7674 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 31 10:43:13.801: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
Jul 31 10:43:13.802: INFO: ExecWithOptions: Clientset creation
Jul 31 10:43:13.802: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7674/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true %!s(MISSING))
Jul 31 10:43:13.943: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:43:13.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-7674" for this suite.

• [SLOW TEST:7.643 seconds]
[sig-node] KubeletManagedEtcHosts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":49,"skipped":997,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:43:13.970: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 31 10:43:18.108: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:43:18.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7165" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":346,"completed":50,"skipped":1008,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:43:18.168: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a Namespace
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:43:18.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-168" for this suite.
STEP: Destroying namespace "nspatchtest-735675a3-a39a-4f2a-9574-864ffbb40ef1-6153" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":346,"completed":51,"skipped":1083,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:43:18.401: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should add annotations for pods in rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating Agnhost RC
Jul 31 10:43:18.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-8144 create -f -'
Jul 31 10:43:18.666: INFO: stderr: ""
Jul 31 10:43:18.666: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jul 31 10:43:19.689: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 31 10:43:19.689: INFO: Found 0 / 1
Jul 31 10:43:20.675: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 31 10:43:20.675: INFO: Found 1 / 1
Jul 31 10:43:20.675: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jul 31 10:43:20.680: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 31 10:43:20.680: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 31 10:43:20.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-8144 patch pod agnhost-primary-6ntmt -p {"metadata":{"annotations":{"x":"y"}}}'
Jul 31 10:43:20.780: INFO: stderr: ""
Jul 31 10:43:20.780: INFO: stdout: "pod/agnhost-primary-6ntmt patched\n"
STEP: checking annotations
Jul 31 10:43:20.789: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 31 10:43:20.789: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:43:20.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8144" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":346,"completed":52,"skipped":1106,"failed":0}
SSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:43:20.807: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward api env vars
Jul 31 10:43:20.909: INFO: Waiting up to 5m0s for pod "downward-api-acf5ad70-6cc5-407e-a308-a404b1e27e90" in namespace "downward-api-6143" to be "Succeeded or Failed"
Jul 31 10:43:20.927: INFO: Pod "downward-api-acf5ad70-6cc5-407e-a308-a404b1e27e90": Phase="Pending", Reason="", readiness=false. Elapsed: 18.089981ms
Jul 31 10:43:22.940: INFO: Pod "downward-api-acf5ad70-6cc5-407e-a308-a404b1e27e90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030778065s
Jul 31 10:43:24.968: INFO: Pod "downward-api-acf5ad70-6cc5-407e-a308-a404b1e27e90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.058862402s
STEP: Saw pod success
Jul 31 10:43:24.968: INFO: Pod "downward-api-acf5ad70-6cc5-407e-a308-a404b1e27e90" satisfied condition "Succeeded or Failed"
Jul 31 10:43:24.975: INFO: Trying to get logs from node p1-nc3cz44465xd41a6poxnmrpeqo pod downward-api-acf5ad70-6cc5-407e-a308-a404b1e27e90 container dapi-container: <nil>
STEP: delete the pod
Jul 31 10:43:25.032: INFO: Waiting for pod downward-api-acf5ad70-6cc5-407e-a308-a404b1e27e90 to disappear
Jul 31 10:43:25.037: INFO: Pod downward-api-acf5ad70-6cc5-407e-a308-a404b1e27e90 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:43:25.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6143" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":346,"completed":53,"skipped":1109,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:43:25.070: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create and stop a working application  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating all guestbook components
Jul 31 10:43:25.154: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jul 31 10:43:25.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-8818 create -f -'
Jul 31 10:43:25.366: INFO: stderr: ""
Jul 31 10:43:25.366: INFO: stdout: "service/agnhost-replica created\n"
Jul 31 10:43:25.367: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jul 31 10:43:25.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-8818 create -f -'
Jul 31 10:43:26.174: INFO: stderr: ""
Jul 31 10:43:26.174: INFO: stdout: "service/agnhost-primary created\n"
Jul 31 10:43:26.174: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jul 31 10:43:26.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-8818 create -f -'
Jul 31 10:43:26.393: INFO: stderr: ""
Jul 31 10:43:26.393: INFO: stdout: "service/frontend created\n"
Jul 31 10:43:26.393: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.39
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jul 31 10:43:26.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-8818 create -f -'
Jul 31 10:43:26.581: INFO: stderr: ""
Jul 31 10:43:26.581: INFO: stdout: "deployment.apps/frontend created\n"
Jul 31 10:43:26.582: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.39
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jul 31 10:43:26.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-8818 create -f -'
Jul 31 10:43:26.777: INFO: stderr: ""
Jul 31 10:43:26.777: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jul 31 10:43:26.777: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.39
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jul 31 10:43:26.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-8818 create -f -'
Jul 31 10:43:26.943: INFO: stderr: ""
Jul 31 10:43:26.943: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Jul 31 10:43:26.943: INFO: Waiting for all frontend pods to be Running.
Jul 31 10:43:31.995: INFO: Waiting for frontend to serve content.
Jul 31 10:43:32.020: INFO: Trying to add a new entry to the guestbook.
Jul 31 10:43:32.040: INFO: Verifying that added entry can be retrieved.
Jul 31 10:43:32.053: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
STEP: using delete to clean up resources
Jul 31 10:43:37.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-8818 delete --grace-period=0 --force -f -'
Jul 31 10:43:37.187: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 31 10:43:37.187: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Jul 31 10:43:37.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-8818 delete --grace-period=0 --force -f -'
Jul 31 10:43:37.272: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 31 10:43:37.272: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jul 31 10:43:37.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-8818 delete --grace-period=0 --force -f -'
Jul 31 10:43:37.405: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 31 10:43:37.405: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jul 31 10:43:37.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-8818 delete --grace-period=0 --force -f -'
Jul 31 10:43:37.483: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 31 10:43:37.483: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jul 31 10:43:37.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-8818 delete --grace-period=0 --force -f -'
Jul 31 10:43:37.582: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 31 10:43:37.582: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Jul 31 10:43:37.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-8818 delete --grace-period=0 --force -f -'
Jul 31 10:43:37.659: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 31 10:43:37.659: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:43:37.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8818" for this suite.

• [SLOW TEST:12.673 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:339
    should create and stop a working application  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":346,"completed":54,"skipped":1161,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:43:37.744: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 31 10:43:38.503: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 31 10:43:40.530: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.July, 31, 10, 43, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 10, 43, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.July, 31, 10, 43, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 10, 43, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6c69dbd86b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 31 10:43:42.542: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.July, 31, 10, 43, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 10, 43, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.July, 31, 10, 43, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 10, 43, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6c69dbd86b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 31 10:43:44.545: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.July, 31, 10, 43, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 10, 43, 38, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.July, 31, 10, 43, 38, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 10, 43, 38, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6c69dbd86b\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 31 10:43:47.589: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 10:43:47.607: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7716-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:43:50.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5109" for this suite.
STEP: Destroying namespace "webhook-5109-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:13.233 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":346,"completed":55,"skipped":1198,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:43:50.978: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a service externalname-service with the type=ExternalName in namespace services-3333
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-3333
I0731 10:43:51.178233      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-3333, replica count: 2
Jul 31 10:43:54.229: INFO: Creating new exec pod
I0731 10:43:54.229347      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 31 10:43:57.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-3333 exec execpodqc2gr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jul 31 10:43:57.495: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jul 31 10:43:57.495: INFO: stdout: ""
Jul 31 10:43:58.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-3333 exec execpodqc2gr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jul 31 10:43:58.717: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jul 31 10:43:58.717: INFO: stdout: "externalname-service-gn9bv"
Jul 31 10:43:58.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-3333 exec execpodqc2gr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.108.107.253 80'
Jul 31 10:43:58.942: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.108.107.253 80\nConnection to 10.108.107.253 80 port [tcp/http] succeeded!\n"
Jul 31 10:43:58.942: INFO: stdout: "externalname-service-l2cth"
Jul 31 10:43:58.942: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:43:59.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3333" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:8.058 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":346,"completed":56,"skipped":1211,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:43:59.038: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test override command
Jul 31 10:43:59.146: INFO: Waiting up to 5m0s for pod "client-containers-455064bd-b128-436d-b968-df126830db32" in namespace "containers-4240" to be "Succeeded or Failed"
Jul 31 10:43:59.169: INFO: Pod "client-containers-455064bd-b128-436d-b968-df126830db32": Phase="Pending", Reason="", readiness=false. Elapsed: 22.857671ms
Jul 31 10:44:01.185: INFO: Pod "client-containers-455064bd-b128-436d-b968-df126830db32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038687575s
Jul 31 10:44:03.196: INFO: Pod "client-containers-455064bd-b128-436d-b968-df126830db32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049911634s
STEP: Saw pod success
Jul 31 10:44:03.196: INFO: Pod "client-containers-455064bd-b128-436d-b968-df126830db32" satisfied condition "Succeeded or Failed"
Jul 31 10:44:03.201: INFO: Trying to get logs from node p1-esqoe7n9eqq76k8ojd9tajh51e pod client-containers-455064bd-b128-436d-b968-df126830db32 container agnhost-container: <nil>
STEP: delete the pod
Jul 31 10:44:03.260: INFO: Waiting for pod client-containers-455064bd-b128-436d-b968-df126830db32 to disappear
Jul 31 10:44:03.268: INFO: Pod client-containers-455064bd-b128-436d-b968-df126830db32 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:44:03.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4240" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":346,"completed":57,"skipped":1260,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:44:03.295: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:44:03.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-354" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":58,"skipped":1289,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:44:03.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-c83f1ae5-b07a-4a87-b698-787357a6655a
STEP: Creating a pod to test consume configMaps
Jul 31 10:44:03.572: INFO: Waiting up to 5m0s for pod "pod-configmaps-1274eb70-3044-4c36-8730-a238ffc169e5" in namespace "configmap-3832" to be "Succeeded or Failed"
Jul 31 10:44:03.577: INFO: Pod "pod-configmaps-1274eb70-3044-4c36-8730-a238ffc169e5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.658277ms
Jul 31 10:44:05.587: INFO: Pod "pod-configmaps-1274eb70-3044-4c36-8730-a238ffc169e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015121315s
Jul 31 10:44:07.602: INFO: Pod "pod-configmaps-1274eb70-3044-4c36-8730-a238ffc169e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029805992s
STEP: Saw pod success
Jul 31 10:44:07.602: INFO: Pod "pod-configmaps-1274eb70-3044-4c36-8730-a238ffc169e5" satisfied condition "Succeeded or Failed"
Jul 31 10:44:07.608: INFO: Trying to get logs from node p1-esqoe7n9eqq76k8ojd9tajh51e pod pod-configmaps-1274eb70-3044-4c36-8730-a238ffc169e5 container agnhost-container: <nil>
STEP: delete the pod
Jul 31 10:44:07.655: INFO: Waiting for pod pod-configmaps-1274eb70-3044-4c36-8730-a238ffc169e5 to disappear
Jul 31 10:44:07.661: INFO: Pod pod-configmaps-1274eb70-3044-4c36-8730-a238ffc169e5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:44:07.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3832" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":59,"skipped":1300,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:44:07.678: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod busybox-8167d57b-3140-498a-b0a8-3cd2a7637f3f in namespace container-probe-837
Jul 31 10:44:09.806: INFO: Started pod busybox-8167d57b-3140-498a-b0a8-3cd2a7637f3f in namespace container-probe-837
STEP: checking the pod's current state and verifying that restartCount is present
Jul 31 10:44:09.812: INFO: Initial restart count of pod busybox-8167d57b-3140-498a-b0a8-3cd2a7637f3f is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:48:09.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-837" for this suite.

• [SLOW TEST:242.274 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":60,"skipped":1353,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:48:09.952: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a service nodeport-service with the type=NodePort in namespace services-4744
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-4744
STEP: creating replication controller externalsvc in namespace services-4744
I0731 10:48:10.151722      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-4744, replica count: 2
I0731 10:48:13.202158      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0731 10:48:16.202381      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Jul 31 10:48:16.288: INFO: Creating new exec pod
Jul 31 10:48:18.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-4744 exec execpodzrkr5 -- /bin/sh -x -c nslookup nodeport-service.services-4744.svc.cluster.local'
Jul 31 10:48:18.589: INFO: stderr: "+ nslookup nodeport-service.services-4744.svc.cluster.local\n"
Jul 31 10:48:18.589: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-4744.svc.cluster.local\tcanonical name = externalsvc.services-4744.svc.cluster.local.\nName:\texternalsvc.services-4744.svc.cluster.local\nAddress: 10.108.226.217\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4744, will wait for the garbage collector to delete the pods
Jul 31 10:48:18.657: INFO: Deleting ReplicationController externalsvc took: 11.574959ms
Jul 31 10:48:18.758: INFO: Terminating ReplicationController externalsvc pods took: 100.812187ms
Jul 31 10:48:20.926: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:48:20.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4744" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:11.053 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","total":346,"completed":61,"skipped":1366,"failed":0}
SS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:48:21.006: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting a starting resourceVersion
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:48:23.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5446" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":346,"completed":62,"skipped":1368,"failed":0}
SSSSSSS
------------------------------
[sig-node] Security Context 
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:48:23.922: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Jul 31 10:48:24.051: INFO: Waiting up to 5m0s for pod "security-context-3742579d-5dd3-44fb-b83e-d240ab9ff760" in namespace "security-context-5913" to be "Succeeded or Failed"
Jul 31 10:48:24.065: INFO: Pod "security-context-3742579d-5dd3-44fb-b83e-d240ab9ff760": Phase="Pending", Reason="", readiness=false. Elapsed: 13.950679ms
Jul 31 10:48:26.077: INFO: Pod "security-context-3742579d-5dd3-44fb-b83e-d240ab9ff760": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025255643s
Jul 31 10:48:28.087: INFO: Pod "security-context-3742579d-5dd3-44fb-b83e-d240ab9ff760": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035881952s
STEP: Saw pod success
Jul 31 10:48:28.087: INFO: Pod "security-context-3742579d-5dd3-44fb-b83e-d240ab9ff760" satisfied condition "Succeeded or Failed"
Jul 31 10:48:28.091: INFO: Trying to get logs from node p1-eipeq63rfgo6ooocmu3kqk3tcc pod security-context-3742579d-5dd3-44fb-b83e-d240ab9ff760 container test-container: <nil>
STEP: delete the pod
Jul 31 10:48:28.146: INFO: Waiting for pod security-context-3742579d-5dd3-44fb-b83e-d240ab9ff760 to disappear
Jul 31 10:48:28.153: INFO: Pod security-context-3742579d-5dd3-44fb-b83e-d240ab9ff760 no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:48:28.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-5913" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":63,"skipped":1375,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:48:28.183: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 31 10:48:29.421: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 31 10:48:32.487: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:48:32.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7838" for this suite.
STEP: Destroying namespace "webhook-7838-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":346,"completed":64,"skipped":1385,"failed":0}
S
------------------------------
[sig-node] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:48:32.686: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Jul 31 10:48:32.830: INFO: observed Pod pod-test in namespace pods-4635 in phase Pending with labels: map[test-pod-static:true] & conditions []
Jul 31 10:48:32.848: INFO: observed Pod pod-test in namespace pods-4635 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-07-31 10:48:32 +0000 UTC  }]
Jul 31 10:48:32.872: INFO: observed Pod pod-test in namespace pods-4635 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-07-31 10:48:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-07-31 10:48:32 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-07-31 10:48:32 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-07-31 10:48:32 +0000 UTC  }]
Jul 31 10:48:34.555: INFO: Found Pod pod-test in namespace pods-4635 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-07-31 10:48:32 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2022-07-31 10:48:34 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2022-07-31 10:48:34 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-07-31 10:48:32 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data
Jul 31 10:48:34.589: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Jul 31 10:48:34.643: INFO: observed event type ADDED
Jul 31 10:48:34.643: INFO: observed event type MODIFIED
Jul 31 10:48:34.646: INFO: observed event type MODIFIED
Jul 31 10:48:34.646: INFO: observed event type MODIFIED
Jul 31 10:48:34.646: INFO: observed event type MODIFIED
Jul 31 10:48:34.646: INFO: observed event type MODIFIED
Jul 31 10:48:34.646: INFO: observed event type MODIFIED
Jul 31 10:48:36.605: INFO: observed event type MODIFIED
Jul 31 10:48:37.651: INFO: observed event type MODIFIED
Jul 31 10:48:37.663: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:48:37.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4635" for this suite.

• [SLOW TEST:5.080 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":346,"completed":65,"skipped":1386,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:48:37.767: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 10:48:37.894: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jul 31 10:48:42.913: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 31 10:48:42.914: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jul 31 10:48:44.931: INFO: Creating deployment "test-rollover-deployment"
Jul 31 10:48:44.964: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jul 31 10:48:46.990: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jul 31 10:48:47.000: INFO: Ensure that both replica sets have 1 created replica
Jul 31 10:48:47.009: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jul 31 10:48:47.022: INFO: Updating deployment test-rollover-deployment
Jul 31 10:48:47.022: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jul 31 10:48:49.047: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jul 31 10:48:49.059: INFO: Make sure deployment "test-rollover-deployment" is complete
Jul 31 10:48:49.069: INFO: all replica sets need to contain the pod-template-hash label
Jul 31 10:48:49.069: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.July, 31, 10, 48, 45, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 10, 48, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.July, 31, 10, 48, 47, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 10, 48, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-77db6f9f48\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 31 10:48:51.092: INFO: all replica sets need to contain the pod-template-hash label
Jul 31 10:48:51.092: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.July, 31, 10, 48, 45, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 10, 48, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.July, 31, 10, 48, 49, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 10, 48, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-77db6f9f48\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 31 10:48:53.086: INFO: all replica sets need to contain the pod-template-hash label
Jul 31 10:48:53.086: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.July, 31, 10, 48, 45, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 10, 48, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.July, 31, 10, 48, 49, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 10, 48, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-77db6f9f48\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 31 10:48:55.083: INFO: all replica sets need to contain the pod-template-hash label
Jul 31 10:48:55.083: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.July, 31, 10, 48, 45, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 10, 48, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.July, 31, 10, 48, 49, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 10, 48, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-77db6f9f48\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 31 10:48:57.085: INFO: all replica sets need to contain the pod-template-hash label
Jul 31 10:48:57.086: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.July, 31, 10, 48, 45, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 10, 48, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.July, 31, 10, 48, 49, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 10, 48, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-77db6f9f48\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 31 10:48:59.091: INFO: all replica sets need to contain the pod-template-hash label
Jul 31 10:48:59.091: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.July, 31, 10, 48, 45, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 10, 48, 45, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.July, 31, 10, 48, 49, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 10, 48, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-77db6f9f48\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 31 10:49:01.093: INFO: 
Jul 31 10:49:01.093: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Jul 31 10:49:01.108: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-8008  7a275223-fdc0-4692-a5e2-fb343faa243c 12164 2 2022-07-31 10:48:44 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-07-31 10:48:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-07-31 10:48:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049bf0a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-07-31 10:48:45 +0000 UTC,LastTransitionTime:2022-07-31 10:48:45 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-77db6f9f48" has successfully progressed.,LastUpdateTime:2022-07-31 10:48:59 +0000 UTC,LastTransitionTime:2022-07-31 10:48:44 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jul 31 10:49:01.114: INFO: New ReplicaSet "test-rollover-deployment-77db6f9f48" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-77db6f9f48  deployment-8008  0e30bea0-2421-40b8-aa07-6db49fbd0e72 12154 2 2022-07-31 10:48:47 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:77db6f9f48] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 7a275223-fdc0-4692-a5e2-fb343faa243c 0xc0049bf577 0xc0049bf578}] []  [{kube-controller-manager Update apps/v1 2022-07-31 10:48:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7a275223-fdc0-4692-a5e2-fb343faa243c\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-07-31 10:48:59 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 77db6f9f48,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:77db6f9f48] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049bf628 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jul 31 10:49:01.114: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jul 31 10:49:01.115: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-8008  5c965d74-f6be-4c38-bcf6-5d37c8788490 12163 2 2022-07-31 10:48:37 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 7a275223-fdc0-4692-a5e2-fb343faa243c 0xc0049bf447 0xc0049bf448}] []  [{e2e.test Update apps/v1 2022-07-31 10:48:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-07-31 10:48:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7a275223-fdc0-4692-a5e2-fb343faa243c\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-07-31 10:48:59 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0049bf508 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 31 10:49:01.115: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-784bc44b77  deployment-8008  ae151f43-d37f-4c95-a98b-5260e4dfcd08 12111 2 2022-07-31 10:48:44 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:784bc44b77] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 7a275223-fdc0-4692-a5e2-fb343faa243c 0xc0049bf697 0xc0049bf698}] []  [{kube-controller-manager Update apps/v1 2022-07-31 10:48:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7a275223-fdc0-4692-a5e2-fb343faa243c\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-07-31 10:48:47 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 784bc44b77,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:784bc44b77] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049bf748 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 31 10:49:01.123: INFO: Pod "test-rollover-deployment-77db6f9f48-ltmhk" is available:
&Pod{ObjectMeta:{test-rollover-deployment-77db6f9f48-ltmhk test-rollover-deployment-77db6f9f48- deployment-8008  9d631b8f-6b97-4cda-8af9-e5c0c421fe10 12124 0 2022-07-31 10:48:47 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:77db6f9f48] map[] [{apps/v1 ReplicaSet test-rollover-deployment-77db6f9f48 0e30bea0-2421-40b8-aa07-6db49fbd0e72 0xc004843d07 0xc004843d08}] []  [{kube-controller-manager Update v1 2022-07-31 10:48:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0e30bea0-2421-40b8-aa07-6db49fbd0e72\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-07-31 10:48:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.28.80.1\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2nszh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2nszh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:p1-ashfcfj4pzo6aemt1j3a9ah7ew,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 10:48:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 10:48:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 10:48:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 10:48:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.27.21.73,PodIP:172.28.80.1,StartTime:2022-07-31 10:48:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-07-31 10:48:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:7e8bdd271312fd25fc5ff5a8f04727be84044eb3d7d8d03611972a6752e2e11e,ContainerID:docker://2414a6a6361e5b7cb960e68b38b9366f8ed82bbd4c0bf3c3481abe597f1fdc13,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.28.80.1,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:49:01.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8008" for this suite.

• [SLOW TEST:23.388 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":346,"completed":66,"skipped":1403,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:49:01.157: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a cronjob
STEP: Ensuring more than one job is running at a time
STEP: Ensuring at least two running jobs exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:51:01.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-7755" for this suite.

• [SLOW TEST:120.150 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","total":346,"completed":67,"skipped":1435,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:51:01.308: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul 31 10:51:01.445: INFO: Waiting up to 5m0s for pod "pod-91a50fd4-acf0-4d7e-9639-7691e9ab2480" in namespace "emptydir-321" to be "Succeeded or Failed"
Jul 31 10:51:01.462: INFO: Pod "pod-91a50fd4-acf0-4d7e-9639-7691e9ab2480": Phase="Pending", Reason="", readiness=false. Elapsed: 16.738601ms
Jul 31 10:51:03.474: INFO: Pod "pod-91a50fd4-acf0-4d7e-9639-7691e9ab2480": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028724818s
Jul 31 10:51:05.489: INFO: Pod "pod-91a50fd4-acf0-4d7e-9639-7691e9ab2480": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043756835s
STEP: Saw pod success
Jul 31 10:51:05.489: INFO: Pod "pod-91a50fd4-acf0-4d7e-9639-7691e9ab2480" satisfied condition "Succeeded or Failed"
Jul 31 10:51:05.495: INFO: Trying to get logs from node p1-ashfcfj4pzo6aemt1j3a9ah7ew pod pod-91a50fd4-acf0-4d7e-9639-7691e9ab2480 container test-container: <nil>
STEP: delete the pod
Jul 31 10:51:05.571: INFO: Waiting for pod pod-91a50fd4-acf0-4d7e-9639-7691e9ab2480 to disappear
Jul 31 10:51:05.576: INFO: Pod pod-91a50fd4-acf0-4d7e-9639-7691e9ab2480 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:51:05.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-321" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":68,"skipped":1510,"failed":0}
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:51:05.591: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with secret that has name projected-secret-test-cb8c7856-bf70-4c5d-bd18-6439a187f1c0
STEP: Creating a pod to test consume secrets
Jul 31 10:51:05.696: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1d9a7c50-8058-4f07-9264-ac8535dc2d32" in namespace "projected-8738" to be "Succeeded or Failed"
Jul 31 10:51:05.710: INFO: Pod "pod-projected-secrets-1d9a7c50-8058-4f07-9264-ac8535dc2d32": Phase="Pending", Reason="", readiness=false. Elapsed: 13.670089ms
Jul 31 10:51:07.728: INFO: Pod "pod-projected-secrets-1d9a7c50-8058-4f07-9264-ac8535dc2d32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031642635s
Jul 31 10:51:09.739: INFO: Pod "pod-projected-secrets-1d9a7c50-8058-4f07-9264-ac8535dc2d32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043146097s
STEP: Saw pod success
Jul 31 10:51:09.739: INFO: Pod "pod-projected-secrets-1d9a7c50-8058-4f07-9264-ac8535dc2d32" satisfied condition "Succeeded or Failed"
Jul 31 10:51:09.744: INFO: Trying to get logs from node p1-ashfcfj4pzo6aemt1j3a9ah7ew pod pod-projected-secrets-1d9a7c50-8058-4f07-9264-ac8535dc2d32 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 31 10:51:09.797: INFO: Waiting for pod pod-projected-secrets-1d9a7c50-8058-4f07-9264-ac8535dc2d32 to disappear
Jul 31 10:51:09.802: INFO: Pod pod-projected-secrets-1d9a7c50-8058-4f07-9264-ac8535dc2d32 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:51:09.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8738" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":69,"skipped":1512,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:51:09.822: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir volume type on tmpfs
Jul 31 10:51:09.931: INFO: Waiting up to 5m0s for pod "pod-3f0f9336-b71f-4b8b-82e7-1f50d2c86976" in namespace "emptydir-1194" to be "Succeeded or Failed"
Jul 31 10:51:09.943: INFO: Pod "pod-3f0f9336-b71f-4b8b-82e7-1f50d2c86976": Phase="Pending", Reason="", readiness=false. Elapsed: 11.050235ms
Jul 31 10:51:11.955: INFO: Pod "pod-3f0f9336-b71f-4b8b-82e7-1f50d2c86976": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023245635s
Jul 31 10:51:13.971: INFO: Pod "pod-3f0f9336-b71f-4b8b-82e7-1f50d2c86976": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039110985s
STEP: Saw pod success
Jul 31 10:51:13.971: INFO: Pod "pod-3f0f9336-b71f-4b8b-82e7-1f50d2c86976" satisfied condition "Succeeded or Failed"
Jul 31 10:51:13.976: INFO: Trying to get logs from node p1-ashfcfj4pzo6aemt1j3a9ah7ew pod pod-3f0f9336-b71f-4b8b-82e7-1f50d2c86976 container test-container: <nil>
STEP: delete the pod
Jul 31 10:51:14.020: INFO: Waiting for pod pod-3f0f9336-b71f-4b8b-82e7-1f50d2c86976 to disappear
Jul 31 10:51:14.026: INFO: Pod pod-3f0f9336-b71f-4b8b-82e7-1f50d2c86976 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:51:14.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1194" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":70,"skipped":1547,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:51:14.042: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Jul 31 10:51:14.121: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering the sample API server.
Jul 31 10:51:15.166: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jul 31 10:51:17.282: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.July, 31, 10, 51, 14, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 10, 51, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.July, 31, 10, 51, 15, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 10, 51, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 31 10:51:19.296: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.July, 31, 10, 51, 14, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 10, 51, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.July, 31, 10, 51, 15, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 10, 51, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 31 10:51:21.300: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.July, 31, 10, 51, 14, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 10, 51, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.July, 31, 10, 51, 15, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 10, 51, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 31 10:51:23.294: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.July, 31, 10, 51, 14, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 10, 51, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.July, 31, 10, 51, 15, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 10, 51, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 31 10:51:25.297: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.July, 31, 10, 51, 14, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 10, 51, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.July, 31, 10, 51, 15, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 10, 51, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 31 10:51:27.292: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.July, 31, 10, 51, 14, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 10, 51, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.July, 31, 10, 51, 15, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 10, 51, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7b4b967944\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 31 10:51:29.709: INFO: Waited 403.911472ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}'
STEP: List APIServices
Jul 31 10:51:29.816: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:51:30.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-4674" for this suite.

• [SLOW TEST:16.256 seconds]
[sig-api-machinery] Aggregator
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":346,"completed":71,"skipped":1594,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:51:30.300: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Jul 31 10:51:30.376: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b021ea44-263b-4c82-bcd5-7b300eb7b819" in namespace "downward-api-6329" to be "Succeeded or Failed"
Jul 31 10:51:30.411: INFO: Pod "downwardapi-volume-b021ea44-263b-4c82-bcd5-7b300eb7b819": Phase="Pending", Reason="", readiness=false. Elapsed: 34.622484ms
Jul 31 10:51:32.430: INFO: Pod "downwardapi-volume-b021ea44-263b-4c82-bcd5-7b300eb7b819": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05421203s
Jul 31 10:51:34.448: INFO: Pod "downwardapi-volume-b021ea44-263b-4c82-bcd5-7b300eb7b819": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.071925238s
STEP: Saw pod success
Jul 31 10:51:34.448: INFO: Pod "downwardapi-volume-b021ea44-263b-4c82-bcd5-7b300eb7b819" satisfied condition "Succeeded or Failed"
Jul 31 10:51:34.453: INFO: Trying to get logs from node p1-ashfcfj4pzo6aemt1j3a9ah7ew pod downwardapi-volume-b021ea44-263b-4c82-bcd5-7b300eb7b819 container client-container: <nil>
STEP: delete the pod
Jul 31 10:51:34.503: INFO: Waiting for pod downwardapi-volume-b021ea44-263b-4c82-bcd5-7b300eb7b819 to disappear
Jul 31 10:51:34.509: INFO: Pod downwardapi-volume-b021ea44-263b-4c82-bcd5-7b300eb7b819 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:51:34.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6329" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":72,"skipped":1601,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:51:34.528: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 10:51:34.635: INFO: The status of Pod server-envvars-077bba9d-05da-47cf-9874-37f303ec89d9 is Pending, waiting for it to be Running (with Ready = true)
Jul 31 10:51:36.647: INFO: The status of Pod server-envvars-077bba9d-05da-47cf-9874-37f303ec89d9 is Running (Ready = true)
Jul 31 10:51:36.715: INFO: Waiting up to 5m0s for pod "client-envvars-911cb7f8-2ae3-45d6-a91e-5c4241bb1ee8" in namespace "pods-9003" to be "Succeeded or Failed"
Jul 31 10:51:36.730: INFO: Pod "client-envvars-911cb7f8-2ae3-45d6-a91e-5c4241bb1ee8": Phase="Pending", Reason="", readiness=false. Elapsed: 15.565922ms
Jul 31 10:51:38.744: INFO: Pod "client-envvars-911cb7f8-2ae3-45d6-a91e-5c4241bb1ee8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02926784s
Jul 31 10:51:40.761: INFO: Pod "client-envvars-911cb7f8-2ae3-45d6-a91e-5c4241bb1ee8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04635613s
Jul 31 10:51:42.775: INFO: Pod "client-envvars-911cb7f8-2ae3-45d6-a91e-5c4241bb1ee8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.060674182s
STEP: Saw pod success
Jul 31 10:51:42.775: INFO: Pod "client-envvars-911cb7f8-2ae3-45d6-a91e-5c4241bb1ee8" satisfied condition "Succeeded or Failed"
Jul 31 10:51:42.781: INFO: Trying to get logs from node p1-nc3cz44465xd41a6poxnmrpeqo pod client-envvars-911cb7f8-2ae3-45d6-a91e-5c4241bb1ee8 container env3cont: <nil>
STEP: delete the pod
Jul 31 10:51:42.858: INFO: Waiting for pod client-envvars-911cb7f8-2ae3-45d6-a91e-5c4241bb1ee8 to disappear
Jul 31 10:51:42.866: INFO: Pod client-envvars-911cb7f8-2ae3-45d6-a91e-5c4241bb1ee8 no longer exists
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:51:42.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9003" for this suite.

• [SLOW TEST:8.363 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":346,"completed":73,"skipped":1679,"failed":0}
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:51:42.891: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 10:51:42.955: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jul 31 10:51:42.987: INFO: Pod name sample-pod: Found 0 pods out of 1
Jul 31 10:51:48.022: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 31 10:51:48.023: INFO: Creating deployment "test-rolling-update-deployment"
Jul 31 10:51:48.040: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jul 31 10:51:48.076: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jul 31 10:51:50.124: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jul 31 10:51:50.146: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:2, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.July, 31, 10, 51, 48, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 10, 51, 48, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.July, 31, 10, 51, 50, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 10, 51, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-8656fc4b57\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 31 10:51:52.159: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Jul 31 10:51:52.173: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-7431  63f490c6-dd72-46f1-be6e-377860168434 13015 1 2022-07-31 10:51:48 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2022-07-31 10:51:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-07-31 10:51:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004183398 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-07-31 10:51:48 +0000 UTC,LastTransitionTime:2022-07-31 10:51:48 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-8656fc4b57" has successfully progressed.,LastUpdateTime:2022-07-31 10:51:50 +0000 UTC,LastTransitionTime:2022-07-31 10:51:48 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jul 31 10:51:52.179: INFO: New ReplicaSet "test-rolling-update-deployment-8656fc4b57" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-8656fc4b57  deployment-7431  2a7958e8-f159-4911-b22b-0cce27022f60 13005 1 2022-07-31 10:51:48 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:8656fc4b57] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 63f490c6-dd72-46f1-be6e-377860168434 0xc0041838a7 0xc0041838a8}] []  [{kube-controller-manager Update apps/v1 2022-07-31 10:51:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"63f490c6-dd72-46f1-be6e-377860168434\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-07-31 10:51:50 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 8656fc4b57,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:8656fc4b57] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004183958 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jul 31 10:51:52.179: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jul 31 10:51:52.179: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-7431  4128aa0b-0d8f-481c-adef-b88acc8a4435 13014 2 2022-07-31 10:51:42 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 63f490c6-dd72-46f1-be6e-377860168434 0xc004183777 0xc004183778}] []  [{e2e.test Update apps/v1 2022-07-31 10:51:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-07-31 10:51:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"63f490c6-dd72-46f1-be6e-377860168434\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2022-07-31 10:51:50 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004183838 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 31 10:51:52.185: INFO: Pod "test-rolling-update-deployment-8656fc4b57-wsljm" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-8656fc4b57-wsljm test-rolling-update-deployment-8656fc4b57- deployment-7431  f33c0722-6f7e-47ac-8a42-d08dc9f6b674 13004 0 2022-07-31 10:51:48 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:8656fc4b57] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-8656fc4b57 2a7958e8-f159-4911-b22b-0cce27022f60 0xc004183dd7 0xc004183dd8}] []  [{kube-controller-manager Update v1 2022-07-31 10:51:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2a7958e8-f159-4911-b22b-0cce27022f60\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-07-31 10:51:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.28.64.1\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-85hm6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-85hm6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:p1-nc3cz44465xd41a6poxnmrpeqo,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 10:51:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 10:51:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 10:51:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 10:51:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.27.21.79,PodIP:172.28.64.1,StartTime:2022-07-31 10:51:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-07-31 10:51:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:7e8bdd271312fd25fc5ff5a8f04727be84044eb3d7d8d03611972a6752e2e11e,ContainerID:docker://774bff844c303476499d1a559f6d06567a1a2785c630da6b70a38ddca6146efb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.28.64.1,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:51:52.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7431" for this suite.

• [SLOW TEST:9.312 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":74,"skipped":1679,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:51:52.205: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-upd-6f429d41-402f-4587-af46-f181b1f94b59
STEP: Creating the pod
Jul 31 10:51:52.338: INFO: The status of Pod pod-configmaps-5911159d-f939-4a67-92ad-f38bd53a28df is Pending, waiting for it to be Running (with Ready = true)
Jul 31 10:51:54.351: INFO: The status of Pod pod-configmaps-5911159d-f939-4a67-92ad-f38bd53a28df is Pending, waiting for it to be Running (with Ready = true)
Jul 31 10:51:56.352: INFO: The status of Pod pod-configmaps-5911159d-f939-4a67-92ad-f38bd53a28df is Running (Ready = true)
STEP: Updating configmap configmap-test-upd-6f429d41-402f-4587-af46-f181b1f94b59
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:53:23.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3334" for this suite.

• [SLOW TEST:91.453 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":75,"skipped":1695,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:53:23.659: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Jul 31 10:53:28.278: INFO: Successfully updated pod "adopt-release-bp2f7"
STEP: Checking that the Job readopts the Pod
Jul 31 10:53:28.278: INFO: Waiting up to 15m0s for pod "adopt-release-bp2f7" in namespace "job-6863" to be "adopted"
Jul 31 10:53:28.286: INFO: Pod "adopt-release-bp2f7": Phase="Running", Reason="", readiness=true. Elapsed: 7.864143ms
Jul 31 10:53:30.507: INFO: Pod "adopt-release-bp2f7": Phase="Running", Reason="", readiness=true. Elapsed: 2.229675473s
Jul 31 10:53:30.508: INFO: Pod "adopt-release-bp2f7" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Jul 31 10:53:31.026: INFO: Successfully updated pod "adopt-release-bp2f7"
STEP: Checking that the Job releases the Pod
Jul 31 10:53:31.026: INFO: Waiting up to 15m0s for pod "adopt-release-bp2f7" in namespace "job-6863" to be "released"
Jul 31 10:53:31.036: INFO: Pod "adopt-release-bp2f7": Phase="Running", Reason="", readiness=true. Elapsed: 9.466134ms
Jul 31 10:53:33.048: INFO: Pod "adopt-release-bp2f7": Phase="Running", Reason="", readiness=true. Elapsed: 2.021775602s
Jul 31 10:53:33.048: INFO: Pod "adopt-release-bp2f7" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:53:33.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6863" for this suite.

• [SLOW TEST:9.407 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":346,"completed":76,"skipped":1736,"failed":0}
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:53:33.066: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
Jul 31 10:53:33.186: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jul 31 10:53:35.201: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the pod with lifecycle hook
Jul 31 10:53:35.257: INFO: The status of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jul 31 10:53:37.270: INFO: The status of Pod pod-with-prestop-http-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Jul 31 10:53:37.290: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 31 10:53:37.298: INFO: Pod pod-with-prestop-http-hook still exists
Jul 31 10:53:39.298: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 31 10:53:39.309: INFO: Pod pod-with-prestop-http-hook still exists
Jul 31 10:53:41.299: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 31 10:53:41.310: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:53:41.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6380" for this suite.

• [SLOW TEST:8.286 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":346,"completed":77,"skipped":1737,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:53:41.352: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:53:45.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8896" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":346,"completed":78,"skipped":1752,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:53:45.505: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-ddba897d-7c7f-4761-9542-8e253e8dd658
STEP: Creating a pod to test consume configMaps
Jul 31 10:53:45.618: INFO: Waiting up to 5m0s for pod "pod-configmaps-de5bcfd1-82e3-4ef7-83b8-36165b88f504" in namespace "configmap-9975" to be "Succeeded or Failed"
Jul 31 10:53:45.637: INFO: Pod "pod-configmaps-de5bcfd1-82e3-4ef7-83b8-36165b88f504": Phase="Pending", Reason="", readiness=false. Elapsed: 19.363613ms
Jul 31 10:53:47.650: INFO: Pod "pod-configmaps-de5bcfd1-82e3-4ef7-83b8-36165b88f504": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032375697s
Jul 31 10:53:49.662: INFO: Pod "pod-configmaps-de5bcfd1-82e3-4ef7-83b8-36165b88f504": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043934538s
Jul 31 10:53:51.680: INFO: Pod "pod-configmaps-de5bcfd1-82e3-4ef7-83b8-36165b88f504": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.062242694s
STEP: Saw pod success
Jul 31 10:53:51.680: INFO: Pod "pod-configmaps-de5bcfd1-82e3-4ef7-83b8-36165b88f504" satisfied condition "Succeeded or Failed"
Jul 31 10:53:51.686: INFO: Trying to get logs from node p1-nc3cz44465xd41a6poxnmrpeqo pod pod-configmaps-de5bcfd1-82e3-4ef7-83b8-36165b88f504 container agnhost-container: <nil>
STEP: delete the pod
Jul 31 10:53:51.738: INFO: Waiting for pod pod-configmaps-de5bcfd1-82e3-4ef7-83b8-36165b88f504 to disappear
Jul 31 10:53:51.743: INFO: Pod pod-configmaps-de5bcfd1-82e3-4ef7-83b8-36165b88f504 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:53:51.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9975" for this suite.

• [SLOW TEST:6.268 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":79,"skipped":1757,"failed":0}
SSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:53:51.773: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:53:51.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2637" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":346,"completed":80,"skipped":1764,"failed":0}
SSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:53:51.886: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test override all
Jul 31 10:53:52.018: INFO: Waiting up to 5m0s for pod "client-containers-2d7e4901-65ae-4a11-b3e5-ac5d03d5d42e" in namespace "containers-9905" to be "Succeeded or Failed"
Jul 31 10:53:52.034: INFO: Pod "client-containers-2d7e4901-65ae-4a11-b3e5-ac5d03d5d42e": Phase="Pending", Reason="", readiness=false. Elapsed: 15.867492ms
Jul 31 10:53:54.048: INFO: Pod "client-containers-2d7e4901-65ae-4a11-b3e5-ac5d03d5d42e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029317899s
Jul 31 10:53:56.060: INFO: Pod "client-containers-2d7e4901-65ae-4a11-b3e5-ac5d03d5d42e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041738564s
STEP: Saw pod success
Jul 31 10:53:56.060: INFO: Pod "client-containers-2d7e4901-65ae-4a11-b3e5-ac5d03d5d42e" satisfied condition "Succeeded or Failed"
Jul 31 10:53:56.064: INFO: Trying to get logs from node p1-nc3cz44465xd41a6poxnmrpeqo pod client-containers-2d7e4901-65ae-4a11-b3e5-ac5d03d5d42e container agnhost-container: <nil>
STEP: delete the pod
Jul 31 10:53:56.116: INFO: Waiting for pod client-containers-2d7e4901-65ae-4a11-b3e5-ac5d03d5d42e to disappear
Jul 31 10:53:56.120: INFO: Pod client-containers-2d7e4901-65ae-4a11-b3e5-ac5d03d5d42e no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:53:56.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9905" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":346,"completed":81,"skipped":1770,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:53:56.155: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support CronJob API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a cronjob
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jul 31 10:53:56.242: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jul 31 10:53:56.249: INFO: starting watch
STEP: patching
STEP: updating
Jul 31 10:53:56.284: INFO: waiting for watch events with expected annotations
Jul 31 10:53:56.285: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:53:56.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-2071" for this suite.
•{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","total":346,"completed":82,"skipped":1794,"failed":0}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:53:56.372: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul 31 10:53:56.446: INFO: Waiting up to 5m0s for pod "pod-c98327d1-cfd3-4641-bffa-d96c1d799c4e" in namespace "emptydir-7758" to be "Succeeded or Failed"
Jul 31 10:53:56.480: INFO: Pod "pod-c98327d1-cfd3-4641-bffa-d96c1d799c4e": Phase="Pending", Reason="", readiness=false. Elapsed: 33.478225ms
Jul 31 10:53:58.492: INFO: Pod "pod-c98327d1-cfd3-4641-bffa-d96c1d799c4e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045796244s
Jul 31 10:54:00.506: INFO: Pod "pod-c98327d1-cfd3-4641-bffa-d96c1d799c4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.059907762s
STEP: Saw pod success
Jul 31 10:54:00.506: INFO: Pod "pod-c98327d1-cfd3-4641-bffa-d96c1d799c4e" satisfied condition "Succeeded or Failed"
Jul 31 10:54:00.510: INFO: Trying to get logs from node p1-esqoe7n9eqq76k8ojd9tajh51e pod pod-c98327d1-cfd3-4641-bffa-d96c1d799c4e container test-container: <nil>
STEP: delete the pod
Jul 31 10:54:00.554: INFO: Waiting for pod pod-c98327d1-cfd3-4641-bffa-d96c1d799c4e to disappear
Jul 31 10:54:00.565: INFO: Pod pod-c98327d1-cfd3-4641-bffa-d96c1d799c4e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:54:00.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7758" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":83,"skipped":1795,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:54:00.584: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Jul 31 10:54:00.683: INFO: Waiting up to 5m0s for pod "downwardapi-volume-73716b3a-1802-456f-a5bd-d951624512c2" in namespace "downward-api-8001" to be "Succeeded or Failed"
Jul 31 10:54:00.723: INFO: Pod "downwardapi-volume-73716b3a-1802-456f-a5bd-d951624512c2": Phase="Pending", Reason="", readiness=false. Elapsed: 40.074937ms
Jul 31 10:54:02.734: INFO: Pod "downwardapi-volume-73716b3a-1802-456f-a5bd-d951624512c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05107801s
Jul 31 10:54:04.746: INFO: Pod "downwardapi-volume-73716b3a-1802-456f-a5bd-d951624512c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.063631887s
STEP: Saw pod success
Jul 31 10:54:04.746: INFO: Pod "downwardapi-volume-73716b3a-1802-456f-a5bd-d951624512c2" satisfied condition "Succeeded or Failed"
Jul 31 10:54:04.752: INFO: Trying to get logs from node p1-nc3cz44465xd41a6poxnmrpeqo pod downwardapi-volume-73716b3a-1802-456f-a5bd-d951624512c2 container client-container: <nil>
STEP: delete the pod
Jul 31 10:54:04.810: INFO: Waiting for pod downwardapi-volume-73716b3a-1802-456f-a5bd-d951624512c2 to disappear
Jul 31 10:54:04.814: INFO: Pod downwardapi-volume-73716b3a-1802-456f-a5bd-d951624512c2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:54:04.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8001" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":346,"completed":84,"skipped":1805,"failed":0}

------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:54:04.840: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename runtimeclass
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Jul 31 10:54:04.935: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Jul 31 10:54:04.977: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:54:05.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-3143" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":346,"completed":85,"skipped":1805,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:54:05.047: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 10:54:05.104: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:54:08.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1869" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":346,"completed":86,"skipped":1848,"failed":0}
SSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:54:08.348: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:54:08.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-9651" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":346,"completed":87,"skipped":1854,"failed":0}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:54:08.680: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Jul 31 10:54:08.772: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f111913c-2da8-41d6-bdf4-043384f8ac3e" in namespace "projected-2944" to be "Succeeded or Failed"
Jul 31 10:54:08.778: INFO: Pod "downwardapi-volume-f111913c-2da8-41d6-bdf4-043384f8ac3e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.681109ms
Jul 31 10:54:10.789: INFO: Pod "downwardapi-volume-f111913c-2da8-41d6-bdf4-043384f8ac3e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017475223s
Jul 31 10:54:12.806: INFO: Pod "downwardapi-volume-f111913c-2da8-41d6-bdf4-043384f8ac3e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034218144s
STEP: Saw pod success
Jul 31 10:54:12.806: INFO: Pod "downwardapi-volume-f111913c-2da8-41d6-bdf4-043384f8ac3e" satisfied condition "Succeeded or Failed"
Jul 31 10:54:12.811: INFO: Trying to get logs from node p1-eipeq63rfgo6ooocmu3kqk3tcc pod downwardapi-volume-f111913c-2da8-41d6-bdf4-043384f8ac3e container client-container: <nil>
STEP: delete the pod
Jul 31 10:54:12.863: INFO: Waiting for pod downwardapi-volume-f111913c-2da8-41d6-bdf4-043384f8ac3e to disappear
Jul 31 10:54:12.869: INFO: Pod downwardapi-volume-f111913c-2da8-41d6-bdf4-043384f8ac3e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:54:12.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2944" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":88,"skipped":1856,"failed":0}
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:54:12.887: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 10:54:13.013: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jul 31 10:54:13.039: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 31 10:54:13.039: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched.
Jul 31 10:54:13.073: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 31 10:54:13.073: INFO: Node p1-esqoe7n9eqq76k8ojd9tajh51e is running 0 daemon pod, expected 1
Jul 31 10:54:14.081: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 31 10:54:14.081: INFO: Node p1-esqoe7n9eqq76k8ojd9tajh51e is running 0 daemon pod, expected 1
Jul 31 10:54:15.089: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jul 31 10:54:15.089: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jul 31 10:54:15.127: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jul 31 10:54:15.128: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Jul 31 10:54:16.138: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 31 10:54:16.138: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jul 31 10:54:16.179: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 31 10:54:16.179: INFO: Node p1-esqoe7n9eqq76k8ojd9tajh51e is running 0 daemon pod, expected 1
Jul 31 10:54:17.190: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 31 10:54:17.190: INFO: Node p1-esqoe7n9eqq76k8ojd9tajh51e is running 0 daemon pod, expected 1
Jul 31 10:54:18.189: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 31 10:54:18.189: INFO: Node p1-esqoe7n9eqq76k8ojd9tajh51e is running 0 daemon pod, expected 1
Jul 31 10:54:19.191: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 31 10:54:19.191: INFO: Node p1-esqoe7n9eqq76k8ojd9tajh51e is running 0 daemon pod, expected 1
Jul 31 10:54:20.192: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jul 31 10:54:20.192: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8554, will wait for the garbage collector to delete the pods
Jul 31 10:54:20.266: INFO: Deleting DaemonSet.extensions daemon-set took: 9.894315ms
Jul 31 10:54:20.368: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.025273ms
Jul 31 10:54:23.082: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 31 10:54:23.082: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jul 31 10:54:23.087: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"13937"},"items":null}

Jul 31 10:54:23.091: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"13937"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:54:23.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8554" for this suite.

• [SLOW TEST:10.279 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":346,"completed":89,"skipped":1859,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:54:23.168: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Jul 31 10:54:23.280: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8860905e-7c8a-493c-a81a-e6f1b335b7e2" in namespace "downward-api-331" to be "Succeeded or Failed"
Jul 31 10:54:23.307: INFO: Pod "downwardapi-volume-8860905e-7c8a-493c-a81a-e6f1b335b7e2": Phase="Pending", Reason="", readiness=false. Elapsed: 27.511242ms
Jul 31 10:54:25.325: INFO: Pod "downwardapi-volume-8860905e-7c8a-493c-a81a-e6f1b335b7e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045389066s
Jul 31 10:54:27.341: INFO: Pod "downwardapi-volume-8860905e-7c8a-493c-a81a-e6f1b335b7e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.061418008s
STEP: Saw pod success
Jul 31 10:54:27.341: INFO: Pod "downwardapi-volume-8860905e-7c8a-493c-a81a-e6f1b335b7e2" satisfied condition "Succeeded or Failed"
Jul 31 10:54:27.346: INFO: Trying to get logs from node p1-nc3cz44465xd41a6poxnmrpeqo pod downwardapi-volume-8860905e-7c8a-493c-a81a-e6f1b335b7e2 container client-container: <nil>
STEP: delete the pod
Jul 31 10:54:27.412: INFO: Waiting for pod downwardapi-volume-8860905e-7c8a-493c-a81a-e6f1b335b7e2 to disappear
Jul 31 10:54:27.431: INFO: Pod downwardapi-volume-8860905e-7c8a-493c-a81a-e6f1b335b7e2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:54:27.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-331" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":90,"skipped":1863,"failed":0}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:54:27.451: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: validating api versions
Jul 31 10:54:27.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-8910 api-versions'
Jul 31 10:54:27.612: INFO: stderr: ""
Jul 31 10:54:27.612: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\npolicy/v1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:54:27.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8910" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":346,"completed":91,"skipped":1866,"failed":0}
SSSSS
------------------------------
[sig-node] Variable Expansion 
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:54:27.633: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Jul 31 10:54:31.753: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-3060 PodName:var-expansion-590a4133-9fab-456a-b389-d71d13f7dddf ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 31 10:54:31.753: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
Jul 31 10:54:31.754: INFO: ExecWithOptions: Clientset creation
Jul 31 10:54:31.754: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-3060/pods/var-expansion-590a4133-9fab-456a-b389-d71d13f7dddf/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true %!s(MISSING))
STEP: test for file in mounted path
Jul 31 10:54:31.901: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-3060 PodName:var-expansion-590a4133-9fab-456a-b389-d71d13f7dddf ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 31 10:54:31.901: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
Jul 31 10:54:31.902: INFO: ExecWithOptions: Clientset creation
Jul 31 10:54:31.902: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-3060/pods/var-expansion-590a4133-9fab-456a-b389-d71d13f7dddf/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true %!s(MISSING))
STEP: updating the annotation value
Jul 31 10:54:32.594: INFO: Successfully updated pod "var-expansion-590a4133-9fab-456a-b389-d71d13f7dddf"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Jul 31 10:54:32.606: INFO: Deleting pod "var-expansion-590a4133-9fab-456a-b389-d71d13f7dddf" in namespace "var-expansion-3060"
Jul 31 10:54:32.632: INFO: Wait up to 5m0s for pod "var-expansion-590a4133-9fab-456a-b389-d71d13f7dddf" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:55:04.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3060" for this suite.

• [SLOW TEST:37.041 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","total":346,"completed":92,"skipped":1871,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:55:04.674: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 31 10:55:05.155: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jul 31 10:55:07.186: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.July, 31, 10, 55, 5, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 10, 55, 5, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.July, 31, 10, 55, 5, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 10, 55, 5, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6c69dbd86b\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 31 10:55:10.232: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Jul 31 10:55:10.268: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:55:10.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1902" for this suite.
STEP: Destroying namespace "webhook-1902-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:5.740 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":346,"completed":93,"skipped":1929,"failed":0}
SS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:55:10.414: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod test-webserver-59b30158-c8da-402d-8e08-d01ed8260eda in namespace container-probe-8805
Jul 31 10:55:14.547: INFO: Started pod test-webserver-59b30158-c8da-402d-8e08-d01ed8260eda in namespace container-probe-8805
STEP: checking the pod's current state and verifying that restartCount is present
Jul 31 10:55:14.552: INFO: Initial restart count of pod test-webserver-59b30158-c8da-402d-8e08-d01ed8260eda is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:59:16.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8805" for this suite.

• [SLOW TEST:246.048 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":94,"skipped":1931,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:59:16.463: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should create services for rc  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating Agnhost RC
Jul 31 10:59:16.549: INFO: namespace kubectl-3460
Jul 31 10:59:16.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-3460 create -f -'
Jul 31 10:59:17.730: INFO: stderr: ""
Jul 31 10:59:17.730: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jul 31 10:59:18.744: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 31 10:59:18.744: INFO: Found 0 / 1
Jul 31 10:59:19.739: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 31 10:59:19.740: INFO: Found 0 / 1
Jul 31 10:59:20.741: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 31 10:59:20.741: INFO: Found 1 / 1
Jul 31 10:59:20.741: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 31 10:59:20.746: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 31 10:59:20.746: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 31 10:59:20.746: INFO: wait on agnhost-primary startup in kubectl-3460 
Jul 31 10:59:20.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-3460 logs agnhost-primary-rvtbb agnhost-primary'
Jul 31 10:59:20.854: INFO: stderr: ""
Jul 31 10:59:20.854: INFO: stdout: "Paused\n"
STEP: exposing RC
Jul 31 10:59:20.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-3460 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jul 31 10:59:20.963: INFO: stderr: ""
Jul 31 10:59:20.963: INFO: stdout: "service/rm2 exposed\n"
Jul 31 10:59:20.992: INFO: Service rm2 in namespace kubectl-3460 found.
STEP: exposing service
Jul 31 10:59:23.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-3460 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jul 31 10:59:23.117: INFO: stderr: ""
Jul 31 10:59:23.117: INFO: stdout: "service/rm3 exposed\n"
Jul 31 10:59:23.131: INFO: Service rm3 in namespace kubectl-3460 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:59:25.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3460" for this suite.

• [SLOW TEST:8.711 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1248
    should create services for rc  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":346,"completed":95,"skipped":1977,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:59:25.175: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Jul 31 10:59:29.298: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-2603 PodName:pod-sharedvolume-7610b7c3-a05b-492c-84a3-cc48d895a0ae ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 31 10:59:29.298: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
Jul 31 10:59:29.299: INFO: ExecWithOptions: Clientset creation
Jul 31 10:59:29.299: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-2603/pods/pod-sharedvolume-7610b7c3-a05b-492c-84a3-cc48d895a0ae/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true %!s(MISSING))
Jul 31 10:59:29.441: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:59:29.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2603" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":346,"completed":96,"skipped":2004,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:59:29.461: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 10:59:40.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5085" for this suite.

• [SLOW TEST:11.213 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":346,"completed":97,"skipped":2039,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 10:59:40.677: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: referencing a single matching pod
STEP: referencing matching pods with named port
STEP: creating empty Endpoints and EndpointSlices for no matching Pods
STEP: recreating EndpointSlices after they've been deleted
Jul 31 11:00:01.143: INFO: EndpointSlice for Service endpointslice-8619/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:00:11.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-8619" for this suite.

• [SLOW TEST:30.533 seconds]
[sig-network] EndpointSlice
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","total":346,"completed":98,"skipped":2061,"failed":0}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:00:11.212: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support proxy with --port 0  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: starting the proxy server
Jul 31 11:00:11.301: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-1458 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:00:11.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1458" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":346,"completed":99,"skipped":2071,"failed":0}
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:00:11.383: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-d4e7602b-c522-49f5-a918-888d3b336cc7
STEP: Creating a pod to test consume configMaps
Jul 31 11:00:11.498: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e648226d-8eb8-4a3d-8244-d04346dd1958" in namespace "projected-9701" to be "Succeeded or Failed"
Jul 31 11:00:11.524: INFO: Pod "pod-projected-configmaps-e648226d-8eb8-4a3d-8244-d04346dd1958": Phase="Pending", Reason="", readiness=false. Elapsed: 25.423553ms
Jul 31 11:00:13.535: INFO: Pod "pod-projected-configmaps-e648226d-8eb8-4a3d-8244-d04346dd1958": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036225847s
Jul 31 11:00:15.553: INFO: Pod "pod-projected-configmaps-e648226d-8eb8-4a3d-8244-d04346dd1958": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054168338s
STEP: Saw pod success
Jul 31 11:00:15.553: INFO: Pod "pod-projected-configmaps-e648226d-8eb8-4a3d-8244-d04346dd1958" satisfied condition "Succeeded or Failed"
Jul 31 11:00:15.558: INFO: Trying to get logs from node p1-ashfcfj4pzo6aemt1j3a9ah7ew pod pod-projected-configmaps-e648226d-8eb8-4a3d-8244-d04346dd1958 container agnhost-container: <nil>
STEP: delete the pod
Jul 31 11:00:15.625: INFO: Waiting for pod pod-projected-configmaps-e648226d-8eb8-4a3d-8244-d04346dd1958 to disappear
Jul 31 11:00:15.632: INFO: Pod pod-projected-configmaps-e648226d-8eb8-4a3d-8244-d04346dd1958 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:00:15.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9701" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":100,"skipped":2075,"failed":0}
SSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:00:15.660: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a volume subpath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test substitution in volume subpath
Jul 31 11:00:15.770: INFO: Waiting up to 5m0s for pod "var-expansion-091e5135-cd3b-49ae-8b43-d24e4474661f" in namespace "var-expansion-7888" to be "Succeeded or Failed"
Jul 31 11:00:15.790: INFO: Pod "var-expansion-091e5135-cd3b-49ae-8b43-d24e4474661f": Phase="Pending", Reason="", readiness=false. Elapsed: 20.064763ms
Jul 31 11:00:17.804: INFO: Pod "var-expansion-091e5135-cd3b-49ae-8b43-d24e4474661f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03389996s
Jul 31 11:00:19.930: INFO: Pod "var-expansion-091e5135-cd3b-49ae-8b43-d24e4474661f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.159638951s
STEP: Saw pod success
Jul 31 11:00:19.930: INFO: Pod "var-expansion-091e5135-cd3b-49ae-8b43-d24e4474661f" satisfied condition "Succeeded or Failed"
Jul 31 11:00:19.936: INFO: Trying to get logs from node p1-ashfcfj4pzo6aemt1j3a9ah7ew pod var-expansion-091e5135-cd3b-49ae-8b43-d24e4474661f container dapi-container: <nil>
STEP: delete the pod
Jul 31 11:00:19.996: INFO: Waiting for pod var-expansion-091e5135-cd3b-49ae-8b43-d24e4474661f to disappear
Jul 31 11:00:20.002: INFO: Pod var-expansion-091e5135-cd3b-49ae-8b43-d24e4474661f no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:00:20.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7888" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","total":346,"completed":101,"skipped":2080,"failed":0}
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:00:20.028: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-upd-9c03fce0-1b0d-4158-8da4-1052dbe05b9d
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:00:24.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6462" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":102,"skipped":2087,"failed":0}
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:00:24.270: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 11:00:24.325: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Jul 31 11:00:27.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=crd-publish-openapi-5648 --namespace=crd-publish-openapi-5648 create -f -'
Jul 31 11:00:28.794: INFO: stderr: ""
Jul 31 11:00:28.794: INFO: stdout: "e2e-test-crd-publish-openapi-7438-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jul 31 11:00:28.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=crd-publish-openapi-5648 --namespace=crd-publish-openapi-5648 delete e2e-test-crd-publish-openapi-7438-crds test-foo'
Jul 31 11:00:28.904: INFO: stderr: ""
Jul 31 11:00:28.904: INFO: stdout: "e2e-test-crd-publish-openapi-7438-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jul 31 11:00:28.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=crd-publish-openapi-5648 --namespace=crd-publish-openapi-5648 apply -f -'
Jul 31 11:00:29.666: INFO: stderr: ""
Jul 31 11:00:29.666: INFO: stdout: "e2e-test-crd-publish-openapi-7438-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jul 31 11:00:29.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=crd-publish-openapi-5648 --namespace=crd-publish-openapi-5648 delete e2e-test-crd-publish-openapi-7438-crds test-foo'
Jul 31 11:00:29.752: INFO: stderr: ""
Jul 31 11:00:29.752: INFO: stdout: "e2e-test-crd-publish-openapi-7438-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with value outside defined enum values
Jul 31 11:00:29.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=crd-publish-openapi-5648 --namespace=crd-publish-openapi-5648 create -f -'
Jul 31 11:00:29.905: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Jul 31 11:00:29.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=crd-publish-openapi-5648 --namespace=crd-publish-openapi-5648 create -f -'
Jul 31 11:00:30.521: INFO: rc: 1
Jul 31 11:00:30.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=crd-publish-openapi-5648 --namespace=crd-publish-openapi-5648 apply -f -'
Jul 31 11:00:30.664: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Jul 31 11:00:30.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=crd-publish-openapi-5648 --namespace=crd-publish-openapi-5648 create -f -'
Jul 31 11:00:30.811: INFO: rc: 1
Jul 31 11:00:30.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=crd-publish-openapi-5648 --namespace=crd-publish-openapi-5648 apply -f -'
Jul 31 11:00:30.953: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Jul 31 11:00:30.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=crd-publish-openapi-5648 explain e2e-test-crd-publish-openapi-7438-crds'
Jul 31 11:00:31.127: INFO: stderr: ""
Jul 31 11:00:31.127: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7438-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Jul 31 11:00:31.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=crd-publish-openapi-5648 explain e2e-test-crd-publish-openapi-7438-crds.metadata'
Jul 31 11:00:31.280: INFO: stderr: ""
Jul 31 11:00:31.280: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7438-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jul 31 11:00:31.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=crd-publish-openapi-5648 explain e2e-test-crd-publish-openapi-7438-crds.spec'
Jul 31 11:00:31.440: INFO: stderr: ""
Jul 31 11:00:31.440: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7438-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jul 31 11:00:31.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=crd-publish-openapi-5648 explain e2e-test-crd-publish-openapi-7438-crds.spec.bars'
Jul 31 11:00:31.590: INFO: stderr: ""
Jul 31 11:00:31.590: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7438-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Jul 31 11:00:31.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=crd-publish-openapi-5648 explain e2e-test-crd-publish-openapi-7438-crds.spec.bars2'
Jul 31 11:00:31.746: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:00:35.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5648" for this suite.

• [SLOW TEST:10.964 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":346,"completed":103,"skipped":2091,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:00:35.234: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward api env vars
Jul 31 11:00:35.333: INFO: Waiting up to 5m0s for pod "downward-api-a0ff0bb0-4e42-46fc-be0b-5a1cf54a44af" in namespace "downward-api-7982" to be "Succeeded or Failed"
Jul 31 11:00:35.359: INFO: Pod "downward-api-a0ff0bb0-4e42-46fc-be0b-5a1cf54a44af": Phase="Pending", Reason="", readiness=false. Elapsed: 26.092203ms
Jul 31 11:00:37.371: INFO: Pod "downward-api-a0ff0bb0-4e42-46fc-be0b-5a1cf54a44af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038664169s
Jul 31 11:00:39.385: INFO: Pod "downward-api-a0ff0bb0-4e42-46fc-be0b-5a1cf54a44af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.052644135s
STEP: Saw pod success
Jul 31 11:00:39.385: INFO: Pod "downward-api-a0ff0bb0-4e42-46fc-be0b-5a1cf54a44af" satisfied condition "Succeeded or Failed"
Jul 31 11:00:39.392: INFO: Trying to get logs from node p1-eipeq63rfgo6ooocmu3kqk3tcc pod downward-api-a0ff0bb0-4e42-46fc-be0b-5a1cf54a44af container dapi-container: <nil>
STEP: delete the pod
Jul 31 11:00:39.452: INFO: Waiting for pod downward-api-a0ff0bb0-4e42-46fc-be0b-5a1cf54a44af to disappear
Jul 31 11:00:39.458: INFO: Pod downward-api-a0ff0bb0-4e42-46fc-be0b-5a1cf54a44af no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:00:39.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7982" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":346,"completed":104,"skipped":2109,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:00:39.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create set of pods
Jul 31 11:00:39.580: INFO: created test-pod-1
Jul 31 11:00:41.619: INFO: running and ready test-pod-1
Jul 31 11:00:41.630: INFO: created test-pod-2
Jul 31 11:00:43.644: INFO: running and ready test-pod-2
Jul 31 11:00:43.653: INFO: created test-pod-3
Jul 31 11:00:47.673: INFO: running and ready test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
Jul 31 11:00:47.787: INFO: Pod quantity 3 is different from expected quantity 0
Jul 31 11:00:48.797: INFO: Pod quantity 1 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:00:49.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1681" for this suite.

• [SLOW TEST:10.340 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","total":346,"completed":105,"skipped":2121,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:00:49.822: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should complete a service status lifecycle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a Service
STEP: watching for the Service to be added
Jul 31 11:00:49.949: INFO: Found Service test-service-w5qpc in namespace services-9916 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Jul 31 11:00:49.949: INFO: Service test-service-w5qpc created
STEP: Getting /status
Jul 31 11:00:49.960: INFO: Service test-service-w5qpc has LoadBalancer: {[]}
STEP: patching the ServiceStatus
STEP: watching for the Service to be patched
Jul 31 11:00:49.971: INFO: observed Service test-service-w5qpc in namespace services-9916 with annotations: map[] & LoadBalancer: {[]}
Jul 31 11:00:49.971: INFO: Found Service test-service-w5qpc in namespace services-9916 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Jul 31 11:00:49.971: INFO: Service test-service-w5qpc has service status patched
STEP: updating the ServiceStatus
Jul 31 11:00:49.983: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated
Jul 31 11:00:49.985: INFO: Observed Service test-service-w5qpc in namespace services-9916 with annotations: map[] & Conditions: {[]}
Jul 31 11:00:49.985: INFO: Observed event: &Service{ObjectMeta:{test-service-w5qpc  services-9916  c374eec8-c678-4c0e-b762-aaad7f38763e 15564 0 2022-07-31 11:00:49 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] []  [{e2e.test Update v1 2022-07-31 11:00:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2022-07-31 11:00:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.103.102.86,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.103.102.86],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Jul 31 11:00:49.985: INFO: Found Service test-service-w5qpc in namespace services-9916 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jul 31 11:00:49.985: INFO: Service test-service-w5qpc has service status updated
STEP: patching the service
STEP: watching for the Service to be patched
Jul 31 11:00:50.007: INFO: observed Service test-service-w5qpc in namespace services-9916 with labels: map[test-service-static:true]
Jul 31 11:00:50.007: INFO: observed Service test-service-w5qpc in namespace services-9916 with labels: map[test-service-static:true]
Jul 31 11:00:50.007: INFO: observed Service test-service-w5qpc in namespace services-9916 with labels: map[test-service-static:true]
Jul 31 11:00:50.007: INFO: Found Service test-service-w5qpc in namespace services-9916 with labels: map[test-service:patched test-service-static:true]
Jul 31 11:00:50.007: INFO: Service test-service-w5qpc patched
STEP: deleting the service
STEP: watching for the Service to be deleted
Jul 31 11:00:50.047: INFO: Observed event: ADDED
Jul 31 11:00:50.048: INFO: Observed event: MODIFIED
Jul 31 11:00:50.048: INFO: Observed event: MODIFIED
Jul 31 11:00:50.048: INFO: Observed event: MODIFIED
Jul 31 11:00:50.048: INFO: Found Service test-service-w5qpc in namespace services-9916 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Jul 31 11:00:50.048: INFO: Service test-service-w5qpc deleted
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:00:50.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9916" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756
•{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","total":346,"completed":106,"skipped":2129,"failed":0}
SSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:00:50.072: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename limitrange
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Jul 31 11:00:50.142: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Jul 31 11:00:50.160: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jul 31 11:00:50.160: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Jul 31 11:00:50.195: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jul 31 11:00:50.195: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Jul 31 11:00:50.242: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jul 31 11:00:50.242: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Jul 31 11:00:57.348: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:00:57.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-8662" for this suite.

• [SLOW TEST:7.349 seconds]
[sig-scheduling] LimitRange
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":346,"completed":107,"skipped":2134,"failed":0}
SSSSSSSS
------------------------------
[sig-network] Services 
  should delete a collection of services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:00:57.422: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should delete a collection of services [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a collection of services
Jul 31 11:00:57.673: INFO: Creating e2e-svc-a-8t72s
Jul 31 11:00:57.727: INFO: Creating e2e-svc-b-lwlkj
Jul 31 11:00:57.763: INFO: Creating e2e-svc-c-hnmnx
STEP: deleting service collection
Jul 31 11:00:57.888: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:00:57.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1400" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756
•{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","total":346,"completed":108,"skipped":2142,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:00:57.925: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Jul 31 11:00:58.007: INFO: Waiting up to 5m0s for pod "downwardapi-volume-497143b4-ca9d-437d-9aa5-ff2246007dc8" in namespace "downward-api-9354" to be "Succeeded or Failed"
Jul 31 11:00:58.037: INFO: Pod "downwardapi-volume-497143b4-ca9d-437d-9aa5-ff2246007dc8": Phase="Pending", Reason="", readiness=false. Elapsed: 29.505626ms
Jul 31 11:01:00.052: INFO: Pod "downwardapi-volume-497143b4-ca9d-437d-9aa5-ff2246007dc8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045184137s
Jul 31 11:01:02.066: INFO: Pod "downwardapi-volume-497143b4-ca9d-437d-9aa5-ff2246007dc8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.059238608s
Jul 31 11:01:04.079: INFO: Pod "downwardapi-volume-497143b4-ca9d-437d-9aa5-ff2246007dc8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.071475912s
STEP: Saw pod success
Jul 31 11:01:04.079: INFO: Pod "downwardapi-volume-497143b4-ca9d-437d-9aa5-ff2246007dc8" satisfied condition "Succeeded or Failed"
Jul 31 11:01:04.083: INFO: Trying to get logs from node p1-ashfcfj4pzo6aemt1j3a9ah7ew pod downwardapi-volume-497143b4-ca9d-437d-9aa5-ff2246007dc8 container client-container: <nil>
STEP: delete the pod
Jul 31 11:01:04.127: INFO: Waiting for pod downwardapi-volume-497143b4-ca9d-437d-9aa5-ff2246007dc8 to disappear
Jul 31 11:01:04.136: INFO: Pod downwardapi-volume-497143b4-ca9d-437d-9aa5-ff2246007dc8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:01:04.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9354" for this suite.

• [SLOW TEST:6.230 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":109,"skipped":2207,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:01:04.160: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod pod-subpath-test-downwardapi-7g5g
STEP: Creating a pod to test atomic-volume-subpath
Jul 31 11:01:04.307: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-7g5g" in namespace "subpath-8921" to be "Succeeded or Failed"
Jul 31 11:01:04.520: INFO: Pod "pod-subpath-test-downwardapi-7g5g": Phase="Pending", Reason="", readiness=false. Elapsed: 213.661151ms
Jul 31 11:01:06.532: INFO: Pod "pod-subpath-test-downwardapi-7g5g": Phase="Running", Reason="", readiness=true. Elapsed: 2.225458874s
Jul 31 11:01:08.547: INFO: Pod "pod-subpath-test-downwardapi-7g5g": Phase="Running", Reason="", readiness=true. Elapsed: 4.239847525s
Jul 31 11:01:10.558: INFO: Pod "pod-subpath-test-downwardapi-7g5g": Phase="Running", Reason="", readiness=true. Elapsed: 6.25078638s
Jul 31 11:01:12.574: INFO: Pod "pod-subpath-test-downwardapi-7g5g": Phase="Running", Reason="", readiness=true. Elapsed: 8.26717868s
Jul 31 11:01:14.589: INFO: Pod "pod-subpath-test-downwardapi-7g5g": Phase="Running", Reason="", readiness=true. Elapsed: 10.282702475s
Jul 31 11:01:16.606: INFO: Pod "pod-subpath-test-downwardapi-7g5g": Phase="Running", Reason="", readiness=true. Elapsed: 12.299091486s
Jul 31 11:01:18.625: INFO: Pod "pod-subpath-test-downwardapi-7g5g": Phase="Running", Reason="", readiness=true. Elapsed: 14.317979781s
Jul 31 11:01:20.639: INFO: Pod "pod-subpath-test-downwardapi-7g5g": Phase="Running", Reason="", readiness=true. Elapsed: 16.332518707s
Jul 31 11:01:22.658: INFO: Pod "pod-subpath-test-downwardapi-7g5g": Phase="Running", Reason="", readiness=true. Elapsed: 18.351166755s
Jul 31 11:01:24.674: INFO: Pod "pod-subpath-test-downwardapi-7g5g": Phase="Running", Reason="", readiness=true. Elapsed: 20.36751408s
Jul 31 11:01:26.689: INFO: Pod "pod-subpath-test-downwardapi-7g5g": Phase="Running", Reason="", readiness=false. Elapsed: 22.381835505s
Jul 31 11:01:28.706: INFO: Pod "pod-subpath-test-downwardapi-7g5g": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.398765679s
STEP: Saw pod success
Jul 31 11:01:28.706: INFO: Pod "pod-subpath-test-downwardapi-7g5g" satisfied condition "Succeeded or Failed"
Jul 31 11:01:28.711: INFO: Trying to get logs from node p1-nc3cz44465xd41a6poxnmrpeqo pod pod-subpath-test-downwardapi-7g5g container test-container-subpath-downwardapi-7g5g: <nil>
STEP: delete the pod
Jul 31 11:01:28.782: INFO: Waiting for pod pod-subpath-test-downwardapi-7g5g to disappear
Jul 31 11:01:28.788: INFO: Pod pod-subpath-test-downwardapi-7g5g no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-7g5g
Jul 31 11:01:28.788: INFO: Deleting pod "pod-subpath-test-downwardapi-7g5g" in namespace "subpath-8921"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:01:28.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8921" for this suite.

• [SLOW TEST:24.650 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":110,"skipped":2227,"failed":0}
S
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:01:28.810: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap configmap-3403/configmap-test-c05fdfe7-60fe-419d-9cdd-bf5676ed3a1e
STEP: Creating a pod to test consume configMaps
Jul 31 11:01:28.910: INFO: Waiting up to 5m0s for pod "pod-configmaps-8a967530-9422-4fe3-ae22-d587e50c621b" in namespace "configmap-3403" to be "Succeeded or Failed"
Jul 31 11:01:28.942: INFO: Pod "pod-configmaps-8a967530-9422-4fe3-ae22-d587e50c621b": Phase="Pending", Reason="", readiness=false. Elapsed: 31.827126ms
Jul 31 11:01:30.955: INFO: Pod "pod-configmaps-8a967530-9422-4fe3-ae22-d587e50c621b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045056548s
Jul 31 11:01:32.971: INFO: Pod "pod-configmaps-8a967530-9422-4fe3-ae22-d587e50c621b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.061456864s
STEP: Saw pod success
Jul 31 11:01:32.971: INFO: Pod "pod-configmaps-8a967530-9422-4fe3-ae22-d587e50c621b" satisfied condition "Succeeded or Failed"
Jul 31 11:01:32.975: INFO: Trying to get logs from node p1-ashfcfj4pzo6aemt1j3a9ah7ew pod pod-configmaps-8a967530-9422-4fe3-ae22-d587e50c621b container env-test: <nil>
STEP: delete the pod
Jul 31 11:01:33.021: INFO: Waiting for pod pod-configmaps-8a967530-9422-4fe3-ae22-d587e50c621b to disappear
Jul 31 11:01:33.026: INFO: Pod pod-configmaps-8a967530-9422-4fe3-ae22-d587e50c621b no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:01:33.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3403" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":111,"skipped":2228,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:01:33.049: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jul 31 11:01:33.960: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jul 31 11:01:35.983: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.July, 31, 11, 1, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 11, 1, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.July, 31, 11, 1, 33, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 11, 1, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-67c86bcf4b\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 31 11:01:39.046: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 11:01:39.055: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:01:42.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5831" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:9.432 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":346,"completed":112,"skipped":2233,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:01:42.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Performing setup for networking test in namespace pod-network-test-2499
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 31 11:01:42.556: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jul 31 11:01:42.756: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 31 11:01:44.768: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 31 11:01:46.772: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 31 11:01:48.767: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 31 11:01:50.769: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 31 11:01:52.773: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 31 11:01:54.767: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 31 11:01:56.766: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 31 11:01:58.774: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 31 11:02:00.772: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 31 11:02:02.769: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 31 11:02:04.768: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jul 31 11:02:04.777: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jul 31 11:02:04.785: INFO: The status of Pod netserver-2 is Running (Ready = true)
Jul 31 11:02:04.794: INFO: The status of Pod netserver-3 is Running (Ready = true)
Jul 31 11:02:04.803: INFO: The status of Pod netserver-4 is Running (Ready = true)
STEP: Creating test pods
Jul 31 11:02:06.851: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
Jul 31 11:02:06.851: INFO: Breadth first check of 172.28.80.1 on host 172.27.21.73...
Jul 31 11:02:06.855: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.28.80.2:9080/dial?request=hostname&protocol=udp&host=172.28.80.1&port=8081&tries=1'] Namespace:pod-network-test-2499 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 31 11:02:06.855: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
Jul 31 11:02:06.856: INFO: ExecWithOptions: Clientset creation
Jul 31 11:02:06.856: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2499/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.28.80.2%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.28.80.1%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Jul 31 11:02:07.005: INFO: Waiting for responses: map[]
Jul 31 11:02:07.005: INFO: reached 172.28.80.1 after 0/1 tries
Jul 31 11:02:07.005: INFO: Breadth first check of 172.28.0.2 on host 172.27.21.69...
Jul 31 11:02:07.012: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.28.80.2:9080/dial?request=hostname&protocol=udp&host=172.28.0.2&port=8081&tries=1'] Namespace:pod-network-test-2499 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 31 11:02:07.012: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
Jul 31 11:02:07.013: INFO: ExecWithOptions: Clientset creation
Jul 31 11:02:07.013: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2499/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.28.80.2%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.28.0.2%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Jul 31 11:02:07.168: INFO: Waiting for responses: map[]
Jul 31 11:02:07.168: INFO: reached 172.28.0.2 after 0/1 tries
Jul 31 11:02:07.168: INFO: Breadth first check of 172.28.176.2 on host 172.27.21.77...
Jul 31 11:02:07.175: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.28.80.2:9080/dial?request=hostname&protocol=udp&host=172.28.176.2&port=8081&tries=1'] Namespace:pod-network-test-2499 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 31 11:02:07.175: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
Jul 31 11:02:07.176: INFO: ExecWithOptions: Clientset creation
Jul 31 11:02:07.176: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2499/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.28.80.2%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.28.176.2%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Jul 31 11:02:07.338: INFO: Waiting for responses: map[]
Jul 31 11:02:07.338: INFO: reached 172.28.176.2 after 0/1 tries
Jul 31 11:02:07.338: INFO: Breadth first check of 172.28.96.2 on host 172.27.21.72...
Jul 31 11:02:07.345: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.28.80.2:9080/dial?request=hostname&protocol=udp&host=172.28.96.2&port=8081&tries=1'] Namespace:pod-network-test-2499 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 31 11:02:07.345: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
Jul 31 11:02:07.346: INFO: ExecWithOptions: Clientset creation
Jul 31 11:02:07.346: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2499/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.28.80.2%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.28.96.2%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Jul 31 11:02:07.505: INFO: Waiting for responses: map[]
Jul 31 11:02:07.505: INFO: reached 172.28.96.2 after 0/1 tries
Jul 31 11:02:07.505: INFO: Breadth first check of 172.28.64.1 on host 172.27.21.79...
Jul 31 11:02:07.511: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.28.80.2:9080/dial?request=hostname&protocol=udp&host=172.28.64.1&port=8081&tries=1'] Namespace:pod-network-test-2499 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 31 11:02:07.511: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
Jul 31 11:02:07.512: INFO: ExecWithOptions: Clientset creation
Jul 31 11:02:07.512: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2499/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.28.80.2%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.28.64.1%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Jul 31 11:02:07.653: INFO: Waiting for responses: map[]
Jul 31 11:02:07.653: INFO: reached 172.28.64.1 after 0/1 tries
Jul 31 11:02:07.654: INFO: Going to retry 0 out of 5 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:02:07.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2499" for this suite.

• [SLOW TEST:25.201 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":346,"completed":113,"skipped":2242,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:02:07.685: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Jul 31 11:02:07.784: INFO: Waiting up to 5m0s for pod "downwardapi-volume-44b5f461-b17d-4de7-9ae8-8278de1c00db" in namespace "projected-3923" to be "Succeeded or Failed"
Jul 31 11:02:07.795: INFO: Pod "downwardapi-volume-44b5f461-b17d-4de7-9ae8-8278de1c00db": Phase="Pending", Reason="", readiness=false. Elapsed: 11.002894ms
Jul 31 11:02:09.808: INFO: Pod "downwardapi-volume-44b5f461-b17d-4de7-9ae8-8278de1c00db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023556832s
Jul 31 11:02:11.819: INFO: Pod "downwardapi-volume-44b5f461-b17d-4de7-9ae8-8278de1c00db": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035168751s
Jul 31 11:02:13.839: INFO: Pod "downwardapi-volume-44b5f461-b17d-4de7-9ae8-8278de1c00db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.054357993s
STEP: Saw pod success
Jul 31 11:02:13.839: INFO: Pod "downwardapi-volume-44b5f461-b17d-4de7-9ae8-8278de1c00db" satisfied condition "Succeeded or Failed"
Jul 31 11:02:13.870: INFO: Trying to get logs from node p1-nc3cz44465xd41a6poxnmrpeqo pod downwardapi-volume-44b5f461-b17d-4de7-9ae8-8278de1c00db container client-container: <nil>
STEP: delete the pod
Jul 31 11:02:13.975: INFO: Waiting for pod downwardapi-volume-44b5f461-b17d-4de7-9ae8-8278de1c00db to disappear
Jul 31 11:02:13.982: INFO: Pod downwardapi-volume-44b5f461-b17d-4de7-9ae8-8278de1c00db no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:02:13.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3923" for this suite.

• [SLOW TEST:6.353 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":114,"skipped":2267,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:02:14.038: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create a ReplicaSet
STEP: Verify that the required pods have come up
Jul 31 11:02:14.186: INFO: Pod name sample-pod: Found 0 pods out of 3
Jul 31 11:02:19.204: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running
Jul 31 11:02:19.211: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets
STEP: DeleteCollection of the ReplicaSets
STEP: After DeleteCollection verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:02:19.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8922" for this suite.

• [SLOW TEST:5.228 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","total":346,"completed":115,"skipped":2280,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:02:19.273: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Jul 31 11:02:19.458: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5c960afd-44b5-4abe-b717-0a8e4e56851a" in namespace "projected-5685" to be "Succeeded or Failed"
Jul 31 11:02:19.488: INFO: Pod "downwardapi-volume-5c960afd-44b5-4abe-b717-0a8e4e56851a": Phase="Pending", Reason="", readiness=false. Elapsed: 29.520483ms
Jul 31 11:02:21.500: INFO: Pod "downwardapi-volume-5c960afd-44b5-4abe-b717-0a8e4e56851a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042085265s
Jul 31 11:02:23.513: INFO: Pod "downwardapi-volume-5c960afd-44b5-4abe-b717-0a8e4e56851a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055018344s
STEP: Saw pod success
Jul 31 11:02:23.513: INFO: Pod "downwardapi-volume-5c960afd-44b5-4abe-b717-0a8e4e56851a" satisfied condition "Succeeded or Failed"
Jul 31 11:02:23.519: INFO: Trying to get logs from node p1-eipeq63rfgo6ooocmu3kqk3tcc pod downwardapi-volume-5c960afd-44b5-4abe-b717-0a8e4e56851a container client-container: <nil>
STEP: delete the pod
Jul 31 11:02:23.582: INFO: Waiting for pod downwardapi-volume-5c960afd-44b5-4abe-b717-0a8e4e56851a to disappear
Jul 31 11:02:23.589: INFO: Pod downwardapi-volume-5c960afd-44b5-4abe-b717-0a8e4e56851a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:02:23.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5685" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":346,"completed":116,"skipped":2322,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:02:23.613: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: validating cluster-info
Jul 31 11:02:23.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-9455 cluster-info'
Jul 31 11:02:23.757: INFO: stderr: ""
Jul 31 11:02:23.757: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:02:23.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9455" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":346,"completed":117,"skipped":2353,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:02:23.777: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename discovery
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 11:02:24.573: INFO: Checking APIGroup: apiregistration.k8s.io
Jul 31 11:02:24.575: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jul 31 11:02:24.575: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Jul 31 11:02:24.575: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jul 31 11:02:24.575: INFO: Checking APIGroup: apps
Jul 31 11:02:24.576: INFO: PreferredVersion.GroupVersion: apps/v1
Jul 31 11:02:24.576: INFO: Versions found [{apps/v1 v1}]
Jul 31 11:02:24.576: INFO: apps/v1 matches apps/v1
Jul 31 11:02:24.576: INFO: Checking APIGroup: events.k8s.io
Jul 31 11:02:24.578: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jul 31 11:02:24.578: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Jul 31 11:02:24.578: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jul 31 11:02:24.578: INFO: Checking APIGroup: authentication.k8s.io
Jul 31 11:02:24.579: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jul 31 11:02:24.579: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Jul 31 11:02:24.579: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jul 31 11:02:24.579: INFO: Checking APIGroup: authorization.k8s.io
Jul 31 11:02:24.580: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jul 31 11:02:24.580: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Jul 31 11:02:24.580: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jul 31 11:02:24.580: INFO: Checking APIGroup: autoscaling
Jul 31 11:02:24.581: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Jul 31 11:02:24.581: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Jul 31 11:02:24.581: INFO: autoscaling/v2 matches autoscaling/v2
Jul 31 11:02:24.581: INFO: Checking APIGroup: batch
Jul 31 11:02:24.582: INFO: PreferredVersion.GroupVersion: batch/v1
Jul 31 11:02:24.582: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Jul 31 11:02:24.582: INFO: batch/v1 matches batch/v1
Jul 31 11:02:24.582: INFO: Checking APIGroup: certificates.k8s.io
Jul 31 11:02:24.583: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jul 31 11:02:24.583: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Jul 31 11:02:24.583: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jul 31 11:02:24.583: INFO: Checking APIGroup: networking.k8s.io
Jul 31 11:02:24.584: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jul 31 11:02:24.584: INFO: Versions found [{networking.k8s.io/v1 v1}]
Jul 31 11:02:24.584: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jul 31 11:02:24.584: INFO: Checking APIGroup: policy
Jul 31 11:02:24.585: INFO: PreferredVersion.GroupVersion: policy/v1
Jul 31 11:02:24.586: INFO: Versions found [{policy/v1 v1} {policy/v1beta1 v1beta1}]
Jul 31 11:02:24.586: INFO: policy/v1 matches policy/v1
Jul 31 11:02:24.586: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jul 31 11:02:24.588: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jul 31 11:02:24.588: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Jul 31 11:02:24.588: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jul 31 11:02:24.588: INFO: Checking APIGroup: storage.k8s.io
Jul 31 11:02:24.589: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jul 31 11:02:24.590: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jul 31 11:02:24.590: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jul 31 11:02:24.590: INFO: Checking APIGroup: admissionregistration.k8s.io
Jul 31 11:02:24.591: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jul 31 11:02:24.591: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Jul 31 11:02:24.591: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jul 31 11:02:24.592: INFO: Checking APIGroup: apiextensions.k8s.io
Jul 31 11:02:24.593: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jul 31 11:02:24.593: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Jul 31 11:02:24.593: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jul 31 11:02:24.593: INFO: Checking APIGroup: scheduling.k8s.io
Jul 31 11:02:24.594: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jul 31 11:02:24.594: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Jul 31 11:02:24.595: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jul 31 11:02:24.595: INFO: Checking APIGroup: coordination.k8s.io
Jul 31 11:02:24.596: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jul 31 11:02:24.596: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Jul 31 11:02:24.596: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jul 31 11:02:24.596: INFO: Checking APIGroup: node.k8s.io
Jul 31 11:02:24.597: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jul 31 11:02:24.597: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Jul 31 11:02:24.598: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jul 31 11:02:24.598: INFO: Checking APIGroup: discovery.k8s.io
Jul 31 11:02:24.599: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Jul 31 11:02:24.599: INFO: Versions found [{discovery.k8s.io/v1 v1} {discovery.k8s.io/v1beta1 v1beta1}]
Jul 31 11:02:24.599: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Jul 31 11:02:24.599: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jul 31 11:02:24.600: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Jul 31 11:02:24.600: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Jul 31 11:02:24.601: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Jul 31 11:02:24.601: INFO: Checking APIGroup: metrics.k8s.io
Jul 31 11:02:24.602: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Jul 31 11:02:24.602: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Jul 31 11:02:24.602: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:02:24.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-7056" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":346,"completed":118,"skipped":2360,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:02:24.624: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jul 31 11:02:24.676: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 31 11:02:24.721: INFO: Waiting for terminating namespaces to be deleted...
Jul 31 11:02:24.725: INFO: 
Logging pods the apiserver thinks is on node p1-ashfcfj4pzo6aemt1j3a9ah7ew before test
Jul 31 11:02:24.752: INFO: csi-ridge-node-hndsv from kube-system started at 2022-07-31 10:18:00 +0000 UTC (1 container statuses recorded)
Jul 31 11:02:24.752: INFO: 	Container csi-ridge-plugin ready: true, restart count 0
Jul 31 11:02:24.752: INFO: kube-proxy-24lml from kube-system started at 2022-07-31 10:17:50 +0000 UTC (1 container statuses recorded)
Jul 31 11:02:24.752: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 31 11:02:24.752: INFO: meta-lqxb8 from kube-system started at 2022-07-31 10:17:50 +0000 UTC (1 container statuses recorded)
Jul 31 11:02:24.752: INFO: 	Container meta ready: true, restart count 0
Jul 31 11:02:24.752: INFO: weave-net-qfsdt from kube-system started at 2022-07-31 10:17:59 +0000 UTC (2 container statuses recorded)
Jul 31 11:02:24.752: INFO: 	Container weave ready: true, restart count 1
Jul 31 11:02:24.752: INFO: 	Container weave-npc ready: true, restart count 0
Jul 31 11:02:24.752: INFO: sonobuoy-systemd-logs-daemon-set-b467c6d60405477d-jk6fz from sonobuoy started at 2022-07-31 10:30:53 +0000 UTC (2 container statuses recorded)
Jul 31 11:02:24.752: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 31 11:02:24.752: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 31 11:02:24.752: INFO: 
Logging pods the apiserver thinks is on node p1-eipeq63rfgo6ooocmu3kqk3tcc before test
Jul 31 11:02:24.775: INFO: csi-ridge-node-zd62w from kube-system started at 2022-07-31 10:18:00 +0000 UTC (1 container statuses recorded)
Jul 31 11:02:24.775: INFO: 	Container csi-ridge-plugin ready: true, restart count 0
Jul 31 11:02:24.775: INFO: kube-proxy-zjtsg from kube-system started at 2022-07-31 10:17:50 +0000 UTC (1 container statuses recorded)
Jul 31 11:02:24.775: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 31 11:02:24.775: INFO: meta-75kgq from kube-system started at 2022-07-31 10:17:50 +0000 UTC (1 container statuses recorded)
Jul 31 11:02:24.775: INFO: 	Container meta ready: true, restart count 0
Jul 31 11:02:24.775: INFO: weave-net-wk9fw from kube-system started at 2022-07-31 10:17:59 +0000 UTC (2 container statuses recorded)
Jul 31 11:02:24.775: INFO: 	Container weave ready: true, restart count 1
Jul 31 11:02:24.775: INFO: 	Container weave-npc ready: true, restart count 0
Jul 31 11:02:24.775: INFO: sonobuoy-systemd-logs-daemon-set-b467c6d60405477d-hs6kj from sonobuoy started at 2022-07-31 10:30:53 +0000 UTC (2 container statuses recorded)
Jul 31 11:02:24.775: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 31 11:02:24.775: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 31 11:02:24.775: INFO: 
Logging pods the apiserver thinks is on node p1-esqoe7n9eqq76k8ojd9tajh51e before test
Jul 31 11:02:24.797: INFO: csi-ridge-node-ds95v from kube-system started at 2022-07-31 10:18:00 +0000 UTC (1 container statuses recorded)
Jul 31 11:02:24.797: INFO: 	Container csi-ridge-plugin ready: true, restart count 0
Jul 31 11:02:24.797: INFO: kube-proxy-wkgc5 from kube-system started at 2022-07-31 10:17:50 +0000 UTC (1 container statuses recorded)
Jul 31 11:02:24.797: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 31 11:02:24.797: INFO: meta-h5cxg from kube-system started at 2022-07-31 10:17:50 +0000 UTC (1 container statuses recorded)
Jul 31 11:02:24.797: INFO: 	Container meta ready: true, restart count 0
Jul 31 11:02:24.797: INFO: weave-net-h6vbc from kube-system started at 2022-07-31 10:17:59 +0000 UTC (2 container statuses recorded)
Jul 31 11:02:24.797: INFO: 	Container weave ready: true, restart count 1
Jul 31 11:02:24.797: INFO: 	Container weave-npc ready: true, restart count 0
Jul 31 11:02:24.797: INFO: sonobuoy from sonobuoy started at 2022-07-31 10:30:49 +0000 UTC (1 container statuses recorded)
Jul 31 11:02:24.797: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 31 11:02:24.797: INFO: sonobuoy-systemd-logs-daemon-set-b467c6d60405477d-grkth from sonobuoy started at 2022-07-31 10:30:53 +0000 UTC (2 container statuses recorded)
Jul 31 11:02:24.797: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 31 11:02:24.797: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 31 11:02:24.797: INFO: 
Logging pods the apiserver thinks is on node p1-mz5rqwt5oar4y4n97cfumgzrxe before test
Jul 31 11:02:24.814: INFO: csi-ridge-node-xs7qk from kube-system started at 2022-07-31 10:18:00 +0000 UTC (1 container statuses recorded)
Jul 31 11:02:24.814: INFO: 	Container csi-ridge-plugin ready: true, restart count 0
Jul 31 11:02:24.814: INFO: kube-proxy-hhznq from kube-system started at 2022-07-31 10:17:50 +0000 UTC (1 container statuses recorded)
Jul 31 11:02:24.814: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 31 11:02:24.814: INFO: meta-dmrhb from kube-system started at 2022-07-31 10:17:50 +0000 UTC (1 container statuses recorded)
Jul 31 11:02:24.814: INFO: 	Container meta ready: true, restart count 0
Jul 31 11:02:24.814: INFO: weave-net-wclcv from kube-system started at 2022-07-31 10:17:59 +0000 UTC (2 container statuses recorded)
Jul 31 11:02:24.814: INFO: 	Container weave ready: true, restart count 1
Jul 31 11:02:24.814: INFO: 	Container weave-npc ready: true, restart count 0
Jul 31 11:02:24.814: INFO: sonobuoy-e2e-job-d80ae37bee83480b from sonobuoy started at 2022-07-31 10:30:53 +0000 UTC (2 container statuses recorded)
Jul 31 11:02:24.814: INFO: 	Container e2e ready: true, restart count 0
Jul 31 11:02:24.814: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 31 11:02:24.814: INFO: sonobuoy-systemd-logs-daemon-set-b467c6d60405477d-p7jjn from sonobuoy started at 2022-07-31 10:30:53 +0000 UTC (2 container statuses recorded)
Jul 31 11:02:24.814: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 31 11:02:24.814: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 31 11:02:24.814: INFO: 
Logging pods the apiserver thinks is on node p1-nc3cz44465xd41a6poxnmrpeqo before test
Jul 31 11:02:24.829: INFO: csi-ridge-node-t7hf9 from kube-system started at 2022-07-31 10:18:00 +0000 UTC (1 container statuses recorded)
Jul 31 11:02:24.829: INFO: 	Container csi-ridge-plugin ready: true, restart count 0
Jul 31 11:02:24.829: INFO: kube-proxy-g26h6 from kube-system started at 2022-07-31 10:17:50 +0000 UTC (1 container statuses recorded)
Jul 31 11:02:24.829: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 31 11:02:24.829: INFO: meta-9crb7 from kube-system started at 2022-07-31 10:17:50 +0000 UTC (1 container statuses recorded)
Jul 31 11:02:24.829: INFO: 	Container meta ready: true, restart count 0
Jul 31 11:02:24.829: INFO: weave-net-cm7ct from kube-system started at 2022-07-31 10:17:59 +0000 UTC (2 container statuses recorded)
Jul 31 11:02:24.829: INFO: 	Container weave ready: true, restart count 1
Jul 31 11:02:24.829: INFO: 	Container weave-npc ready: true, restart count 0
Jul 31 11:02:24.829: INFO: sonobuoy-systemd-logs-daemon-set-b467c6d60405477d-w6txb from sonobuoy started at 2022-07-31 10:30:53 +0000 UTC (2 container statuses recorded)
Jul 31 11:02:24.829: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 31 11:02:24.829: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-36a03269-84e4-46b3-ab54-0a9048f18a65 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.27.21.73 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-36a03269-84e4-46b3-ab54-0a9048f18a65 off the node p1-ashfcfj4pzo6aemt1j3a9ah7ew
STEP: verifying the node doesn't have the label kubernetes.io/e2e-36a03269-84e4-46b3-ab54-0a9048f18a65
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:07:33.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:308.565 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":346,"completed":119,"skipped":2371,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:07:33.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-6080
[It] should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating statefulset ss in namespace statefulset-6080
Jul 31 11:07:33.336: INFO: Found 0 stateful pods, waiting for 1
Jul 31 11:07:43.357: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
STEP: Patch a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Jul 31 11:07:43.411: INFO: Deleting all statefulset in ns statefulset-6080
Jul 31 11:07:43.457: INFO: Scaling statefulset ss to 0
Jul 31 11:07:53.515: INFO: Waiting for statefulset status.replicas updated to 0
Jul 31 11:07:53.520: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:07:53.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6080" for this suite.

• [SLOW TEST:20.389 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should have a working scale subresource [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":346,"completed":120,"skipped":2443,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:07:53.581: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service in namespace services-9000
STEP: creating service affinity-clusterip in namespace services-9000
STEP: creating replication controller affinity-clusterip in namespace services-9000
I0731 11:07:53.747115      23 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-9000, replica count: 3
I0731 11:07:56.798162      23 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 31 11:07:56.818: INFO: Creating new exec pod
Jul 31 11:07:59.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-9000 exec execpod-affinityfzn8z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Jul 31 11:08:00.099: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jul 31 11:08:00.099: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 31 11:08:00.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-9000 exec execpod-affinityfzn8z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.105.73.71 80'
Jul 31 11:08:00.326: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.105.73.71 80\nConnection to 10.105.73.71 80 port [tcp/http] succeeded!\n"
Jul 31 11:08:00.326: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 31 11:08:00.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-9000 exec execpod-affinityfzn8z -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.105.73.71:80/ ; done'
Jul 31 11:08:00.648: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.73.71:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.73.71:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.73.71:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.73.71:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.73.71:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.73.71:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.73.71:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.73.71:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.73.71:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.73.71:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.73.71:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.73.71:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.73.71:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.73.71:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.73.71:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.105.73.71:80/\n"
Jul 31 11:08:00.648: INFO: stdout: "\naffinity-clusterip-hjht2\naffinity-clusterip-hjht2\naffinity-clusterip-hjht2\naffinity-clusterip-hjht2\naffinity-clusterip-hjht2\naffinity-clusterip-hjht2\naffinity-clusterip-hjht2\naffinity-clusterip-hjht2\naffinity-clusterip-hjht2\naffinity-clusterip-hjht2\naffinity-clusterip-hjht2\naffinity-clusterip-hjht2\naffinity-clusterip-hjht2\naffinity-clusterip-hjht2\naffinity-clusterip-hjht2\naffinity-clusterip-hjht2"
Jul 31 11:08:00.648: INFO: Received response from host: affinity-clusterip-hjht2
Jul 31 11:08:00.648: INFO: Received response from host: affinity-clusterip-hjht2
Jul 31 11:08:00.648: INFO: Received response from host: affinity-clusterip-hjht2
Jul 31 11:08:00.648: INFO: Received response from host: affinity-clusterip-hjht2
Jul 31 11:08:00.648: INFO: Received response from host: affinity-clusterip-hjht2
Jul 31 11:08:00.648: INFO: Received response from host: affinity-clusterip-hjht2
Jul 31 11:08:00.648: INFO: Received response from host: affinity-clusterip-hjht2
Jul 31 11:08:00.648: INFO: Received response from host: affinity-clusterip-hjht2
Jul 31 11:08:00.648: INFO: Received response from host: affinity-clusterip-hjht2
Jul 31 11:08:00.648: INFO: Received response from host: affinity-clusterip-hjht2
Jul 31 11:08:00.648: INFO: Received response from host: affinity-clusterip-hjht2
Jul 31 11:08:00.648: INFO: Received response from host: affinity-clusterip-hjht2
Jul 31 11:08:00.648: INFO: Received response from host: affinity-clusterip-hjht2
Jul 31 11:08:00.648: INFO: Received response from host: affinity-clusterip-hjht2
Jul 31 11:08:00.648: INFO: Received response from host: affinity-clusterip-hjht2
Jul 31 11:08:00.648: INFO: Received response from host: affinity-clusterip-hjht2
Jul 31 11:08:00.648: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-9000, will wait for the garbage collector to delete the pods
Jul 31 11:08:00.780: INFO: Deleting ReplicationController affinity-clusterip took: 16.576993ms
Jul 31 11:08:00.880: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.190711ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:08:03.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9000" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:9.594 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":121,"skipped":2466,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:08:03.176: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a suspended cronjob
STEP: Ensuring no jobs are scheduled
STEP: Ensuring no job exists by listing jobs explicitly
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:13:03.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-7292" for this suite.

• [SLOW TEST:300.150 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","total":346,"completed":122,"skipped":2489,"failed":0}
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:13:03.328: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-487f0a19-9732-4cd8-a19d-471992fd7d96
STEP: Creating a pod to test consume configMaps
Jul 31 11:13:03.440: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e2f84345-8610-4c02-8846-4dbbcb7de7d4" in namespace "projected-1231" to be "Succeeded or Failed"
Jul 31 11:13:03.468: INFO: Pod "pod-projected-configmaps-e2f84345-8610-4c02-8846-4dbbcb7de7d4": Phase="Pending", Reason="", readiness=false. Elapsed: 27.712168ms
Jul 31 11:13:05.481: INFO: Pod "pod-projected-configmaps-e2f84345-8610-4c02-8846-4dbbcb7de7d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041372921s
Jul 31 11:13:07.492: INFO: Pod "pod-projected-configmaps-e2f84345-8610-4c02-8846-4dbbcb7de7d4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052523905s
Jul 31 11:13:09.503: INFO: Pod "pod-projected-configmaps-e2f84345-8610-4c02-8846-4dbbcb7de7d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.063399552s
STEP: Saw pod success
Jul 31 11:13:09.503: INFO: Pod "pod-projected-configmaps-e2f84345-8610-4c02-8846-4dbbcb7de7d4" satisfied condition "Succeeded or Failed"
Jul 31 11:13:09.508: INFO: Trying to get logs from node p1-nc3cz44465xd41a6poxnmrpeqo pod pod-projected-configmaps-e2f84345-8610-4c02-8846-4dbbcb7de7d4 container agnhost-container: <nil>
STEP: delete the pod
Jul 31 11:13:09.571: INFO: Waiting for pod pod-projected-configmaps-e2f84345-8610-4c02-8846-4dbbcb7de7d4 to disappear
Jul 31 11:13:09.576: INFO: Pod pod-projected-configmaps-e2f84345-8610-4c02-8846-4dbbcb7de7d4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:13:09.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1231" for this suite.

• [SLOW TEST:6.265 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":123,"skipped":2492,"failed":0}
SSSS
------------------------------
[sig-apps] Deployment 
  should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:13:09.593: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should validate Deployment Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a Deployment
Jul 31 11:13:09.634: INFO: Creating simple deployment test-deployment-5s4k4
Jul 31 11:13:09.665: INFO: new replicaset for deployment "test-deployment-5s4k4" is yet to be created
STEP: Getting /status
Jul 31 11:13:11.697: INFO: Deployment test-deployment-5s4k4 has Conditions: [{Available True 2022-07-31 11:13:11 +0000 UTC 2022-07-31 11:13:11 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2022-07-31 11:13:11 +0000 UTC 2022-07-31 11:13:09 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-5s4k4-764bc7c4b7" has successfully progressed.}]
STEP: updating Deployment Status
Jul 31 11:13:11.711: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2022, time.July, 31, 11, 13, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 11, 13, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.July, 31, 11, 13, 11, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 11, 13, 9, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-5s4k4-764bc7c4b7\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated
Jul 31 11:13:11.714: INFO: Observed &Deployment event: ADDED
Jul 31 11:13:11.714: INFO: Observed Deployment test-deployment-5s4k4 in namespace deployment-6409 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-07-31 11:13:09 +0000 UTC 2022-07-31 11:13:09 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-5s4k4-764bc7c4b7"}
Jul 31 11:13:11.715: INFO: Observed &Deployment event: MODIFIED
Jul 31 11:13:11.715: INFO: Observed Deployment test-deployment-5s4k4 in namespace deployment-6409 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-07-31 11:13:09 +0000 UTC 2022-07-31 11:13:09 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-5s4k4-764bc7c4b7"}
Jul 31 11:13:11.715: INFO: Observed Deployment test-deployment-5s4k4 in namespace deployment-6409 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-07-31 11:13:09 +0000 UTC 2022-07-31 11:13:09 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jul 31 11:13:11.715: INFO: Observed &Deployment event: MODIFIED
Jul 31 11:13:11.715: INFO: Observed Deployment test-deployment-5s4k4 in namespace deployment-6409 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-07-31 11:13:09 +0000 UTC 2022-07-31 11:13:09 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jul 31 11:13:11.715: INFO: Observed Deployment test-deployment-5s4k4 in namespace deployment-6409 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-07-31 11:13:09 +0000 UTC 2022-07-31 11:13:09 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-5s4k4-764bc7c4b7" is progressing.}
Jul 31 11:13:11.716: INFO: Observed &Deployment event: MODIFIED
Jul 31 11:13:11.716: INFO: Observed Deployment test-deployment-5s4k4 in namespace deployment-6409 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-07-31 11:13:11 +0000 UTC 2022-07-31 11:13:11 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jul 31 11:13:11.716: INFO: Observed Deployment test-deployment-5s4k4 in namespace deployment-6409 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-07-31 11:13:11 +0000 UTC 2022-07-31 11:13:09 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-5s4k4-764bc7c4b7" has successfully progressed.}
Jul 31 11:13:11.717: INFO: Observed &Deployment event: MODIFIED
Jul 31 11:13:11.717: INFO: Observed Deployment test-deployment-5s4k4 in namespace deployment-6409 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-07-31 11:13:11 +0000 UTC 2022-07-31 11:13:11 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jul 31 11:13:11.717: INFO: Observed Deployment test-deployment-5s4k4 in namespace deployment-6409 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-07-31 11:13:11 +0000 UTC 2022-07-31 11:13:09 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-5s4k4-764bc7c4b7" has successfully progressed.}
Jul 31 11:13:11.717: INFO: Found Deployment test-deployment-5s4k4 in namespace deployment-6409 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jul 31 11:13:11.717: INFO: Deployment test-deployment-5s4k4 has an updated status
STEP: patching the Statefulset Status
Jul 31 11:13:11.717: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jul 31 11:13:11.737: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched
Jul 31 11:13:11.739: INFO: Observed &Deployment event: ADDED
Jul 31 11:13:11.739: INFO: Observed deployment test-deployment-5s4k4 in namespace deployment-6409 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-07-31 11:13:09 +0000 UTC 2022-07-31 11:13:09 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-5s4k4-764bc7c4b7"}
Jul 31 11:13:11.739: INFO: Observed &Deployment event: MODIFIED
Jul 31 11:13:11.739: INFO: Observed deployment test-deployment-5s4k4 in namespace deployment-6409 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-07-31 11:13:09 +0000 UTC 2022-07-31 11:13:09 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-5s4k4-764bc7c4b7"}
Jul 31 11:13:11.739: INFO: Observed deployment test-deployment-5s4k4 in namespace deployment-6409 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-07-31 11:13:09 +0000 UTC 2022-07-31 11:13:09 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jul 31 11:13:11.740: INFO: Observed &Deployment event: MODIFIED
Jul 31 11:13:11.740: INFO: Observed deployment test-deployment-5s4k4 in namespace deployment-6409 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2022-07-31 11:13:09 +0000 UTC 2022-07-31 11:13:09 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jul 31 11:13:11.740: INFO: Observed deployment test-deployment-5s4k4 in namespace deployment-6409 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-07-31 11:13:09 +0000 UTC 2022-07-31 11:13:09 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-5s4k4-764bc7c4b7" is progressing.}
Jul 31 11:13:11.740: INFO: Observed &Deployment event: MODIFIED
Jul 31 11:13:11.740: INFO: Observed deployment test-deployment-5s4k4 in namespace deployment-6409 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-07-31 11:13:11 +0000 UTC 2022-07-31 11:13:11 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jul 31 11:13:11.740: INFO: Observed deployment test-deployment-5s4k4 in namespace deployment-6409 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-07-31 11:13:11 +0000 UTC 2022-07-31 11:13:09 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-5s4k4-764bc7c4b7" has successfully progressed.}
Jul 31 11:13:11.740: INFO: Observed &Deployment event: MODIFIED
Jul 31 11:13:11.740: INFO: Observed deployment test-deployment-5s4k4 in namespace deployment-6409 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2022-07-31 11:13:11 +0000 UTC 2022-07-31 11:13:11 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jul 31 11:13:11.740: INFO: Observed deployment test-deployment-5s4k4 in namespace deployment-6409 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2022-07-31 11:13:11 +0000 UTC 2022-07-31 11:13:09 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-5s4k4-764bc7c4b7" has successfully progressed.}
Jul 31 11:13:11.740: INFO: Observed deployment test-deployment-5s4k4 in namespace deployment-6409 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jul 31 11:13:11.740: INFO: Observed &Deployment event: MODIFIED
Jul 31 11:13:11.740: INFO: Found deployment test-deployment-5s4k4 in namespace deployment-6409 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Jul 31 11:13:11.740: INFO: Deployment test-deployment-5s4k4 has a patched status
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Jul 31 11:13:11.749: INFO: Deployment "test-deployment-5s4k4":
&Deployment{ObjectMeta:{test-deployment-5s4k4  deployment-6409  0bb7d19a-595e-4cc2-941f-e352020f10f6 18418 1 2022-07-31 11:13:09 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] []  [{e2e.test Update apps/v1 2022-07-31 11:13:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2022-07-31 11:13:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2022-07-31 11:13:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00590d0c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-5s4k4-764bc7c4b7",LastUpdateTime:2022-07-31 11:13:11 +0000 UTC,LastTransitionTime:2022-07-31 11:13:11 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jul 31 11:13:11.760: INFO: New ReplicaSet "test-deployment-5s4k4-764bc7c4b7" of Deployment "test-deployment-5s4k4":
&ReplicaSet{ObjectMeta:{test-deployment-5s4k4-764bc7c4b7  deployment-6409  fd66cb17-a298-400c-ab96-8c09e31f166e 18413 1 2022-07-31 11:13:09 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:764bc7c4b7] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-5s4k4 0bb7d19a-595e-4cc2-941f-e352020f10f6 0xc00590d4c0 0xc00590d4c1}] []  [{kube-controller-manager Update apps/v1 2022-07-31 11:13:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0bb7d19a-595e-4cc2-941f-e352020f10f6\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-07-31 11:13:11 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 764bc7c4b7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:764bc7c4b7] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00590d568 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jul 31 11:13:11.775: INFO: Pod "test-deployment-5s4k4-764bc7c4b7-gl99q" is available:
&Pod{ObjectMeta:{test-deployment-5s4k4-764bc7c4b7-gl99q test-deployment-5s4k4-764bc7c4b7- deployment-6409  23031368-5707-4afb-aebc-e0c9b7e7e7b8 18412 0 2022-07-31 11:13:09 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:764bc7c4b7] map[] [{apps/v1 ReplicaSet test-deployment-5s4k4-764bc7c4b7 fd66cb17-a298-400c-ab96-8c09e31f166e 0xc00590d930 0xc00590d931}] []  [{kube-controller-manager Update v1 2022-07-31 11:13:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fd66cb17-a298-400c-ab96-8c09e31f166e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-07-31 11:13:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.28.0.2\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5gsg2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5gsg2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:p1-eipeq63rfgo6ooocmu3kqk3tcc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:13:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:13:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:13:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:13:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.27.21.69,PodIP:172.28.0.2,StartTime:2022-07-31 11:13:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-07-31 11:13:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:docker://49aa441fce0694f9c2756798871639b3a52c66424cba30aceefc04a9276eac77,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.28.0.2,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:13:11.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6409" for this suite.
•{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","total":346,"completed":124,"skipped":2496,"failed":0}
SSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:13:11.792: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name s-test-opt-del-4547ca0a-d41d-4648-a5d6-4b9e9ad5bc09
STEP: Creating secret with name s-test-opt-upd-5241d5a7-6824-4482-8c6e-92c963d28283
STEP: Creating the pod
Jul 31 11:13:11.939: INFO: The status of Pod pod-secrets-5dc03b11-d854-4554-b4e1-856f3fd91d34 is Pending, waiting for it to be Running (with Ready = true)
Jul 31 11:13:13.955: INFO: The status of Pod pod-secrets-5dc03b11-d854-4554-b4e1-856f3fd91d34 is Pending, waiting for it to be Running (with Ready = true)
Jul 31 11:13:15.951: INFO: The status of Pod pod-secrets-5dc03b11-d854-4554-b4e1-856f3fd91d34 is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-4547ca0a-d41d-4648-a5d6-4b9e9ad5bc09
STEP: Updating secret s-test-opt-upd-5241d5a7-6824-4482-8c6e-92c963d28283
STEP: Creating secret with name s-test-opt-create-3a8d384c-39f2-4cc9-bd57-5da106625e66
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:14:40.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7787" for this suite.

• [SLOW TEST:89.105 seconds]
[sig-storage] Secrets
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":125,"skipped":2499,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:14:40.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-3fd02f50-239e-4473-a67e-ec088e3b964e
STEP: Creating a pod to test consume secrets
Jul 31 11:14:41.012: INFO: Waiting up to 5m0s for pod "pod-secrets-a8b112da-0fe1-4993-919a-f0eec693622e" in namespace "secrets-6999" to be "Succeeded or Failed"
Jul 31 11:14:41.040: INFO: Pod "pod-secrets-a8b112da-0fe1-4993-919a-f0eec693622e": Phase="Pending", Reason="", readiness=false. Elapsed: 28.351273ms
Jul 31 11:14:43.051: INFO: Pod "pod-secrets-a8b112da-0fe1-4993-919a-f0eec693622e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038914588s
Jul 31 11:14:45.065: INFO: Pod "pod-secrets-a8b112da-0fe1-4993-919a-f0eec693622e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053326352s
STEP: Saw pod success
Jul 31 11:14:45.065: INFO: Pod "pod-secrets-a8b112da-0fe1-4993-919a-f0eec693622e" satisfied condition "Succeeded or Failed"
Jul 31 11:14:45.069: INFO: Trying to get logs from node p1-ashfcfj4pzo6aemt1j3a9ah7ew pod pod-secrets-a8b112da-0fe1-4993-919a-f0eec693622e container secret-volume-test: <nil>
STEP: delete the pod
Jul 31 11:14:45.120: INFO: Waiting for pod pod-secrets-a8b112da-0fe1-4993-919a-f0eec693622e to disappear
Jul 31 11:14:45.124: INFO: Pod pod-secrets-a8b112da-0fe1-4993-919a-f0eec693622e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:14:45.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6999" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":126,"skipped":2501,"failed":0}
SSS
------------------------------
[sig-node] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:14:45.140: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:59
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod liveness-b2e09d89-dd70-4b56-b38c-06dd60291b83 in namespace container-probe-6078
Jul 31 11:14:47.277: INFO: Started pod liveness-b2e09d89-dd70-4b56-b38c-06dd60291b83 in namespace container-probe-6078
STEP: checking the pod's current state and verifying that restartCount is present
Jul 31 11:14:47.282: INFO: Initial restart count of pod liveness-b2e09d89-dd70-4b56-b38c-06dd60291b83 is 0
Jul 31 11:15:07.418: INFO: Restart count of pod container-probe-6078/liveness-b2e09d89-dd70-4b56-b38c-06dd60291b83 is now 1 (20.135923764s elapsed)
Jul 31 11:15:27.550: INFO: Restart count of pod container-probe-6078/liveness-b2e09d89-dd70-4b56-b38c-06dd60291b83 is now 2 (40.267485025s elapsed)
Jul 31 11:15:47.785: INFO: Restart count of pod container-probe-6078/liveness-b2e09d89-dd70-4b56-b38c-06dd60291b83 is now 3 (1m0.5025699s elapsed)
Jul 31 11:16:08.132: INFO: Restart count of pod container-probe-6078/liveness-b2e09d89-dd70-4b56-b38c-06dd60291b83 is now 4 (1m20.849052019s elapsed)
Jul 31 11:17:08.531: INFO: Restart count of pod container-probe-6078/liveness-b2e09d89-dd70-4b56-b38c-06dd60291b83 is now 5 (2m21.248207622s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:17:08.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6078" for this suite.

• [SLOW TEST:143.476 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":346,"completed":127,"skipped":2504,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:17:08.618: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 31 11:17:09.839: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 31 11:17:12.923: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:17:13.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2889" for this suite.
STEP: Destroying namespace "webhook-2889-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":346,"completed":128,"skipped":2615,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:17:13.472: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 31 11:17:18.682: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:17:18.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1378" for this suite.

• [SLOW TEST:5.306 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    on terminated container
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:134
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]","total":346,"completed":129,"skipped":2625,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:17:18.780: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-4547, will wait for the garbage collector to delete the pods
Jul 31 11:17:22.944: INFO: Deleting Job.batch foo took: 12.618286ms
Jul 31 11:17:23.045: INFO: Terminating Job.batch foo pods took: 100.668233ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:17:55.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4547" for this suite.

• [SLOW TEST:37.205 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":346,"completed":130,"skipped":2644,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:17:55.985: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:18:00.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-1623" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","total":346,"completed":131,"skipped":2667,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice 
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:18:00.276: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename endpointslice
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/endpointslice.go:49
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 11:18:00.394: INFO: Endpoints addresses: [172.27.21.68 172.27.21.74 172.27.21.80] , ports: [6443]
Jul 31 11:18:00.394: INFO: EndpointSlices addresses: [172.27.21.68 172.27.21.74 172.27.21.80] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:18:00.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-2101" for this suite.
•{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","total":346,"completed":132,"skipped":2680,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:18:00.425: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward api env vars
Jul 31 11:18:00.487: INFO: Waiting up to 5m0s for pod "downward-api-1720824c-1471-436e-8ea8-1ab60f4b06da" in namespace "downward-api-5836" to be "Succeeded or Failed"
Jul 31 11:18:00.492: INFO: Pod "downward-api-1720824c-1471-436e-8ea8-1ab60f4b06da": Phase="Pending", Reason="", readiness=false. Elapsed: 4.793921ms
Jul 31 11:18:02.505: INFO: Pod "downward-api-1720824c-1471-436e-8ea8-1ab60f4b06da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017672694s
Jul 31 11:18:04.515: INFO: Pod "downward-api-1720824c-1471-436e-8ea8-1ab60f4b06da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02767271s
STEP: Saw pod success
Jul 31 11:18:04.515: INFO: Pod "downward-api-1720824c-1471-436e-8ea8-1ab60f4b06da" satisfied condition "Succeeded or Failed"
Jul 31 11:18:04.520: INFO: Trying to get logs from node p1-ashfcfj4pzo6aemt1j3a9ah7ew pod downward-api-1720824c-1471-436e-8ea8-1ab60f4b06da container dapi-container: <nil>
STEP: delete the pod
Jul 31 11:18:04.575: INFO: Waiting for pod downward-api-1720824c-1471-436e-8ea8-1ab60f4b06da to disappear
Jul 31 11:18:04.580: INFO: Pod downward-api-1720824c-1471-436e-8ea8-1ab60f4b06da no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:18:04.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5836" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":346,"completed":133,"skipped":2712,"failed":0}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:18:04.601: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:18:11.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1682" for this suite.

• [SLOW TEST:7.123 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":346,"completed":134,"skipped":2716,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:18:11.724: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename server-version
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should find the server version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Request ServerVersion
STEP: Confirm major version
Jul 31 11:18:11.783: INFO: Major version: 1
STEP: Confirm minor version
Jul 31 11:18:11.783: INFO: cleanMinorVersion: 23
Jul 31 11:18:11.783: INFO: Minor version: 23
[AfterEach] [sig-api-machinery] server version
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:18:11.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-8724" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":346,"completed":135,"skipped":2731,"failed":0}
SSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:18:11.822: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test service account token: 
Jul 31 11:18:11.930: INFO: Waiting up to 5m0s for pod "test-pod-a45bc14c-08c1-4e9a-8013-70789954bb56" in namespace "svcaccounts-9828" to be "Succeeded or Failed"
Jul 31 11:18:11.941: INFO: Pod "test-pod-a45bc14c-08c1-4e9a-8013-70789954bb56": Phase="Pending", Reason="", readiness=false. Elapsed: 10.917336ms
Jul 31 11:18:13.956: INFO: Pod "test-pod-a45bc14c-08c1-4e9a-8013-70789954bb56": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025447354s
Jul 31 11:18:15.967: INFO: Pod "test-pod-a45bc14c-08c1-4e9a-8013-70789954bb56": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037107387s
STEP: Saw pod success
Jul 31 11:18:15.967: INFO: Pod "test-pod-a45bc14c-08c1-4e9a-8013-70789954bb56" satisfied condition "Succeeded or Failed"
Jul 31 11:18:15.973: INFO: Trying to get logs from node p1-ashfcfj4pzo6aemt1j3a9ah7ew pod test-pod-a45bc14c-08c1-4e9a-8013-70789954bb56 container agnhost-container: <nil>
STEP: delete the pod
Jul 31 11:18:16.021: INFO: Waiting for pod test-pod-a45bc14c-08c1-4e9a-8013-70789954bb56 to disappear
Jul 31 11:18:16.026: INFO: Pod test-pod-a45bc14c-08c1-4e9a-8013-70789954bb56 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:18:16.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9828" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":346,"completed":136,"skipped":2734,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:18:16.051: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 11:18:16.162: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jul 31 11:18:21.184: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 31 11:18:21.184: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Jul 31 11:18:21.229: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-1793  0cbde258-57b2-48e4-9dd2-8207d9991519 19710 1 2022-07-31 11:18:21 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2022-07-31 11:18:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001f7fb78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Jul 31 11:18:21.253: INFO: New ReplicaSet "test-cleanup-deployment-5dbdbf94dc" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-5dbdbf94dc  deployment-1793  d8b7ee1f-93d4-42c7-8704-78469cd0059d 19712 1 2022-07-31 11:18:21 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5dbdbf94dc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 0cbde258-57b2-48e4-9dd2-8207d9991519 0xc00460a6b7 0xc00460a6b8}] []  [{kube-controller-manager Update apps/v1 2022-07-31 11:18:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0cbde258-57b2-48e4-9dd2-8207d9991519\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 5dbdbf94dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5dbdbf94dc] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00460a748 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 31 11:18:21.253: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jul 31 11:18:21.253: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-1793  2894c011-5a57-4514-9502-7280a8fbd2c0 19711 1 2022-07-31 11:18:16 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 0cbde258-57b2-48e4-9dd2-8207d9991519 0xc00460a587 0xc00460a588}] []  [{e2e.test Update apps/v1 2022-07-31 11:18:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-07-31 11:18:18 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2022-07-31 11:18:21 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"0cbde258-57b2-48e4-9dd2-8207d9991519\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00460a648 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jul 31 11:18:21.303: INFO: Pod "test-cleanup-controller-p2r66" is available:
&Pod{ObjectMeta:{test-cleanup-controller-p2r66 test-cleanup-controller- deployment-1793  14bd697e-5bc6-4de8-b36a-8d5dc5769558 19700 0 2022-07-31 11:18:16 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller 2894c011-5a57-4514-9502-7280a8fbd2c0 0xc00460ada7 0xc00460ada8}] []  [{kube-controller-manager Update v1 2022-07-31 11:18:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2894c011-5a57-4514-9502-7280a8fbd2c0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-07-31 11:18:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.28.0.2\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sv5lk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sv5lk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:p1-eipeq63rfgo6ooocmu3kqk3tcc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:18:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:18:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:18:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:18:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.27.21.69,PodIP:172.28.0.2,StartTime:2022-07-31 11:18:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-07-31 11:18:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:docker://e8a30d4a94a451f469d59ffcb3e1e9856f4bad813dd9cdb27dd6c98b76fc8216,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.28.0.2,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 11:18:21.303: INFO: Pod "test-cleanup-deployment-5dbdbf94dc-t84vd" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-5dbdbf94dc-t84vd test-cleanup-deployment-5dbdbf94dc- deployment-1793  ca98ad9c-8672-4467-a8a2-390266144783 19714 0 2022-07-31 11:18:21 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5dbdbf94dc] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-5dbdbf94dc d8b7ee1f-93d4-42c7-8704-78469cd0059d 0xc00460af77 0xc00460af78}] []  [{kube-controller-manager Update v1 2022-07-31 11:18:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d8b7ee1f-93d4-42c7-8704-78469cd0059d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cpk6z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cpk6z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:p1-ashfcfj4pzo6aemt1j3a9ah7ew,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:18:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:18:21.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1793" for this suite.

• [SLOW TEST:5.292 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":346,"completed":137,"skipped":2746,"failed":0}
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:18:21.344: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-6651
[It] Should recreate evicted statefulset [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-6651
STEP: Waiting until pod test-pod will start running in namespace statefulset-6651
STEP: Creating statefulset with conflicting port in namespace statefulset-6651
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6651
Jul 31 11:18:25.537: INFO: Observed stateful pod in namespace: statefulset-6651, name: ss-0, uid: abe4f274-f2f4-400b-a2e1-8358db2a8e6f, status phase: Pending. Waiting for statefulset controller to delete.
Jul 31 11:18:25.560: INFO: Observed stateful pod in namespace: statefulset-6651, name: ss-0, uid: abe4f274-f2f4-400b-a2e1-8358db2a8e6f, status phase: Failed. Waiting for statefulset controller to delete.
Jul 31 11:18:25.608: INFO: Observed stateful pod in namespace: statefulset-6651, name: ss-0, uid: abe4f274-f2f4-400b-a2e1-8358db2a8e6f, status phase: Failed. Waiting for statefulset controller to delete.
Jul 31 11:18:25.617: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6651
STEP: Removing pod with conflicting port in namespace statefulset-6651
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6651 and will be in running state
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Jul 31 11:18:29.734: INFO: Deleting all statefulset in ns statefulset-6651
Jul 31 11:18:29.738: INFO: Scaling statefulset ss to 0
Jul 31 11:18:39.789: INFO: Waiting for statefulset status.replicas updated to 0
Jul 31 11:18:39.796: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:18:39.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6651" for this suite.

• [SLOW TEST:18.500 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    Should recreate evicted statefulset [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","total":346,"completed":138,"skipped":2747,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:18:39.847: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:18:40.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3047" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":346,"completed":139,"skipped":2782,"failed":0}

------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:18:40.031: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-38
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-38
STEP: creating replication controller externalsvc in namespace services-38
I0731 11:18:40.237732      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-38, replica count: 2
I0731 11:18:43.288582      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Jul 31 11:18:43.356: INFO: Creating new exec pod
Jul 31 11:18:45.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-38 exec execpodpf9r6 -- /bin/sh -x -c nslookup clusterip-service.services-38.svc.cluster.local'
Jul 31 11:18:45.841: INFO: stderr: "+ nslookup clusterip-service.services-38.svc.cluster.local\n"
Jul 31 11:18:45.841: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-38.svc.cluster.local\tcanonical name = externalsvc.services-38.svc.cluster.local.\nName:\texternalsvc.services-38.svc.cluster.local\nAddress: 10.97.248.12\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-38, will wait for the garbage collector to delete the pods
Jul 31 11:18:45.922: INFO: Deleting ReplicationController externalsvc took: 22.922436ms
Jul 31 11:18:46.022: INFO: Terminating ReplicationController externalsvc pods took: 100.154995ms
Jul 31 11:18:48.470: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:18:48.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-38" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:8.524 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":346,"completed":140,"skipped":2782,"failed":0}
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:18:48.556: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod pod-subpath-test-secret-h6wj
STEP: Creating a pod to test atomic-volume-subpath
Jul 31 11:18:48.664: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-h6wj" in namespace "subpath-6643" to be "Succeeded or Failed"
Jul 31 11:18:48.686: INFO: Pod "pod-subpath-test-secret-h6wj": Phase="Pending", Reason="", readiness=false. Elapsed: 21.946659ms
Jul 31 11:18:50.700: INFO: Pod "pod-subpath-test-secret-h6wj": Phase="Running", Reason="", readiness=true. Elapsed: 2.035944112s
Jul 31 11:18:52.721: INFO: Pod "pod-subpath-test-secret-h6wj": Phase="Running", Reason="", readiness=true. Elapsed: 4.057375322s
Jul 31 11:18:54.739: INFO: Pod "pod-subpath-test-secret-h6wj": Phase="Running", Reason="", readiness=true. Elapsed: 6.074679282s
Jul 31 11:18:56.752: INFO: Pod "pod-subpath-test-secret-h6wj": Phase="Running", Reason="", readiness=true. Elapsed: 8.087616351s
Jul 31 11:18:58.767: INFO: Pod "pod-subpath-test-secret-h6wj": Phase="Running", Reason="", readiness=true. Elapsed: 10.103234438s
Jul 31 11:19:00.780: INFO: Pod "pod-subpath-test-secret-h6wj": Phase="Running", Reason="", readiness=true. Elapsed: 12.116197634s
Jul 31 11:19:02.792: INFO: Pod "pod-subpath-test-secret-h6wj": Phase="Running", Reason="", readiness=true. Elapsed: 14.128556053s
Jul 31 11:19:04.806: INFO: Pod "pod-subpath-test-secret-h6wj": Phase="Running", Reason="", readiness=true. Elapsed: 16.14212438s
Jul 31 11:19:06.818: INFO: Pod "pod-subpath-test-secret-h6wj": Phase="Running", Reason="", readiness=true. Elapsed: 18.154388592s
Jul 31 11:19:08.835: INFO: Pod "pod-subpath-test-secret-h6wj": Phase="Running", Reason="", readiness=true. Elapsed: 20.17088619s
Jul 31 11:19:10.849: INFO: Pod "pod-subpath-test-secret-h6wj": Phase="Running", Reason="", readiness=false. Elapsed: 22.185193998s
Jul 31 11:19:12.863: INFO: Pod "pod-subpath-test-secret-h6wj": Phase="Running", Reason="", readiness=false. Elapsed: 24.199346021s
Jul 31 11:19:14.879: INFO: Pod "pod-subpath-test-secret-h6wj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.215243234s
STEP: Saw pod success
Jul 31 11:19:14.879: INFO: Pod "pod-subpath-test-secret-h6wj" satisfied condition "Succeeded or Failed"
Jul 31 11:19:14.885: INFO: Trying to get logs from node p1-ashfcfj4pzo6aemt1j3a9ah7ew pod pod-subpath-test-secret-h6wj container test-container-subpath-secret-h6wj: <nil>
STEP: delete the pod
Jul 31 11:19:14.951: INFO: Waiting for pod pod-subpath-test-secret-h6wj to disappear
Jul 31 11:19:14.968: INFO: Pod pod-subpath-test-secret-h6wj no longer exists
STEP: Deleting pod pod-subpath-test-secret-h6wj
Jul 31 11:19:14.968: INFO: Deleting pod "pod-subpath-test-secret-h6wj" in namespace "subpath-6643"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:19:14.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6643" for this suite.

• [SLOW TEST:26.439 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Excluded:WindowsDocker] [Conformance]","total":346,"completed":141,"skipped":2785,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:19:14.996: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:19:27.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9058" for this suite.

• [SLOW TEST:12.144 seconds]
[sig-apps] Job
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":346,"completed":142,"skipped":2803,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:19:27.140: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should observe PodDisruptionBudget status updated [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Jul 31 11:19:29.408: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:19:31.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-8" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","total":346,"completed":143,"skipped":2835,"failed":0}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:19:31.441: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-546
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a new StatefulSet
Jul 31 11:19:31.547: INFO: Found 0 stateful pods, waiting for 3
Jul 31 11:19:41.568: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 31 11:19:41.568: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 31 11:19:41.568: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jul 31 11:19:41.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=statefulset-546 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 31 11:19:41.828: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 31 11:19:41.828: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 31 11:19:41.828: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 to k8s.gcr.io/e2e-test-images/httpd:2.4.39-2
Jul 31 11:19:51.895: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jul 31 11:20:01.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=statefulset-546 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 31 11:20:02.179: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 31 11:20:02.179: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 31 11:20:02.179: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 31 11:20:12.231: INFO: Waiting for StatefulSet statefulset-546/ss2 to complete update
STEP: Rolling back to a previous revision
Jul 31 11:20:22.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=statefulset-546 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 31 11:20:22.484: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 31 11:20:22.484: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 31 11:20:22.484: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 31 11:20:32.559: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jul 31 11:20:42.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=statefulset-546 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 31 11:20:42.812: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 31 11:20:42.812: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 31 11:20:42.812: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Jul 31 11:20:52.858: INFO: Deleting all statefulset in ns statefulset-546
Jul 31 11:20:52.862: INFO: Scaling statefulset ss2 to 0
Jul 31 11:21:02.905: INFO: Waiting for statefulset status.replicas updated to 0
Jul 31 11:21:02.911: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:21:02.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-546" for this suite.

• [SLOW TEST:91.521 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":346,"completed":144,"skipped":2854,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:21:02.963: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Jul 31 11:21:03.082: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-8211  e078d139-9a3d-41fc-8074-49f0a2b05e12 20993 0 2022-07-31 11:21:03 +0000 UTC <nil> <nil> map[] map[] [] []  [{e2e.test Update v1 2022-07-31 11:21:03 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8cwp8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.39,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8cwp8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 11:21:03.100: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jul 31 11:21:05.115: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Jul 31 11:21:05.115: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-8211 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 31 11:21:05.115: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
Jul 31 11:21:05.115: INFO: ExecWithOptions: Clientset creation
Jul 31 11:21:05.116: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-8211/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
STEP: Verifying customized DNS server is configured on pod...
Jul 31 11:21:05.282: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-8211 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 31 11:21:05.282: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
Jul 31 11:21:05.283: INFO: ExecWithOptions: Clientset creation
Jul 31 11:21:05.283: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-8211/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Jul 31 11:21:05.442: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:21:05.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8211" for this suite.
•{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":346,"completed":145,"skipped":2866,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:21:05.539: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name projected-secret-test-43f84fe8-7aa5-49af-8820-06dfef7b625f
STEP: Creating a pod to test consume secrets
Jul 31 11:21:05.662: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7442a0db-6a8e-41df-82e2-c6887720989b" in namespace "projected-9574" to be "Succeeded or Failed"
Jul 31 11:21:05.693: INFO: Pod "pod-projected-secrets-7442a0db-6a8e-41df-82e2-c6887720989b": Phase="Pending", Reason="", readiness=false. Elapsed: 30.5431ms
Jul 31 11:21:07.703: INFO: Pod "pod-projected-secrets-7442a0db-6a8e-41df-82e2-c6887720989b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041486326s
Jul 31 11:21:09.716: INFO: Pod "pod-projected-secrets-7442a0db-6a8e-41df-82e2-c6887720989b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053896929s
Jul 31 11:21:11.731: INFO: Pod "pod-projected-secrets-7442a0db-6a8e-41df-82e2-c6887720989b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.068801241s
STEP: Saw pod success
Jul 31 11:21:11.731: INFO: Pod "pod-projected-secrets-7442a0db-6a8e-41df-82e2-c6887720989b" satisfied condition "Succeeded or Failed"
Jul 31 11:21:11.735: INFO: Trying to get logs from node p1-eipeq63rfgo6ooocmu3kqk3tcc pod pod-projected-secrets-7442a0db-6a8e-41df-82e2-c6887720989b container secret-volume-test: <nil>
STEP: delete the pod
Jul 31 11:21:11.797: INFO: Waiting for pod pod-projected-secrets-7442a0db-6a8e-41df-82e2-c6887720989b to disappear
Jul 31 11:21:11.804: INFO: Pod pod-projected-secrets-7442a0db-6a8e-41df-82e2-c6887720989b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:21:11.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9574" for this suite.

• [SLOW TEST:6.302 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":146,"skipped":2900,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:21:11.842: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-ef7a7bc8-d104-4240-b37b-006ea944baa2
STEP: Creating a pod to test consume configMaps
Jul 31 11:21:11.913: INFO: Waiting up to 5m0s for pod "pod-configmaps-c9276126-512b-4bd3-bde9-b4bcb82e2d75" in namespace "configmap-995" to be "Succeeded or Failed"
Jul 31 11:21:11.932: INFO: Pod "pod-configmaps-c9276126-512b-4bd3-bde9-b4bcb82e2d75": Phase="Pending", Reason="", readiness=false. Elapsed: 19.030317ms
Jul 31 11:21:13.945: INFO: Pod "pod-configmaps-c9276126-512b-4bd3-bde9-b4bcb82e2d75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03206512s
Jul 31 11:21:15.962: INFO: Pod "pod-configmaps-c9276126-512b-4bd3-bde9-b4bcb82e2d75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04935726s
STEP: Saw pod success
Jul 31 11:21:15.962: INFO: Pod "pod-configmaps-c9276126-512b-4bd3-bde9-b4bcb82e2d75" satisfied condition "Succeeded or Failed"
Jul 31 11:21:15.967: INFO: Trying to get logs from node p1-nc3cz44465xd41a6poxnmrpeqo pod pod-configmaps-c9276126-512b-4bd3-bde9-b4bcb82e2d75 container agnhost-container: <nil>
STEP: delete the pod
Jul 31 11:21:16.032: INFO: Waiting for pod pod-configmaps-c9276126-512b-4bd3-bde9-b4bcb82e2d75 to disappear
Jul 31 11:21:16.047: INFO: Pod pod-configmaps-c9276126-512b-4bd3-bde9-b4bcb82e2d75 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:21:16.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-995" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":147,"skipped":2929,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:21:16.072: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Jul 31 11:21:16.158: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 31 11:22:16.215: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 11:22:16.221: INFO: Starting informer...
STEP: Starting pods...
Jul 31 11:22:16.475: INFO: Pod1 is running on p1-nc3cz44465xd41a6poxnmrpeqo. Tainting Node
Jul 31 11:22:20.722: INFO: Pod2 is running on p1-nc3cz44465xd41a6poxnmrpeqo. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Jul 31 11:22:27.177: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jul 31 11:22:47.495: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:22:47.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-9832" for this suite.

• [SLOW TEST:91.488 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":346,"completed":148,"skipped":2961,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:22:47.560: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Given a Pod with a 'name' label pod-adoption is created
Jul 31 11:22:47.672: INFO: The status of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jul 31 11:22:49.685: INFO: The status of Pod pod-adoption is Running (Ready = true)
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:22:50.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5212" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":346,"completed":149,"skipped":2995,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:22:50.744: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating the pod
Jul 31 11:22:50.844: INFO: The status of Pod annotationupdatea803020b-c9b9-4d4a-8aee-cd63d4ac3e73 is Pending, waiting for it to be Running (with Ready = true)
Jul 31 11:22:52.856: INFO: The status of Pod annotationupdatea803020b-c9b9-4d4a-8aee-cd63d4ac3e73 is Pending, waiting for it to be Running (with Ready = true)
Jul 31 11:22:54.858: INFO: The status of Pod annotationupdatea803020b-c9b9-4d4a-8aee-cd63d4ac3e73 is Running (Ready = true)
Jul 31 11:22:55.415: INFO: Successfully updated pod "annotationupdatea803020b-c9b9-4d4a-8aee-cd63d4ac3e73"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:22:57.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7603" for this suite.

• [SLOW TEST:6.719 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":150,"skipped":3050,"failed":0}
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:22:57.463: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name s-test-opt-del-d1c17cdc-afff-4b1a-b254-8501a8188e6c
STEP: Creating secret with name s-test-opt-upd-4bd48ac2-a54a-45c6-b741-de0137c68c0b
STEP: Creating the pod
Jul 31 11:22:57.624: INFO: The status of Pod pod-projected-secrets-2b6c6d13-4ba2-48cc-bbb7-6fd999c77a8a is Pending, waiting for it to be Running (with Ready = true)
Jul 31 11:22:59.844: INFO: The status of Pod pod-projected-secrets-2b6c6d13-4ba2-48cc-bbb7-6fd999c77a8a is Pending, waiting for it to be Running (with Ready = true)
Jul 31 11:23:01.639: INFO: The status of Pod pod-projected-secrets-2b6c6d13-4ba2-48cc-bbb7-6fd999c77a8a is Running (Ready = true)
STEP: Deleting secret s-test-opt-del-d1c17cdc-afff-4b1a-b254-8501a8188e6c
STEP: Updating secret s-test-opt-upd-4bd48ac2-a54a-45c6-b741-de0137c68c0b
STEP: Creating secret with name s-test-opt-create-2fa58e7e-11d5-4c18-ab31-6c892af72591
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:23:03.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2140" for this suite.

• [SLOW TEST:6.361 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":151,"skipped":3050,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:23:03.825: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 11:23:03.875: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jul 31 11:23:06.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=crd-publish-openapi-405 --namespace=crd-publish-openapi-405 create -f -'
Jul 31 11:23:07.928: INFO: stderr: ""
Jul 31 11:23:07.928: INFO: stdout: "e2e-test-crd-publish-openapi-1147-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jul 31 11:23:07.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=crd-publish-openapi-405 --namespace=crd-publish-openapi-405 delete e2e-test-crd-publish-openapi-1147-crds test-cr'
Jul 31 11:23:08.047: INFO: stderr: ""
Jul 31 11:23:08.047: INFO: stdout: "e2e-test-crd-publish-openapi-1147-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jul 31 11:23:08.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=crd-publish-openapi-405 --namespace=crd-publish-openapi-405 apply -f -'
Jul 31 11:23:09.286: INFO: stderr: ""
Jul 31 11:23:09.286: INFO: stdout: "e2e-test-crd-publish-openapi-1147-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jul 31 11:23:09.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=crd-publish-openapi-405 --namespace=crd-publish-openapi-405 delete e2e-test-crd-publish-openapi-1147-crds test-cr'
Jul 31 11:23:09.372: INFO: stderr: ""
Jul 31 11:23:09.372: INFO: stdout: "e2e-test-crd-publish-openapi-1147-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Jul 31 11:23:09.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=crd-publish-openapi-405 explain e2e-test-crd-publish-openapi-1147-crds'
Jul 31 11:23:10.090: INFO: stderr: ""
Jul 31 11:23:10.090: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1147-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:23:12.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-405" for this suite.

• [SLOW TEST:9.208 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":346,"completed":152,"skipped":3093,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:23:13.033: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Jul 31 11:23:53.324: INFO: The status of Pod kube-controller-manager-master-um8faxf3d4468jbdm49zh4o57y is Running (Ready = true)
Jul 31 11:23:53.503: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jul 31 11:23:53.503: INFO: Deleting pod "simpletest.rc-24cng" in namespace "gc-8500"
Jul 31 11:23:53.552: INFO: Deleting pod "simpletest.rc-2cxcp" in namespace "gc-8500"
Jul 31 11:23:53.580: INFO: Deleting pod "simpletest.rc-2tw5r" in namespace "gc-8500"
Jul 31 11:23:53.612: INFO: Deleting pod "simpletest.rc-49knw" in namespace "gc-8500"
Jul 31 11:23:53.673: INFO: Deleting pod "simpletest.rc-4bmw6" in namespace "gc-8500"
Jul 31 11:23:53.705: INFO: Deleting pod "simpletest.rc-4hmvz" in namespace "gc-8500"
Jul 31 11:23:53.766: INFO: Deleting pod "simpletest.rc-5bsg2" in namespace "gc-8500"
Jul 31 11:23:53.814: INFO: Deleting pod "simpletest.rc-5sjmc" in namespace "gc-8500"
Jul 31 11:23:53.882: INFO: Deleting pod "simpletest.rc-5vp4t" in namespace "gc-8500"
Jul 31 11:23:53.926: INFO: Deleting pod "simpletest.rc-64kvb" in namespace "gc-8500"
Jul 31 11:23:54.234: INFO: Deleting pod "simpletest.rc-67fn6" in namespace "gc-8500"
Jul 31 11:23:54.289: INFO: Deleting pod "simpletest.rc-6nmp4" in namespace "gc-8500"
Jul 31 11:23:54.341: INFO: Deleting pod "simpletest.rc-78wmb" in namespace "gc-8500"
Jul 31 11:23:54.383: INFO: Deleting pod "simpletest.rc-7d6r7" in namespace "gc-8500"
Jul 31 11:23:54.457: INFO: Deleting pod "simpletest.rc-7ghrf" in namespace "gc-8500"
Jul 31 11:23:54.611: INFO: Deleting pod "simpletest.rc-7j7mq" in namespace "gc-8500"
Jul 31 11:23:54.692: INFO: Deleting pod "simpletest.rc-8c95p" in namespace "gc-8500"
Jul 31 11:23:54.760: INFO: Deleting pod "simpletest.rc-8gz7k" in namespace "gc-8500"
Jul 31 11:23:54.811: INFO: Deleting pod "simpletest.rc-8rc8b" in namespace "gc-8500"
Jul 31 11:23:54.861: INFO: Deleting pod "simpletest.rc-8rwwq" in namespace "gc-8500"
Jul 31 11:23:55.138: INFO: Deleting pod "simpletest.rc-8whft" in namespace "gc-8500"
Jul 31 11:23:55.176: INFO: Deleting pod "simpletest.rc-8zp96" in namespace "gc-8500"
Jul 31 11:23:55.219: INFO: Deleting pod "simpletest.rc-9c2xm" in namespace "gc-8500"
Jul 31 11:23:55.309: INFO: Deleting pod "simpletest.rc-9cch6" in namespace "gc-8500"
Jul 31 11:23:55.374: INFO: Deleting pod "simpletest.rc-9t7km" in namespace "gc-8500"
Jul 31 11:23:55.458: INFO: Deleting pod "simpletest.rc-bj6m6" in namespace "gc-8500"
Jul 31 11:23:55.578: INFO: Deleting pod "simpletest.rc-bkt6n" in namespace "gc-8500"
Jul 31 11:23:55.614: INFO: Deleting pod "simpletest.rc-bl9cw" in namespace "gc-8500"
Jul 31 11:23:55.714: INFO: Deleting pod "simpletest.rc-brx8d" in namespace "gc-8500"
Jul 31 11:23:55.775: INFO: Deleting pod "simpletest.rc-bs6tw" in namespace "gc-8500"
Jul 31 11:23:55.857: INFO: Deleting pod "simpletest.rc-bvtr6" in namespace "gc-8500"
Jul 31 11:23:55.972: INFO: Deleting pod "simpletest.rc-cfnnd" in namespace "gc-8500"
Jul 31 11:23:56.074: INFO: Deleting pod "simpletest.rc-cgj8j" in namespace "gc-8500"
Jul 31 11:23:56.105: INFO: Deleting pod "simpletest.rc-czbkj" in namespace "gc-8500"
Jul 31 11:23:56.206: INFO: Deleting pod "simpletest.rc-d7wgd" in namespace "gc-8500"
Jul 31 11:23:56.244: INFO: Deleting pod "simpletest.rc-f6rnw" in namespace "gc-8500"
Jul 31 11:23:56.322: INFO: Deleting pod "simpletest.rc-f72dq" in namespace "gc-8500"
Jul 31 11:23:56.397: INFO: Deleting pod "simpletest.rc-fgf6l" in namespace "gc-8500"
Jul 31 11:23:56.477: INFO: Deleting pod "simpletest.rc-fwnlj" in namespace "gc-8500"
Jul 31 11:23:56.596: INFO: Deleting pod "simpletest.rc-fx42k" in namespace "gc-8500"
Jul 31 11:23:56.658: INFO: Deleting pod "simpletest.rc-g48dt" in namespace "gc-8500"
Jul 31 11:23:56.724: INFO: Deleting pod "simpletest.rc-g4t4l" in namespace "gc-8500"
Jul 31 11:23:56.803: INFO: Deleting pod "simpletest.rc-g4vtz" in namespace "gc-8500"
Jul 31 11:23:56.861: INFO: Deleting pod "simpletest.rc-g7p52" in namespace "gc-8500"
Jul 31 11:23:56.980: INFO: Deleting pod "simpletest.rc-gbttp" in namespace "gc-8500"
Jul 31 11:23:57.065: INFO: Deleting pod "simpletest.rc-gvtwm" in namespace "gc-8500"
Jul 31 11:23:57.158: INFO: Deleting pod "simpletest.rc-h4tsg" in namespace "gc-8500"
Jul 31 11:23:57.211: INFO: Deleting pod "simpletest.rc-h695f" in namespace "gc-8500"
Jul 31 11:23:57.475: INFO: Deleting pod "simpletest.rc-h9ttg" in namespace "gc-8500"
Jul 31 11:23:57.505: INFO: Deleting pod "simpletest.rc-hfzcx" in namespace "gc-8500"
Jul 31 11:23:57.569: INFO: Deleting pod "simpletest.rc-hkml9" in namespace "gc-8500"
Jul 31 11:23:57.687: INFO: Deleting pod "simpletest.rc-hkmq7" in namespace "gc-8500"
Jul 31 11:23:57.744: INFO: Deleting pod "simpletest.rc-hspkh" in namespace "gc-8500"
Jul 31 11:23:57.830: INFO: Deleting pod "simpletest.rc-jjfwn" in namespace "gc-8500"
Jul 31 11:23:57.875: INFO: Deleting pod "simpletest.rc-jtgnn" in namespace "gc-8500"
Jul 31 11:23:57.974: INFO: Deleting pod "simpletest.rc-k4l5j" in namespace "gc-8500"
Jul 31 11:23:58.084: INFO: Deleting pod "simpletest.rc-kdj2l" in namespace "gc-8500"
Jul 31 11:23:58.180: INFO: Deleting pod "simpletest.rc-khrm8" in namespace "gc-8500"
Jul 31 11:23:58.242: INFO: Deleting pod "simpletest.rc-kjkqk" in namespace "gc-8500"
Jul 31 11:23:58.300: INFO: Deleting pod "simpletest.rc-kjx4s" in namespace "gc-8500"
Jul 31 11:23:58.364: INFO: Deleting pod "simpletest.rc-klp4n" in namespace "gc-8500"
Jul 31 11:23:58.418: INFO: Deleting pod "simpletest.rc-lpsks" in namespace "gc-8500"
Jul 31 11:23:58.494: INFO: Deleting pod "simpletest.rc-mfzqn" in namespace "gc-8500"
Jul 31 11:23:58.598: INFO: Deleting pod "simpletest.rc-mjb4d" in namespace "gc-8500"
Jul 31 11:23:58.652: INFO: Deleting pod "simpletest.rc-mjg5n" in namespace "gc-8500"
Jul 31 11:23:58.705: INFO: Deleting pod "simpletest.rc-n79fv" in namespace "gc-8500"
Jul 31 11:23:58.766: INFO: Deleting pod "simpletest.rc-nj42z" in namespace "gc-8500"
Jul 31 11:23:58.802: INFO: Deleting pod "simpletest.rc-nj4bj" in namespace "gc-8500"
Jul 31 11:23:58.878: INFO: Deleting pod "simpletest.rc-njngf" in namespace "gc-8500"
Jul 31 11:23:58.911: INFO: Deleting pod "simpletest.rc-nwwcd" in namespace "gc-8500"
Jul 31 11:23:58.970: INFO: Deleting pod "simpletest.rc-pnw6j" in namespace "gc-8500"
Jul 31 11:23:59.037: INFO: Deleting pod "simpletest.rc-pps54" in namespace "gc-8500"
Jul 31 11:23:59.098: INFO: Deleting pod "simpletest.rc-pqrkb" in namespace "gc-8500"
Jul 31 11:23:59.163: INFO: Deleting pod "simpletest.rc-q4mnn" in namespace "gc-8500"
Jul 31 11:23:59.240: INFO: Deleting pod "simpletest.rc-qdn9t" in namespace "gc-8500"
Jul 31 11:23:59.292: INFO: Deleting pod "simpletest.rc-r2mnt" in namespace "gc-8500"
Jul 31 11:23:59.336: INFO: Deleting pod "simpletest.rc-r5clm" in namespace "gc-8500"
Jul 31 11:23:59.388: INFO: Deleting pod "simpletest.rc-r7jls" in namespace "gc-8500"
Jul 31 11:23:59.552: INFO: Deleting pod "simpletest.rc-rb2r2" in namespace "gc-8500"
Jul 31 11:23:59.629: INFO: Deleting pod "simpletest.rc-rhq2q" in namespace "gc-8500"
Jul 31 11:23:59.729: INFO: Deleting pod "simpletest.rc-shrss" in namespace "gc-8500"
Jul 31 11:23:59.835: INFO: Deleting pod "simpletest.rc-t92ql" in namespace "gc-8500"
Jul 31 11:23:59.927: INFO: Deleting pod "simpletest.rc-tg2sr" in namespace "gc-8500"
Jul 31 11:23:59.988: INFO: Deleting pod "simpletest.rc-tpf8w" in namespace "gc-8500"
Jul 31 11:24:00.061: INFO: Deleting pod "simpletest.rc-v2j44" in namespace "gc-8500"
Jul 31 11:24:00.115: INFO: Deleting pod "simpletest.rc-vgxfg" in namespace "gc-8500"
Jul 31 11:24:00.198: INFO: Deleting pod "simpletest.rc-vn5s4" in namespace "gc-8500"
Jul 31 11:24:00.268: INFO: Deleting pod "simpletest.rc-vxczh" in namespace "gc-8500"
Jul 31 11:24:00.324: INFO: Deleting pod "simpletest.rc-wwjrl" in namespace "gc-8500"
Jul 31 11:24:00.428: INFO: Deleting pod "simpletest.rc-wwjvq" in namespace "gc-8500"
Jul 31 11:24:00.542: INFO: Deleting pod "simpletest.rc-wzs2b" in namespace "gc-8500"
Jul 31 11:24:00.599: INFO: Deleting pod "simpletest.rc-x2w7c" in namespace "gc-8500"
Jul 31 11:24:00.658: INFO: Deleting pod "simpletest.rc-x4tcq" in namespace "gc-8500"
Jul 31 11:24:00.714: INFO: Deleting pod "simpletest.rc-x6hvf" in namespace "gc-8500"
Jul 31 11:24:00.808: INFO: Deleting pod "simpletest.rc-xhkz5" in namespace "gc-8500"
Jul 31 11:24:00.903: INFO: Deleting pod "simpletest.rc-xhnd4" in namespace "gc-8500"
Jul 31 11:24:00.967: INFO: Deleting pod "simpletest.rc-xmjlc" in namespace "gc-8500"
Jul 31 11:24:01.011: INFO: Deleting pod "simpletest.rc-zg7hb" in namespace "gc-8500"
Jul 31 11:24:01.053: INFO: Deleting pod "simpletest.rc-zljft" in namespace "gc-8500"
Jul 31 11:24:01.110: INFO: Deleting pod "simpletest.rc-zwkn2" in namespace "gc-8500"
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:24:01.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8500" for this suite.

• [SLOW TEST:48.156 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":346,"completed":153,"skipped":3100,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:24:01.190: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create deployment with httpd image
Jul 31 11:24:01.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-2214 create -f -'
Jul 31 11:24:02.145: INFO: stderr: ""
Jul 31 11:24:02.145: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Jul 31 11:24:02.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-2214 diff -f -'
Jul 31 11:24:02.847: INFO: rc: 1
Jul 31 11:24:02.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-2214 delete -f -'
Jul 31 11:24:02.924: INFO: stderr: ""
Jul 31 11:24:02.924: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:24:02.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2214" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":346,"completed":154,"skipped":3112,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:24:02.955: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Jul 31 11:24:03.009: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
Jul 31 11:24:08.380: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:24:23.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8753" for this suite.

• [SLOW TEST:20.429 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":346,"completed":155,"skipped":3133,"failed":0}
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:24:23.384: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul 31 11:24:23.521: INFO: Waiting up to 5m0s for pod "pod-810d0898-8ff2-40b8-9707-ae17bb9ef821" in namespace "emptydir-9800" to be "Succeeded or Failed"
Jul 31 11:24:23.542: INFO: Pod "pod-810d0898-8ff2-40b8-9707-ae17bb9ef821": Phase="Pending", Reason="", readiness=false. Elapsed: 20.77993ms
Jul 31 11:24:25.555: INFO: Pod "pod-810d0898-8ff2-40b8-9707-ae17bb9ef821": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033479298s
Jul 31 11:24:27.572: INFO: Pod "pod-810d0898-8ff2-40b8-9707-ae17bb9ef821": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051070618s
STEP: Saw pod success
Jul 31 11:24:27.572: INFO: Pod "pod-810d0898-8ff2-40b8-9707-ae17bb9ef821" satisfied condition "Succeeded or Failed"
Jul 31 11:24:27.577: INFO: Trying to get logs from node p1-ashfcfj4pzo6aemt1j3a9ah7ew pod pod-810d0898-8ff2-40b8-9707-ae17bb9ef821 container test-container: <nil>
STEP: delete the pod
Jul 31 11:24:27.646: INFO: Waiting for pod pod-810d0898-8ff2-40b8-9707-ae17bb9ef821 to disappear
Jul 31 11:24:27.650: INFO: Pod pod-810d0898-8ff2-40b8-9707-ae17bb9ef821 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:24:27.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9800" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":156,"skipped":3140,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:24:27.676: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename ingress
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jul 31 11:24:27.827: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Jul 31 11:24:27.833: INFO: starting watch
STEP: patching
STEP: updating
Jul 31 11:24:27.877: INFO: waiting for watch events with expected annotations
Jul 31 11:24:27.877: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:24:27.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-3555" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":346,"completed":157,"skipped":3174,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:24:27.991: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Jul 31 11:24:29.381: INFO: The status of Pod kube-controller-manager-master-um8faxf3d4468jbdm49zh4o57y is Running (Ready = true)
Jul 31 11:24:29.483: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:24:29.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4642" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":346,"completed":158,"skipped":3208,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:24:29.512: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 31 11:24:30.622: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jul 31 11:24:32.644: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.July, 31, 11, 24, 30, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 11, 24, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.July, 31, 11, 24, 30, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 11, 24, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6c69dbd86b\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 31 11:24:35.713: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 11:24:35.722: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:24:38.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7610" for this suite.
STEP: Destroying namespace "webhook-7610-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.617 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":346,"completed":159,"skipped":3222,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:24:39.129: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:59
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod liveness-44ad525f-b88a-4a38-8738-252214ca8f61 in namespace container-probe-9941
Jul 31 11:24:41.231: INFO: Started pod liveness-44ad525f-b88a-4a38-8738-252214ca8f61 in namespace container-probe-9941
STEP: checking the pod's current state and verifying that restartCount is present
Jul 31 11:24:41.236: INFO: Initial restart count of pod liveness-44ad525f-b88a-4a38-8738-252214ca8f61 is 0
Jul 31 11:25:01.415: INFO: Restart count of pod container-probe-9941/liveness-44ad525f-b88a-4a38-8738-252214ca8f61 is now 1 (20.1782167s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:25:01.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9941" for this suite.

• [SLOW TEST:22.371 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":346,"completed":160,"skipped":3233,"failed":0}
SSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:25:01.503: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:25:01.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6913" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":346,"completed":161,"skipped":3243,"failed":0}
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:25:01.673: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should list and delete a collection of DaemonSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul 31 11:25:01.879: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:25:01.879: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:25:01.879: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:25:01.914: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 31 11:25:01.914: INFO: Node p1-ashfcfj4pzo6aemt1j3a9ah7ew is running 0 daemon pod, expected 1
Jul 31 11:25:02.930: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:25:02.930: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:25:02.930: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:25:02.936: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 31 11:25:02.937: INFO: Node p1-ashfcfj4pzo6aemt1j3a9ah7ew is running 0 daemon pod, expected 1
Jul 31 11:25:03.926: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:25:03.926: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:25:03.926: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:25:03.932: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jul 31 11:25:03.932: INFO: Node p1-eipeq63rfgo6ooocmu3kqk3tcc is running 0 daemon pod, expected 1
Jul 31 11:25:04.926: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:25:04.927: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:25:04.927: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:25:04.937: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Jul 31 11:25:04.937: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
STEP: listing all DeamonSets
STEP: DeleteCollection of the DaemonSets
STEP: Verify that ReplicaSets have been deleted
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
Jul 31 11:25:05.024: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"24352"},"items":null}

Jul 31 11:25:05.034: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"24353"},"items":[{"metadata":{"name":"daemon-set-2q7tg","generateName":"daemon-set-","namespace":"daemonsets-3394","uid":"66cda074-68e9-48eb-8d3c-8762d86568fb","resourceVersion":"24348","creationTimestamp":"2022-07-31T11:25:01Z","deletionTimestamp":"2022-07-31T11:25:34Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"5b46c58f6f","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"a4229332-8d47-40d5-b7e7-0779f599bcb6","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-07-31T11:25:01Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a4229332-8d47-40d5-b7e7-0779f599bcb6\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-07-31T11:25:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.28.0.2\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-kzdtn","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-kzdtn","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"Always","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"p1-eipeq63rfgo6ooocmu3kqk3tcc","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["p1-eipeq63rfgo6ooocmu3kqk3tcc"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-07-31T11:25:01Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-07-31T11:25:04Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-07-31T11:25:04Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-07-31T11:25:01Z"}],"hostIP":"172.27.21.69","podIP":"172.28.0.2","podIPs":[{"ip":"172.28.0.2"}],"startTime":"2022-07-31T11:25:01Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-07-31T11:25:03Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"docker://efe4cffbb7030751cf6d12d188a26f4c8a96be58f69ebad13ee12b3891bbf9e9","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-5wzqx","generateName":"daemon-set-","namespace":"daemonsets-3394","uid":"9cf3a660-7484-4d03-b72a-2d7ab9da40d5","resourceVersion":"24350","creationTimestamp":"2022-07-31T11:25:01Z","deletionTimestamp":"2022-07-31T11:25:34Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"5b46c58f6f","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"a4229332-8d47-40d5-b7e7-0779f599bcb6","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-07-31T11:25:01Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a4229332-8d47-40d5-b7e7-0779f599bcb6\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-07-31T11:25:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.28.176.2\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-hnzlf","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-hnzlf","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"Always","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"p1-esqoe7n9eqq76k8ojd9tajh51e","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["p1-esqoe7n9eqq76k8ojd9tajh51e"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-07-31T11:25:02Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-07-31T11:25:04Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-07-31T11:25:04Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-07-31T11:25:01Z"}],"hostIP":"172.27.21.77","podIP":"172.28.176.2","podIPs":[{"ip":"172.28.176.2"}],"startTime":"2022-07-31T11:25:02Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-07-31T11:25:03Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"docker://694cfa6a0410708efbff8b13c6bb2ead9e6be77d4d1559db7ff19fc53eba9f59","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-9jw27","generateName":"daemon-set-","namespace":"daemonsets-3394","uid":"c51b820d-82d3-4071-8578-ffd216070351","resourceVersion":"24351","creationTimestamp":"2022-07-31T11:25:01Z","deletionTimestamp":"2022-07-31T11:25:34Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"5b46c58f6f","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"a4229332-8d47-40d5-b7e7-0779f599bcb6","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-07-31T11:25:01Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a4229332-8d47-40d5-b7e7-0779f599bcb6\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-07-31T11:25:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.28.64.1\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-gb5tc","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-gb5tc","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"Always","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"p1-nc3cz44465xd41a6poxnmrpeqo","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["p1-nc3cz44465xd41a6poxnmrpeqo"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-07-31T11:25:01Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-07-31T11:25:04Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-07-31T11:25:04Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-07-31T11:25:01Z"}],"hostIP":"172.27.21.79","podIP":"172.28.64.1","podIPs":[{"ip":"172.28.64.1"}],"startTime":"2022-07-31T11:25:01Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-07-31T11:25:04Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"docker://a5fa6feade3622297ca7a310a97f39e25a8558769f8aafad5ab8bdff7f4aaa27","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-fb9gt","generateName":"daemon-set-","namespace":"daemonsets-3394","uid":"98252e7e-1c8a-418c-a138-6e605ab531ad","resourceVersion":"24352","creationTimestamp":"2022-07-31T11:25:01Z","deletionTimestamp":"2022-07-31T11:25:34Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"5b46c58f6f","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"a4229332-8d47-40d5-b7e7-0779f599bcb6","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-07-31T11:25:01Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a4229332-8d47-40d5-b7e7-0779f599bcb6\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-07-31T11:25:03Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.28.80.1\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-6jlwv","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-6jlwv","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"Always","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"p1-ashfcfj4pzo6aemt1j3a9ah7ew","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["p1-ashfcfj4pzo6aemt1j3a9ah7ew"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-07-31T11:25:01Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-07-31T11:25:03Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-07-31T11:25:03Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-07-31T11:25:01Z"}],"hostIP":"172.27.21.73","podIP":"172.28.80.1","podIPs":[{"ip":"172.28.80.1"}],"startTime":"2022-07-31T11:25:01Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-07-31T11:25:03Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"docker://d05aa9aaf1873fbdd31a864d79faa3515ede8c8028d4f1f47c90e7035cf1a007","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-lv49r","generateName":"daemon-set-","namespace":"daemonsets-3394","uid":"0773f1e2-c9ad-48d7-9c97-6f2b3d4adbe7","resourceVersion":"24353","creationTimestamp":"2022-07-31T11:25:01Z","deletionTimestamp":"2022-07-31T11:25:34Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"5b46c58f6f","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"a4229332-8d47-40d5-b7e7-0779f599bcb6","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2022-07-31T11:25:01Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a4229332-8d47-40d5-b7e7-0779f599bcb6\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2022-07-31T11:25:03Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.28.96.2\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-ccns8","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-ccns8","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"Always","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"p1-mz5rqwt5oar4y4n97cfumgzrxe","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["p1-mz5rqwt5oar4y4n97cfumgzrxe"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-07-31T11:25:01Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-07-31T11:25:03Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-07-31T11:25:03Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2022-07-31T11:25:01Z"}],"hostIP":"172.27.21.72","podIP":"172.28.96.2","podIPs":[{"ip":"172.28.96.2"}],"startTime":"2022-07-31T11:25:01Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2022-07-31T11:25:03Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2","imageID":"docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"docker://ebdf84020d0b5aeae56eeb86b24a2db16e42a55967068b0df5eaf0be23488531","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:25:05.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3394" for this suite.
•{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","total":346,"completed":162,"skipped":3244,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:25:05.099: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a ReplaceConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring the job is replaced with a new one
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:27:01.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-3663" for this suite.

• [SLOW TEST:116.205 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","total":346,"completed":163,"skipped":3275,"failed":0}
SSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:27:01.304: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:46
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 11:27:01.431: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-de3bb4a3-4292-4fae-ac35-47e1f06861ac" in namespace "security-context-test-8985" to be "Succeeded or Failed"
Jul 31 11:27:01.445: INFO: Pod "busybox-readonly-false-de3bb4a3-4292-4fae-ac35-47e1f06861ac": Phase="Pending", Reason="", readiness=false. Elapsed: 13.328845ms
Jul 31 11:27:03.462: INFO: Pod "busybox-readonly-false-de3bb4a3-4292-4fae-ac35-47e1f06861ac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030492841s
Jul 31 11:27:05.473: INFO: Pod "busybox-readonly-false-de3bb4a3-4292-4fae-ac35-47e1f06861ac": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041514119s
Jul 31 11:27:07.484: INFO: Pod "busybox-readonly-false-de3bb4a3-4292-4fae-ac35-47e1f06861ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.052331966s
Jul 31 11:27:07.484: INFO: Pod "busybox-readonly-false-de3bb4a3-4292-4fae-ac35-47e1f06861ac" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:27:07.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8985" for this suite.

• [SLOW TEST:6.196 seconds]
[sig-node] Security Context
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/security_context.go:171
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":346,"completed":164,"skipped":3280,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:27:07.501: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 31 11:27:08.067: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jul 31 11:27:10.094: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.July, 31, 11, 27, 8, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 11, 27, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.July, 31, 11, 27, 8, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 11, 27, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6c69dbd86b\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 31 11:27:13.149: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Jul 31 11:27:17.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=webhook-4060 attach --namespace=webhook-4060 to-be-attached-pod -i -c=container1'
Jul 31 11:27:17.382: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:27:17.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4060" for this suite.
STEP: Destroying namespace "webhook-4060-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:10.041 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":346,"completed":165,"skipped":3287,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:27:17.542: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Jul 31 11:27:17.657: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
Jul 31 11:27:21.076: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:27:34.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7296" for this suite.

• [SLOW TEST:16.737 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":346,"completed":166,"skipped":3295,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController 
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:27:34.280: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pdb that targets all three pods in a test replica set
STEP: Waiting for the pdb to be processed
STEP: First trying to evict a pod which shouldn't be evictable
STEP: Waiting for all pods to be running
Jul 31 11:27:36.609: INFO: pods: 0 < 3
Jul 31 11:27:38.620: INFO: running pods: 1 < 3
STEP: locating a running pod
STEP: Updating the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
STEP: Waiting for the pdb to observed all healthy pods
STEP: Patching the pdb to disallow a pod to be evicted
STEP: Waiting for the pdb to be processed
STEP: Waiting for all pods to be running
Jul 31 11:27:44.807: INFO: running pods: 2 < 3
STEP: locating a running pod
STEP: Deleting the pdb to allow a pod to be evicted
STEP: Waiting for the pdb to be deleted
STEP: Trying to evict the same pod we tried earlier which should now be evictable
STEP: Waiting for all pods to be running
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:27:46.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1909" for this suite.

• [SLOW TEST:12.667 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","total":346,"completed":167,"skipped":3319,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context 
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:27:46.948: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename security-context
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser
Jul 31 11:27:47.269: INFO: Waiting up to 5m0s for pod "security-context-3663b34d-86c9-45eb-a0b1-24ef9e7192b5" in namespace "security-context-6566" to be "Succeeded or Failed"
Jul 31 11:27:47.298: INFO: Pod "security-context-3663b34d-86c9-45eb-a0b1-24ef9e7192b5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.747021ms
Jul 31 11:27:49.313: INFO: Pod "security-context-3663b34d-86c9-45eb-a0b1-24ef9e7192b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043252219s
Jul 31 11:27:51.326: INFO: Pod "security-context-3663b34d-86c9-45eb-a0b1-24ef9e7192b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.056641583s
STEP: Saw pod success
Jul 31 11:27:51.326: INFO: Pod "security-context-3663b34d-86c9-45eb-a0b1-24ef9e7192b5" satisfied condition "Succeeded or Failed"
Jul 31 11:27:51.333: INFO: Trying to get logs from node p1-ashfcfj4pzo6aemt1j3a9ah7ew pod security-context-3663b34d-86c9-45eb-a0b1-24ef9e7192b5 container test-container: <nil>
STEP: delete the pod
Jul 31 11:27:51.412: INFO: Waiting for pod security-context-3663b34d-86c9-45eb-a0b1-24ef9e7192b5 to disappear
Jul 31 11:27:51.417: INFO: Pod security-context-3663b34d-86c9-45eb-a0b1-24ef9e7192b5 no longer exists
[AfterEach] [sig-node] Security Context
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:27:51.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-6566" for this suite.
•{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","total":346,"completed":168,"skipped":3334,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:27:51.435: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a service externalname-service with the type=ExternalName in namespace services-7401
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-7401
I0731 11:27:51.621049      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-7401, replica count: 2
Jul 31 11:27:54.672: INFO: Creating new exec pod
I0731 11:27:54.672781      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 31 11:27:57.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-7401 exec execpod6bflz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jul 31 11:27:57.990: INFO: stderr: "+ + nc -vecho -t hostName -w\n 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jul 31 11:27:57.990: INFO: stdout: ""
Jul 31 11:27:58.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-7401 exec execpod6bflz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jul 31 11:27:59.208: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jul 31 11:27:59.208: INFO: stdout: ""
Jul 31 11:27:59.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-7401 exec execpod6bflz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jul 31 11:28:00.216: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jul 31 11:28:00.216: INFO: stdout: "externalname-service-2b8fg"
Jul 31 11:28:00.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-7401 exec execpod6bflz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.107.241.103 80'
Jul 31 11:28:00.453: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.107.241.103 80\nConnection to 10.107.241.103 80 port [tcp/http] succeeded!\n"
Jul 31 11:28:00.453: INFO: stdout: "externalname-service-gwpsm"
Jul 31 11:28:00.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-7401 exec execpod6bflz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.27.21.77 30128'
Jul 31 11:28:00.676: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.27.21.77 30128\nConnection to 172.27.21.77 30128 port [tcp/*] succeeded!\n"
Jul 31 11:28:00.676: INFO: stdout: "externalname-service-2b8fg"
Jul 31 11:28:00.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-7401 exec execpod6bflz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.27.21.79 30128'
Jul 31 11:28:00.909: INFO: stderr: "+ + echonc hostName -v\n -t -w 2 172.27.21.79 30128\nConnection to 172.27.21.79 30128 port [tcp/*] succeeded!\n"
Jul 31 11:28:00.909: INFO: stdout: "externalname-service-2b8fg"
Jul 31 11:28:00.909: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:28:00.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7401" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:9.583 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","total":346,"completed":169,"skipped":3351,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:28:01.019: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 11:28:01.102: INFO: Creating ReplicaSet my-hostname-basic-25d3df23-5e43-496a-962e-220cd1689c8a
Jul 31 11:28:01.116: INFO: Pod name my-hostname-basic-25d3df23-5e43-496a-962e-220cd1689c8a: Found 0 pods out of 1
Jul 31 11:28:06.135: INFO: Pod name my-hostname-basic-25d3df23-5e43-496a-962e-220cd1689c8a: Found 1 pods out of 1
Jul 31 11:28:06.135: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-25d3df23-5e43-496a-962e-220cd1689c8a" is running
Jul 31 11:28:06.141: INFO: Pod "my-hostname-basic-25d3df23-5e43-496a-962e-220cd1689c8a-76tfq" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-07-31 11:28:01 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-07-31 11:28:03 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-07-31 11:28:03 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-07-31 11:28:01 +0000 UTC Reason: Message:}])
Jul 31 11:28:06.141: INFO: Trying to dial the pod
Jul 31 11:28:11.166: INFO: Controller my-hostname-basic-25d3df23-5e43-496a-962e-220cd1689c8a: Got expected result from replica 1 [my-hostname-basic-25d3df23-5e43-496a-962e-220cd1689c8a-76tfq]: "my-hostname-basic-25d3df23-5e43-496a-962e-220cd1689c8a-76tfq", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:28:11.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9523" for this suite.

• [SLOW TEST:10.168 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":170,"skipped":3366,"failed":0}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:28:11.188: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating the pod
Jul 31 11:28:11.304: INFO: The status of Pod labelsupdate69ddfb9c-45b8-42c0-9e89-03eb1f6175b1 is Pending, waiting for it to be Running (with Ready = true)
Jul 31 11:28:13.313: INFO: The status of Pod labelsupdate69ddfb9c-45b8-42c0-9e89-03eb1f6175b1 is Pending, waiting for it to be Running (with Ready = true)
Jul 31 11:28:15.321: INFO: The status of Pod labelsupdate69ddfb9c-45b8-42c0-9e89-03eb1f6175b1 is Running (Ready = true)
Jul 31 11:28:15.890: INFO: Successfully updated pod "labelsupdate69ddfb9c-45b8-42c0-9e89-03eb1f6175b1"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:28:17.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5840" for this suite.

• [SLOW TEST:6.764 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":171,"skipped":3372,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:28:17.953: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 11:28:18.087: INFO: created pod
Jul 31 11:28:18.087: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-5965" to be "Succeeded or Failed"
Jul 31 11:28:18.113: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 26.664038ms
Jul 31 11:28:20.126: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039087735s
Jul 31 11:28:22.141: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054337176s
STEP: Saw pod success
Jul 31 11:28:22.141: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Jul 31 11:28:52.142: INFO: polling logs
Jul 31 11:28:52.170: INFO: Pod logs: 
I0731 11:28:19.285182       1 log.go:195] OK: Got token
I0731 11:28:19.285225       1 log.go:195] validating with in-cluster discovery
I0731 11:28:19.285655       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
I0731 11:28:19.285707       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-5965:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1659267498, NotBefore:1659266898, IssuedAt:1659266898, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5965", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"100f22d9-6c37-47e9-88c0-51f923752429"}}}
I0731 11:28:19.320387       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0731 11:28:19.332792       1 log.go:195] OK: Validated signature on JWT
I0731 11:28:19.332926       1 log.go:195] OK: Got valid claims from token!
I0731 11:28:19.332954       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-5965:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1659267498, NotBefore:1659266898, IssuedAt:1659266898, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-5965", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"100f22d9-6c37-47e9-88c0-51f923752429"}}}

Jul 31 11:28:52.170: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:28:52.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5965" for this suite.

• [SLOW TEST:34.290 seconds]
[sig-auth] ServiceAccounts
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","total":346,"completed":172,"skipped":3392,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:28:52.244: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 31 11:28:53.142: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 31 11:28:55.170: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.July, 31, 11, 28, 53, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 11, 28, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.July, 31, 11, 28, 53, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 11, 28, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6c69dbd86b\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 31 11:28:58.229: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:28:58.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2219" for this suite.
STEP: Destroying namespace "webhook-2219-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.263 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":346,"completed":173,"skipped":3431,"failed":0}
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:28:58.508: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-map-476e3785-c014-4090-903d-6125dedaa117
STEP: Creating a pod to test consume secrets
Jul 31 11:28:58.601: INFO: Waiting up to 5m0s for pod "pod-secrets-bf744dac-1f05-4b61-a9b2-eeae1d161e84" in namespace "secrets-8684" to be "Succeeded or Failed"
Jul 31 11:28:58.619: INFO: Pod "pod-secrets-bf744dac-1f05-4b61-a9b2-eeae1d161e84": Phase="Pending", Reason="", readiness=false. Elapsed: 17.035163ms
Jul 31 11:29:00.634: INFO: Pod "pod-secrets-bf744dac-1f05-4b61-a9b2-eeae1d161e84": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032857955s
Jul 31 11:29:02.645: INFO: Pod "pod-secrets-bf744dac-1f05-4b61-a9b2-eeae1d161e84": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043215122s
STEP: Saw pod success
Jul 31 11:29:02.645: INFO: Pod "pod-secrets-bf744dac-1f05-4b61-a9b2-eeae1d161e84" satisfied condition "Succeeded or Failed"
Jul 31 11:29:02.650: INFO: Trying to get logs from node p1-ashfcfj4pzo6aemt1j3a9ah7ew pod pod-secrets-bf744dac-1f05-4b61-a9b2-eeae1d161e84 container secret-volume-test: <nil>
STEP: delete the pod
Jul 31 11:29:02.712: INFO: Waiting for pod pod-secrets-bf744dac-1f05-4b61-a9b2-eeae1d161e84 to disappear
Jul 31 11:29:02.730: INFO: Pod pod-secrets-bf744dac-1f05-4b61-a9b2-eeae1d161e84 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:29:02.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8684" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":174,"skipped":3433,"failed":0}
SS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:29:02.747: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap configmap-7732/configmap-test-d441bd5e-94a6-4c90-8da0-b6e942e4473a
STEP: Creating a pod to test consume configMaps
Jul 31 11:29:02.851: INFO: Waiting up to 5m0s for pod "pod-configmaps-7122c138-50ad-42fe-ae09-6c6e33e6d2aa" in namespace "configmap-7732" to be "Succeeded or Failed"
Jul 31 11:29:02.866: INFO: Pod "pod-configmaps-7122c138-50ad-42fe-ae09-6c6e33e6d2aa": Phase="Pending", Reason="", readiness=false. Elapsed: 15.150777ms
Jul 31 11:29:04.879: INFO: Pod "pod-configmaps-7122c138-50ad-42fe-ae09-6c6e33e6d2aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027880836s
Jul 31 11:29:06.895: INFO: Pod "pod-configmaps-7122c138-50ad-42fe-ae09-6c6e33e6d2aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043900124s
STEP: Saw pod success
Jul 31 11:29:06.895: INFO: Pod "pod-configmaps-7122c138-50ad-42fe-ae09-6c6e33e6d2aa" satisfied condition "Succeeded or Failed"
Jul 31 11:29:06.900: INFO: Trying to get logs from node p1-ashfcfj4pzo6aemt1j3a9ah7ew pod pod-configmaps-7122c138-50ad-42fe-ae09-6c6e33e6d2aa container env-test: <nil>
STEP: delete the pod
Jul 31 11:29:06.950: INFO: Waiting for pod pod-configmaps-7122c138-50ad-42fe-ae09-6c6e33e6d2aa to disappear
Jul 31 11:29:06.955: INFO: Pod pod-configmaps-7122c138-50ad-42fe-ae09-6c6e33e6d2aa no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:29:06.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7732" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":346,"completed":175,"skipped":3435,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:29:06.983: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8807 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8807;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8807 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8807;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8807.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8807.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8807.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8807.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8807.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8807.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8807.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8807.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8807.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8807.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8807.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8807.svc;check="$$(dig +notcp +noall +answer +search 232.135.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.135.232_udp@PTR;check="$$(dig +tcp +noall +answer +search 232.135.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.135.232_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8807 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8807;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8807 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8807;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8807.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8807.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8807.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8807.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8807.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8807.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8807.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8807.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8807.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8807.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8807.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8807.svc;check="$$(dig +notcp +noall +answer +search 232.135.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.135.232_udp@PTR;check="$$(dig +tcp +noall +answer +search 232.135.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.135.232_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 31 11:29:21.203: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:21.208: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:21.213: INFO: Unable to read wheezy_udp@dns-test-service.dns-8807 from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:21.229: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8807 from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:21.234: INFO: Unable to read wheezy_udp@dns-test-service.dns-8807.svc from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:21.239: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8807.svc from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:21.246: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8807.svc from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:21.251: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8807.svc from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:21.281: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:21.287: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:21.292: INFO: Unable to read jessie_udp@dns-test-service.dns-8807 from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:21.298: INFO: Unable to read jessie_tcp@dns-test-service.dns-8807 from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:21.304: INFO: Unable to read jessie_udp@dns-test-service.dns-8807.svc from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:21.309: INFO: Unable to read jessie_tcp@dns-test-service.dns-8807.svc from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:21.315: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8807.svc from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:21.321: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8807.svc from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:21.346: INFO: Lookups using dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8807 wheezy_tcp@dns-test-service.dns-8807 wheezy_udp@dns-test-service.dns-8807.svc wheezy_tcp@dns-test-service.dns-8807.svc wheezy_udp@_http._tcp.dns-test-service.dns-8807.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8807.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8807 jessie_tcp@dns-test-service.dns-8807 jessie_udp@dns-test-service.dns-8807.svc jessie_tcp@dns-test-service.dns-8807.svc jessie_udp@_http._tcp.dns-test-service.dns-8807.svc jessie_tcp@_http._tcp.dns-test-service.dns-8807.svc]

Jul 31 11:29:26.357: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:26.363: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:26.369: INFO: Unable to read wheezy_udp@dns-test-service.dns-8807 from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:26.375: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8807 from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:26.380: INFO: Unable to read wheezy_udp@dns-test-service.dns-8807.svc from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:26.387: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8807.svc from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:26.392: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8807.svc from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:26.398: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8807.svc from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:26.434: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:26.441: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:26.447: INFO: Unable to read jessie_udp@dns-test-service.dns-8807 from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:26.453: INFO: Unable to read jessie_tcp@dns-test-service.dns-8807 from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:26.463: INFO: Unable to read jessie_udp@dns-test-service.dns-8807.svc from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:26.471: INFO: Unable to read jessie_tcp@dns-test-service.dns-8807.svc from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:26.478: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8807.svc from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:26.483: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8807.svc from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:26.508: INFO: Lookups using dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8807 wheezy_tcp@dns-test-service.dns-8807 wheezy_udp@dns-test-service.dns-8807.svc wheezy_tcp@dns-test-service.dns-8807.svc wheezy_udp@_http._tcp.dns-test-service.dns-8807.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8807.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8807 jessie_tcp@dns-test-service.dns-8807 jessie_udp@dns-test-service.dns-8807.svc jessie_tcp@dns-test-service.dns-8807.svc jessie_udp@_http._tcp.dns-test-service.dns-8807.svc jessie_tcp@_http._tcp.dns-test-service.dns-8807.svc]

Jul 31 11:29:31.354: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:31.359: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:31.364: INFO: Unable to read wheezy_udp@dns-test-service.dns-8807 from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:31.370: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8807 from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:31.374: INFO: Unable to read wheezy_udp@dns-test-service.dns-8807.svc from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:31.379: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8807.svc from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:31.384: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8807.svc from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:31.389: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8807.svc from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:31.418: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:31.423: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:31.427: INFO: Unable to read jessie_udp@dns-test-service.dns-8807 from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:31.433: INFO: Unable to read jessie_tcp@dns-test-service.dns-8807 from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:31.438: INFO: Unable to read jessie_udp@dns-test-service.dns-8807.svc from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:31.443: INFO: Unable to read jessie_tcp@dns-test-service.dns-8807.svc from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:31.448: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8807.svc from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:31.458: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8807.svc from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:31.482: INFO: Lookups using dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8807 wheezy_tcp@dns-test-service.dns-8807 wheezy_udp@dns-test-service.dns-8807.svc wheezy_tcp@dns-test-service.dns-8807.svc wheezy_udp@_http._tcp.dns-test-service.dns-8807.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8807.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8807 jessie_tcp@dns-test-service.dns-8807 jessie_udp@dns-test-service.dns-8807.svc jessie_tcp@dns-test-service.dns-8807.svc jessie_udp@_http._tcp.dns-test-service.dns-8807.svc jessie_tcp@_http._tcp.dns-test-service.dns-8807.svc]

Jul 31 11:29:36.357: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:36.364: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:36.369: INFO: Unable to read wheezy_udp@dns-test-service.dns-8807 from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:36.374: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8807 from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:36.379: INFO: Unable to read wheezy_udp@dns-test-service.dns-8807.svc from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:36.384: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8807.svc from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:36.390: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8807.svc from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:36.397: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8807.svc from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:36.423: INFO: Unable to read jessie_udp@dns-test-service from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:36.428: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:36.434: INFO: Unable to read jessie_udp@dns-test-service.dns-8807 from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:36.440: INFO: Unable to read jessie_tcp@dns-test-service.dns-8807 from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:36.445: INFO: Unable to read jessie_udp@dns-test-service.dns-8807.svc from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:36.451: INFO: Unable to read jessie_tcp@dns-test-service.dns-8807.svc from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:36.458: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8807.svc from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:36.467: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8807.svc from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:36.487: INFO: Lookups using dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-8807 wheezy_tcp@dns-test-service.dns-8807 wheezy_udp@dns-test-service.dns-8807.svc wheezy_tcp@dns-test-service.dns-8807.svc wheezy_udp@_http._tcp.dns-test-service.dns-8807.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8807.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-8807 jessie_tcp@dns-test-service.dns-8807 jessie_udp@dns-test-service.dns-8807.svc jessie_tcp@dns-test-service.dns-8807.svc jessie_udp@_http._tcp.dns-test-service.dns-8807.svc jessie_tcp@_http._tcp.dns-test-service.dns-8807.svc]

Jul 31 11:29:41.388: INFO: Unable to read wheezy_udp@dns-test-service.dns-8807.svc from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:41.394: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8807.svc from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:41.405: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8807.svc from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:41.419: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8807.svc from pod dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185: the server could not find the requested resource (get pods dns-test-84502274-d56f-4e77-8c61-59dff84c1185)
Jul 31 11:29:41.547: INFO: Lookups using dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185 failed for: [wheezy_udp@dns-test-service.dns-8807.svc wheezy_tcp@dns-test-service.dns-8807.svc wheezy_udp@_http._tcp.dns-test-service.dns-8807.svc wheezy_tcp@_http._tcp.dns-test-service.dns-8807.svc]

Jul 31 11:29:46.491: INFO: DNS probes using dns-8807/dns-test-84502274-d56f-4e77-8c61-59dff84c1185 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:29:46.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8807" for this suite.

• [SLOW TEST:39.958 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":346,"completed":176,"skipped":3455,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:29:46.942: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 31 11:29:51.105: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:29:51.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7642" for this suite.
•{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]","total":346,"completed":177,"skipped":3493,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:29:51.180: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
Jul 31 11:29:51.265: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:29:56.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-541" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":346,"completed":178,"skipped":3542,"failed":0}
SSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:29:56.172: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8907.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-8907.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8907.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-8907.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 31 11:30:00.346: INFO: DNS probes using dns-8907/dns-test-8b1085f6-51b8-47b9-87a0-1d241f8093c9 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:30:00.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8907" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":346,"completed":179,"skipped":3547,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:30:00.441: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:30:00.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-280" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","total":346,"completed":180,"skipped":3596,"failed":0}
SSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:30:00.630: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 11:30:00.774: INFO: The status of Pod busybox-readonly-fsfa301233-7dd4-4972-998d-992357ab8e98 is Pending, waiting for it to be Running (with Ready = true)
Jul 31 11:30:02.782: INFO: The status of Pod busybox-readonly-fsfa301233-7dd4-4972-998d-992357ab8e98 is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:30:02.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7249" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":181,"skipped":3602,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:30:02.832: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 31 11:30:03.533: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Jul 31 11:30:05.561: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.July, 31, 11, 30, 3, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 11, 30, 3, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.July, 31, 11, 30, 3, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 11, 30, 3, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6c69dbd86b\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 31 11:30:08.621: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:30:08.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3165" for this suite.
STEP: Destroying namespace "webhook-3165-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:6.002 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":346,"completed":182,"skipped":3617,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:30:08.835: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-8564
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 11:30:08.944: INFO: Found 0 stateful pods, waiting for 1
Jul 31 11:30:18.968: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet
Jul 31 11:30:19.023: INFO: Found 1 stateful pods, waiting for 2
Jul 31 11:30:29.033: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 31 11:30:29.033: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets
STEP: Delete all of the StatefulSets
STEP: Verify that StatefulSets have been deleted
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Jul 31 11:30:29.077: INFO: Deleting all statefulset in ns statefulset-8564
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:30:29.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8564" for this suite.

• [SLOW TEST:20.294 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should list, patch and delete a collection of StatefulSets [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","total":346,"completed":183,"skipped":3639,"failed":0}
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:30:29.129: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Jul 31 11:30:30.350: INFO: The status of Pod kube-controller-manager-master-um8faxf3d4468jbdm49zh4o57y is Running (Ready = true)
Jul 31 11:30:30.474: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:30:30.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2123" for this suite.
•{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":346,"completed":184,"skipped":3639,"failed":0}

------------------------------
[sig-node] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:30:30.499: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with secret that has name secret-emptykey-test-ff68d2a8-1bcf-434e-8d90-15cb13ccc8a0
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:30:30.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8139" for this suite.
•{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","total":346,"completed":185,"skipped":3639,"failed":0}
SSSSSSS
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:30:30.597: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Jul 31 11:30:30.697: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul 31 11:30:30.697: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul 31 11:30:30.733: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul 31 11:30:30.733: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul 31 11:30:30.817: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul 31 11:30:30.817: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul 31 11:30:30.886: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul 31 11:30:30.886: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jul 31 11:30:32.604: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jul 31 11:30:32.604: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jul 31 11:30:32.655: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Jul 31 11:30:32.683: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Jul 31 11:30:32.686: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 0
Jul 31 11:30:32.686: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 0
Jul 31 11:30:32.686: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 0
Jul 31 11:30:32.686: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 0
Jul 31 11:30:32.686: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 0
Jul 31 11:30:32.686: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 0
Jul 31 11:30:32.686: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 0
Jul 31 11:30:32.686: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 0
Jul 31 11:30:32.686: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 1
Jul 31 11:30:32.686: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 1
Jul 31 11:30:32.686: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 2
Jul 31 11:30:32.686: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 2
Jul 31 11:30:32.686: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 2
Jul 31 11:30:32.686: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 2
Jul 31 11:30:32.705: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 2
Jul 31 11:30:32.705: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 2
Jul 31 11:30:32.818: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 2
Jul 31 11:30:32.818: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 2
Jul 31 11:30:32.849: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 1
Jul 31 11:30:32.849: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 1
Jul 31 11:30:32.875: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 1
Jul 31 11:30:32.875: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 1
Jul 31 11:30:34.596: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 2
Jul 31 11:30:34.596: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 2
Jul 31 11:30:34.656: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 1
STEP: listing Deployments
Jul 31 11:30:34.672: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Jul 31 11:30:34.708: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Jul 31 11:30:34.736: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jul 31 11:30:34.752: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jul 31 11:30:34.820: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jul 31 11:30:34.889: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jul 31 11:30:34.916: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jul 31 11:30:36.715: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jul 31 11:30:36.787: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Jul 31 11:30:36.803: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jul 31 11:30:36.844: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jul 31 11:30:38.634: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Jul 31 11:30:38.698: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 1
Jul 31 11:30:38.698: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 1
Jul 31 11:30:38.698: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 1
Jul 31 11:30:38.698: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 1
Jul 31 11:30:38.698: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 1
Jul 31 11:30:38.698: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 2
Jul 31 11:30:38.699: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 3
Jul 31 11:30:38.699: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 2
Jul 31 11:30:38.699: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 2
Jul 31 11:30:38.699: INFO: observed Deployment test-deployment in namespace deployment-9205 with ReadyReplicas 3
STEP: deleting the Deployment
Jul 31 11:30:38.715: INFO: observed event type MODIFIED
Jul 31 11:30:38.715: INFO: observed event type MODIFIED
Jul 31 11:30:38.715: INFO: observed event type MODIFIED
Jul 31 11:30:38.716: INFO: observed event type MODIFIED
Jul 31 11:30:38.716: INFO: observed event type MODIFIED
Jul 31 11:30:38.716: INFO: observed event type MODIFIED
Jul 31 11:30:38.716: INFO: observed event type MODIFIED
Jul 31 11:30:38.716: INFO: observed event type MODIFIED
Jul 31 11:30:38.716: INFO: observed event type MODIFIED
Jul 31 11:30:38.716: INFO: observed event type MODIFIED
Jul 31 11:30:38.717: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Jul 31 11:30:38.728: INFO: Log out all the ReplicaSets if there is no deployment created
Jul 31 11:30:38.733: INFO: ReplicaSet "test-deployment-5ddd8b47d8":
&ReplicaSet{ObjectMeta:{test-deployment-5ddd8b47d8  deployment-9205  97b09d31-c654-4aa4-9e30-c5a8b857ccfd 26678 4 2022-07-31 11:30:32 +0000 UTC <nil> <nil> map[pod-template-hash:5ddd8b47d8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 66644256-11f9-407c-9f30-729f9252b10f 0xc0025dc587 0xc0025dc588}] []  [{kube-controller-manager Update apps/v1 2022-07-31 11:30:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66644256-11f9-407c-9f30-729f9252b10f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-07-31 11:30:38 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 5ddd8b47d8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:5ddd8b47d8 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.6 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0025dc610 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jul 31 11:30:38.738: INFO: pod: "test-deployment-5ddd8b47d8-ftdzd":
&Pod{ObjectMeta:{test-deployment-5ddd8b47d8-ftdzd test-deployment-5ddd8b47d8- deployment-9205  6033a045-7f85-437f-840b-f2322e553c43 26673 0 2022-07-31 11:30:34 +0000 UTC 2022-07-31 11:30:39 +0000 UTC 0xc00265ea88 map[pod-template-hash:5ddd8b47d8 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-5ddd8b47d8 97b09d31-c654-4aa4-9e30-c5a8b857ccfd 0xc00265eab7 0xc00265eab8}] []  [{kube-controller-manager Update v1 2022-07-31 11:30:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"97b09d31-c654-4aa4-9e30-c5a8b857ccfd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-07-31 11:30:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.28.176.2\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-74nbw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/pause:3.6,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-74nbw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:p1-esqoe7n9eqq76k8ojd9tajh51e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:30:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:30:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:30:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:30:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.27.21.77,PodIP:172.28.176.2,StartTime:2022-07-31 11:30:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-07-31 11:30:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/pause:3.6,ImageID:docker-pullable://k8s.gcr.io/pause@sha256:3d380ca8864549e74af4b29c10f9cb0956236dfb01c40ca076fb6c37253234db,ContainerID:docker://7e756a7d7d79987227a1a178b1c9b0327f131b62dd4c471bfe72c6f62a02c4bf,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.28.176.2,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jul 31 11:30:38.738: INFO: ReplicaSet "test-deployment-6d7ffcf7fb":
&ReplicaSet{ObjectMeta:{test-deployment-6d7ffcf7fb  deployment-9205  80ff37a3-de90-4030-8cb2-5c6e036933ac 26536 3 2022-07-31 11:30:30 +0000 UTC <nil> <nil> map[pod-template-hash:6d7ffcf7fb test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 66644256-11f9-407c-9f30-729f9252b10f 0xc0025dc677 0xc0025dc678}] []  [{kube-controller-manager Update apps/v1 2022-07-31 11:30:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66644256-11f9-407c-9f30-729f9252b10f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-07-31 11:30:34 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6d7ffcf7fb,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6d7ffcf7fb test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0025dc700 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jul 31 11:30:38.743: INFO: ReplicaSet "test-deployment-854fdc678":
&ReplicaSet{ObjectMeta:{test-deployment-854fdc678  deployment-9205  394e001a-92fe-4fcd-a641-7c3405e13f64 26670 2 2022-07-31 11:30:34 +0000 UTC <nil> <nil> map[pod-template-hash:854fdc678 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 66644256-11f9-407c-9f30-729f9252b10f 0xc0025dc767 0xc0025dc768}] []  [{kube-controller-manager Update apps/v1 2022-07-31 11:30:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66644256-11f9-407c-9f30-729f9252b10f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-07-31 11:30:36 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 854fdc678,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:854fdc678 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0025dc7f0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Jul 31 11:30:38.748: INFO: pod: "test-deployment-854fdc678-svndc":
&Pod{ObjectMeta:{test-deployment-854fdc678-svndc test-deployment-854fdc678- deployment-9205  617724f7-7fab-4228-a53f-1519b06d182a 26633 0 2022-07-31 11:30:34 +0000 UTC <nil> <nil> map[pod-template-hash:854fdc678 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-854fdc678 394e001a-92fe-4fcd-a641-7c3405e13f64 0xc0041e1e37 0xc0041e1e38}] []  [{kube-controller-manager Update v1 2022-07-31 11:30:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"394e001a-92fe-4fcd-a641-7c3405e13f64\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-07-31 11:30:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.28.80.2\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jcdhk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jcdhk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:p1-ashfcfj4pzo6aemt1j3a9ah7ew,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:30:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:30:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:30:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:30:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.27.21.73,PodIP:172.28.80.2,StartTime:2022-07-31 11:30:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-07-31 11:30:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:docker://a48f91299e68a6a422a45dc054cc2e0e896710340a2e052f2f3e9acf8672e3d5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.28.80.2,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jul 31 11:30:38.748: INFO: pod: "test-deployment-854fdc678-tv7nr":
&Pod{ObjectMeta:{test-deployment-854fdc678-tv7nr test-deployment-854fdc678- deployment-9205  a676d3a4-7768-4656-a038-6ffe71b36e4f 26669 0 2022-07-31 11:30:36 +0000 UTC <nil> <nil> map[pod-template-hash:854fdc678 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-854fdc678 394e001a-92fe-4fcd-a641-7c3405e13f64 0xc003ec2017 0xc003ec2018}] []  [{kube-controller-manager Update v1 2022-07-31 11:30:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"394e001a-92fe-4fcd-a641-7c3405e13f64\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-07-31 11:30:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.28.0.3\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6jrmc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6jrmc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:p1-eipeq63rfgo6ooocmu3kqk3tcc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:30:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:30:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:30:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:30:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.27.21.69,PodIP:172.28.0.3,StartTime:2022-07-31 11:30:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-07-31 11:30:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:docker://df1baecc250d5263b08a33dcc706a675af2f8ce776ef52fc17959c97a5fbdbd1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.28.0.3,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:30:38.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9205" for this suite.

• [SLOW TEST:8.168 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":346,"completed":186,"skipped":3646,"failed":0}
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:30:38.765: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-99eb0267-b708-4678-ad39-7f93575d6f30
STEP: Creating a pod to test consume configMaps
Jul 31 11:30:38.977: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a0e2050a-39fa-413b-bdf5-d304415d8f4a" in namespace "projected-8373" to be "Succeeded or Failed"
Jul 31 11:30:38.987: INFO: Pod "pod-projected-configmaps-a0e2050a-39fa-413b-bdf5-d304415d8f4a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.793955ms
Jul 31 11:30:40.999: INFO: Pod "pod-projected-configmaps-a0e2050a-39fa-413b-bdf5-d304415d8f4a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021580639s
Jul 31 11:30:43.010: INFO: Pod "pod-projected-configmaps-a0e2050a-39fa-413b-bdf5-d304415d8f4a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032504707s
STEP: Saw pod success
Jul 31 11:30:43.010: INFO: Pod "pod-projected-configmaps-a0e2050a-39fa-413b-bdf5-d304415d8f4a" satisfied condition "Succeeded or Failed"
Jul 31 11:30:43.015: INFO: Trying to get logs from node p1-eipeq63rfgo6ooocmu3kqk3tcc pod pod-projected-configmaps-a0e2050a-39fa-413b-bdf5-d304415d8f4a container agnhost-container: <nil>
STEP: delete the pod
Jul 31 11:30:43.093: INFO: Waiting for pod pod-projected-configmaps-a0e2050a-39fa-413b-bdf5-d304415d8f4a to disappear
Jul 31 11:30:43.099: INFO: Pod pod-projected-configmaps-a0e2050a-39fa-413b-bdf5-d304415d8f4a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:30:43.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8373" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":346,"completed":187,"skipped":3646,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:30:43.118: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:59
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod busybox-5fc4713c-0636-4572-a425-0d5a69cc1e08 in namespace container-probe-7203
Jul 31 11:30:45.254: INFO: Started pod busybox-5fc4713c-0636-4572-a425-0d5a69cc1e08 in namespace container-probe-7203
STEP: checking the pod's current state and verifying that restartCount is present
Jul 31 11:30:45.259: INFO: Initial restart count of pod busybox-5fc4713c-0636-4572-a425-0d5a69cc1e08 is 0
Jul 31 11:31:35.572: INFO: Restart count of pod container-probe-7203/busybox-5fc4713c-0636-4572-a425-0d5a69cc1e08 is now 1 (50.312802656s elapsed)
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:31:35.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7203" for this suite.

• [SLOW TEST:52.521 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":346,"completed":188,"skipped":3662,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:31:35.639: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with secret that has name projected-secret-test-503a2b60-e726-4669-9372-be8c9aefd201
STEP: Creating a pod to test consume secrets
Jul 31 11:31:35.749: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-afd8f14a-769b-401b-ab34-35632a5a53a6" in namespace "projected-1613" to be "Succeeded or Failed"
Jul 31 11:31:35.770: INFO: Pod "pod-projected-secrets-afd8f14a-769b-401b-ab34-35632a5a53a6": Phase="Pending", Reason="", readiness=false. Elapsed: 21.054724ms
Jul 31 11:31:37.788: INFO: Pod "pod-projected-secrets-afd8f14a-769b-401b-ab34-35632a5a53a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0385719s
Jul 31 11:31:39.798: INFO: Pod "pod-projected-secrets-afd8f14a-769b-401b-ab34-35632a5a53a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049140912s
STEP: Saw pod success
Jul 31 11:31:39.798: INFO: Pod "pod-projected-secrets-afd8f14a-769b-401b-ab34-35632a5a53a6" satisfied condition "Succeeded or Failed"
Jul 31 11:31:39.813: INFO: Trying to get logs from node p1-eipeq63rfgo6ooocmu3kqk3tcc pod pod-projected-secrets-afd8f14a-769b-401b-ab34-35632a5a53a6 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 31 11:31:39.872: INFO: Waiting for pod pod-projected-secrets-afd8f14a-769b-401b-ab34-35632a5a53a6 to disappear
Jul 31 11:31:39.878: INFO: Pod pod-projected-secrets-afd8f14a-769b-401b-ab34-35632a5a53a6 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:31:39.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1613" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":346,"completed":189,"skipped":3675,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:31:39.895: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service nodeport-test with type=NodePort in namespace services-2547
STEP: creating replication controller nodeport-test in namespace services-2547
I0731 11:31:40.059933      23 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-2547, replica count: 2
I0731 11:31:43.111256      23 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 31 11:31:43.111: INFO: Creating new exec pod
Jul 31 11:31:46.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-2547 exec execpod2lvbs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jul 31 11:31:46.421: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jul 31 11:31:46.421: INFO: stdout: ""
Jul 31 11:31:47.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-2547 exec execpod2lvbs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jul 31 11:31:47.646: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jul 31 11:31:47.646: INFO: stdout: "nodeport-test-wkdg8"
Jul 31 11:31:47.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-2547 exec execpod2lvbs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.105.204.2 80'
Jul 31 11:31:47.871: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.105.204.2 80\nConnection to 10.105.204.2 80 port [tcp/http] succeeded!\n"
Jul 31 11:31:47.871: INFO: stdout: "nodeport-test-mnvht"
Jul 31 11:31:47.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-2547 exec execpod2lvbs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.27.21.69 32032'
Jul 31 11:31:48.092: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.27.21.69 32032\nConnection to 172.27.21.69 32032 port [tcp/*] succeeded!\n"
Jul 31 11:31:48.092: INFO: stdout: "nodeport-test-wkdg8"
Jul 31 11:31:48.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-2547 exec execpod2lvbs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.27.21.72 32032'
Jul 31 11:31:48.315: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.27.21.72 32032\nConnection to 172.27.21.72 32032 port [tcp/*] succeeded!\n"
Jul 31 11:31:48.315: INFO: stdout: "nodeport-test-wkdg8"
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:31:48.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2547" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:8.443 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","total":346,"completed":190,"skipped":3688,"failed":0}
SSSSSSS
------------------------------
[sig-node] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:31:48.338: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test override arguments
Jul 31 11:31:48.438: INFO: Waiting up to 5m0s for pod "client-containers-22919b01-dfcf-4056-afb9-7bcd38673d78" in namespace "containers-4114" to be "Succeeded or Failed"
Jul 31 11:31:48.454: INFO: Pod "client-containers-22919b01-dfcf-4056-afb9-7bcd38673d78": Phase="Pending", Reason="", readiness=false. Elapsed: 16.572975ms
Jul 31 11:31:50.472: INFO: Pod "client-containers-22919b01-dfcf-4056-afb9-7bcd38673d78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034270652s
Jul 31 11:31:52.484: INFO: Pod "client-containers-22919b01-dfcf-4056-afb9-7bcd38673d78": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.04597404s
STEP: Saw pod success
Jul 31 11:31:52.484: INFO: Pod "client-containers-22919b01-dfcf-4056-afb9-7bcd38673d78" satisfied condition "Succeeded or Failed"
Jul 31 11:31:52.488: INFO: Trying to get logs from node p1-ashfcfj4pzo6aemt1j3a9ah7ew pod client-containers-22919b01-dfcf-4056-afb9-7bcd38673d78 container agnhost-container: <nil>
STEP: delete the pod
Jul 31 11:31:52.553: INFO: Waiting for pod client-containers-22919b01-dfcf-4056-afb9-7bcd38673d78 to disappear
Jul 31 11:31:52.562: INFO: Pod client-containers-22919b01-dfcf-4056-afb9-7bcd38673d78 no longer exists
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:31:52.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4114" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":346,"completed":191,"skipped":3695,"failed":0}
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:31:52.581: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Performing setup for networking test in namespace pod-network-test-5680
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 31 11:31:52.636: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jul 31 11:31:52.812: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 31 11:31:54.826: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 31 11:31:56.825: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 31 11:31:58.825: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 31 11:32:00.826: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 31 11:32:02.825: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 31 11:32:04.826: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 31 11:32:06.827: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 31 11:32:08.826: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 31 11:32:10.825: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 31 11:32:12.826: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 31 11:32:14.829: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jul 31 11:32:14.839: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jul 31 11:32:14.853: INFO: The status of Pod netserver-2 is Running (Ready = true)
Jul 31 11:32:14.863: INFO: The status of Pod netserver-3 is Running (Ready = true)
Jul 31 11:32:14.874: INFO: The status of Pod netserver-4 is Running (Ready = true)
STEP: Creating test pods
Jul 31 11:32:18.909: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
Jul 31 11:32:18.909: INFO: Breadth first check of 172.28.80.1 on host 172.27.21.73...
Jul 31 11:32:18.914: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.28.0.2:9080/dial?request=hostname&protocol=http&host=172.28.80.1&port=8083&tries=1'] Namespace:pod-network-test-5680 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 31 11:32:18.914: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
Jul 31 11:32:18.915: INFO: ExecWithOptions: Clientset creation
Jul 31 11:32:18.915: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5680/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.28.0.2%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.28.80.1%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Jul 31 11:32:19.105: INFO: Waiting for responses: map[]
Jul 31 11:32:19.105: INFO: reached 172.28.80.1 after 0/1 tries
Jul 31 11:32:19.105: INFO: Breadth first check of 172.28.0.4 on host 172.27.21.69...
Jul 31 11:32:19.111: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.28.0.2:9080/dial?request=hostname&protocol=http&host=172.28.0.4&port=8083&tries=1'] Namespace:pod-network-test-5680 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 31 11:32:19.111: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
Jul 31 11:32:19.112: INFO: ExecWithOptions: Clientset creation
Jul 31 11:32:19.112: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5680/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.28.0.2%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.28.0.4%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Jul 31 11:32:19.265: INFO: Waiting for responses: map[]
Jul 31 11:32:19.265: INFO: reached 172.28.0.4 after 0/1 tries
Jul 31 11:32:19.265: INFO: Breadth first check of 172.28.176.2 on host 172.27.21.77...
Jul 31 11:32:19.272: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.28.0.2:9080/dial?request=hostname&protocol=http&host=172.28.176.2&port=8083&tries=1'] Namespace:pod-network-test-5680 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 31 11:32:19.272: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
Jul 31 11:32:19.272: INFO: ExecWithOptions: Clientset creation
Jul 31 11:32:19.273: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5680/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.28.0.2%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.28.176.2%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Jul 31 11:32:20.488: INFO: Waiting for responses: map[]
Jul 31 11:32:20.489: INFO: reached 172.28.176.2 after 0/1 tries
Jul 31 11:32:20.489: INFO: Breadth first check of 172.28.96.2 on host 172.27.21.72...
Jul 31 11:32:20.497: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.28.0.2:9080/dial?request=hostname&protocol=http&host=172.28.96.2&port=8083&tries=1'] Namespace:pod-network-test-5680 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 31 11:32:20.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
Jul 31 11:32:20.498: INFO: ExecWithOptions: Clientset creation
Jul 31 11:32:20.498: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5680/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.28.0.2%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.28.96.2%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Jul 31 11:32:20.645: INFO: Waiting for responses: map[]
Jul 31 11:32:20.645: INFO: reached 172.28.96.2 after 0/1 tries
Jul 31 11:32:20.645: INFO: Breadth first check of 172.28.64.2 on host 172.27.21.79...
Jul 31 11:32:20.654: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.28.0.2:9080/dial?request=hostname&protocol=http&host=172.28.64.2&port=8083&tries=1'] Namespace:pod-network-test-5680 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 31 11:32:20.654: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
Jul 31 11:32:20.654: INFO: ExecWithOptions: Clientset creation
Jul 31 11:32:20.655: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5680/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.28.0.2%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.28.64.2%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true %!s(MISSING))
Jul 31 11:32:20.807: INFO: Waiting for responses: map[]
Jul 31 11:32:20.807: INFO: reached 172.28.64.2 after 0/1 tries
Jul 31 11:32:20.807: INFO: Going to retry 0 out of 5 pods....
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:32:20.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5680" for this suite.

• [SLOW TEST:28.253 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":346,"completed":192,"skipped":3704,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:32:20.836: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jul 31 11:32:20.892: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 31 11:32:20.913: INFO: Waiting for terminating namespaces to be deleted...
Jul 31 11:32:20.918: INFO: 
Logging pods the apiserver thinks is on node p1-ashfcfj4pzo6aemt1j3a9ah7ew before test
Jul 31 11:32:20.935: INFO: csi-ridge-node-hndsv from kube-system started at 2022-07-31 10:18:00 +0000 UTC (1 container statuses recorded)
Jul 31 11:32:20.935: INFO: 	Container csi-ridge-plugin ready: true, restart count 0
Jul 31 11:32:20.935: INFO: kube-proxy-24lml from kube-system started at 2022-07-31 10:17:50 +0000 UTC (1 container statuses recorded)
Jul 31 11:32:20.935: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 31 11:32:20.935: INFO: meta-lqxb8 from kube-system started at 2022-07-31 10:17:50 +0000 UTC (1 container statuses recorded)
Jul 31 11:32:20.935: INFO: 	Container meta ready: true, restart count 0
Jul 31 11:32:20.935: INFO: weave-net-qfsdt from kube-system started at 2022-07-31 10:17:59 +0000 UTC (2 container statuses recorded)
Jul 31 11:32:20.935: INFO: 	Container weave ready: true, restart count 1
Jul 31 11:32:20.935: INFO: 	Container weave-npc ready: true, restart count 0
Jul 31 11:32:20.936: INFO: netserver-0 from pod-network-test-5680 started at 2022-07-31 11:31:52 +0000 UTC (1 container statuses recorded)
Jul 31 11:32:20.936: INFO: 	Container webserver ready: true, restart count 0
Jul 31 11:32:20.936: INFO: sonobuoy-systemd-logs-daemon-set-b467c6d60405477d-jk6fz from sonobuoy started at 2022-07-31 10:30:53 +0000 UTC (2 container statuses recorded)
Jul 31 11:32:20.936: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 31 11:32:20.936: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 31 11:32:20.936: INFO: 
Logging pods the apiserver thinks is on node p1-eipeq63rfgo6ooocmu3kqk3tcc before test
Jul 31 11:32:20.953: INFO: csi-ridge-node-zd62w from kube-system started at 2022-07-31 10:18:00 +0000 UTC (1 container statuses recorded)
Jul 31 11:32:20.953: INFO: 	Container csi-ridge-plugin ready: true, restart count 0
Jul 31 11:32:20.953: INFO: kube-proxy-zjtsg from kube-system started at 2022-07-31 10:17:50 +0000 UTC (1 container statuses recorded)
Jul 31 11:32:20.953: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 31 11:32:20.953: INFO: meta-75kgq from kube-system started at 2022-07-31 10:17:50 +0000 UTC (1 container statuses recorded)
Jul 31 11:32:20.953: INFO: 	Container meta ready: true, restart count 0
Jul 31 11:32:20.953: INFO: weave-net-wk9fw from kube-system started at 2022-07-31 10:17:59 +0000 UTC (2 container statuses recorded)
Jul 31 11:32:20.953: INFO: 	Container weave ready: true, restart count 1
Jul 31 11:32:20.953: INFO: 	Container weave-npc ready: true, restart count 0
Jul 31 11:32:20.953: INFO: netserver-1 from pod-network-test-5680 started at 2022-07-31 11:31:52 +0000 UTC (1 container statuses recorded)
Jul 31 11:32:20.953: INFO: 	Container webserver ready: true, restart count 0
Jul 31 11:32:20.953: INFO: test-container-pod from pod-network-test-5680 started at 2022-07-31 11:32:14 +0000 UTC (1 container statuses recorded)
Jul 31 11:32:20.953: INFO: 	Container webserver ready: true, restart count 0
Jul 31 11:32:20.953: INFO: sonobuoy-systemd-logs-daemon-set-b467c6d60405477d-hs6kj from sonobuoy started at 2022-07-31 10:30:53 +0000 UTC (2 container statuses recorded)
Jul 31 11:32:20.953: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 31 11:32:20.953: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 31 11:32:20.953: INFO: 
Logging pods the apiserver thinks is on node p1-esqoe7n9eqq76k8ojd9tajh51e before test
Jul 31 11:32:20.974: INFO: csi-ridge-node-ds95v from kube-system started at 2022-07-31 10:18:00 +0000 UTC (1 container statuses recorded)
Jul 31 11:32:20.974: INFO: 	Container csi-ridge-plugin ready: true, restart count 0
Jul 31 11:32:20.974: INFO: kube-proxy-wkgc5 from kube-system started at 2022-07-31 10:17:50 +0000 UTC (1 container statuses recorded)
Jul 31 11:32:20.974: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 31 11:32:20.974: INFO: meta-h5cxg from kube-system started at 2022-07-31 10:17:50 +0000 UTC (1 container statuses recorded)
Jul 31 11:32:20.974: INFO: 	Container meta ready: true, restart count 0
Jul 31 11:32:20.974: INFO: weave-net-h6vbc from kube-system started at 2022-07-31 10:17:59 +0000 UTC (2 container statuses recorded)
Jul 31 11:32:20.974: INFO: 	Container weave ready: true, restart count 1
Jul 31 11:32:20.974: INFO: 	Container weave-npc ready: true, restart count 0
Jul 31 11:32:20.974: INFO: netserver-2 from pod-network-test-5680 started at 2022-07-31 11:31:52 +0000 UTC (1 container statuses recorded)
Jul 31 11:32:20.974: INFO: 	Container webserver ready: true, restart count 0
Jul 31 11:32:20.974: INFO: sonobuoy from sonobuoy started at 2022-07-31 10:30:49 +0000 UTC (1 container statuses recorded)
Jul 31 11:32:20.974: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 31 11:32:20.974: INFO: sonobuoy-systemd-logs-daemon-set-b467c6d60405477d-grkth from sonobuoy started at 2022-07-31 10:30:53 +0000 UTC (2 container statuses recorded)
Jul 31 11:32:20.974: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 31 11:32:20.974: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 31 11:32:20.974: INFO: 
Logging pods the apiserver thinks is on node p1-mz5rqwt5oar4y4n97cfumgzrxe before test
Jul 31 11:32:21.010: INFO: csi-ridge-node-xs7qk from kube-system started at 2022-07-31 10:18:00 +0000 UTC (1 container statuses recorded)
Jul 31 11:32:21.010: INFO: 	Container csi-ridge-plugin ready: true, restart count 0
Jul 31 11:32:21.011: INFO: kube-proxy-hhznq from kube-system started at 2022-07-31 10:17:50 +0000 UTC (1 container statuses recorded)
Jul 31 11:32:21.011: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 31 11:32:21.011: INFO: meta-dmrhb from kube-system started at 2022-07-31 10:17:50 +0000 UTC (1 container statuses recorded)
Jul 31 11:32:21.011: INFO: 	Container meta ready: true, restart count 0
Jul 31 11:32:21.011: INFO: weave-net-wclcv from kube-system started at 2022-07-31 10:17:59 +0000 UTC (2 container statuses recorded)
Jul 31 11:32:21.011: INFO: 	Container weave ready: true, restart count 1
Jul 31 11:32:21.011: INFO: 	Container weave-npc ready: true, restart count 0
Jul 31 11:32:21.011: INFO: netserver-3 from pod-network-test-5680 started at 2022-07-31 11:31:52 +0000 UTC (1 container statuses recorded)
Jul 31 11:32:21.011: INFO: 	Container webserver ready: true, restart count 0
Jul 31 11:32:21.011: INFO: sonobuoy-e2e-job-d80ae37bee83480b from sonobuoy started at 2022-07-31 10:30:53 +0000 UTC (2 container statuses recorded)
Jul 31 11:32:21.011: INFO: 	Container e2e ready: true, restart count 0
Jul 31 11:32:21.011: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 31 11:32:21.011: INFO: sonobuoy-systemd-logs-daemon-set-b467c6d60405477d-p7jjn from sonobuoy started at 2022-07-31 10:30:53 +0000 UTC (2 container statuses recorded)
Jul 31 11:32:21.011: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 31 11:32:21.011: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 31 11:32:21.011: INFO: 
Logging pods the apiserver thinks is on node p1-nc3cz44465xd41a6poxnmrpeqo before test
Jul 31 11:32:21.049: INFO: csi-ridge-node-t7hf9 from kube-system started at 2022-07-31 10:18:00 +0000 UTC (1 container statuses recorded)
Jul 31 11:32:21.049: INFO: 	Container csi-ridge-plugin ready: true, restart count 0
Jul 31 11:32:21.049: INFO: kube-proxy-g26h6 from kube-system started at 2022-07-31 10:17:50 +0000 UTC (1 container statuses recorded)
Jul 31 11:32:21.049: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 31 11:32:21.049: INFO: meta-9crb7 from kube-system started at 2022-07-31 10:17:50 +0000 UTC (1 container statuses recorded)
Jul 31 11:32:21.049: INFO: 	Container meta ready: true, restart count 0
Jul 31 11:32:21.049: INFO: weave-net-cm7ct from kube-system started at 2022-07-31 10:17:59 +0000 UTC (2 container statuses recorded)
Jul 31 11:32:21.049: INFO: 	Container weave ready: true, restart count 1
Jul 31 11:32:21.049: INFO: 	Container weave-npc ready: true, restart count 0
Jul 31 11:32:21.049: INFO: netserver-4 from pod-network-test-5680 started at 2022-07-31 11:31:52 +0000 UTC (1 container statuses recorded)
Jul 31 11:32:21.049: INFO: 	Container webserver ready: true, restart count 0
Jul 31 11:32:21.049: INFO: sonobuoy-systemd-logs-daemon-set-b467c6d60405477d-w6txb from sonobuoy started at 2022-07-31 10:30:53 +0000 UTC (2 container statuses recorded)
Jul 31 11:32:21.049: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 31 11:32:21.049: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: verifying the node has the label node p1-ashfcfj4pzo6aemt1j3a9ah7ew
STEP: verifying the node has the label node p1-eipeq63rfgo6ooocmu3kqk3tcc
STEP: verifying the node has the label node p1-esqoe7n9eqq76k8ojd9tajh51e
STEP: verifying the node has the label node p1-mz5rqwt5oar4y4n97cfumgzrxe
STEP: verifying the node has the label node p1-nc3cz44465xd41a6poxnmrpeqo
Jul 31 11:32:21.242: INFO: Pod csi-ridge-node-ds95v requesting resource cpu=0m on Node p1-esqoe7n9eqq76k8ojd9tajh51e
Jul 31 11:32:21.242: INFO: Pod csi-ridge-node-hndsv requesting resource cpu=0m on Node p1-ashfcfj4pzo6aemt1j3a9ah7ew
Jul 31 11:32:21.242: INFO: Pod csi-ridge-node-t7hf9 requesting resource cpu=0m on Node p1-nc3cz44465xd41a6poxnmrpeqo
Jul 31 11:32:21.242: INFO: Pod csi-ridge-node-xs7qk requesting resource cpu=0m on Node p1-mz5rqwt5oar4y4n97cfumgzrxe
Jul 31 11:32:21.242: INFO: Pod csi-ridge-node-zd62w requesting resource cpu=0m on Node p1-eipeq63rfgo6ooocmu3kqk3tcc
Jul 31 11:32:21.242: INFO: Pod kube-proxy-24lml requesting resource cpu=0m on Node p1-ashfcfj4pzo6aemt1j3a9ah7ew
Jul 31 11:32:21.242: INFO: Pod kube-proxy-g26h6 requesting resource cpu=0m on Node p1-nc3cz44465xd41a6poxnmrpeqo
Jul 31 11:32:21.242: INFO: Pod kube-proxy-hhznq requesting resource cpu=0m on Node p1-mz5rqwt5oar4y4n97cfumgzrxe
Jul 31 11:32:21.242: INFO: Pod kube-proxy-wkgc5 requesting resource cpu=0m on Node p1-esqoe7n9eqq76k8ojd9tajh51e
Jul 31 11:32:21.242: INFO: Pod kube-proxy-zjtsg requesting resource cpu=0m on Node p1-eipeq63rfgo6ooocmu3kqk3tcc
Jul 31 11:32:21.242: INFO: Pod meta-75kgq requesting resource cpu=0m on Node p1-eipeq63rfgo6ooocmu3kqk3tcc
Jul 31 11:32:21.242: INFO: Pod meta-9crb7 requesting resource cpu=0m on Node p1-nc3cz44465xd41a6poxnmrpeqo
Jul 31 11:32:21.242: INFO: Pod meta-dmrhb requesting resource cpu=0m on Node p1-mz5rqwt5oar4y4n97cfumgzrxe
Jul 31 11:32:21.242: INFO: Pod meta-h5cxg requesting resource cpu=0m on Node p1-esqoe7n9eqq76k8ojd9tajh51e
Jul 31 11:32:21.242: INFO: Pod meta-lqxb8 requesting resource cpu=0m on Node p1-ashfcfj4pzo6aemt1j3a9ah7ew
Jul 31 11:32:21.243: INFO: Pod weave-net-cm7ct requesting resource cpu=100m on Node p1-nc3cz44465xd41a6poxnmrpeqo
Jul 31 11:32:21.243: INFO: Pod weave-net-h6vbc requesting resource cpu=100m on Node p1-esqoe7n9eqq76k8ojd9tajh51e
Jul 31 11:32:21.243: INFO: Pod weave-net-qfsdt requesting resource cpu=100m on Node p1-ashfcfj4pzo6aemt1j3a9ah7ew
Jul 31 11:32:21.243: INFO: Pod weave-net-wclcv requesting resource cpu=100m on Node p1-mz5rqwt5oar4y4n97cfumgzrxe
Jul 31 11:32:21.243: INFO: Pod weave-net-wk9fw requesting resource cpu=100m on Node p1-eipeq63rfgo6ooocmu3kqk3tcc
Jul 31 11:32:21.243: INFO: Pod netserver-0 requesting resource cpu=0m on Node p1-ashfcfj4pzo6aemt1j3a9ah7ew
Jul 31 11:32:21.243: INFO: Pod netserver-1 requesting resource cpu=0m on Node p1-eipeq63rfgo6ooocmu3kqk3tcc
Jul 31 11:32:21.243: INFO: Pod netserver-2 requesting resource cpu=0m on Node p1-esqoe7n9eqq76k8ojd9tajh51e
Jul 31 11:32:21.243: INFO: Pod netserver-3 requesting resource cpu=0m on Node p1-mz5rqwt5oar4y4n97cfumgzrxe
Jul 31 11:32:21.243: INFO: Pod netserver-4 requesting resource cpu=0m on Node p1-nc3cz44465xd41a6poxnmrpeqo
Jul 31 11:32:21.243: INFO: Pod test-container-pod requesting resource cpu=0m on Node p1-eipeq63rfgo6ooocmu3kqk3tcc
Jul 31 11:32:21.243: INFO: Pod sonobuoy requesting resource cpu=0m on Node p1-esqoe7n9eqq76k8ojd9tajh51e
Jul 31 11:32:21.243: INFO: Pod sonobuoy-e2e-job-d80ae37bee83480b requesting resource cpu=0m on Node p1-mz5rqwt5oar4y4n97cfumgzrxe
Jul 31 11:32:21.243: INFO: Pod sonobuoy-systemd-logs-daemon-set-b467c6d60405477d-grkth requesting resource cpu=0m on Node p1-esqoe7n9eqq76k8ojd9tajh51e
Jul 31 11:32:21.243: INFO: Pod sonobuoy-systemd-logs-daemon-set-b467c6d60405477d-hs6kj requesting resource cpu=0m on Node p1-eipeq63rfgo6ooocmu3kqk3tcc
Jul 31 11:32:21.243: INFO: Pod sonobuoy-systemd-logs-daemon-set-b467c6d60405477d-jk6fz requesting resource cpu=0m on Node p1-ashfcfj4pzo6aemt1j3a9ah7ew
Jul 31 11:32:21.243: INFO: Pod sonobuoy-systemd-logs-daemon-set-b467c6d60405477d-p7jjn requesting resource cpu=0m on Node p1-mz5rqwt5oar4y4n97cfumgzrxe
Jul 31 11:32:21.243: INFO: Pod sonobuoy-systemd-logs-daemon-set-b467c6d60405477d-w6txb requesting resource cpu=0m on Node p1-nc3cz44465xd41a6poxnmrpeqo
STEP: Starting Pods to consume most of the cluster CPU.
Jul 31 11:32:21.243: INFO: Creating a pod which consumes cpu=2730m on Node p1-mz5rqwt5oar4y4n97cfumgzrxe
Jul 31 11:32:21.259: INFO: Creating a pod which consumes cpu=2730m on Node p1-nc3cz44465xd41a6poxnmrpeqo
Jul 31 11:32:21.269: INFO: Creating a pod which consumes cpu=2730m on Node p1-ashfcfj4pzo6aemt1j3a9ah7ew
Jul 31 11:32:21.302: INFO: Creating a pod which consumes cpu=2730m on Node p1-eipeq63rfgo6ooocmu3kqk3tcc
Jul 31 11:32:21.323: INFO: Creating a pod which consumes cpu=2730m on Node p1-esqoe7n9eqq76k8ojd9tajh51e
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-48c050c0-c36c-475c-9e9e-7c07e8cd15a7.1706e6ac80244dd2], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4816/filler-pod-48c050c0-c36c-475c-9e9e-7c07e8cd15a7 to p1-nc3cz44465xd41a6poxnmrpeqo]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-48c050c0-c36c-475c-9e9e-7c07e8cd15a7.1706e6acd4816422], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.6"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-48c050c0-c36c-475c-9e9e-7c07e8cd15a7.1706e6ace66473c6], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.6" in 300.077298ms]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-48c050c0-c36c-475c-9e9e-7c07e8cd15a7.1706e6acea050eaf], Reason = [Created], Message = [Created container filler-pod-48c050c0-c36c-475c-9e9e-7c07e8cd15a7]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-48c050c0-c36c-475c-9e9e-7c07e8cd15a7.1706e6acf0f8f0ae], Reason = [Started], Message = [Started container filler-pod-48c050c0-c36c-475c-9e9e-7c07e8cd15a7]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-778b7326-b3a6-429f-912f-59034be16d97.1706e6ac81a0997c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4816/filler-pod-778b7326-b3a6-429f-912f-59034be16d97 to p1-ashfcfj4pzo6aemt1j3a9ah7ew]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-778b7326-b3a6-429f-912f-59034be16d97.1706e6acb34d200e], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.6"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-778b7326-b3a6-429f-912f-59034be16d97.1706e6acc81801e8], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.6" in 348.825695ms]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-778b7326-b3a6-429f-912f-59034be16d97.1706e6accdeeca1e], Reason = [Created], Message = [Created container filler-pod-778b7326-b3a6-429f-912f-59034be16d97]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-778b7326-b3a6-429f-912f-59034be16d97.1706e6acd4f58f81], Reason = [Started], Message = [Started container filler-pod-778b7326-b3a6-429f-912f-59034be16d97]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9363d36e-673a-439f-ba32-5476f2cfb17b.1706e6ac85d6bb4e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4816/filler-pod-9363d36e-673a-439f-ba32-5476f2cfb17b to p1-esqoe7n9eqq76k8ojd9tajh51e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9363d36e-673a-439f-ba32-5476f2cfb17b.1706e6acdce8f093], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.6"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9363d36e-673a-439f-ba32-5476f2cfb17b.1706e6acedc69fac], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.6" in 282.948346ms]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9363d36e-673a-439f-ba32-5476f2cfb17b.1706e6acf24380f8], Reason = [Created], Message = [Created container filler-pod-9363d36e-673a-439f-ba32-5476f2cfb17b]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9363d36e-673a-439f-ba32-5476f2cfb17b.1706e6acfaa7ea26], Reason = [Started], Message = [Started container filler-pod-9363d36e-673a-439f-ba32-5476f2cfb17b]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b2dcb153-2ee3-4ff8-b8e5-6de7f1deee8c.1706e6ac7ee2c6ae], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4816/filler-pod-b2dcb153-2ee3-4ff8-b8e5-6de7f1deee8c to p1-mz5rqwt5oar4y4n97cfumgzrxe]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b2dcb153-2ee3-4ff8-b8e5-6de7f1deee8c.1706e6acad57585d], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.6"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b2dcb153-2ee3-4ff8-b8e5-6de7f1deee8c.1706e6acbe893c10], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.6" in 288.463759ms]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b2dcb153-2ee3-4ff8-b8e5-6de7f1deee8c.1706e6acc3bd74d1], Reason = [Created], Message = [Created container filler-pod-b2dcb153-2ee3-4ff8-b8e5-6de7f1deee8c]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b2dcb153-2ee3-4ff8-b8e5-6de7f1deee8c.1706e6accb1079e6], Reason = [Started], Message = [Started container filler-pod-b2dcb153-2ee3-4ff8-b8e5-6de7f1deee8c]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bf055709-f808-4820-8cb3-f7c56a5562a5.1706e6ac837a222f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4816/filler-pod-bf055709-f808-4820-8cb3-f7c56a5562a5 to p1-eipeq63rfgo6ooocmu3kqk3tcc]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bf055709-f808-4820-8cb3-f7c56a5562a5.1706e6acb30313e6], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.6"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bf055709-f808-4820-8cb3-f7c56a5562a5.1706e6accb79f272], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.6" in 410.395858ms]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bf055709-f808-4820-8cb3-f7c56a5562a5.1706e6acce4a8d7f], Reason = [Created], Message = [Created container filler-pod-bf055709-f808-4820-8cb3-f7c56a5562a5]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bf055709-f808-4820-8cb3-f7c56a5562a5.1706e6acd51c0b83], Reason = [Started], Message = [Started container filler-pod-bf055709-f808-4820-8cb3-f7c56a5562a5]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1706e6ad79727733], Reason = [FailedScheduling], Message = [0/8 nodes are available: 3 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 5 Insufficient cpu.]
STEP: removing the label node off the node p1-esqoe7n9eqq76k8ojd9tajh51e
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node p1-mz5rqwt5oar4y4n97cfumgzrxe
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node p1-nc3cz44465xd41a6poxnmrpeqo
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node p1-ashfcfj4pzo6aemt1j3a9ah7ew
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node p1-eipeq63rfgo6ooocmu3kqk3tcc
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:32:26.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4816" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

• [SLOW TEST:5.888 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":346,"completed":193,"skipped":3719,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:32:26.724: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 11:32:28.872: INFO: Deleting pod "var-expansion-b7a79aff-0842-463b-a3a2-cb830f50201b" in namespace "var-expansion-703"
Jul 31 11:32:28.897: INFO: Wait up to 5m0s for pod "var-expansion-b7a79aff-0842-463b-a3a2-cb830f50201b" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:32:30.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-703" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","total":346,"completed":194,"skipped":3741,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:32:30.938: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:32:34.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9013" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":346,"completed":195,"skipped":3757,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:32:34.569: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap that has name configmap-test-emptyKey-a0ebf631-3496-4dd2-9442-fb2ec6a2b425
[AfterEach] [sig-node] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:32:34.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3674" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":346,"completed":196,"skipped":3768,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:32:34.645: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 11:32:34.740: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Creating first CR 
Jul 31 11:32:37.341: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-07-31T11:32:37Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-07-31T11:32:37Z]] name:name1 resourceVersion:27647 uid:83a9e0ad-3191-420c-843c-98774aa2242e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Jul 31 11:32:47.576: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-07-31T11:32:47Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-07-31T11:32:47Z]] name:name2 resourceVersion:27707 uid:938b1274-507b-49b0-8026-a290666f53b2] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Jul 31 11:32:57.610: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-07-31T11:32:37Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-07-31T11:32:57Z]] name:name1 resourceVersion:27736 uid:83a9e0ad-3191-420c-843c-98774aa2242e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Jul 31 11:33:07.649: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-07-31T11:32:47Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-07-31T11:33:07Z]] name:name2 resourceVersion:27764 uid:938b1274-507b-49b0-8026-a290666f53b2] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Jul 31 11:33:17.663: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-07-31T11:32:37Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-07-31T11:32:57Z]] name:name1 resourceVersion:27790 uid:83a9e0ad-3191-420c-843c-98774aa2242e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Jul 31 11:33:27.690: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2022-07-31T11:32:47Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2022-07-31T11:33:07Z]] name:name2 resourceVersion:27816 uid:938b1274-507b-49b0-8026-a290666f53b2] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:33:38.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-5839" for this suite.

• [SLOW TEST:63.629 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":346,"completed":197,"skipped":3788,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:33:38.276: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename podtemplate
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create set of pod templates
Jul 31 11:33:38.378: INFO: created test-podtemplate-1
Jul 31 11:33:38.386: INFO: created test-podtemplate-2
Jul 31 11:33:38.394: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Jul 31 11:33:38.399: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Jul 31 11:33:38.438: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:33:38.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-5417" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":346,"completed":198,"skipped":3806,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:33:38.467: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 11:33:38.558: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:33:39.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2846" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":346,"completed":199,"skipped":3818,"failed":0}
SSSSSSSSS
------------------------------
[sig-apps] CronJob 
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:33:39.211: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename cronjob
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a ForbidConcurrent cronjob
STEP: Ensuring a job is scheduled
STEP: Ensuring exactly one is scheduled
STEP: Ensuring exactly one running job exists by listing jobs explicitly
STEP: Ensuring no more jobs are scheduled
STEP: Removing cronjob
[AfterEach] [sig-apps] CronJob
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:39:01.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-8016" for this suite.

• [SLOW TEST:322.174 seconds]
[sig-apps] CronJob
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","total":346,"completed":200,"skipped":3827,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:39:01.385: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul 31 11:39:01.553: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:39:01.553: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:39:01.553: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:39:01.577: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 31 11:39:01.577: INFO: Node p1-ashfcfj4pzo6aemt1j3a9ah7ew is running 0 daemon pod, expected 1
Jul 31 11:39:02.594: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:39:02.594: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:39:02.595: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:39:02.601: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 31 11:39:02.601: INFO: Node p1-ashfcfj4pzo6aemt1j3a9ah7ew is running 0 daemon pod, expected 1
Jul 31 11:39:03.588: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:39:03.588: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:39:03.588: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:39:03.593: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jul 31 11:39:03.594: INFO: Node p1-ashfcfj4pzo6aemt1j3a9ah7ew is running 0 daemon pod, expected 1
Jul 31 11:39:04.599: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:39:04.599: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:39:04.599: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:39:04.605: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Jul 31 11:39:04.605: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jul 31 11:39:04.637: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:39:04.637: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:39:04.637: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:39:04.645: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jul 31 11:39:04.645: INFO: Node p1-mz5rqwt5oar4y4n97cfumgzrxe is running 0 daemon pod, expected 1
Jul 31 11:39:05.663: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:39:05.663: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:39:05.663: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:39:05.667: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jul 31 11:39:05.667: INFO: Node p1-mz5rqwt5oar4y4n97cfumgzrxe is running 0 daemon pod, expected 1
Jul 31 11:39:06.658: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:39:06.658: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:39:06.658: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:39:06.664: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jul 31 11:39:06.664: INFO: Node p1-mz5rqwt5oar4y4n97cfumgzrxe is running 0 daemon pod, expected 1
Jul 31 11:39:07.658: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:39:07.658: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:39:07.658: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:39:07.665: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jul 31 11:39:07.665: INFO: Node p1-mz5rqwt5oar4y4n97cfumgzrxe is running 0 daemon pod, expected 1
Jul 31 11:39:08.661: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:39:08.661: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:39:08.661: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:39:08.667: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jul 31 11:39:08.667: INFO: Node p1-mz5rqwt5oar4y4n97cfumgzrxe is running 0 daemon pod, expected 1
Jul 31 11:39:09.659: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:39:09.659: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:39:09.659: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:39:09.664: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Jul 31 11:39:09.664: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9415, will wait for the garbage collector to delete the pods
Jul 31 11:39:09.740: INFO: Deleting DaemonSet.extensions daemon-set took: 15.520176ms
Jul 31 11:39:09.940: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.627688ms
Jul 31 11:39:12.649: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 31 11:39:12.649: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jul 31 11:39:12.653: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"28915"},"items":null}

Jul 31 11:39:12.657: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"28915"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:39:12.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9415" for this suite.

• [SLOW TEST:11.323 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":346,"completed":201,"skipped":3850,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:39:12.709: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 11:39:12.789: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Jul 31 11:39:16.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=crd-publish-openapi-9484 --namespace=crd-publish-openapi-9484 create -f -'
Jul 31 11:39:17.256: INFO: stderr: ""
Jul 31 11:39:17.256: INFO: stdout: "e2e-test-crd-publish-openapi-4350-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jul 31 11:39:17.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=crd-publish-openapi-9484 --namespace=crd-publish-openapi-9484 delete e2e-test-crd-publish-openapi-4350-crds test-cr'
Jul 31 11:39:17.372: INFO: stderr: ""
Jul 31 11:39:17.372: INFO: stdout: "e2e-test-crd-publish-openapi-4350-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jul 31 11:39:17.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=crd-publish-openapi-9484 --namespace=crd-publish-openapi-9484 apply -f -'
Jul 31 11:39:17.554: INFO: stderr: ""
Jul 31 11:39:17.554: INFO: stdout: "e2e-test-crd-publish-openapi-4350-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jul 31 11:39:17.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=crd-publish-openapi-9484 --namespace=crd-publish-openapi-9484 delete e2e-test-crd-publish-openapi-4350-crds test-cr'
Jul 31 11:39:17.667: INFO: stderr: ""
Jul 31 11:39:17.667: INFO: stdout: "e2e-test-crd-publish-openapi-4350-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Jul 31 11:39:17.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=crd-publish-openapi-9484 explain e2e-test-crd-publish-openapi-4350-crds'
Jul 31 11:39:18.527: INFO: stderr: ""
Jul 31 11:39:18.527: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4350-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:39:21.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9484" for this suite.

• [SLOW TEST:9.051 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":346,"completed":202,"skipped":3867,"failed":0}
SSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:39:21.760: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service endpoint-test2 in namespace services-6487
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6487 to expose endpoints map[]
Jul 31 11:39:21.895: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Jul 31 11:39:23.117: INFO: successfully validated that service endpoint-test2 in namespace services-6487 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-6487
Jul 31 11:39:23.153: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jul 31 11:39:25.170: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jul 31 11:39:27.162: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6487 to expose endpoints map[pod1:[80]]
Jul 31 11:39:27.184: INFO: successfully validated that service endpoint-test2 in namespace services-6487 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1
Jul 31 11:39:27.184: INFO: Creating new exec pod
Jul 31 11:39:32.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-6487 exec execpodlj55b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jul 31 11:39:32.697: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jul 31 11:39:32.697: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 31 11:39:32.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-6487 exec execpodlj55b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.26.122 80'
Jul 31 11:39:33.922: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.102.26.122 80\nConnection to 10.102.26.122 80 port [tcp/http] succeeded!\n"
Jul 31 11:39:33.922: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-6487
Jul 31 11:39:33.945: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jul 31 11:39:35.962: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6487 to expose endpoints map[pod1:[80] pod2:[80]]
Jul 31 11:39:35.996: INFO: successfully validated that service endpoint-test2 in namespace services-6487 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2
Jul 31 11:39:36.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-6487 exec execpodlj55b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jul 31 11:39:37.263: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jul 31 11:39:37.264: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 31 11:39:37.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-6487 exec execpodlj55b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.26.122 80'
Jul 31 11:39:37.477: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.102.26.122 80\nConnection to 10.102.26.122 80 port [tcp/http] succeeded!\n"
Jul 31 11:39:37.477: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-6487
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6487 to expose endpoints map[pod2:[80]]
Jul 31 11:39:37.550: INFO: successfully validated that service endpoint-test2 in namespace services-6487 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2
Jul 31 11:39:38.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-6487 exec execpodlj55b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jul 31 11:39:38.772: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jul 31 11:39:38.772: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 31 11:39:38.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-6487 exec execpodlj55b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.26.122 80'
Jul 31 11:39:38.984: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.102.26.122 80\nConnection to 10.102.26.122 80 port [tcp/http] succeeded!\n"
Jul 31 11:39:38.984: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-6487
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6487 to expose endpoints map[]
Jul 31 11:39:40.056: INFO: successfully validated that service endpoint-test2 in namespace services-6487 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:39:40.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6487" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:18.358 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":346,"completed":203,"skipped":3872,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:39:40.119: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul 31 11:39:40.199: INFO: Waiting up to 5m0s for pod "pod-a6217503-5c69-4212-9538-f53a5e1838b7" in namespace "emptydir-3407" to be "Succeeded or Failed"
Jul 31 11:39:40.204: INFO: Pod "pod-a6217503-5c69-4212-9538-f53a5e1838b7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.344482ms
Jul 31 11:39:42.218: INFO: Pod "pod-a6217503-5c69-4212-9538-f53a5e1838b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018899917s
Jul 31 11:39:44.228: INFO: Pod "pod-a6217503-5c69-4212-9538-f53a5e1838b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028908238s
STEP: Saw pod success
Jul 31 11:39:44.228: INFO: Pod "pod-a6217503-5c69-4212-9538-f53a5e1838b7" satisfied condition "Succeeded or Failed"
Jul 31 11:39:44.233: INFO: Trying to get logs from node p1-nc3cz44465xd41a6poxnmrpeqo pod pod-a6217503-5c69-4212-9538-f53a5e1838b7 container test-container: <nil>
STEP: delete the pod
Jul 31 11:39:44.298: INFO: Waiting for pod pod-a6217503-5c69-4212-9538-f53a5e1838b7 to disappear
Jul 31 11:39:44.303: INFO: Pod pod-a6217503-5c69-4212-9538-f53a5e1838b7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:39:44.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3407" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":204,"skipped":3892,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:39:44.331: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:59
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 11:39:44.444: INFO: The status of Pod test-webserver-2674d32d-660d-4d4e-b885-4fdc519ce647 is Pending, waiting for it to be Running (with Ready = true)
Jul 31 11:39:46.455: INFO: The status of Pod test-webserver-2674d32d-660d-4d4e-b885-4fdc519ce647 is Running (Ready = false)
Jul 31 11:39:48.461: INFO: The status of Pod test-webserver-2674d32d-660d-4d4e-b885-4fdc519ce647 is Running (Ready = false)
Jul 31 11:39:50.461: INFO: The status of Pod test-webserver-2674d32d-660d-4d4e-b885-4fdc519ce647 is Running (Ready = false)
Jul 31 11:39:52.458: INFO: The status of Pod test-webserver-2674d32d-660d-4d4e-b885-4fdc519ce647 is Running (Ready = false)
Jul 31 11:39:54.453: INFO: The status of Pod test-webserver-2674d32d-660d-4d4e-b885-4fdc519ce647 is Running (Ready = false)
Jul 31 11:39:56.453: INFO: The status of Pod test-webserver-2674d32d-660d-4d4e-b885-4fdc519ce647 is Running (Ready = false)
Jul 31 11:39:58.461: INFO: The status of Pod test-webserver-2674d32d-660d-4d4e-b885-4fdc519ce647 is Running (Ready = false)
Jul 31 11:40:00.463: INFO: The status of Pod test-webserver-2674d32d-660d-4d4e-b885-4fdc519ce647 is Running (Ready = false)
Jul 31 11:40:02.456: INFO: The status of Pod test-webserver-2674d32d-660d-4d4e-b885-4fdc519ce647 is Running (Ready = false)
Jul 31 11:40:04.453: INFO: The status of Pod test-webserver-2674d32d-660d-4d4e-b885-4fdc519ce647 is Running (Ready = false)
Jul 31 11:40:06.455: INFO: The status of Pod test-webserver-2674d32d-660d-4d4e-b885-4fdc519ce647 is Running (Ready = true)
Jul 31 11:40:06.463: INFO: Container started at 2022-07-31 11:39:45 +0000 UTC, pod became ready at 2022-07-31 11:40:04 +0000 UTC
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:40:06.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3608" for this suite.

• [SLOW TEST:22.178 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":346,"completed":205,"skipped":3904,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:40:06.510: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create a Replicaset
STEP: Verify that the required pods have come up.
Jul 31 11:40:06.608: INFO: Pod name sample-pod: Found 0 pods out of 1
Jul 31 11:40:11.632: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Getting /status
Jul 31 11:40:11.639: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status
Jul 31 11:40:11.672: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated
Jul 31 11:40:11.675: INFO: Observed &ReplicaSet event: ADDED
Jul 31 11:40:11.675: INFO: Observed &ReplicaSet event: MODIFIED
Jul 31 11:40:11.675: INFO: Observed &ReplicaSet event: MODIFIED
Jul 31 11:40:11.676: INFO: Observed &ReplicaSet event: MODIFIED
Jul 31 11:40:11.676: INFO: Found replicaset test-rs in namespace replicaset-8502 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jul 31 11:40:11.676: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status
Jul 31 11:40:11.676: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jul 31 11:40:11.695: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched
Jul 31 11:40:11.698: INFO: Observed &ReplicaSet event: ADDED
Jul 31 11:40:11.698: INFO: Observed &ReplicaSet event: MODIFIED
Jul 31 11:40:11.698: INFO: Observed &ReplicaSet event: MODIFIED
Jul 31 11:40:11.698: INFO: Observed &ReplicaSet event: MODIFIED
Jul 31 11:40:11.699: INFO: Observed replicaset test-rs in namespace replicaset-8502 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jul 31 11:40:11.699: INFO: Observed &ReplicaSet event: MODIFIED
Jul 31 11:40:11.699: INFO: Found replicaset test-rs in namespace replicaset-8502 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Jul 31 11:40:11.699: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:40:11.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8502" for this suite.

• [SLOW TEST:5.207 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","total":346,"completed":206,"skipped":3939,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:40:11.720: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 31 11:40:12.181: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jul 31 11:40:14.198: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.July, 31, 11, 40, 12, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 11, 40, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.July, 31, 11, 40, 12, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 11, 40, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6c69dbd86b\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 31 11:40:17.271: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 11:40:17.280: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-215-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:40:20.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8385" for this suite.
STEP: Destroying namespace "webhook-8385-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:9.132 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":346,"completed":207,"skipped":3972,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:40:20.853: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul 31 11:40:20.952: INFO: Waiting up to 5m0s for pod "pod-494bb4b0-cd55-431c-87e2-3fc4a193b933" in namespace "emptydir-4194" to be "Succeeded or Failed"
Jul 31 11:40:20.967: INFO: Pod "pod-494bb4b0-cd55-431c-87e2-3fc4a193b933": Phase="Pending", Reason="", readiness=false. Elapsed: 15.064555ms
Jul 31 11:40:22.980: INFO: Pod "pod-494bb4b0-cd55-431c-87e2-3fc4a193b933": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028170743s
Jul 31 11:40:24.991: INFO: Pod "pod-494bb4b0-cd55-431c-87e2-3fc4a193b933": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039134977s
STEP: Saw pod success
Jul 31 11:40:24.991: INFO: Pod "pod-494bb4b0-cd55-431c-87e2-3fc4a193b933" satisfied condition "Succeeded or Failed"
Jul 31 11:40:24.997: INFO: Trying to get logs from node p1-eipeq63rfgo6ooocmu3kqk3tcc pod pod-494bb4b0-cd55-431c-87e2-3fc4a193b933 container test-container: <nil>
STEP: delete the pod
Jul 31 11:40:25.063: INFO: Waiting for pod pod-494bb4b0-cd55-431c-87e2-3fc4a193b933 to disappear
Jul 31 11:40:25.069: INFO: Pod pod-494bb4b0-cd55-431c-87e2-3fc4a193b933 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:40:25.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4194" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":208,"skipped":4002,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container 
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:40:25.092: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 31 11:40:30.265: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:40:30.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1093" for this suite.

• [SLOW TEST:5.474 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    on terminated container
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:134
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [Excluded:WindowsDocker] [NodeConformance] [Conformance]","total":346,"completed":209,"skipped":4023,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:40:30.567: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 31 11:40:31.073: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 31 11:40:34.146: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:40:44.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-542" for this suite.
STEP: Destroying namespace "webhook-542-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:13.940 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":346,"completed":210,"skipped":4031,"failed":0}
[sig-node] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:40:44.508: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-905f0b8d-63a2-4bf1-b7fb-ffc3b072b319
STEP: Creating a pod to test consume secrets
Jul 31 11:40:44.635: INFO: Waiting up to 5m0s for pod "pod-secrets-69774ae6-600a-45a4-a208-7ea7e0cbec76" in namespace "secrets-9790" to be "Succeeded or Failed"
Jul 31 11:40:44.639: INFO: Pod "pod-secrets-69774ae6-600a-45a4-a208-7ea7e0cbec76": Phase="Pending", Reason="", readiness=false. Elapsed: 3.850761ms
Jul 31 11:40:46.650: INFO: Pod "pod-secrets-69774ae6-600a-45a4-a208-7ea7e0cbec76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015122581s
Jul 31 11:40:48.665: INFO: Pod "pod-secrets-69774ae6-600a-45a4-a208-7ea7e0cbec76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029935793s
STEP: Saw pod success
Jul 31 11:40:48.665: INFO: Pod "pod-secrets-69774ae6-600a-45a4-a208-7ea7e0cbec76" satisfied condition "Succeeded or Failed"
Jul 31 11:40:48.935: INFO: Trying to get logs from node p1-nc3cz44465xd41a6poxnmrpeqo pod pod-secrets-69774ae6-600a-45a4-a208-7ea7e0cbec76 container secret-env-test: <nil>
STEP: delete the pod
Jul 31 11:40:49.004: INFO: Waiting for pod pod-secrets-69774ae6-600a-45a4-a208-7ea7e0cbec76 to disappear
Jul 31 11:40:49.009: INFO: Pod pod-secrets-69774ae6-600a-45a4-a208-7ea7e0cbec76 no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:40:49.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9790" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":346,"completed":211,"skipped":4031,"failed":0}
S
------------------------------
[sig-node] Lease 
  lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:40:49.030: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename lease-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] lease API should be available [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-node] Lease
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:40:49.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-435" for this suite.
•{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","total":346,"completed":212,"skipped":4032,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:40:49.257: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create set of events
Jul 31 11:40:49.381: INFO: created test-event-1
Jul 31 11:40:49.387: INFO: created test-event-2
Jul 31 11:40:49.400: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Jul 31 11:40:49.405: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Jul 31 11:40:49.477: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:40:49.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4905" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","total":346,"completed":213,"skipped":4054,"failed":0}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:40:49.508: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 31 11:40:50.520: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 31 11:40:53.589: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:40:53.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3240" for this suite.
STEP: Destroying namespace "webhook-3240-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":346,"completed":214,"skipped":4060,"failed":0}
SSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:40:53.865: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 11:40:53.917: INFO: Creating deployment "webserver-deployment"
Jul 31 11:40:53.939: INFO: Waiting for observed generation 1
Jul 31 11:40:55.975: INFO: Waiting for all required pods to come up
Jul 31 11:40:56.025: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Jul 31 11:40:58.064: INFO: Waiting for deployment "webserver-deployment" to complete
Jul 31 11:40:58.076: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jul 31 11:40:58.092: INFO: Updating deployment webserver-deployment
Jul 31 11:40:58.092: INFO: Waiting for observed generation 2
Jul 31 11:41:00.112: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jul 31 11:41:00.118: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jul 31 11:41:00.124: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jul 31 11:41:00.137: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jul 31 11:41:00.137: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jul 31 11:41:00.142: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jul 31 11:41:00.152: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jul 31 11:41:00.152: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jul 31 11:41:00.175: INFO: Updating deployment webserver-deployment
Jul 31 11:41:00.175: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jul 31 11:41:00.184: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jul 31 11:41:00.199: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Jul 31 11:41:00.223: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-8474  fe668e0e-af79-495c-8ec9-24870721f701 30029 3 2022-07-31 11:40:53 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-07-31 11:40:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-07-31 11:40:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00597f4a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2022-07-31 11:40:56 +0000 UTC,LastTransitionTime:2022-07-31 11:40:56 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-566f96c878" is progressing.,LastUpdateTime:2022-07-31 11:40:58 +0000 UTC,LastTransitionTime:2022-07-31 11:40:53 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jul 31 11:41:00.259: INFO: New ReplicaSet "webserver-deployment-566f96c878" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-566f96c878  deployment-8474  fa3dbf92-3061-4eef-9318-f0055f759336 30033 3 2022-07-31 11:40:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment fe668e0e-af79-495c-8ec9-24870721f701 0xc00597f8a7 0xc00597f8a8}] []  [{kube-controller-manager Update apps/v1 2022-07-31 11:40:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe668e0e-af79-495c-8ec9-24870721f701\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-07-31 11:40:58 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 566f96c878,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00597f948 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 31 11:41:00.259: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jul 31 11:41:00.259: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-5d9fdcc779  deployment-8474  0f292292-3b3d-4905-9906-86d10b758df1 30030 3 2022-07-31 11:40:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment fe668e0e-af79-495c-8ec9-24870721f701 0xc00597f9a7 0xc00597f9a8}] []  [{kube-controller-manager Update apps/v1 2022-07-31 11:40:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fe668e0e-af79-495c-8ec9-24870721f701\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-07-31 11:40:55 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 5d9fdcc779,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00597fa38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jul 31 11:41:00.285: INFO: Pod "webserver-deployment-566f96c878-bm5ct" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-bm5ct webserver-deployment-566f96c878- deployment-8474  13f28c56-a153-4078-accf-996579f589b9 30022 0 2022-07-31 11:40:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 fa3dbf92-3061-4eef-9318-f0055f759336 0xc0033b67f7 0xc0033b67f8}] []  [{kube-controller-manager Update v1 2022-07-31 11:40:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa3dbf92-3061-4eef-9318-f0055f759336\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-07-31 11:40:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.28.64.3\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dj8rv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dj8rv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:p1-nc3cz44465xd41a6poxnmrpeqo,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.27.21.79,PodIP:172.28.64.3,StartTime:2022-07-31 11:40:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: pull access denied for webserver, repository does not exist or may require 'docker login': denied: requested access to the resource is denied,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.28.64.3,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 11:41:00.286: INFO: Pod "webserver-deployment-566f96c878-gfv92" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-gfv92 webserver-deployment-566f96c878- deployment-8474  39a4fdc4-e01d-49dc-9f82-d30931d7bf92 30042 0 2022-07-31 11:41:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 fa3dbf92-3061-4eef-9318-f0055f759336 0xc0033b69f0 0xc0033b69f1}] []  [{kube-controller-manager Update v1 2022-07-31 11:41:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa3dbf92-3061-4eef-9318-f0055f759336\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7hbt4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7hbt4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 11:41:00.286: INFO: Pod "webserver-deployment-566f96c878-k9tch" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-k9tch webserver-deployment-566f96c878- deployment-8474  75859afd-40ab-471b-8e1e-a7e9f74e8c1c 29998 0 2022-07-31 11:40:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 fa3dbf92-3061-4eef-9318-f0055f759336 0xc0033b6b37 0xc0033b6b38}] []  [{kube-controller-manager Update v1 2022-07-31 11:40:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa3dbf92-3061-4eef-9318-f0055f759336\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-07-31 11:40:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.28.0.2\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s5hs7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s5hs7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:p1-eipeq63rfgo6ooocmu3kqk3tcc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.27.21.69,PodIP:172.28.0.2,StartTime:2022-07-31 11:40:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: pull access denied for webserver, repository does not exist or may require 'docker login': denied: requested access to the resource is denied,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.28.0.2,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 11:41:00.286: INFO: Pod "webserver-deployment-566f96c878-nbp8l" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-nbp8l webserver-deployment-566f96c878- deployment-8474  c240084e-88f2-40d2-9593-6d4f584d1c1e 30025 0 2022-07-31 11:40:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 fa3dbf92-3061-4eef-9318-f0055f759336 0xc0033b6d30 0xc0033b6d31}] []  [{kube-controller-manager Update v1 2022-07-31 11:40:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa3dbf92-3061-4eef-9318-f0055f759336\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-07-31 11:40:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.28.96.4\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h5h4l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h5h4l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:p1-mz5rqwt5oar4y4n97cfumgzrxe,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.27.21.72,PodIP:172.28.96.4,StartTime:2022-07-31 11:40:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: pull access denied for webserver, repository does not exist or may require 'docker login': denied: requested access to the resource is denied,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.28.96.4,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 11:41:00.287: INFO: Pod "webserver-deployment-566f96c878-nlxl2" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-nlxl2 webserver-deployment-566f96c878- deployment-8474  d3b60c15-e2ab-437d-9c7d-0a34356282c7 30043 0 2022-07-31 11:41:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 fa3dbf92-3061-4eef-9318-f0055f759336 0xc0033b6f30 0xc0033b6f31}] []  [{kube-controller-manager Update v1 2022-07-31 11:41:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa3dbf92-3061-4eef-9318-f0055f759336\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tlkfd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tlkfd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 11:41:00.287: INFO: Pod "webserver-deployment-566f96c878-pk4zm" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-pk4zm webserver-deployment-566f96c878- deployment-8474  31b628e0-fd35-49ac-b3c5-a6d64b1e9ea2 30037 0 2022-07-31 11:41:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 fa3dbf92-3061-4eef-9318-f0055f759336 0xc0033b7077 0xc0033b7078}] []  [{kube-controller-manager Update v1 2022-07-31 11:41:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa3dbf92-3061-4eef-9318-f0055f759336\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rc7jc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rc7jc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:p1-eipeq63rfgo6ooocmu3kqk3tcc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:41:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 11:41:00.287: INFO: Pod "webserver-deployment-566f96c878-xgk8p" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-xgk8p webserver-deployment-566f96c878- deployment-8474  34f1f4b8-02dd-46ac-876a-ec46064115fb 30019 0 2022-07-31 11:40:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 fa3dbf92-3061-4eef-9318-f0055f759336 0xc0033b71d0 0xc0033b71d1}] []  [{kube-controller-manager Update v1 2022-07-31 11:40:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa3dbf92-3061-4eef-9318-f0055f759336\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-07-31 11:40:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.28.176.4\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-84qkf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-84qkf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:p1-esqoe7n9eqq76k8ojd9tajh51e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.27.21.77,PodIP:172.28.176.4,StartTime:2022-07-31 11:40:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: pull access denied for webserver, repository does not exist or may require 'docker login': denied: requested access to the resource is denied,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.28.176.4,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 11:41:00.288: INFO: Pod "webserver-deployment-566f96c878-xpqdg" is not available:
&Pod{ObjectMeta:{webserver-deployment-566f96c878-xpqdg webserver-deployment-566f96c878- deployment-8474  cf48ec52-a7ec-4631-9db8-f5e1b63c381b 30028 0 2022-07-31 11:40:58 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:566f96c878] map[] [{apps/v1 ReplicaSet webserver-deployment-566f96c878 fa3dbf92-3061-4eef-9318-f0055f759336 0xc0033b73d7 0xc0033b73d8}] []  [{kube-controller-manager Update v1 2022-07-31 11:40:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fa3dbf92-3061-4eef-9318-f0055f759336\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-07-31 11:40:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.28.80.1\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-22gsm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-22gsm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:p1-ashfcfj4pzo6aemt1j3a9ah7ew,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.27.21.73,PodIP:172.28.80.1,StartTime:2022-07-31 11:40:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: pull access denied for webserver, repository does not exist or may require 'docker login': denied: requested access to the resource is denied,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.28.80.1,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 11:41:00.288: INFO: Pod "webserver-deployment-5d9fdcc779-22kxp" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-22kxp webserver-deployment-5d9fdcc779- deployment-8474  42c6b4d5-edd1-49ff-a24d-18c6ceea9461 29911 0 2022-07-31 11:40:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 0f292292-3b3d-4905-9906-86d10b758df1 0xc0033b75e0 0xc0033b75e1}] []  [{kube-controller-manager Update v1 2022-07-31 11:40:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0f292292-3b3d-4905-9906-86d10b758df1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-07-31 11:40:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.28.96.3\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kb8zr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kb8zr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:p1-mz5rqwt5oar4y4n97cfumgzrxe,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.27.21.72,PodIP:172.28.96.3,StartTime:2022-07-31 11:40:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-07-31 11:40:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:docker://a7fa1e7228dc78ab577f506a768a481d120ae418fdf7e0f925b92bfd4601691c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.28.96.3,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 11:41:00.288: INFO: Pod "webserver-deployment-5d9fdcc779-5dmzt" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-5dmzt webserver-deployment-5d9fdcc779- deployment-8474  a5f0027a-9c36-4e36-b5ab-264b79026472 29914 0 2022-07-31 11:40:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 0f292292-3b3d-4905-9906-86d10b758df1 0xc0033b77b0 0xc0033b77b1}] []  [{kube-controller-manager Update v1 2022-07-31 11:40:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0f292292-3b3d-4905-9906-86d10b758df1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-07-31 11:40:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.28.80.2\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t7zk4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t7zk4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:p1-ashfcfj4pzo6aemt1j3a9ah7ew,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.27.21.73,PodIP:172.28.80.2,StartTime:2022-07-31 11:40:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-07-31 11:40:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:docker://3d8421352fec33bfbed66fbe0e21c2296eccbaad42c3871cef3f5af67d314257,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.28.80.2,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 11:41:00.289: INFO: Pod "webserver-deployment-5d9fdcc779-6c2m9" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-6c2m9 webserver-deployment-5d9fdcc779- deployment-8474  b9d516c9-8d5e-4af9-9ec3-9936e96ff0a8 29901 0 2022-07-31 11:40:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 0f292292-3b3d-4905-9906-86d10b758df1 0xc0033b7980 0xc0033b7981}] []  [{kube-controller-manager Update v1 2022-07-31 11:40:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0f292292-3b3d-4905-9906-86d10b758df1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-07-31 11:40:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.28.64.1\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kv7vv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kv7vv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:p1-nc3cz44465xd41a6poxnmrpeqo,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.27.21.79,PodIP:172.28.64.1,StartTime:2022-07-31 11:40:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-07-31 11:40:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:docker://619e14c7d17f47cadf1e01559fbc1e69f307b6c1ac19a7b97ee73c481ba250f5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.28.64.1,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 11:41:00.289: INFO: Pod "webserver-deployment-5d9fdcc779-7sj87" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-7sj87 webserver-deployment-5d9fdcc779- deployment-8474  31698a16-16ed-4bb8-bc28-b99f35dc1fa3 29908 0 2022-07-31 11:40:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 0f292292-3b3d-4905-9906-86d10b758df1 0xc0033b7b50 0xc0033b7b51}] []  [{kube-controller-manager Update v1 2022-07-31 11:40:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0f292292-3b3d-4905-9906-86d10b758df1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-07-31 11:40:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.28.96.2\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rcsds,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rcsds,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:p1-mz5rqwt5oar4y4n97cfumgzrxe,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.27.21.72,PodIP:172.28.96.2,StartTime:2022-07-31 11:40:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-07-31 11:40:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:docker://e8c810644b33fafdddb445f04c5e3f9a4ae677fd656d9f75257cee663dd885af,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.28.96.2,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 11:41:00.289: INFO: Pod "webserver-deployment-5d9fdcc779-bg7mg" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-bg7mg webserver-deployment-5d9fdcc779- deployment-8474  740483f2-e144-474d-9a11-5ecb0c85449a 29906 0 2022-07-31 11:40:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 0f292292-3b3d-4905-9906-86d10b758df1 0xc0033b7d20 0xc0033b7d21}] []  [{kube-controller-manager Update v1 2022-07-31 11:40:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0f292292-3b3d-4905-9906-86d10b758df1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-07-31 11:40:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.28.64.2\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2mt44,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2mt44,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:p1-nc3cz44465xd41a6poxnmrpeqo,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.27.21.79,PodIP:172.28.64.2,StartTime:2022-07-31 11:40:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-07-31 11:40:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:docker://b639fc5ed10eea233e74edff0d07385ccb950f317386eafea28cba9f6a50ba89,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.28.64.2,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 11:41:00.289: INFO: Pod "webserver-deployment-5d9fdcc779-fpfhq" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-fpfhq webserver-deployment-5d9fdcc779- deployment-8474  2bca1171-de20-4028-86b3-99db6134e82b 30040 0 2022-07-31 11:41:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 0f292292-3b3d-4905-9906-86d10b758df1 0xc0033b7ef0 0xc0033b7ef1}] []  [{kube-controller-manager Update v1 2022-07-31 11:41:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0f292292-3b3d-4905-9906-86d10b758df1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-plphm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-plphm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 11:41:00.289: INFO: Pod "webserver-deployment-5d9fdcc779-gqx96" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-gqx96 webserver-deployment-5d9fdcc779- deployment-8474  15a8d078-bc32-437f-b667-6729a222e7cb 29899 0 2022-07-31 11:40:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 0f292292-3b3d-4905-9906-86d10b758df1 0xc004a68027 0xc004a68028}] []  [{kube-controller-manager Update v1 2022-07-31 11:40:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0f292292-3b3d-4905-9906-86d10b758df1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-07-31 11:40:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.28.176.3\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6m29t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6m29t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:p1-esqoe7n9eqq76k8ojd9tajh51e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.27.21.77,PodIP:172.28.176.3,StartTime:2022-07-31 11:40:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-07-31 11:40:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:docker://e29e8048d8fdb26c9a9f3d31ce5230974fbfa303e151893b3203ec795a70863a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.28.176.3,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 11:41:00.290: INFO: Pod "webserver-deployment-5d9fdcc779-hws8p" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-hws8p webserver-deployment-5d9fdcc779- deployment-8474  56d28b16-a1e6-46f0-8565-ceded661c8a5 29917 0 2022-07-31 11:40:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 0f292292-3b3d-4905-9906-86d10b758df1 0xc004a681f7 0xc004a681f8}] []  [{kube-controller-manager Update v1 2022-07-31 11:40:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0f292292-3b3d-4905-9906-86d10b758df1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-07-31 11:40:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.28.80.3\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hml7n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hml7n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:p1-ashfcfj4pzo6aemt1j3a9ah7ew,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.27.21.73,PodIP:172.28.80.3,StartTime:2022-07-31 11:40:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-07-31 11:40:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:docker://53c0250174a091bb2db96ede3e91da785059fdbaaf9e390975e464d9f8608141,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.28.80.3,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 11:41:00.290: INFO: Pod "webserver-deployment-5d9fdcc779-kvmk4" is available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-kvmk4 webserver-deployment-5d9fdcc779- deployment-8474  2c04c688-9dc2-4950-8963-5191ff3b9df3 29896 0 2022-07-31 11:40:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 0f292292-3b3d-4905-9906-86d10b758df1 0xc004a683e0 0xc004a683e1}] []  [{kube-controller-manager Update v1 2022-07-31 11:40:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0f292292-3b3d-4905-9906-86d10b758df1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-07-31 11:40:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.28.176.2\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cgthz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cgthz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:p1-esqoe7n9eqq76k8ojd9tajh51e,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:40:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.27.21.77,PodIP:172.28.176.2,StartTime:2022-07-31 11:40:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2022-07-31 11:40:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:docker://fd4183cb2a740618e0e0d3b6bc74d14040a583402ace59265a62a439a6c61714,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.28.176.2,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 11:41:00.290: INFO: Pod "webserver-deployment-5d9fdcc779-r72rv" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-r72rv webserver-deployment-5d9fdcc779- deployment-8474  673110d0-8740-4fbf-a115-53989cff20af 30034 0 2022-07-31 11:41:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 0f292292-3b3d-4905-9906-86d10b758df1 0xc004a685b7 0xc004a685b8}] []  [{kube-controller-manager Update v1 2022-07-31 11:41:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0f292292-3b3d-4905-9906-86d10b758df1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f5drq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f5drq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:p1-eipeq63rfgo6ooocmu3kqk3tcc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:41:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jul 31 11:41:00.290: INFO: Pod "webserver-deployment-5d9fdcc779-wphxc" is not available:
&Pod{ObjectMeta:{webserver-deployment-5d9fdcc779-wphxc webserver-deployment-5d9fdcc779- deployment-8474  ccbe82a1-da08-49c1-8a7e-4f4543fa09b5 30039 0 2022-07-31 11:41:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:5d9fdcc779] map[] [{apps/v1 ReplicaSet webserver-deployment-5d9fdcc779 0f292292-3b3d-4905-9906-86d10b758df1 0xc004a68700 0xc004a68701}] []  [{kube-controller-manager Update v1 2022-07-31 11:41:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0f292292-3b3d-4905-9906-86d10b758df1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-59dhh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-59dhh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:41:00.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8474" for this suite.

• [SLOW TEST:6.589 seconds]
[sig-apps] Deployment
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":346,"completed":215,"skipped":4066,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:41:00.455: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with secret that has name projected-secret-test-map-f7caa0d2-2806-416f-8b2a-99abc6b78341
STEP: Creating a pod to test consume secrets
Jul 31 11:41:00.693: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0606ef95-5bb1-4962-a393-8352600cadcb" in namespace "projected-9604" to be "Succeeded or Failed"
Jul 31 11:41:00.719: INFO: Pod "pod-projected-secrets-0606ef95-5bb1-4962-a393-8352600cadcb": Phase="Pending", Reason="", readiness=false. Elapsed: 25.921529ms
Jul 31 11:41:02.731: INFO: Pod "pod-projected-secrets-0606ef95-5bb1-4962-a393-8352600cadcb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037996083s
Jul 31 11:41:04.745: INFO: Pod "pod-projected-secrets-0606ef95-5bb1-4962-a393-8352600cadcb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052142035s
Jul 31 11:41:06.768: INFO: Pod "pod-projected-secrets-0606ef95-5bb1-4962-a393-8352600cadcb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.075129593s
STEP: Saw pod success
Jul 31 11:41:06.768: INFO: Pod "pod-projected-secrets-0606ef95-5bb1-4962-a393-8352600cadcb" satisfied condition "Succeeded or Failed"
Jul 31 11:41:06.788: INFO: Trying to get logs from node p1-eipeq63rfgo6ooocmu3kqk3tcc pod pod-projected-secrets-0606ef95-5bb1-4962-a393-8352600cadcb container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 31 11:41:06.853: INFO: Waiting for pod pod-projected-secrets-0606ef95-5bb1-4962-a393-8352600cadcb to disappear
Jul 31 11:41:06.858: INFO: Pod pod-projected-secrets-0606ef95-5bb1-4962-a393-8352600cadcb no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:41:06.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9604" for this suite.

• [SLOW TEST:6.432 seconds]
[sig-storage] Projected secret
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":216,"skipped":4084,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:41:06.888: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul 31 11:41:07.005: INFO: Waiting up to 5m0s for pod "pod-6e7c7556-39f7-44f7-abd1-725d3ea2afb6" in namespace "emptydir-2931" to be "Succeeded or Failed"
Jul 31 11:41:07.023: INFO: Pod "pod-6e7c7556-39f7-44f7-abd1-725d3ea2afb6": Phase="Pending", Reason="", readiness=false. Elapsed: 17.907164ms
Jul 31 11:41:09.037: INFO: Pod "pod-6e7c7556-39f7-44f7-abd1-725d3ea2afb6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031686227s
Jul 31 11:41:11.055: INFO: Pod "pod-6e7c7556-39f7-44f7-abd1-725d3ea2afb6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049955324s
STEP: Saw pod success
Jul 31 11:41:11.055: INFO: Pod "pod-6e7c7556-39f7-44f7-abd1-725d3ea2afb6" satisfied condition "Succeeded or Failed"
Jul 31 11:41:11.060: INFO: Trying to get logs from node p1-eipeq63rfgo6ooocmu3kqk3tcc pod pod-6e7c7556-39f7-44f7-abd1-725d3ea2afb6 container test-container: <nil>
STEP: delete the pod
Jul 31 11:41:11.120: INFO: Waiting for pod pod-6e7c7556-39f7-44f7-abd1-725d3ea2afb6 to disappear
Jul 31 11:41:11.125: INFO: Pod pod-6e7c7556-39f7-44f7-abd1-725d3ea2afb6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:41:11.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2931" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":217,"skipped":4089,"failed":0}
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:41:11.141: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Jul 31 11:41:11.248: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f5d5419c-e334-4f54-8a89-c09311dcf8c5" in namespace "downward-api-1445" to be "Succeeded or Failed"
Jul 31 11:41:11.272: INFO: Pod "downwardapi-volume-f5d5419c-e334-4f54-8a89-c09311dcf8c5": Phase="Pending", Reason="", readiness=false. Elapsed: 23.989384ms
Jul 31 11:41:13.286: INFO: Pod "downwardapi-volume-f5d5419c-e334-4f54-8a89-c09311dcf8c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03785431s
Jul 31 11:41:15.402: INFO: Pod "downwardapi-volume-f5d5419c-e334-4f54-8a89-c09311dcf8c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.154008172s
STEP: Saw pod success
Jul 31 11:41:15.402: INFO: Pod "downwardapi-volume-f5d5419c-e334-4f54-8a89-c09311dcf8c5" satisfied condition "Succeeded or Failed"
Jul 31 11:41:15.631: INFO: Trying to get logs from node p1-ashfcfj4pzo6aemt1j3a9ah7ew pod downwardapi-volume-f5d5419c-e334-4f54-8a89-c09311dcf8c5 container client-container: <nil>
STEP: delete the pod
Jul 31 11:41:15.718: INFO: Waiting for pod downwardapi-volume-f5d5419c-e334-4f54-8a89-c09311dcf8c5 to disappear
Jul 31 11:41:15.724: INFO: Pod downwardapi-volume-f5d5419c-e334-4f54-8a89-c09311dcf8c5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:41:15.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1445" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":346,"completed":218,"skipped":4093,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:41:15.757: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-5786
[It] should validate Statefulset Status endpoints [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating statefulset ss in namespace statefulset-5786
Jul 31 11:41:15.894: INFO: Found 0 stateful pods, waiting for 1
Jul 31 11:41:25.911: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label
STEP: Getting /status
Jul 31 11:41:25.942: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status
Jul 31 11:41:25.963: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated
Jul 31 11:41:25.966: INFO: Observed &StatefulSet event: ADDED
Jul 31 11:41:25.966: INFO: Found Statefulset ss in namespace statefulset-5786 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jul 31 11:41:25.966: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status
Jul 31 11:41:25.966: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jul 31 11:41:25.985: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched
Jul 31 11:41:25.987: INFO: Observed &StatefulSet event: ADDED
Jul 31 11:41:25.987: INFO: Observed Statefulset ss in namespace statefulset-5786 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jul 31 11:41:25.987: INFO: Observed &StatefulSet event: MODIFIED
Jul 31 11:41:25.987: INFO: Found Statefulset ss in namespace statefulset-5786 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Jul 31 11:41:25.987: INFO: Deleting all statefulset in ns statefulset-5786
Jul 31 11:41:25.993: INFO: Scaling statefulset ss to 0
Jul 31 11:41:36.028: INFO: Waiting for statefulset status.replicas updated to 0
Jul 31 11:41:36.032: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:41:36.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5786" for this suite.

• [SLOW TEST:20.344 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    should validate Statefulset Status endpoints [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","total":346,"completed":219,"skipped":4124,"failed":0}
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:41:36.102: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
Jul 31 11:41:36.189: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:41:42.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-532" for this suite.

• [SLOW TEST:6.136 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":346,"completed":220,"skipped":4124,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:41:42.241: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service in namespace services-9549
STEP: creating service affinity-nodeport-transition in namespace services-9549
STEP: creating replication controller affinity-nodeport-transition in namespace services-9549
I0731 11:41:42.394951      23 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-9549, replica count: 3
I0731 11:41:45.446283      23 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 31 11:41:45.478: INFO: Creating new exec pod
Jul 31 11:41:48.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-9549 exec execpod-affinitypjswd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Jul 31 11:41:48.810: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jul 31 11:41:48.810: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 31 11:41:48.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-9549 exec execpod-affinitypjswd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.103.85 80'
Jul 31 11:41:49.029: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.103.85 80\nConnection to 10.100.103.85 80 port [tcp/http] succeeded!\n"
Jul 31 11:41:49.029: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 31 11:41:49.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-9549 exec execpod-affinitypjswd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.27.21.69 30982'
Jul 31 11:41:49.247: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.27.21.69 30982\nConnection to 172.27.21.69 30982 port [tcp/*] succeeded!\n"
Jul 31 11:41:49.247: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 31 11:41:49.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-9549 exec execpod-affinitypjswd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.27.21.79 30982'
Jul 31 11:41:49.455: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.27.21.79 30982\nConnection to 172.27.21.79 30982 port [tcp/*] succeeded!\n"
Jul 31 11:41:49.455: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 31 11:41:49.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-9549 exec execpod-affinitypjswd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.27.21.73:30982/ ; done'
Jul 31 11:41:49.804: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:30982/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:30982/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:30982/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:30982/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:30982/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:30982/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:30982/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:30982/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:30982/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:30982/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:30982/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:30982/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:30982/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:30982/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:30982/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:30982/\n"
Jul 31 11:41:49.804: INFO: stdout: "\naffinity-nodeport-transition-h8cwt\naffinity-nodeport-transition-kvch4\naffinity-nodeport-transition-pjdzv\naffinity-nodeport-transition-pjdzv\naffinity-nodeport-transition-kvch4\naffinity-nodeport-transition-pjdzv\naffinity-nodeport-transition-kvch4\naffinity-nodeport-transition-pjdzv\naffinity-nodeport-transition-h8cwt\naffinity-nodeport-transition-pjdzv\naffinity-nodeport-transition-h8cwt\naffinity-nodeport-transition-pjdzv\naffinity-nodeport-transition-kvch4\naffinity-nodeport-transition-pjdzv\naffinity-nodeport-transition-pjdzv\naffinity-nodeport-transition-pjdzv"
Jul 31 11:41:49.804: INFO: Received response from host: affinity-nodeport-transition-h8cwt
Jul 31 11:41:49.804: INFO: Received response from host: affinity-nodeport-transition-kvch4
Jul 31 11:41:49.804: INFO: Received response from host: affinity-nodeport-transition-pjdzv
Jul 31 11:41:49.804: INFO: Received response from host: affinity-nodeport-transition-pjdzv
Jul 31 11:41:49.804: INFO: Received response from host: affinity-nodeport-transition-kvch4
Jul 31 11:41:49.804: INFO: Received response from host: affinity-nodeport-transition-pjdzv
Jul 31 11:41:49.804: INFO: Received response from host: affinity-nodeport-transition-kvch4
Jul 31 11:41:49.804: INFO: Received response from host: affinity-nodeport-transition-pjdzv
Jul 31 11:41:49.804: INFO: Received response from host: affinity-nodeport-transition-h8cwt
Jul 31 11:41:49.804: INFO: Received response from host: affinity-nodeport-transition-pjdzv
Jul 31 11:41:49.805: INFO: Received response from host: affinity-nodeport-transition-h8cwt
Jul 31 11:41:49.805: INFO: Received response from host: affinity-nodeport-transition-pjdzv
Jul 31 11:41:49.805: INFO: Received response from host: affinity-nodeport-transition-kvch4
Jul 31 11:41:49.805: INFO: Received response from host: affinity-nodeport-transition-pjdzv
Jul 31 11:41:49.805: INFO: Received response from host: affinity-nodeport-transition-pjdzv
Jul 31 11:41:49.805: INFO: Received response from host: affinity-nodeport-transition-pjdzv
Jul 31 11:41:49.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-9549 exec execpod-affinitypjswd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.27.21.73:30982/ ; done'
Jul 31 11:41:50.130: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:30982/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:30982/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:30982/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:30982/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:30982/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:30982/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:30982/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:30982/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:30982/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:30982/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:30982/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:30982/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:30982/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:30982/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:30982/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:30982/\n"
Jul 31 11:41:50.130: INFO: stdout: "\naffinity-nodeport-transition-pjdzv\naffinity-nodeport-transition-pjdzv\naffinity-nodeport-transition-pjdzv\naffinity-nodeport-transition-pjdzv\naffinity-nodeport-transition-pjdzv\naffinity-nodeport-transition-pjdzv\naffinity-nodeport-transition-pjdzv\naffinity-nodeport-transition-pjdzv\naffinity-nodeport-transition-pjdzv\naffinity-nodeport-transition-pjdzv\naffinity-nodeport-transition-pjdzv\naffinity-nodeport-transition-pjdzv\naffinity-nodeport-transition-pjdzv\naffinity-nodeport-transition-pjdzv\naffinity-nodeport-transition-pjdzv\naffinity-nodeport-transition-pjdzv"
Jul 31 11:41:50.130: INFO: Received response from host: affinity-nodeport-transition-pjdzv
Jul 31 11:41:50.130: INFO: Received response from host: affinity-nodeport-transition-pjdzv
Jul 31 11:41:50.130: INFO: Received response from host: affinity-nodeport-transition-pjdzv
Jul 31 11:41:50.130: INFO: Received response from host: affinity-nodeport-transition-pjdzv
Jul 31 11:41:50.130: INFO: Received response from host: affinity-nodeport-transition-pjdzv
Jul 31 11:41:50.130: INFO: Received response from host: affinity-nodeport-transition-pjdzv
Jul 31 11:41:50.130: INFO: Received response from host: affinity-nodeport-transition-pjdzv
Jul 31 11:41:50.130: INFO: Received response from host: affinity-nodeport-transition-pjdzv
Jul 31 11:41:50.130: INFO: Received response from host: affinity-nodeport-transition-pjdzv
Jul 31 11:41:50.130: INFO: Received response from host: affinity-nodeport-transition-pjdzv
Jul 31 11:41:50.130: INFO: Received response from host: affinity-nodeport-transition-pjdzv
Jul 31 11:41:50.130: INFO: Received response from host: affinity-nodeport-transition-pjdzv
Jul 31 11:41:50.130: INFO: Received response from host: affinity-nodeport-transition-pjdzv
Jul 31 11:41:50.130: INFO: Received response from host: affinity-nodeport-transition-pjdzv
Jul 31 11:41:50.130: INFO: Received response from host: affinity-nodeport-transition-pjdzv
Jul 31 11:41:50.130: INFO: Received response from host: affinity-nodeport-transition-pjdzv
Jul 31 11:41:50.130: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-9549, will wait for the garbage collector to delete the pods
Jul 31 11:41:50.241: INFO: Deleting ReplicationController affinity-nodeport-transition took: 18.51671ms
Jul 31 11:41:50.342: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.74792ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:41:52.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9549" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:10.392 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":221,"skipped":4161,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:41:52.635: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:42:07.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-381" for this suite.
STEP: Destroying namespace "nsdeletetest-2476" for this suite.
Jul 31 11:42:07.988: INFO: Namespace nsdeletetest-2476 was already deleted
STEP: Destroying namespace "nsdeletetest-186" for this suite.

• [SLOW TEST:15.367 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":346,"completed":222,"skipped":4166,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:42:08.002: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:42:08.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-6114" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":346,"completed":223,"skipped":4175,"failed":0}

------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:42:08.106: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:89
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 11:42:08.198: INFO: Creating deployment "test-recreate-deployment"
Jul 31 11:42:08.218: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jul 31 11:42:08.245: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jul 31 11:42:10.262: INFO: Waiting deployment "test-recreate-deployment" to complete
Jul 31 11:42:10.267: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jul 31 11:42:10.286: INFO: Updating deployment test-recreate-deployment
Jul 31 11:42:10.286: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:83
Jul 31 11:42:10.505: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-1338  3fbad677-6b68-44e7-8cf2-ba3b501255d6 31008 2 2022-07-31 11:42:08 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2022-07-31 11:42:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-07-31 11:42:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003731238 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2022-07-31 11:42:10 +0000 UTC,LastTransitionTime:2022-07-31 11:42:10 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5b99bd5487" is progressing.,LastUpdateTime:2022-07-31 11:42:10 +0000 UTC,LastTransitionTime:2022-07-31 11:42:08 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jul 31 11:42:10.512: INFO: New ReplicaSet "test-recreate-deployment-5b99bd5487" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5b99bd5487  deployment-1338  f97b6fba-86e0-4a56-ab39-f402157f73a9 31006 1 2022-07-31 11:42:10 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5b99bd5487] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 3fbad677-6b68-44e7-8cf2-ba3b501255d6 0xc004842697 0xc004842698}] []  [{kube-controller-manager Update apps/v1 2022-07-31 11:42:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3fbad677-6b68-44e7-8cf2-ba3b501255d6\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-07-31 11:42:10 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5b99bd5487,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5b99bd5487] map[] [] []  []} {[] [] [{httpd k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004842738 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 31 11:42:10.512: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jul 31 11:42:10.512: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-594f666cd9  deployment-1338  b352eac7-1f66-4db0-a5c9-7cef04f514ca 30996 2 2022-07-31 11:42:08 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:594f666cd9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 3fbad677-6b68-44e7-8cf2-ba3b501255d6 0xc004842577 0xc004842578}] []  [{kube-controller-manager Update apps/v1 2022-07-31 11:42:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3fbad677-6b68-44e7-8cf2-ba3b501255d6\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2022-07-31 11:42:10 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 594f666cd9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:594f666cd9] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.39 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004842628 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jul 31 11:42:10.518: INFO: Pod "test-recreate-deployment-5b99bd5487-zd668" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5b99bd5487-zd668 test-recreate-deployment-5b99bd5487- deployment-1338  a950f93d-ba44-4091-be87-d7e4e2706090 31005 0 2022-07-31 11:42:10 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5b99bd5487] map[] [{apps/v1 ReplicaSet test-recreate-deployment-5b99bd5487 f97b6fba-86e0-4a56-ab39-f402157f73a9 0xc004842bc7 0xc004842bc8}] []  [{kube-controller-manager Update v1 2022-07-31 11:42:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f97b6fba-86e0-4a56-ab39-f402157f73a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2022-07-31 11:42:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8wffh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8wffh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:p1-ashfcfj4pzo6aemt1j3a9ah7ew,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:42:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:42:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:42:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2022-07-31 11:42:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.27.21.73,PodIP:,StartTime:2022-07-31 11:42:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:42:10.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1338" for this suite.
•{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":346,"completed":224,"skipped":4175,"failed":0}
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:42:10.544: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul 31 11:42:10.615: INFO: Waiting up to 5m0s for pod "pod-03eb44bb-276e-47e7-806f-d7dfbed26b6a" in namespace "emptydir-3658" to be "Succeeded or Failed"
Jul 31 11:42:10.633: INFO: Pod "pod-03eb44bb-276e-47e7-806f-d7dfbed26b6a": Phase="Pending", Reason="", readiness=false. Elapsed: 18.317743ms
Jul 31 11:42:12.640: INFO: Pod "pod-03eb44bb-276e-47e7-806f-d7dfbed26b6a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025134895s
Jul 31 11:42:14.648: INFO: Pod "pod-03eb44bb-276e-47e7-806f-d7dfbed26b6a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032958883s
Jul 31 11:42:16.664: INFO: Pod "pod-03eb44bb-276e-47e7-806f-d7dfbed26b6a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.049102382s
STEP: Saw pod success
Jul 31 11:42:16.664: INFO: Pod "pod-03eb44bb-276e-47e7-806f-d7dfbed26b6a" satisfied condition "Succeeded or Failed"
Jul 31 11:42:16.670: INFO: Trying to get logs from node p1-nc3cz44465xd41a6poxnmrpeqo pod pod-03eb44bb-276e-47e7-806f-d7dfbed26b6a container test-container: <nil>
STEP: delete the pod
Jul 31 11:42:16.716: INFO: Waiting for pod pod-03eb44bb-276e-47e7-806f-d7dfbed26b6a to disappear
Jul 31 11:42:16.720: INFO: Pod pod-03eb44bb-276e-47e7-806f-d7dfbed26b6a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:42:16.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3658" for this suite.

• [SLOW TEST:6.201 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":225,"skipped":4177,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:42:16.747: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward api env vars
Jul 31 11:42:16.850: INFO: Waiting up to 5m0s for pod "downward-api-313a3939-d0de-4b68-999e-15cc1a927be1" in namespace "downward-api-8636" to be "Succeeded or Failed"
Jul 31 11:42:16.887: INFO: Pod "downward-api-313a3939-d0de-4b68-999e-15cc1a927be1": Phase="Pending", Reason="", readiness=false. Elapsed: 36.136462ms
Jul 31 11:42:18.903: INFO: Pod "downward-api-313a3939-d0de-4b68-999e-15cc1a927be1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052894591s
Jul 31 11:42:20.919: INFO: Pod "downward-api-313a3939-d0de-4b68-999e-15cc1a927be1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.069003277s
STEP: Saw pod success
Jul 31 11:42:20.920: INFO: Pod "downward-api-313a3939-d0de-4b68-999e-15cc1a927be1" satisfied condition "Succeeded or Failed"
Jul 31 11:42:20.924: INFO: Trying to get logs from node p1-ashfcfj4pzo6aemt1j3a9ah7ew pod downward-api-313a3939-d0de-4b68-999e-15cc1a927be1 container dapi-container: <nil>
STEP: delete the pod
Jul 31 11:42:20.982: INFO: Waiting for pod downward-api-313a3939-d0de-4b68-999e-15cc1a927be1 to disappear
Jul 31 11:42:20.990: INFO: Pod downward-api-313a3939-d0de-4b68-999e-15cc1a927be1 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:42:20.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8636" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":346,"completed":226,"skipped":4193,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:42:21.011: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-6090
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-6090
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6090
Jul 31 11:42:21.160: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Jul 31 11:42:31.176: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jul 31 11:42:31.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=statefulset-6090 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 31 11:42:31.459: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 31 11:42:31.459: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 31 11:42:31.459: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 31 11:42:31.552: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 31 11:42:31.552: INFO: Waiting for statefulset status.replicas updated to 0
Jul 31 11:42:31.594: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999739s
Jul 31 11:42:32.605: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994881363s
Jul 31 11:42:33.616: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.983361674s
Jul 31 11:42:34.628: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.972479034s
Jul 31 11:42:35.639: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.961142752s
Jul 31 11:42:36.652: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.950262061s
Jul 31 11:42:37.662: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.936553854s
Jul 31 11:42:38.672: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.926454482s
Jul 31 11:42:39.683: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.917177469s
Jul 31 11:42:40.695: INFO: Verifying statefulset ss doesn't scale past 1 for another 905.835404ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6090
Jul 31 11:42:41.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=statefulset-6090 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 31 11:42:41.934: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 31 11:42:41.934: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 31 11:42:41.934: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 31 11:42:41.940: INFO: Found 1 stateful pods, waiting for 3
Jul 31 11:42:51.956: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 31 11:42:51.957: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 31 11:42:51.957: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jul 31 11:42:51.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=statefulset-6090 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 31 11:42:52.212: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 31 11:42:52.212: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 31 11:42:52.212: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 31 11:42:52.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=statefulset-6090 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 31 11:42:52.431: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 31 11:42:52.431: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 31 11:42:52.431: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 31 11:42:52.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=statefulset-6090 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 31 11:42:52.649: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 31 11:42:52.649: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 31 11:42:52.649: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 31 11:42:52.649: INFO: Waiting for statefulset status.replicas updated to 0
Jul 31 11:42:52.656: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jul 31 11:43:02.682: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 31 11:43:02.682: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul 31 11:43:02.682: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul 31 11:43:02.725: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999732s
Jul 31 11:43:03.737: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.983369889s
Jul 31 11:43:04.746: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.970569211s
Jul 31 11:43:05.766: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.96139086s
Jul 31 11:43:06.776: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.94228653s
Jul 31 11:43:07.789: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.932606499s
Jul 31 11:43:08.799: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.919356492s
Jul 31 11:43:09.809: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.909548783s
Jul 31 11:43:10.820: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.898967211s
Jul 31 11:43:11.831: INFO: Verifying statefulset ss doesn't scale past 3 for another 888.41582ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6090
Jul 31 11:43:12.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=statefulset-6090 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 31 11:43:13.090: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 31 11:43:13.090: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 31 11:43:13.090: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 31 11:43:13.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=statefulset-6090 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 31 11:43:13.328: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 31 11:43:13.328: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 31 11:43:13.328: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 31 11:43:13.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=statefulset-6090 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 31 11:43:13.551: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 31 11:43:13.551: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 31 11:43:13.551: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 31 11:43:13.551: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Jul 31 11:43:23.592: INFO: Deleting all statefulset in ns statefulset-6090
Jul 31 11:43:23.596: INFO: Scaling statefulset ss to 0
Jul 31 11:43:23.615: INFO: Waiting for statefulset status.replicas updated to 0
Jul 31 11:43:23.620: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:43:23.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6090" for this suite.

• [SLOW TEST:62.660 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":346,"completed":227,"skipped":4211,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:43:23.673: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name cm-test-opt-del-4a28815e-c741-4353-872b-745b7df362ae
STEP: Creating configMap with name cm-test-opt-upd-9e748e0a-3eca-4e96-b4dd-8b2f4481b89a
STEP: Creating the pod
Jul 31 11:43:23.870: INFO: The status of Pod pod-configmaps-382e96ac-0b40-4d09-b52c-d2e218e92856 is Pending, waiting for it to be Running (with Ready = true)
Jul 31 11:43:25.882: INFO: The status of Pod pod-configmaps-382e96ac-0b40-4d09-b52c-d2e218e92856 is Pending, waiting for it to be Running (with Ready = true)
Jul 31 11:43:27.882: INFO: The status of Pod pod-configmaps-382e96ac-0b40-4d09-b52c-d2e218e92856 is Running (Ready = true)
STEP: Deleting configmap cm-test-opt-del-4a28815e-c741-4353-872b-745b7df362ae
STEP: Updating configmap cm-test-opt-upd-9e748e0a-3eca-4e96-b4dd-8b2f4481b89a
STEP: Creating configMap with name cm-test-opt-create-480324b5-da86-448a-8e21-f6cb37afa693
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:44:42.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4408" for this suite.

• [SLOW TEST:79.013 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":228,"skipped":4233,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:44:42.689: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jul 31 11:44:42.802: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3789  23fcb2b9-a04f-498d-b844-6e2450393cf8 31773 0 2022-07-31 11:44:42 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-07-31 11:44:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 31 11:44:42.803: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3789  23fcb2b9-a04f-498d-b844-6e2450393cf8 31776 0 2022-07-31 11:44:42 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-07-31 11:44:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jul 31 11:44:42.845: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3789  23fcb2b9-a04f-498d-b844-6e2450393cf8 31777 0 2022-07-31 11:44:42 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-07-31 11:44:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 31 11:44:42.846: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3789  23fcb2b9-a04f-498d-b844-6e2450393cf8 31778 0 2022-07-31 11:44:42 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2022-07-31 11:44:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:44:42.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3789" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":346,"completed":229,"skipped":4313,"failed":0}
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:44:42.873: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be immutable if `immutable` field is set [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:44:43.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1712" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","total":346,"completed":230,"skipped":4322,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:44:43.061: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Jul 31 11:44:43.183: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b2737a02-ec51-46e1-9049-ab855da718a4" in namespace "projected-9134" to be "Succeeded or Failed"
Jul 31 11:44:43.206: INFO: Pod "downwardapi-volume-b2737a02-ec51-46e1-9049-ab855da718a4": Phase="Pending", Reason="", readiness=false. Elapsed: 22.611611ms
Jul 31 11:44:45.217: INFO: Pod "downwardapi-volume-b2737a02-ec51-46e1-9049-ab855da718a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033370624s
Jul 31 11:44:47.228: INFO: Pod "downwardapi-volume-b2737a02-ec51-46e1-9049-ab855da718a4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044922041s
Jul 31 11:44:49.238: INFO: Pod "downwardapi-volume-b2737a02-ec51-46e1-9049-ab855da718a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.053981511s
STEP: Saw pod success
Jul 31 11:44:49.238: INFO: Pod "downwardapi-volume-b2737a02-ec51-46e1-9049-ab855da718a4" satisfied condition "Succeeded or Failed"
Jul 31 11:44:49.243: INFO: Trying to get logs from node p1-nc3cz44465xd41a6poxnmrpeqo pod downwardapi-volume-b2737a02-ec51-46e1-9049-ab855da718a4 container client-container: <nil>
STEP: delete the pod
Jul 31 11:44:49.336: INFO: Waiting for pod downwardapi-volume-b2737a02-ec51-46e1-9049-ab855da718a4 to disappear
Jul 31 11:44:49.341: INFO: Pod downwardapi-volume-b2737a02-ec51-46e1-9049-ab855da718a4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:44:49.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9134" for this suite.

• [SLOW TEST:6.308 seconds]
[sig-storage] Projected downwardAPI
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":346,"completed":231,"skipped":4323,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:44:49.371: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jul 31 11:44:49.473: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:44:54.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3383" for this suite.
•{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","total":346,"completed":232,"skipped":4336,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:44:54.366: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Jul 31 11:44:55.361: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
Jul 31 11:44:57.402: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2022, time.July, 31, 11, 44, 55, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 11, 44, 55, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2022, time.July, 31, 11, 44, 55, 0, time.Local), LastTransitionTime:time.Date(2022, time.July, 31, 11, 44, 55, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-67c86bcf4b\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 31 11:45:00.473: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 11:45:00.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:45:03.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-9990" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:9.526 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":346,"completed":233,"skipped":4343,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:45:03.894: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Jul 31 11:45:04.015: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Jul 31 11:45:18.703: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
Jul 31 11:45:22.040: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:45:36.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3070" for this suite.

• [SLOW TEST:32.624 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":346,"completed":234,"skipped":4350,"failed":0}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:45:36.518: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 11:45:36.662: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"503bc20d-3ce9-4aa0-90bb-38664a9cd1c7", Controller:(*bool)(0xc0055de8f6), BlockOwnerDeletion:(*bool)(0xc0055de8f7)}}
Jul 31 11:45:36.687: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"cc0bf12b-5032-44d2-b4d3-b40cd9200210", Controller:(*bool)(0xc0055deb9e), BlockOwnerDeletion:(*bool)(0xc0055deb9f)}}
Jul 31 11:45:36.743: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"e6d46873-bd7a-4b4b-bac3-c675cc8d24a2", Controller:(*bool)(0xc005425aee), BlockOwnerDeletion:(*bool)(0xc005425aef)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:45:41.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8928" for this suite.

• [SLOW TEST:5.286 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":346,"completed":235,"skipped":4368,"failed":0}
[sig-node] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:45:41.804: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test env composition
Jul 31 11:45:41.889: INFO: Waiting up to 5m0s for pod "var-expansion-96f9d1c1-ab2c-4825-ba94-1d03e5d7564e" in namespace "var-expansion-2527" to be "Succeeded or Failed"
Jul 31 11:45:41.911: INFO: Pod "var-expansion-96f9d1c1-ab2c-4825-ba94-1d03e5d7564e": Phase="Pending", Reason="", readiness=false. Elapsed: 21.895922ms
Jul 31 11:45:43.923: INFO: Pod "var-expansion-96f9d1c1-ab2c-4825-ba94-1d03e5d7564e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033282615s
Jul 31 11:45:45.937: INFO: Pod "var-expansion-96f9d1c1-ab2c-4825-ba94-1d03e5d7564e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047947542s
STEP: Saw pod success
Jul 31 11:45:45.937: INFO: Pod "var-expansion-96f9d1c1-ab2c-4825-ba94-1d03e5d7564e" satisfied condition "Succeeded or Failed"
Jul 31 11:45:45.941: INFO: Trying to get logs from node p1-nc3cz44465xd41a6poxnmrpeqo pod var-expansion-96f9d1c1-ab2c-4825-ba94-1d03e5d7564e container dapi-container: <nil>
STEP: delete the pod
Jul 31 11:45:45.985: INFO: Waiting for pod var-expansion-96f9d1c1-ab2c-4825-ba94-1d03e5d7564e to disappear
Jul 31 11:45:45.990: INFO: Pod var-expansion-96f9d1c1-ab2c-4825-ba94-1d03e5d7564e no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:45:45.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2527" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":346,"completed":236,"skipped":4368,"failed":0}
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:45:46.009: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jul 31 11:45:46.093: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 31 11:45:46.106: INFO: Waiting for terminating namespaces to be deleted...
Jul 31 11:45:46.109: INFO: 
Logging pods the apiserver thinks is on node p1-ashfcfj4pzo6aemt1j3a9ah7ew before test
Jul 31 11:45:46.124: INFO: csi-ridge-node-hndsv from kube-system started at 2022-07-31 10:18:00 +0000 UTC (1 container statuses recorded)
Jul 31 11:45:46.124: INFO: 	Container csi-ridge-plugin ready: true, restart count 0
Jul 31 11:45:46.124: INFO: kube-proxy-24lml from kube-system started at 2022-07-31 10:17:50 +0000 UTC (1 container statuses recorded)
Jul 31 11:45:46.124: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 31 11:45:46.124: INFO: meta-lqxb8 from kube-system started at 2022-07-31 10:17:50 +0000 UTC (1 container statuses recorded)
Jul 31 11:45:46.124: INFO: 	Container meta ready: true, restart count 0
Jul 31 11:45:46.124: INFO: weave-net-qfsdt from kube-system started at 2022-07-31 10:17:59 +0000 UTC (2 container statuses recorded)
Jul 31 11:45:46.124: INFO: 	Container weave ready: true, restart count 1
Jul 31 11:45:46.124: INFO: 	Container weave-npc ready: true, restart count 0
Jul 31 11:45:46.124: INFO: sonobuoy-systemd-logs-daemon-set-b467c6d60405477d-jk6fz from sonobuoy started at 2022-07-31 10:30:53 +0000 UTC (2 container statuses recorded)
Jul 31 11:45:46.124: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 31 11:45:46.124: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 31 11:45:46.124: INFO: 
Logging pods the apiserver thinks is on node p1-eipeq63rfgo6ooocmu3kqk3tcc before test
Jul 31 11:45:46.137: INFO: csi-ridge-node-zd62w from kube-system started at 2022-07-31 10:18:00 +0000 UTC (1 container statuses recorded)
Jul 31 11:45:46.137: INFO: 	Container csi-ridge-plugin ready: true, restart count 0
Jul 31 11:45:46.137: INFO: kube-proxy-zjtsg from kube-system started at 2022-07-31 10:17:50 +0000 UTC (1 container statuses recorded)
Jul 31 11:45:46.137: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 31 11:45:46.137: INFO: meta-75kgq from kube-system started at 2022-07-31 10:17:50 +0000 UTC (1 container statuses recorded)
Jul 31 11:45:46.138: INFO: 	Container meta ready: true, restart count 0
Jul 31 11:45:46.138: INFO: weave-net-wk9fw from kube-system started at 2022-07-31 10:17:59 +0000 UTC (2 container statuses recorded)
Jul 31 11:45:46.138: INFO: 	Container weave ready: true, restart count 1
Jul 31 11:45:46.138: INFO: 	Container weave-npc ready: true, restart count 0
Jul 31 11:45:46.138: INFO: sonobuoy-systemd-logs-daemon-set-b467c6d60405477d-hs6kj from sonobuoy started at 2022-07-31 10:30:53 +0000 UTC (2 container statuses recorded)
Jul 31 11:45:46.138: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 31 11:45:46.138: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 31 11:45:46.138: INFO: 
Logging pods the apiserver thinks is on node p1-esqoe7n9eqq76k8ojd9tajh51e before test
Jul 31 11:45:46.151: INFO: csi-ridge-node-ds95v from kube-system started at 2022-07-31 10:18:00 +0000 UTC (1 container statuses recorded)
Jul 31 11:45:46.151: INFO: 	Container csi-ridge-plugin ready: true, restart count 0
Jul 31 11:45:46.151: INFO: kube-proxy-wkgc5 from kube-system started at 2022-07-31 10:17:50 +0000 UTC (1 container statuses recorded)
Jul 31 11:45:46.151: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 31 11:45:46.151: INFO: meta-h5cxg from kube-system started at 2022-07-31 10:17:50 +0000 UTC (1 container statuses recorded)
Jul 31 11:45:46.152: INFO: 	Container meta ready: true, restart count 0
Jul 31 11:45:46.152: INFO: weave-net-h6vbc from kube-system started at 2022-07-31 10:17:59 +0000 UTC (2 container statuses recorded)
Jul 31 11:45:46.152: INFO: 	Container weave ready: true, restart count 1
Jul 31 11:45:46.152: INFO: 	Container weave-npc ready: true, restart count 0
Jul 31 11:45:46.152: INFO: sonobuoy from sonobuoy started at 2022-07-31 10:30:49 +0000 UTC (1 container statuses recorded)
Jul 31 11:45:46.152: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 31 11:45:46.152: INFO: sonobuoy-systemd-logs-daemon-set-b467c6d60405477d-grkth from sonobuoy started at 2022-07-31 10:30:53 +0000 UTC (2 container statuses recorded)
Jul 31 11:45:46.152: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 31 11:45:46.152: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 31 11:45:46.152: INFO: 
Logging pods the apiserver thinks is on node p1-mz5rqwt5oar4y4n97cfumgzrxe before test
Jul 31 11:45:46.163: INFO: csi-ridge-node-xs7qk from kube-system started at 2022-07-31 10:18:00 +0000 UTC (1 container statuses recorded)
Jul 31 11:45:46.163: INFO: 	Container csi-ridge-plugin ready: true, restart count 0
Jul 31 11:45:46.163: INFO: kube-proxy-hhznq from kube-system started at 2022-07-31 10:17:50 +0000 UTC (1 container statuses recorded)
Jul 31 11:45:46.163: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 31 11:45:46.163: INFO: meta-dmrhb from kube-system started at 2022-07-31 10:17:50 +0000 UTC (1 container statuses recorded)
Jul 31 11:45:46.163: INFO: 	Container meta ready: true, restart count 0
Jul 31 11:45:46.163: INFO: weave-net-wclcv from kube-system started at 2022-07-31 10:17:59 +0000 UTC (2 container statuses recorded)
Jul 31 11:45:46.163: INFO: 	Container weave ready: true, restart count 1
Jul 31 11:45:46.163: INFO: 	Container weave-npc ready: true, restart count 0
Jul 31 11:45:46.163: INFO: sonobuoy-e2e-job-d80ae37bee83480b from sonobuoy started at 2022-07-31 10:30:53 +0000 UTC (2 container statuses recorded)
Jul 31 11:45:46.163: INFO: 	Container e2e ready: true, restart count 0
Jul 31 11:45:46.163: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 31 11:45:46.163: INFO: sonobuoy-systemd-logs-daemon-set-b467c6d60405477d-p7jjn from sonobuoy started at 2022-07-31 10:30:53 +0000 UTC (2 container statuses recorded)
Jul 31 11:45:46.163: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 31 11:45:46.163: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 31 11:45:46.163: INFO: 
Logging pods the apiserver thinks is on node p1-nc3cz44465xd41a6poxnmrpeqo before test
Jul 31 11:45:46.185: INFO: csi-ridge-node-t7hf9 from kube-system started at 2022-07-31 10:18:00 +0000 UTC (1 container statuses recorded)
Jul 31 11:45:46.185: INFO: 	Container csi-ridge-plugin ready: true, restart count 0
Jul 31 11:45:46.185: INFO: kube-proxy-g26h6 from kube-system started at 2022-07-31 10:17:50 +0000 UTC (1 container statuses recorded)
Jul 31 11:45:46.185: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 31 11:45:46.185: INFO: meta-9crb7 from kube-system started at 2022-07-31 10:17:50 +0000 UTC (1 container statuses recorded)
Jul 31 11:45:46.185: INFO: 	Container meta ready: true, restart count 0
Jul 31 11:45:46.185: INFO: weave-net-cm7ct from kube-system started at 2022-07-31 10:17:59 +0000 UTC (2 container statuses recorded)
Jul 31 11:45:46.185: INFO: 	Container weave ready: true, restart count 1
Jul 31 11:45:46.185: INFO: 	Container weave-npc ready: true, restart count 0
Jul 31 11:45:46.185: INFO: sonobuoy-systemd-logs-daemon-set-b467c6d60405477d-w6txb from sonobuoy started at 2022-07-31 10:30:53 +0000 UTC (2 container statuses recorded)
Jul 31 11:45:46.185: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 31 11:45:46.185: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-828509da-9e85-44d6-b622-d35ff2c14d32 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-828509da-9e85-44d6-b622-d35ff2c14d32 off the node p1-eipeq63rfgo6ooocmu3kqk3tcc
STEP: verifying the node doesn't have the label kubernetes.io/e2e-828509da-9e85-44d6-b622-d35ff2c14d32
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:45:50.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9774" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":346,"completed":237,"skipped":4370,"failed":0}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:45:50.337: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 31 11:45:50.921: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 31 11:45:53.952: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:45:54.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6199" for this suite.
STEP: Destroying namespace "webhook-6199-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":346,"completed":238,"skipped":4373,"failed":0}
S
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:45:54.159: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl
STEP: Watching for error events or started pod
STEP: Waiting for pod completion
STEP: Checking that the pod succeeded
STEP: Getting logs from the pod
STEP: Checking that the sysctl is actually updated
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:45:58.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-2177" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":239,"skipped":4374,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:45:58.300: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating replication controller my-hostname-basic-9eadf63c-273f-4358-87ec-c2120d970309
Jul 31 11:45:58.368: INFO: Pod name my-hostname-basic-9eadf63c-273f-4358-87ec-c2120d970309: Found 0 pods out of 1
Jul 31 11:46:03.380: INFO: Pod name my-hostname-basic-9eadf63c-273f-4358-87ec-c2120d970309: Found 1 pods out of 1
Jul 31 11:46:03.380: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-9eadf63c-273f-4358-87ec-c2120d970309" are running
Jul 31 11:46:03.383: INFO: Pod "my-hostname-basic-9eadf63c-273f-4358-87ec-c2120d970309-89h8w" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-07-31 11:45:58 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-07-31 11:46:00 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-07-31 11:46:00 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2022-07-31 11:45:58 +0000 UTC Reason: Message:}])
Jul 31 11:46:03.383: INFO: Trying to dial the pod
Jul 31 11:46:08.411: INFO: Controller my-hostname-basic-9eadf63c-273f-4358-87ec-c2120d970309: Got expected result from replica 1 [my-hostname-basic-9eadf63c-273f-4358-87ec-c2120d970309-89h8w]: "my-hostname-basic-9eadf63c-273f-4358-87ec-c2120d970309-89h8w", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:46:08.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9914" for this suite.

• [SLOW TEST:10.132 seconds]
[sig-apps] ReplicationController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":346,"completed":240,"skipped":4419,"failed":0}
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:46:08.433: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-map-1ee356ea-f27a-4589-8b90-e15d76371b91
STEP: Creating a pod to test consume configMaps
Jul 31 11:46:08.532: INFO: Waiting up to 5m0s for pod "pod-configmaps-8c19d1c1-b75c-41f4-8543-aca4c33f4ee3" in namespace "configmap-5217" to be "Succeeded or Failed"
Jul 31 11:46:08.554: INFO: Pod "pod-configmaps-8c19d1c1-b75c-41f4-8543-aca4c33f4ee3": Phase="Pending", Reason="", readiness=false. Elapsed: 22.203655ms
Jul 31 11:46:10.570: INFO: Pod "pod-configmaps-8c19d1c1-b75c-41f4-8543-aca4c33f4ee3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038560442s
Jul 31 11:46:12.578: INFO: Pod "pod-configmaps-8c19d1c1-b75c-41f4-8543-aca4c33f4ee3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046296511s
Jul 31 11:46:14.589: INFO: Pod "pod-configmaps-8c19d1c1-b75c-41f4-8543-aca4c33f4ee3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.057076447s
STEP: Saw pod success
Jul 31 11:46:14.589: INFO: Pod "pod-configmaps-8c19d1c1-b75c-41f4-8543-aca4c33f4ee3" satisfied condition "Succeeded or Failed"
Jul 31 11:46:14.593: INFO: Trying to get logs from node p1-eipeq63rfgo6ooocmu3kqk3tcc pod pod-configmaps-8c19d1c1-b75c-41f4-8543-aca4c33f4ee3 container agnhost-container: <nil>
STEP: delete the pod
Jul 31 11:46:14.646: INFO: Waiting for pod pod-configmaps-8c19d1c1-b75c-41f4-8543-aca4c33f4ee3 to disappear
Jul 31 11:46:14.651: INFO: Pod pod-configmaps-8c19d1c1-b75c-41f4-8543-aca4c33f4ee3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:46:14.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5217" for this suite.

• [SLOW TEST:6.242 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":241,"skipped":4423,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:46:14.677: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
Jul 31 11:46:14.824: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jul 31 11:46:16.833: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jul 31 11:46:18.836: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the pod with lifecycle hook
Jul 31 11:46:18.852: INFO: The status of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jul 31 11:46:20.858: INFO: The status of Pod pod-with-prestop-exec-hook is Running (Ready = true)
STEP: delete the pod with lifecycle hook
Jul 31 11:46:20.872: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 31 11:46:20.876: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 31 11:46:22.876: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 31 11:46:22.883: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 31 11:46:24.877: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 31 11:46:24.891: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:46:24.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5905" for this suite.

• [SLOW TEST:10.247 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":346,"completed":242,"skipped":4497,"failed":0}
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:46:24.924: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
Jul 31 11:46:25.031: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jul 31 11:46:27.044: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the pod with lifecycle hook
Jul 31 11:46:27.058: INFO: The status of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jul 31 11:46:29.072: INFO: The status of Pod pod-with-poststart-http-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jul 31 11:46:29.098: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 31 11:46:29.103: INFO: Pod pod-with-poststart-http-hook still exists
Jul 31 11:46:31.104: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 31 11:46:31.117: INFO: Pod pod-with-poststart-http-hook still exists
Jul 31 11:46:33.105: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 31 11:46:33.114: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:46:33.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4274" for this suite.

• [SLOW TEST:8.207 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":346,"completed":243,"skipped":4498,"failed":0}
SSSSSSSSS
------------------------------
[sig-node] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:46:33.138: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:59
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:47:33.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-507" for this suite.

• [SLOW TEST:60.122 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":346,"completed":244,"skipped":4507,"failed":0}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:47:33.261: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Jul 31 11:47:33.346: INFO: Waiting up to 5m0s for pod "downwardapi-volume-108bea12-6669-44da-8af8-313c2a8b33d7" in namespace "projected-1771" to be "Succeeded or Failed"
Jul 31 11:47:33.350: INFO: Pod "downwardapi-volume-108bea12-6669-44da-8af8-313c2a8b33d7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.818493ms
Jul 31 11:47:35.366: INFO: Pod "downwardapi-volume-108bea12-6669-44da-8af8-313c2a8b33d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020219121s
Jul 31 11:47:37.379: INFO: Pod "downwardapi-volume-108bea12-6669-44da-8af8-313c2a8b33d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03307748s
STEP: Saw pod success
Jul 31 11:47:37.379: INFO: Pod "downwardapi-volume-108bea12-6669-44da-8af8-313c2a8b33d7" satisfied condition "Succeeded or Failed"
Jul 31 11:47:37.382: INFO: Trying to get logs from node p1-eipeq63rfgo6ooocmu3kqk3tcc pod downwardapi-volume-108bea12-6669-44da-8af8-313c2a8b33d7 container client-container: <nil>
STEP: delete the pod
Jul 31 11:47:37.432: INFO: Waiting for pod downwardapi-volume-108bea12-6669-44da-8af8-313c2a8b33d7 to disappear
Jul 31 11:47:37.438: INFO: Pod downwardapi-volume-108bea12-6669-44da-8af8-313c2a8b33d7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:47:37.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1771" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":346,"completed":245,"skipped":4508,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:47:37.465: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 31 11:47:38.853: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 31 11:47:41.922: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:47:54.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5883" for this suite.
STEP: Destroying namespace "webhook-5883-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:16.791 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":346,"completed":246,"skipped":4541,"failed":0}
SSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:47:54.256: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should support --unix-socket=/path  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Starting the proxy
Jul 31 11:47:54.319: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-1699 proxy --unix-socket=/tmp/kubectl-proxy-unix4281137690/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:47:54.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1699" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":346,"completed":247,"skipped":4547,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:47:54.405: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:48:10.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7245" for this suite.

• [SLOW TEST:16.340 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":346,"completed":248,"skipped":4571,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:48:10.745: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1333
STEP: creating the pod
Jul 31 11:48:10.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-4816 create -f -'
Jul 31 11:48:11.604: INFO: stderr: ""
Jul 31 11:48:11.604: INFO: stdout: "pod/pause created\n"
Jul 31 11:48:11.604: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jul 31 11:48:11.604: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-4816" to be "running and ready"
Jul 31 11:48:11.623: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 19.040171ms
Jul 31 11:48:13.637: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.033087836s
Jul 31 11:48:13.637: INFO: Pod "pause" satisfied condition "running and ready"
Jul 31 11:48:13.637: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: adding the label testing-label with value testing-label-value to a pod
Jul 31 11:48:13.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-4816 label pods pause testing-label=testing-label-value'
Jul 31 11:48:13.729: INFO: stderr: ""
Jul 31 11:48:13.729: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jul 31 11:48:13.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-4816 get pod pause -L testing-label'
Jul 31 11:48:13.801: INFO: stderr: ""
Jul 31 11:48:13.801: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jul 31 11:48:13.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-4816 label pods pause testing-label-'
Jul 31 11:48:13.893: INFO: stderr: ""
Jul 31 11:48:13.893: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jul 31 11:48:13.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-4816 get pod pause -L testing-label'
Jul 31 11:48:13.974: INFO: stderr: ""
Jul 31 11:48:13.974: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1339
STEP: using delete to clean up resources
Jul 31 11:48:13.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-4816 delete --grace-period=0 --force -f -'
Jul 31 11:48:15.105: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 31 11:48:15.105: INFO: stdout: "pod \"pause\" force deleted\n"
Jul 31 11:48:15.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-4816 get rc,svc -l name=pause --no-headers'
Jul 31 11:48:15.189: INFO: stderr: "No resources found in kubectl-4816 namespace.\n"
Jul 31 11:48:15.189: INFO: stdout: ""
Jul 31 11:48:15.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-4816 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 31 11:48:15.286: INFO: stderr: ""
Jul 31 11:48:15.286: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:48:15.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4816" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":346,"completed":249,"skipped":4593,"failed":0}
SS
------------------------------
[sig-apps] ReplicaSet 
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:48:15.306: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 11:48:15.384: INFO: Pod name sample-pod: Found 0 pods out of 1
Jul 31 11:48:20.397: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: Scaling up "test-rs" replicaset 
Jul 31 11:48:20.407: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet
Jul 31 11:48:20.419: INFO: observed ReplicaSet test-rs in namespace replicaset-4349 with ReadyReplicas 1, AvailableReplicas 1
Jul 31 11:48:20.505: INFO: observed ReplicaSet test-rs in namespace replicaset-4349 with ReadyReplicas 1, AvailableReplicas 1
Jul 31 11:48:20.544: INFO: observed ReplicaSet test-rs in namespace replicaset-4349 with ReadyReplicas 1, AvailableReplicas 1
Jul 31 11:48:20.559: INFO: observed ReplicaSet test-rs in namespace replicaset-4349 with ReadyReplicas 1, AvailableReplicas 1
Jul 31 11:48:22.672: INFO: observed ReplicaSet test-rs in namespace replicaset-4349 with ReadyReplicas 2, AvailableReplicas 2
Jul 31 11:48:22.805: INFO: observed Replicaset test-rs in namespace replicaset-4349 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:48:22.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4349" for this suite.

• [SLOW TEST:7.516 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","total":346,"completed":250,"skipped":4595,"failed":0}
SSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:48:22.822: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Jul 31 11:48:22.957: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:48:22.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4384" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":346,"completed":251,"skipped":4600,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:48:23.008: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 11:48:23.115: INFO: The status of Pod pod-secrets-a3459ff1-0e26-4957-99e8-064f6cdefa11 is Pending, waiting for it to be Running (with Ready = true)
Jul 31 11:48:25.127: INFO: The status of Pod pod-secrets-a3459ff1-0e26-4957-99e8-064f6cdefa11 is Running (Ready = true)
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:48:25.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-2850" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":346,"completed":252,"skipped":4617,"failed":0}

------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:48:25.216: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Given a Pod with a 'name' label pod-adoption-release is created
Jul 31 11:48:25.327: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jul 31 11:48:27.339: INFO: The status of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jul 31 11:48:29.341: INFO: The status of Pod pod-adoption-release is Running (Ready = true)
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jul 31 11:48:30.373: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:48:31.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3451" for this suite.

• [SLOW TEST:6.205 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":346,"completed":253,"skipped":4617,"failed":0}
SSSSSS
------------------------------
[sig-apps] DisruptionController 
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:48:31.422: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[It] should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pdb
STEP: Waiting for the pdb to be processed
STEP: updating the pdb
STEP: Waiting for the pdb to be processed
STEP: patching the pdb
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be deleted
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:48:37.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-348" for this suite.

• [SLOW TEST:6.177 seconds]
[sig-apps] DisruptionController
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","total":346,"completed":254,"skipped":4623,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:48:37.601: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
Jul 31 11:48:37.650: INFO: PodSpec: initContainers in spec.initContainers
Jul 31 11:49:20.038: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-e9017547-2974-4d17-84d1-bbf2dc0482af", GenerateName:"", Namespace:"init-container-3514", SelfLink:"", UID:"459d5053-01d5-40d3-905a-d4a2e036bb3b", ResourceVersion:"33621", Generation:0, CreationTimestamp:time.Date(2022, time.July, 31, 11, 48, 37, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"650009654"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.July, 31, 11, 48, 37, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004066078), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2022, time.July, 31, 11, 48, 39, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0040660a8), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-lh9q2", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0044c7e00), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-lh9q2", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"Always", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-lh9q2", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"Always", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.6", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-lh9q2", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"Always", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001f7e908), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"p1-nc3cz44465xd41a6poxnmrpeqo", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0014b4000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001f7e980)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001f7e9a0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001f7e9a8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc001f7e9ac), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0021ee2f0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.July, 31, 11, 48, 37, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.July, 31, 11, 48, 37, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.July, 31, 11, 48, 37, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2022, time.July, 31, 11, 48, 37, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.27.21.79", PodIP:"172.28.64.1", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.28.64.1"}}, StartTime:time.Date(2022, time.July, 31, 11, 48, 37, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0014b4150)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0014b41c0)}, Ready:false, RestartCount:3, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"docker-pullable://k8s.gcr.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"docker://109d72e0cd5e13fd6225630fc5d709276aa25fd765e1e74442eaef0c7e05f81f", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0044c7e80), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0044c7e60), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.6", ImageID:"", ContainerID:"", Started:(*bool)(0xc001f7eb2f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:49:20.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3514" for this suite.

• [SLOW TEST:42.474 seconds]
[sig-node] InitContainer [NodeConformance]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":346,"completed":255,"skipped":4638,"failed":0}
SSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:49:20.076: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Pods Set QOS Class
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [sig-node] Pods Extended
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:49:20.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8369" for this suite.
•{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":346,"completed":256,"skipped":4643,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:49:20.345: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 11:49:20.490: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jul 31 11:49:20.501: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:20.502: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:20.502: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:20.506: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 31 11:49:20.506: INFO: Node p1-ashfcfj4pzo6aemt1j3a9ah7ew is running 0 daemon pod, expected 1
Jul 31 11:49:21.517: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:21.518: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:21.518: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:21.522: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 31 11:49:21.522: INFO: Node p1-ashfcfj4pzo6aemt1j3a9ah7ew is running 0 daemon pod, expected 1
Jul 31 11:49:22.527: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:22.528: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:22.528: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:22.533: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jul 31 11:49:22.533: INFO: Node p1-ashfcfj4pzo6aemt1j3a9ah7ew is running 0 daemon pod, expected 1
Jul 31 11:49:23.516: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:23.516: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:23.516: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:23.521: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jul 31 11:49:23.521: INFO: Node p1-nc3cz44465xd41a6poxnmrpeqo is running 0 daemon pod, expected 1
Jul 31 11:49:24.520: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:24.520: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:24.520: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:24.525: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Jul 31 11:49:24.525: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jul 31 11:49:24.563: INFO: Wrong image for pod: daemon-set-5wmkt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jul 31 11:49:24.563: INFO: Wrong image for pod: daemon-set-7t6zn. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jul 31 11:49:24.563: INFO: Wrong image for pod: daemon-set-bwpcm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jul 31 11:49:24.563: INFO: Wrong image for pod: daemon-set-l8lvw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jul 31 11:49:24.563: INFO: Wrong image for pod: daemon-set-zvpx9. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jul 31 11:49:24.568: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:24.568: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:24.569: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:25.585: INFO: Wrong image for pod: daemon-set-5wmkt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jul 31 11:49:25.585: INFO: Wrong image for pod: daemon-set-bwpcm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jul 31 11:49:25.585: INFO: Wrong image for pod: daemon-set-l8lvw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jul 31 11:49:25.585: INFO: Wrong image for pod: daemon-set-zvpx9. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jul 31 11:49:25.592: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:25.592: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:25.592: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:26.582: INFO: Wrong image for pod: daemon-set-5wmkt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jul 31 11:49:26.582: INFO: Wrong image for pod: daemon-set-bwpcm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jul 31 11:49:26.582: INFO: Wrong image for pod: daemon-set-l8lvw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jul 31 11:49:26.582: INFO: Wrong image for pod: daemon-set-zvpx9. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jul 31 11:49:26.588: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:26.588: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:26.588: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:27.577: INFO: Pod daemon-set-5p5xh is not available
Jul 31 11:49:27.577: INFO: Wrong image for pod: daemon-set-5wmkt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jul 31 11:49:27.577: INFO: Wrong image for pod: daemon-set-bwpcm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jul 31 11:49:27.577: INFO: Wrong image for pod: daemon-set-l8lvw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jul 31 11:49:27.577: INFO: Wrong image for pod: daemon-set-zvpx9. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jul 31 11:49:27.585: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:27.585: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:27.585: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:28.581: INFO: Pod daemon-set-5p5xh is not available
Jul 31 11:49:28.581: INFO: Wrong image for pod: daemon-set-5wmkt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jul 31 11:49:28.581: INFO: Wrong image for pod: daemon-set-bwpcm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jul 31 11:49:28.581: INFO: Wrong image for pod: daemon-set-l8lvw. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jul 31 11:49:28.581: INFO: Wrong image for pod: daemon-set-zvpx9. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jul 31 11:49:28.587: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:28.588: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:28.588: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:29.579: INFO: Wrong image for pod: daemon-set-5wmkt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jul 31 11:49:29.579: INFO: Wrong image for pod: daemon-set-bwpcm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jul 31 11:49:29.579: INFO: Wrong image for pod: daemon-set-zvpx9. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jul 31 11:49:29.587: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:29.587: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:29.587: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:30.580: INFO: Wrong image for pod: daemon-set-5wmkt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jul 31 11:49:30.580: INFO: Wrong image for pod: daemon-set-bwpcm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jul 31 11:49:30.580: INFO: Wrong image for pod: daemon-set-zvpx9. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jul 31 11:49:30.587: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:30.587: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:30.587: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:31.582: INFO: Wrong image for pod: daemon-set-5wmkt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jul 31 11:49:31.582: INFO: Wrong image for pod: daemon-set-bwpcm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jul 31 11:49:31.582: INFO: Pod daemon-set-rdppx is not available
Jul 31 11:49:31.582: INFO: Wrong image for pod: daemon-set-zvpx9. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jul 31 11:49:31.590: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:31.590: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:31.590: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:32.580: INFO: Wrong image for pod: daemon-set-5wmkt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jul 31 11:49:32.580: INFO: Wrong image for pod: daemon-set-bwpcm. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jul 31 11:49:32.580: INFO: Pod daemon-set-rdppx is not available
Jul 31 11:49:32.580: INFO: Wrong image for pod: daemon-set-zvpx9. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jul 31 11:49:32.587: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:32.587: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:32.587: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:33.582: INFO: Wrong image for pod: daemon-set-5wmkt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jul 31 11:49:33.582: INFO: Wrong image for pod: daemon-set-zvpx9. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jul 31 11:49:33.590: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:33.590: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:33.590: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:34.580: INFO: Wrong image for pod: daemon-set-5wmkt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jul 31 11:49:34.580: INFO: Pod daemon-set-k2krz is not available
Jul 31 11:49:34.580: INFO: Wrong image for pod: daemon-set-zvpx9. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jul 31 11:49:34.587: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:34.587: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:34.587: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:35.580: INFO: Wrong image for pod: daemon-set-5wmkt. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jul 31 11:49:35.580: INFO: Pod daemon-set-k2krz is not available
Jul 31 11:49:35.580: INFO: Wrong image for pod: daemon-set-zvpx9. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jul 31 11:49:35.586: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:35.586: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:35.586: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:36.582: INFO: Wrong image for pod: daemon-set-zvpx9. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jul 31 11:49:36.589: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:36.589: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:36.589: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:37.577: INFO: Pod daemon-set-scmcw is not available
Jul 31 11:49:37.577: INFO: Wrong image for pod: daemon-set-zvpx9. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jul 31 11:49:37.586: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:37.586: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:37.586: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:38.585: INFO: Pod daemon-set-scmcw is not available
Jul 31 11:49:38.585: INFO: Wrong image for pod: daemon-set-zvpx9. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.39, got: k8s.gcr.io/e2e-test-images/httpd:2.4.38-2.
Jul 31 11:49:38.593: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:38.594: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:38.594: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:39.588: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:39.588: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:39.588: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:40.789: INFO: Pod daemon-set-ptjz2 is not available
Jul 31 11:49:40.796: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:40.796: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:40.796: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Jul 31 11:49:40.802: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:40.802: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:40.802: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:40.806: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jul 31 11:49:40.806: INFO: Node p1-eipeq63rfgo6ooocmu3kqk3tcc is running 0 daemon pod, expected 1
Jul 31 11:49:41.820: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:41.820: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:41.820: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:41.826: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Jul 31 11:49:41.826: INFO: Node p1-eipeq63rfgo6ooocmu3kqk3tcc is running 0 daemon pod, expected 1
Jul 31 11:49:42.816: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:42.816: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:42.816: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 11:49:42.820: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Jul 31 11:49:42.820: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3532, will wait for the garbage collector to delete the pods
Jul 31 11:49:42.905: INFO: Deleting DaemonSet.extensions daemon-set took: 11.281544ms
Jul 31 11:49:43.106: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.90692ms
Jul 31 11:49:45.136: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 31 11:49:45.136: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jul 31 11:49:45.141: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33913"},"items":null}

Jul 31 11:49:45.144: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33913"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:49:45.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3532" for this suite.

• [SLOW TEST:24.843 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":346,"completed":257,"skipped":4659,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:49:45.191: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Performing setup for networking test in namespace pod-network-test-5668
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 31 11:49:45.266: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jul 31 11:49:45.459: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 31 11:49:47.465: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 31 11:49:49.470: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 31 11:49:51.467: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 31 11:49:53.471: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 31 11:49:55.476: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 31 11:49:57.468: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 31 11:49:59.474: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 31 11:50:01.474: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 31 11:50:03.470: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 31 11:50:05.470: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 31 11:50:07.465: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jul 31 11:50:07.472: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jul 31 11:50:07.482: INFO: The status of Pod netserver-2 is Running (Ready = true)
Jul 31 11:50:07.489: INFO: The status of Pod netserver-3 is Running (Ready = true)
Jul 31 11:50:07.496: INFO: The status of Pod netserver-4 is Running (Ready = true)
STEP: Creating test pods
Jul 31 11:50:09.583: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
Jul 31 11:50:09.583: INFO: Going to poll 172.28.80.1 on port 8083 at least 0 times, with a maximum of 55 tries before failing
Jul 31 11:50:09.588: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.28.80.1:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5668 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 31 11:50:09.588: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
Jul 31 11:50:09.589: INFO: ExecWithOptions: Clientset creation
Jul 31 11:50:09.589: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5668/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.28.80.1%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Jul 31 11:50:09.782: INFO: Found all 1 expected endpoints: [netserver-0]
Jul 31 11:50:09.782: INFO: Going to poll 172.28.0.2 on port 8083 at least 0 times, with a maximum of 55 tries before failing
Jul 31 11:50:09.789: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.28.0.2:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5668 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 31 11:50:09.789: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
Jul 31 11:50:09.790: INFO: ExecWithOptions: Clientset creation
Jul 31 11:50:09.790: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5668/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.28.0.2%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Jul 31 11:50:09.963: INFO: Found all 1 expected endpoints: [netserver-1]
Jul 31 11:50:09.963: INFO: Going to poll 172.28.176.2 on port 8083 at least 0 times, with a maximum of 55 tries before failing
Jul 31 11:50:09.970: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.28.176.2:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5668 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 31 11:50:09.971: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
Jul 31 11:50:09.972: INFO: ExecWithOptions: Clientset creation
Jul 31 11:50:09.972: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5668/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.28.176.2%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Jul 31 11:50:10.127: INFO: Found all 1 expected endpoints: [netserver-2]
Jul 31 11:50:10.127: INFO: Going to poll 172.28.96.2 on port 8083 at least 0 times, with a maximum of 55 tries before failing
Jul 31 11:50:10.132: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.28.96.2:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5668 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 31 11:50:10.132: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
Jul 31 11:50:10.133: INFO: ExecWithOptions: Clientset creation
Jul 31 11:50:10.133: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5668/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.28.96.2%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Jul 31 11:50:10.274: INFO: Found all 1 expected endpoints: [netserver-3]
Jul 31 11:50:10.274: INFO: Going to poll 172.28.64.1 on port 8083 at least 0 times, with a maximum of 55 tries before failing
Jul 31 11:50:10.283: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.28.64.1:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5668 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 31 11:50:10.283: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
Jul 31 11:50:10.284: INFO: ExecWithOptions: Clientset creation
Jul 31 11:50:10.284: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5668/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.28.64.1%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Jul 31 11:50:10.449: INFO: Found all 1 expected endpoints: [netserver-4]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:50:10.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5668" for this suite.

• [SLOW TEST:25.280 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":258,"skipped":4672,"failed":0}
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:50:10.472: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with secret that has name projected-secret-test-a4ada9ae-f3ba-4990-8bcf-915638abccb4
STEP: Creating a pod to test consume secrets
Jul 31 11:50:10.566: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-17da7968-219f-4477-9e13-70f0d3829b51" in namespace "projected-9851" to be "Succeeded or Failed"
Jul 31 11:50:10.588: INFO: Pod "pod-projected-secrets-17da7968-219f-4477-9e13-70f0d3829b51": Phase="Pending", Reason="", readiness=false. Elapsed: 21.784274ms
Jul 31 11:50:12.595: INFO: Pod "pod-projected-secrets-17da7968-219f-4477-9e13-70f0d3829b51": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02910168s
Jul 31 11:50:14.614: INFO: Pod "pod-projected-secrets-17da7968-219f-4477-9e13-70f0d3829b51": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047736427s
STEP: Saw pod success
Jul 31 11:50:14.614: INFO: Pod "pod-projected-secrets-17da7968-219f-4477-9e13-70f0d3829b51" satisfied condition "Succeeded or Failed"
Jul 31 11:50:14.618: INFO: Trying to get logs from node p1-nc3cz44465xd41a6poxnmrpeqo pod pod-projected-secrets-17da7968-219f-4477-9e13-70f0d3829b51 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 31 11:50:14.678: INFO: Waiting for pod pod-projected-secrets-17da7968-219f-4477-9e13-70f0d3829b51 to disappear
Jul 31 11:50:14.682: INFO: Pod pod-projected-secrets-17da7968-219f-4477-9e13-70f0d3829b51 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:50:14.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9851" for this suite.
•{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":259,"skipped":4677,"failed":0}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:50:14.705: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Jul 31 11:50:14.788: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 31 11:50:14.799: INFO: Waiting for terminating namespaces to be deleted...
Jul 31 11:50:14.802: INFO: 
Logging pods the apiserver thinks is on node p1-ashfcfj4pzo6aemt1j3a9ah7ew before test
Jul 31 11:50:14.814: INFO: csi-ridge-node-hndsv from kube-system started at 2022-07-31 10:18:00 +0000 UTC (1 container statuses recorded)
Jul 31 11:50:14.815: INFO: 	Container csi-ridge-plugin ready: true, restart count 0
Jul 31 11:50:14.815: INFO: kube-proxy-24lml from kube-system started at 2022-07-31 10:17:50 +0000 UTC (1 container statuses recorded)
Jul 31 11:50:14.815: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 31 11:50:14.815: INFO: meta-lqxb8 from kube-system started at 2022-07-31 10:17:50 +0000 UTC (1 container statuses recorded)
Jul 31 11:50:14.815: INFO: 	Container meta ready: true, restart count 0
Jul 31 11:50:14.815: INFO: weave-net-qfsdt from kube-system started at 2022-07-31 10:17:59 +0000 UTC (2 container statuses recorded)
Jul 31 11:50:14.815: INFO: 	Container weave ready: true, restart count 1
Jul 31 11:50:14.815: INFO: 	Container weave-npc ready: true, restart count 0
Jul 31 11:50:14.815: INFO: netserver-0 from pod-network-test-5668 started at 2022-07-31 11:49:45 +0000 UTC (1 container statuses recorded)
Jul 31 11:50:14.815: INFO: 	Container webserver ready: true, restart count 0
Jul 31 11:50:14.816: INFO: pod-qos-class-d052f481-2c73-42e4-b524-ba56d3f64ae6 from pods-8369 started at 2022-07-31 11:49:20 +0000 UTC (1 container statuses recorded)
Jul 31 11:50:14.816: INFO: 	Container agnhost ready: false, restart count 0
Jul 31 11:50:14.816: INFO: sonobuoy-systemd-logs-daemon-set-b467c6d60405477d-jk6fz from sonobuoy started at 2022-07-31 10:30:53 +0000 UTC (2 container statuses recorded)
Jul 31 11:50:14.816: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 31 11:50:14.816: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 31 11:50:14.816: INFO: 
Logging pods the apiserver thinks is on node p1-eipeq63rfgo6ooocmu3kqk3tcc before test
Jul 31 11:50:14.829: INFO: csi-ridge-node-zd62w from kube-system started at 2022-07-31 10:18:00 +0000 UTC (1 container statuses recorded)
Jul 31 11:50:14.829: INFO: 	Container csi-ridge-plugin ready: true, restart count 0
Jul 31 11:50:14.830: INFO: kube-proxy-zjtsg from kube-system started at 2022-07-31 10:17:50 +0000 UTC (1 container statuses recorded)
Jul 31 11:50:14.830: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 31 11:50:14.830: INFO: meta-75kgq from kube-system started at 2022-07-31 10:17:50 +0000 UTC (1 container statuses recorded)
Jul 31 11:50:14.830: INFO: 	Container meta ready: true, restart count 0
Jul 31 11:50:14.830: INFO: weave-net-wk9fw from kube-system started at 2022-07-31 10:17:59 +0000 UTC (2 container statuses recorded)
Jul 31 11:50:14.830: INFO: 	Container weave ready: true, restart count 1
Jul 31 11:50:14.830: INFO: 	Container weave-npc ready: true, restart count 0
Jul 31 11:50:14.830: INFO: host-test-container-pod from pod-network-test-5668 started at 2022-07-31 11:50:07 +0000 UTC (1 container statuses recorded)
Jul 31 11:50:14.830: INFO: 	Container agnhost-container ready: true, restart count 0
Jul 31 11:50:14.830: INFO: netserver-1 from pod-network-test-5668 started at 2022-07-31 11:49:45 +0000 UTC (1 container statuses recorded)
Jul 31 11:50:14.830: INFO: 	Container webserver ready: true, restart count 0
Jul 31 11:50:14.830: INFO: sonobuoy-systemd-logs-daemon-set-b467c6d60405477d-hs6kj from sonobuoy started at 2022-07-31 10:30:53 +0000 UTC (2 container statuses recorded)
Jul 31 11:50:14.830: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 31 11:50:14.830: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 31 11:50:14.830: INFO: 
Logging pods the apiserver thinks is on node p1-esqoe7n9eqq76k8ojd9tajh51e before test
Jul 31 11:50:14.842: INFO: csi-ridge-node-ds95v from kube-system started at 2022-07-31 10:18:00 +0000 UTC (1 container statuses recorded)
Jul 31 11:50:14.842: INFO: 	Container csi-ridge-plugin ready: true, restart count 0
Jul 31 11:50:14.842: INFO: kube-proxy-wkgc5 from kube-system started at 2022-07-31 10:17:50 +0000 UTC (1 container statuses recorded)
Jul 31 11:50:14.842: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 31 11:50:14.842: INFO: meta-h5cxg from kube-system started at 2022-07-31 10:17:50 +0000 UTC (1 container statuses recorded)
Jul 31 11:50:14.842: INFO: 	Container meta ready: true, restart count 0
Jul 31 11:50:14.842: INFO: weave-net-h6vbc from kube-system started at 2022-07-31 10:17:59 +0000 UTC (2 container statuses recorded)
Jul 31 11:50:14.842: INFO: 	Container weave ready: true, restart count 1
Jul 31 11:50:14.842: INFO: 	Container weave-npc ready: true, restart count 0
Jul 31 11:50:14.842: INFO: netserver-2 from pod-network-test-5668 started at 2022-07-31 11:49:45 +0000 UTC (1 container statuses recorded)
Jul 31 11:50:14.842: INFO: 	Container webserver ready: true, restart count 0
Jul 31 11:50:14.842: INFO: sonobuoy from sonobuoy started at 2022-07-31 10:30:49 +0000 UTC (1 container statuses recorded)
Jul 31 11:50:14.842: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 31 11:50:14.842: INFO: sonobuoy-systemd-logs-daemon-set-b467c6d60405477d-grkth from sonobuoy started at 2022-07-31 10:30:53 +0000 UTC (2 container statuses recorded)
Jul 31 11:50:14.842: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 31 11:50:14.842: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 31 11:50:14.842: INFO: 
Logging pods the apiserver thinks is on node p1-mz5rqwt5oar4y4n97cfumgzrxe before test
Jul 31 11:50:14.854: INFO: csi-ridge-node-xs7qk from kube-system started at 2022-07-31 10:18:00 +0000 UTC (1 container statuses recorded)
Jul 31 11:50:14.854: INFO: 	Container csi-ridge-plugin ready: true, restart count 0
Jul 31 11:50:14.855: INFO: kube-proxy-hhznq from kube-system started at 2022-07-31 10:17:50 +0000 UTC (1 container statuses recorded)
Jul 31 11:50:14.855: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 31 11:50:14.855: INFO: meta-dmrhb from kube-system started at 2022-07-31 10:17:50 +0000 UTC (1 container statuses recorded)
Jul 31 11:50:14.855: INFO: 	Container meta ready: true, restart count 0
Jul 31 11:50:14.855: INFO: weave-net-wclcv from kube-system started at 2022-07-31 10:17:59 +0000 UTC (2 container statuses recorded)
Jul 31 11:50:14.855: INFO: 	Container weave ready: true, restart count 1
Jul 31 11:50:14.855: INFO: 	Container weave-npc ready: true, restart count 0
Jul 31 11:50:14.856: INFO: netserver-3 from pod-network-test-5668 started at 2022-07-31 11:49:45 +0000 UTC (1 container statuses recorded)
Jul 31 11:50:14.856: INFO: 	Container webserver ready: true, restart count 0
Jul 31 11:50:14.856: INFO: sonobuoy-e2e-job-d80ae37bee83480b from sonobuoy started at 2022-07-31 10:30:53 +0000 UTC (2 container statuses recorded)
Jul 31 11:50:14.856: INFO: 	Container e2e ready: true, restart count 0
Jul 31 11:50:14.856: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 31 11:50:14.856: INFO: sonobuoy-systemd-logs-daemon-set-b467c6d60405477d-p7jjn from sonobuoy started at 2022-07-31 10:30:53 +0000 UTC (2 container statuses recorded)
Jul 31 11:50:14.856: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 31 11:50:14.856: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 31 11:50:14.857: INFO: 
Logging pods the apiserver thinks is on node p1-nc3cz44465xd41a6poxnmrpeqo before test
Jul 31 11:50:14.870: INFO: csi-ridge-node-t7hf9 from kube-system started at 2022-07-31 10:18:00 +0000 UTC (1 container statuses recorded)
Jul 31 11:50:14.870: INFO: 	Container csi-ridge-plugin ready: true, restart count 0
Jul 31 11:50:14.870: INFO: kube-proxy-g26h6 from kube-system started at 2022-07-31 10:17:50 +0000 UTC (1 container statuses recorded)
Jul 31 11:50:14.870: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 31 11:50:14.870: INFO: meta-9crb7 from kube-system started at 2022-07-31 10:17:50 +0000 UTC (1 container statuses recorded)
Jul 31 11:50:14.870: INFO: 	Container meta ready: true, restart count 0
Jul 31 11:50:14.870: INFO: weave-net-cm7ct from kube-system started at 2022-07-31 10:17:59 +0000 UTC (2 container statuses recorded)
Jul 31 11:50:14.870: INFO: 	Container weave ready: true, restart count 1
Jul 31 11:50:14.870: INFO: 	Container weave-npc ready: true, restart count 0
Jul 31 11:50:14.870: INFO: netserver-4 from pod-network-test-5668 started at 2022-07-31 11:49:45 +0000 UTC (1 container statuses recorded)
Jul 31 11:50:14.870: INFO: 	Container webserver ready: true, restart count 0
Jul 31 11:50:14.870: INFO: test-container-pod from pod-network-test-5668 started at 2022-07-31 11:50:07 +0000 UTC (1 container statuses recorded)
Jul 31 11:50:14.870: INFO: 	Container webserver ready: true, restart count 0
Jul 31 11:50:14.870: INFO: sonobuoy-systemd-logs-daemon-set-b467c6d60405477d-w6txb from sonobuoy started at 2022-07-31 10:30:53 +0000 UTC (2 container statuses recorded)
Jul 31 11:50:14.870: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 31 11:50:14.870: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1706e7a679629a56], Reason = [FailedScheduling], Message = [0/8 nodes are available: 3 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 5 node(s) didn't match Pod's node affinity/selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:50:15.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3787" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":346,"completed":260,"skipped":4698,"failed":0}
SSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath 
  runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:50:15.953: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jul 31 11:50:16.076: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 31 11:51:16.158: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:51:16.162: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:488
STEP: Finding an available node
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
Jul 31 11:51:20.287: INFO: found a healthy node: p1-eipeq63rfgo6ooocmu3kqk3tcc
[It] runs ReplicaSets to verify preemption running path [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 11:51:34.434: INFO: pods created so far: [1 1 1]
Jul 31 11:51:34.434: INFO: length of pods created so far: 3
Jul 31 11:51:36.455: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:51:43.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-8336" for this suite.
[AfterEach] PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:462
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:51:43.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-2895" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:87.669 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:451
    runs ReplicaSets to verify preemption running path [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","total":346,"completed":261,"skipped":4703,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:51:43.623: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-42669100-985d-4a00-8cc6-1d2f7b59ca3d
STEP: Creating a pod to test consume configMaps
Jul 31 11:51:43.706: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9901ff93-ea38-4f9f-8a4f-f8da38b6f37e" in namespace "projected-9490" to be "Succeeded or Failed"
Jul 31 11:51:43.727: INFO: Pod "pod-projected-configmaps-9901ff93-ea38-4f9f-8a4f-f8da38b6f37e": Phase="Pending", Reason="", readiness=false. Elapsed: 20.938221ms
Jul 31 11:51:45.743: INFO: Pod "pod-projected-configmaps-9901ff93-ea38-4f9f-8a4f-f8da38b6f37e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037193583s
Jul 31 11:51:47.753: INFO: Pod "pod-projected-configmaps-9901ff93-ea38-4f9f-8a4f-f8da38b6f37e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046968606s
Jul 31 11:51:49.765: INFO: Pod "pod-projected-configmaps-9901ff93-ea38-4f9f-8a4f-f8da38b6f37e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.058648559s
STEP: Saw pod success
Jul 31 11:51:49.765: INFO: Pod "pod-projected-configmaps-9901ff93-ea38-4f9f-8a4f-f8da38b6f37e" satisfied condition "Succeeded or Failed"
Jul 31 11:51:49.769: INFO: Trying to get logs from node p1-nc3cz44465xd41a6poxnmrpeqo pod pod-projected-configmaps-9901ff93-ea38-4f9f-8a4f-f8da38b6f37e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 31 11:51:49.810: INFO: Waiting for pod pod-projected-configmaps-9901ff93-ea38-4f9f-8a4f-f8da38b6f37e to disappear
Jul 31 11:51:49.815: INFO: Pod pod-projected-configmaps-9901ff93-ea38-4f9f-8a4f-f8da38b6f37e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:51:49.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9490" for this suite.

• [SLOW TEST:6.208 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":262,"skipped":4725,"failed":0}
SSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:51:49.833: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6347.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-6347.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6347.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-6347.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6347.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-6347.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6347.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-6347.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 31 11:51:53.931: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local from pod dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db: the server could not find the requested resource (get pods dns-test-622baedc-69d6-470e-9064-42509c91e3db)
Jul 31 11:51:53.935: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local from pod dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db: the server could not find the requested resource (get pods dns-test-622baedc-69d6-470e-9064-42509c91e3db)
Jul 31 11:51:53.939: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6347.svc.cluster.local from pod dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db: the server could not find the requested resource (get pods dns-test-622baedc-69d6-470e-9064-42509c91e3db)
Jul 31 11:51:53.943: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6347.svc.cluster.local from pod dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db: the server could not find the requested resource (get pods dns-test-622baedc-69d6-470e-9064-42509c91e3db)
Jul 31 11:51:53.947: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local from pod dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db: the server could not find the requested resource (get pods dns-test-622baedc-69d6-470e-9064-42509c91e3db)
Jul 31 11:51:53.952: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local from pod dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db: the server could not find the requested resource (get pods dns-test-622baedc-69d6-470e-9064-42509c91e3db)
Jul 31 11:51:53.956: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6347.svc.cluster.local from pod dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db: the server could not find the requested resource (get pods dns-test-622baedc-69d6-470e-9064-42509c91e3db)
Jul 31 11:51:53.960: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6347.svc.cluster.local from pod dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db: the server could not find the requested resource (get pods dns-test-622baedc-69d6-470e-9064-42509c91e3db)
Jul 31 11:51:53.960: INFO: Lookups using dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6347.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6347.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local jessie_udp@dns-test-service-2.dns-6347.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6347.svc.cluster.local]

Jul 31 11:51:58.971: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local from pod dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db: the server could not find the requested resource (get pods dns-test-622baedc-69d6-470e-9064-42509c91e3db)
Jul 31 11:51:58.977: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local from pod dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db: the server could not find the requested resource (get pods dns-test-622baedc-69d6-470e-9064-42509c91e3db)
Jul 31 11:51:58.981: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6347.svc.cluster.local from pod dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db: the server could not find the requested resource (get pods dns-test-622baedc-69d6-470e-9064-42509c91e3db)
Jul 31 11:51:58.986: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6347.svc.cluster.local from pod dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db: the server could not find the requested resource (get pods dns-test-622baedc-69d6-470e-9064-42509c91e3db)
Jul 31 11:51:58.990: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local from pod dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db: the server could not find the requested resource (get pods dns-test-622baedc-69d6-470e-9064-42509c91e3db)
Jul 31 11:51:58.994: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local from pod dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db: the server could not find the requested resource (get pods dns-test-622baedc-69d6-470e-9064-42509c91e3db)
Jul 31 11:51:58.998: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6347.svc.cluster.local from pod dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db: the server could not find the requested resource (get pods dns-test-622baedc-69d6-470e-9064-42509c91e3db)
Jul 31 11:51:59.004: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6347.svc.cluster.local from pod dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db: the server could not find the requested resource (get pods dns-test-622baedc-69d6-470e-9064-42509c91e3db)
Jul 31 11:51:59.005: INFO: Lookups using dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6347.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6347.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local jessie_udp@dns-test-service-2.dns-6347.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6347.svc.cluster.local]

Jul 31 11:52:03.972: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local from pod dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db: the server could not find the requested resource (get pods dns-test-622baedc-69d6-470e-9064-42509c91e3db)
Jul 31 11:52:03.977: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local from pod dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db: the server could not find the requested resource (get pods dns-test-622baedc-69d6-470e-9064-42509c91e3db)
Jul 31 11:52:03.981: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6347.svc.cluster.local from pod dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db: the server could not find the requested resource (get pods dns-test-622baedc-69d6-470e-9064-42509c91e3db)
Jul 31 11:52:03.985: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6347.svc.cluster.local from pod dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db: the server could not find the requested resource (get pods dns-test-622baedc-69d6-470e-9064-42509c91e3db)
Jul 31 11:52:03.989: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local from pod dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db: the server could not find the requested resource (get pods dns-test-622baedc-69d6-470e-9064-42509c91e3db)
Jul 31 11:52:03.993: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local from pod dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db: the server could not find the requested resource (get pods dns-test-622baedc-69d6-470e-9064-42509c91e3db)
Jul 31 11:52:04.000: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6347.svc.cluster.local from pod dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db: the server could not find the requested resource (get pods dns-test-622baedc-69d6-470e-9064-42509c91e3db)
Jul 31 11:52:04.004: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6347.svc.cluster.local from pod dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db: the server could not find the requested resource (get pods dns-test-622baedc-69d6-470e-9064-42509c91e3db)
Jul 31 11:52:04.004: INFO: Lookups using dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6347.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6347.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local jessie_udp@dns-test-service-2.dns-6347.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6347.svc.cluster.local]

Jul 31 11:52:08.970: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local from pod dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db: the server could not find the requested resource (get pods dns-test-622baedc-69d6-470e-9064-42509c91e3db)
Jul 31 11:52:08.976: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local from pod dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db: the server could not find the requested resource (get pods dns-test-622baedc-69d6-470e-9064-42509c91e3db)
Jul 31 11:52:08.980: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6347.svc.cluster.local from pod dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db: the server could not find the requested resource (get pods dns-test-622baedc-69d6-470e-9064-42509c91e3db)
Jul 31 11:52:08.984: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6347.svc.cluster.local from pod dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db: the server could not find the requested resource (get pods dns-test-622baedc-69d6-470e-9064-42509c91e3db)
Jul 31 11:52:08.989: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local from pod dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db: the server could not find the requested resource (get pods dns-test-622baedc-69d6-470e-9064-42509c91e3db)
Jul 31 11:52:08.994: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local from pod dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db: the server could not find the requested resource (get pods dns-test-622baedc-69d6-470e-9064-42509c91e3db)
Jul 31 11:52:08.998: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6347.svc.cluster.local from pod dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db: the server could not find the requested resource (get pods dns-test-622baedc-69d6-470e-9064-42509c91e3db)
Jul 31 11:52:09.002: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6347.svc.cluster.local from pod dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db: the server could not find the requested resource (get pods dns-test-622baedc-69d6-470e-9064-42509c91e3db)
Jul 31 11:52:09.002: INFO: Lookups using dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6347.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6347.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local jessie_udp@dns-test-service-2.dns-6347.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6347.svc.cluster.local]

Jul 31 11:52:13.971: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local from pod dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db: the server could not find the requested resource (get pods dns-test-622baedc-69d6-470e-9064-42509c91e3db)
Jul 31 11:52:13.976: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local from pod dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db: the server could not find the requested resource (get pods dns-test-622baedc-69d6-470e-9064-42509c91e3db)
Jul 31 11:52:13.980: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6347.svc.cluster.local from pod dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db: the server could not find the requested resource (get pods dns-test-622baedc-69d6-470e-9064-42509c91e3db)
Jul 31 11:52:13.984: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6347.svc.cluster.local from pod dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db: the server could not find the requested resource (get pods dns-test-622baedc-69d6-470e-9064-42509c91e3db)
Jul 31 11:52:13.990: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local from pod dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db: the server could not find the requested resource (get pods dns-test-622baedc-69d6-470e-9064-42509c91e3db)
Jul 31 11:52:13.995: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local from pod dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db: the server could not find the requested resource (get pods dns-test-622baedc-69d6-470e-9064-42509c91e3db)
Jul 31 11:52:13.999: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6347.svc.cluster.local from pod dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db: the server could not find the requested resource (get pods dns-test-622baedc-69d6-470e-9064-42509c91e3db)
Jul 31 11:52:14.005: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6347.svc.cluster.local from pod dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db: the server could not find the requested resource (get pods dns-test-622baedc-69d6-470e-9064-42509c91e3db)
Jul 31 11:52:14.005: INFO: Lookups using dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6347.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6347.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local jessie_udp@dns-test-service-2.dns-6347.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6347.svc.cluster.local]

Jul 31 11:52:18.970: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local from pod dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db: the server could not find the requested resource (get pods dns-test-622baedc-69d6-470e-9064-42509c91e3db)
Jul 31 11:52:18.976: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local from pod dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db: the server could not find the requested resource (get pods dns-test-622baedc-69d6-470e-9064-42509c91e3db)
Jul 31 11:52:18.980: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6347.svc.cluster.local from pod dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db: the server could not find the requested resource (get pods dns-test-622baedc-69d6-470e-9064-42509c91e3db)
Jul 31 11:52:18.984: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6347.svc.cluster.local from pod dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db: the server could not find the requested resource (get pods dns-test-622baedc-69d6-470e-9064-42509c91e3db)
Jul 31 11:52:18.989: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local from pod dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db: the server could not find the requested resource (get pods dns-test-622baedc-69d6-470e-9064-42509c91e3db)
Jul 31 11:52:18.993: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local from pod dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db: the server could not find the requested resource (get pods dns-test-622baedc-69d6-470e-9064-42509c91e3db)
Jul 31 11:52:18.997: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6347.svc.cluster.local from pod dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db: the server could not find the requested resource (get pods dns-test-622baedc-69d6-470e-9064-42509c91e3db)
Jul 31 11:52:19.001: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6347.svc.cluster.local from pod dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db: the server could not find the requested resource (get pods dns-test-622baedc-69d6-470e-9064-42509c91e3db)
Jul 31 11:52:19.001: INFO: Lookups using dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6347.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6347.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local jessie_udp@dns-test-service-2.dns-6347.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6347.svc.cluster.local]

Jul 31 11:52:23.969: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local from pod dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db: the server could not find the requested resource (get pods dns-test-622baedc-69d6-470e-9064-42509c91e3db)
Jul 31 11:52:23.975: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local from pod dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db: the server could not find the requested resource (get pods dns-test-622baedc-69d6-470e-9064-42509c91e3db)
Jul 31 11:52:23.979: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6347.svc.cluster.local from pod dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db: the server could not find the requested resource (get pods dns-test-622baedc-69d6-470e-9064-42509c91e3db)
Jul 31 11:52:24.000: INFO: Lookups using dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6347.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6347.svc.cluster.local]

Jul 31 11:52:29.008: INFO: DNS probes using dns-6347/dns-test-622baedc-69d6-470e-9064-42509c91e3db succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:52:29.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6347" for this suite.

• [SLOW TEST:39.297 seconds]
[sig-network] DNS
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":346,"completed":263,"skipped":4733,"failed":0}
[sig-node] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:52:29.133: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 11:52:29.219: INFO: The status of Pod busybox-host-aliasesfc9c9502-996f-41cb-ba67-ebf1345c35fc is Pending, waiting for it to be Running (with Ready = true)
Jul 31 11:52:31.235: INFO: The status of Pod busybox-host-aliasesfc9c9502-996f-41cb-ba67-ebf1345c35fc is Pending, waiting for it to be Running (with Ready = true)
Jul 31 11:52:33.226: INFO: The status of Pod busybox-host-aliasesfc9c9502-996f-41cb-ba67-ebf1345c35fc is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:52:33.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4810" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":264,"skipped":4733,"failed":0}
SSSSSS
------------------------------
[sig-apps] ReplicaSet 
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:52:33.257: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota
Jul 31 11:52:33.351: INFO: Pod name sample-pod: Found 0 pods out of 1
Jul 31 11:52:38.357: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the replicaset Spec.Replicas was modified
STEP: Patch a scale subresource
[AfterEach] [sig-apps] ReplicaSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:52:38.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9576" for this suite.

• [SLOW TEST:5.183 seconds]
[sig-apps] ReplicaSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","total":346,"completed":265,"skipped":4739,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:52:38.442: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 11:52:38.500: INFO: Creating pod...
Jul 31 11:52:38.517: INFO: Pod Quantity: 1 Status: Pending
Jul 31 11:52:39.529: INFO: Pod Quantity: 1 Status: Pending
Jul 31 11:52:40.529: INFO: Pod Status: Running
Jul 31 11:52:40.529: INFO: Creating service...
Jul 31 11:52:40.553: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9046/pods/agnhost/proxy/some/path/with/DELETE
Jul 31 11:52:40.564: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jul 31 11:52:40.564: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9046/pods/agnhost/proxy/some/path/with/GET
Jul 31 11:52:40.570: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jul 31 11:52:40.570: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9046/pods/agnhost/proxy/some/path/with/HEAD
Jul 31 11:52:40.574: INFO: http.Client request:HEAD | StatusCode:200
Jul 31 11:52:40.574: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9046/pods/agnhost/proxy/some/path/with/OPTIONS
Jul 31 11:52:40.579: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jul 31 11:52:40.579: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9046/pods/agnhost/proxy/some/path/with/PATCH
Jul 31 11:52:40.588: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jul 31 11:52:40.588: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9046/pods/agnhost/proxy/some/path/with/POST
Jul 31 11:52:40.592: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jul 31 11:52:40.592: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9046/pods/agnhost/proxy/some/path/with/PUT
Jul 31 11:52:40.597: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jul 31 11:52:40.597: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9046/services/test-service/proxy/some/path/with/DELETE
Jul 31 11:52:40.604: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jul 31 11:52:40.604: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9046/services/test-service/proxy/some/path/with/GET
Jul 31 11:52:40.610: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jul 31 11:52:40.610: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9046/services/test-service/proxy/some/path/with/HEAD
Jul 31 11:52:40.615: INFO: http.Client request:HEAD | StatusCode:200
Jul 31 11:52:40.616: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9046/services/test-service/proxy/some/path/with/OPTIONS
Jul 31 11:52:40.620: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jul 31 11:52:40.620: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9046/services/test-service/proxy/some/path/with/PATCH
Jul 31 11:52:40.626: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jul 31 11:52:40.626: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9046/services/test-service/proxy/some/path/with/POST
Jul 31 11:52:40.631: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jul 31 11:52:40.631: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9046/services/test-service/proxy/some/path/with/PUT
Jul 31 11:52:40.636: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:52:40.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9046" for this suite.
•{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","total":346,"completed":266,"skipped":4763,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:52:40.656: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service in namespace services-9051
STEP: creating service affinity-clusterip-transition in namespace services-9051
STEP: creating replication controller affinity-clusterip-transition in namespace services-9051
I0731 11:52:40.800861      23 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-9051, replica count: 3
I0731 11:52:43.851988      23 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 31 11:52:43.864: INFO: Creating new exec pod
Jul 31 11:52:46.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-9051 exec execpod-affinityvjjss -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Jul 31 11:52:47.326: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jul 31 11:52:47.327: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 31 11:52:47.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-9051 exec execpod-affinityvjjss -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.103.107.47 80'
Jul 31 11:52:47.596: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.103.107.47 80\nConnection to 10.103.107.47 80 port [tcp/http] succeeded!\n"
Jul 31 11:52:47.596: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 31 11:52:47.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-9051 exec execpod-affinityvjjss -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.103.107.47:80/ ; done'
Jul 31 11:52:49.005: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.107.47:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.107.47:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.107.47:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.107.47:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.107.47:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.107.47:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.107.47:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.107.47:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.107.47:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.107.47:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.107.47:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.107.47:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.107.47:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.107.47:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.107.47:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.107.47:80/\n"
Jul 31 11:52:49.005: INFO: stdout: "\naffinity-clusterip-transition-fczcm\naffinity-clusterip-transition-h4qpn\naffinity-clusterip-transition-fczcm\naffinity-clusterip-transition-h4qpn\naffinity-clusterip-transition-f7hwk\naffinity-clusterip-transition-h4qpn\naffinity-clusterip-transition-f7hwk\naffinity-clusterip-transition-f7hwk\naffinity-clusterip-transition-f7hwk\naffinity-clusterip-transition-h4qpn\naffinity-clusterip-transition-f7hwk\naffinity-clusterip-transition-fczcm\naffinity-clusterip-transition-fczcm\naffinity-clusterip-transition-f7hwk\naffinity-clusterip-transition-h4qpn\naffinity-clusterip-transition-f7hwk"
Jul 31 11:52:49.005: INFO: Received response from host: affinity-clusterip-transition-fczcm
Jul 31 11:52:49.005: INFO: Received response from host: affinity-clusterip-transition-h4qpn
Jul 31 11:52:49.005: INFO: Received response from host: affinity-clusterip-transition-fczcm
Jul 31 11:52:49.005: INFO: Received response from host: affinity-clusterip-transition-h4qpn
Jul 31 11:52:49.005: INFO: Received response from host: affinity-clusterip-transition-f7hwk
Jul 31 11:52:49.005: INFO: Received response from host: affinity-clusterip-transition-h4qpn
Jul 31 11:52:49.005: INFO: Received response from host: affinity-clusterip-transition-f7hwk
Jul 31 11:52:49.005: INFO: Received response from host: affinity-clusterip-transition-f7hwk
Jul 31 11:52:49.005: INFO: Received response from host: affinity-clusterip-transition-f7hwk
Jul 31 11:52:49.005: INFO: Received response from host: affinity-clusterip-transition-h4qpn
Jul 31 11:52:49.005: INFO: Received response from host: affinity-clusterip-transition-f7hwk
Jul 31 11:52:49.005: INFO: Received response from host: affinity-clusterip-transition-fczcm
Jul 31 11:52:49.005: INFO: Received response from host: affinity-clusterip-transition-fczcm
Jul 31 11:52:49.005: INFO: Received response from host: affinity-clusterip-transition-f7hwk
Jul 31 11:52:49.005: INFO: Received response from host: affinity-clusterip-transition-h4qpn
Jul 31 11:52:49.005: INFO: Received response from host: affinity-clusterip-transition-f7hwk
Jul 31 11:52:49.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-9051 exec execpod-affinityvjjss -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.103.107.47:80/ ; done'
Jul 31 11:52:49.336: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.107.47:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.107.47:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.107.47:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.107.47:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.107.47:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.107.47:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.107.47:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.107.47:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.107.47:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.107.47:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.107.47:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.107.47:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.107.47:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.107.47:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.107.47:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.107.47:80/\n"
Jul 31 11:52:49.336: INFO: stdout: "\naffinity-clusterip-transition-h4qpn\naffinity-clusterip-transition-h4qpn\naffinity-clusterip-transition-h4qpn\naffinity-clusterip-transition-h4qpn\naffinity-clusterip-transition-h4qpn\naffinity-clusterip-transition-h4qpn\naffinity-clusterip-transition-h4qpn\naffinity-clusterip-transition-h4qpn\naffinity-clusterip-transition-h4qpn\naffinity-clusterip-transition-h4qpn\naffinity-clusterip-transition-h4qpn\naffinity-clusterip-transition-h4qpn\naffinity-clusterip-transition-h4qpn\naffinity-clusterip-transition-h4qpn\naffinity-clusterip-transition-h4qpn\naffinity-clusterip-transition-h4qpn"
Jul 31 11:52:49.336: INFO: Received response from host: affinity-clusterip-transition-h4qpn
Jul 31 11:52:49.336: INFO: Received response from host: affinity-clusterip-transition-h4qpn
Jul 31 11:52:49.336: INFO: Received response from host: affinity-clusterip-transition-h4qpn
Jul 31 11:52:49.336: INFO: Received response from host: affinity-clusterip-transition-h4qpn
Jul 31 11:52:49.336: INFO: Received response from host: affinity-clusterip-transition-h4qpn
Jul 31 11:52:49.336: INFO: Received response from host: affinity-clusterip-transition-h4qpn
Jul 31 11:52:49.336: INFO: Received response from host: affinity-clusterip-transition-h4qpn
Jul 31 11:52:49.336: INFO: Received response from host: affinity-clusterip-transition-h4qpn
Jul 31 11:52:49.336: INFO: Received response from host: affinity-clusterip-transition-h4qpn
Jul 31 11:52:49.336: INFO: Received response from host: affinity-clusterip-transition-h4qpn
Jul 31 11:52:49.336: INFO: Received response from host: affinity-clusterip-transition-h4qpn
Jul 31 11:52:49.336: INFO: Received response from host: affinity-clusterip-transition-h4qpn
Jul 31 11:52:49.336: INFO: Received response from host: affinity-clusterip-transition-h4qpn
Jul 31 11:52:49.336: INFO: Received response from host: affinity-clusterip-transition-h4qpn
Jul 31 11:52:49.336: INFO: Received response from host: affinity-clusterip-transition-h4qpn
Jul 31 11:52:49.337: INFO: Received response from host: affinity-clusterip-transition-h4qpn
Jul 31 11:52:49.337: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-9051, will wait for the garbage collector to delete the pods
Jul 31 11:52:49.429: INFO: Deleting ReplicationController affinity-clusterip-transition took: 12.794792ms
Jul 31 11:52:49.529: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.244473ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:52:51.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9051" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:10.832 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":267,"skipped":4805,"failed":0}
SSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:52:51.489: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [sig-node] Container Runtime
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:53:20.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3998" for this suite.

• [SLOW TEST:29.070 seconds]
[sig-node] Container Runtime
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  blackbox test
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:41
    when starting a container that exits
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":346,"completed":268,"skipped":4809,"failed":0}
SSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:53:20.560: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jul 31 11:53:20.670: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3478  11c4cc09-46bc-4819-9348-6f8305aad36f 35360 0 2022-07-31 11:53:20 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-07-31 11:53:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 31 11:53:20.671: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3478  11c4cc09-46bc-4819-9348-6f8305aad36f 35361 0 2022-07-31 11:53:20 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2022-07-31 11:53:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:53:20.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3478" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":346,"completed":269,"skipped":4812,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:53:20.690: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul 31 11:53:20.763: INFO: Waiting up to 5m0s for pod "pod-d782bfed-ed3e-4190-90fe-ebc86bdc5c25" in namespace "emptydir-9529" to be "Succeeded or Failed"
Jul 31 11:53:20.767: INFO: Pod "pod-d782bfed-ed3e-4190-90fe-ebc86bdc5c25": Phase="Pending", Reason="", readiness=false. Elapsed: 3.80317ms
Jul 31 11:53:22.779: INFO: Pod "pod-d782bfed-ed3e-4190-90fe-ebc86bdc5c25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015451661s
Jul 31 11:53:24.796: INFO: Pod "pod-d782bfed-ed3e-4190-90fe-ebc86bdc5c25": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032153207s
Jul 31 11:53:26.812: INFO: Pod "pod-d782bfed-ed3e-4190-90fe-ebc86bdc5c25": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.048210866s
STEP: Saw pod success
Jul 31 11:53:26.812: INFO: Pod "pod-d782bfed-ed3e-4190-90fe-ebc86bdc5c25" satisfied condition "Succeeded or Failed"
Jul 31 11:53:26.815: INFO: Trying to get logs from node p1-eipeq63rfgo6ooocmu3kqk3tcc pod pod-d782bfed-ed3e-4190-90fe-ebc86bdc5c25 container test-container: <nil>
STEP: delete the pod
Jul 31 11:53:26.871: INFO: Waiting for pod pod-d782bfed-ed3e-4190-90fe-ebc86bdc5c25 to disappear
Jul 31 11:53:26.875: INFO: Pod pod-d782bfed-ed3e-4190-90fe-ebc86bdc5c25 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:53:26.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9529" for this suite.

• [SLOW TEST:6.199 seconds]
[sig-storage] EmptyDir volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":270,"skipped":4909,"failed":0}
SSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] 
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:36
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:53:26.890: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename sysctl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/sysctl.go:65
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod with one valid and two invalid sysctls
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:53:26.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-9045" for this suite.
•{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","total":346,"completed":271,"skipped":4920,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-node] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:53:26.959: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-node] Docker Containers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:53:29.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-152" for this suite.
•{"msg":"PASSED [sig-node] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":346,"completed":272,"skipped":4932,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:53:29.156: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating the pod
Jul 31 11:53:29.256: INFO: The status of Pod annotationupdate48ca96b4-0180-4c74-9497-93f94712ca53 is Pending, waiting for it to be Running (with Ready = true)
Jul 31 11:53:31.476: INFO: The status of Pod annotationupdate48ca96b4-0180-4c74-9497-93f94712ca53 is Running (Ready = true)
Jul 31 11:53:32.007: INFO: Successfully updated pod "annotationupdate48ca96b4-0180-4c74-9497-93f94712ca53"
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:53:34.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1708" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":346,"completed":273,"skipped":4966,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:53:34.056: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Jul 31 11:53:44.220: INFO: The status of Pod kube-controller-manager-master-um8faxf3d4468jbdm49zh4o57y is Running (Ready = true)
Jul 31 11:53:44.350: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:53:44.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8331" for this suite.

• [SLOW TEST:10.312 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":346,"completed":274,"skipped":5002,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:53:44.371: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test substitution in container's command
Jul 31 11:53:44.460: INFO: Waiting up to 5m0s for pod "var-expansion-f78d42ac-175a-4ea9-b52a-a295608aadf0" in namespace "var-expansion-5463" to be "Succeeded or Failed"
Jul 31 11:53:44.481: INFO: Pod "var-expansion-f78d42ac-175a-4ea9-b52a-a295608aadf0": Phase="Pending", Reason="", readiness=false. Elapsed: 21.044175ms
Jul 31 11:53:46.496: INFO: Pod "var-expansion-f78d42ac-175a-4ea9-b52a-a295608aadf0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035978941s
Jul 31 11:53:48.510: INFO: Pod "var-expansion-f78d42ac-175a-4ea9-b52a-a295608aadf0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.049618224s
STEP: Saw pod success
Jul 31 11:53:48.510: INFO: Pod "var-expansion-f78d42ac-175a-4ea9-b52a-a295608aadf0" satisfied condition "Succeeded or Failed"
Jul 31 11:53:48.513: INFO: Trying to get logs from node p1-nc3cz44465xd41a6poxnmrpeqo pod var-expansion-f78d42ac-175a-4ea9-b52a-a295608aadf0 container dapi-container: <nil>
STEP: delete the pod
Jul 31 11:53:48.542: INFO: Waiting for pod var-expansion-f78d42ac-175a-4ea9-b52a-a295608aadf0 to disappear
Jul 31 11:53:48.545: INFO: Pod var-expansion-f78d42ac-175a-4ea9-b52a-a295608aadf0 no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:53:48.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5463" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":346,"completed":275,"skipped":5044,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:53:48.571: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jul 31 11:53:48.680: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-510  8d4a7e23-39b4-4578-83b3-93c43b23cd46 35636 0 2022-07-31 11:53:48 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-07-31 11:53:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 31 11:53:48.680: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-510  8d4a7e23-39b4-4578-83b3-93c43b23cd46 35638 0 2022-07-31 11:53:48 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-07-31 11:53:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 31 11:53:48.680: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-510  8d4a7e23-39b4-4578-83b3-93c43b23cd46 35640 0 2022-07-31 11:53:48 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-07-31 11:53:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jul 31 11:53:58.724: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-510  8d4a7e23-39b4-4578-83b3-93c43b23cd46 35700 0 2022-07-31 11:53:48 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-07-31 11:53:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 31 11:53:58.724: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-510  8d4a7e23-39b4-4578-83b3-93c43b23cd46 35701 0 2022-07-31 11:53:48 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-07-31 11:53:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jul 31 11:53:58.724: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-510  8d4a7e23-39b4-4578-83b3-93c43b23cd46 35702 0 2022-07-31 11:53:48 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2022-07-31 11:53:48 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:53:58.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-510" for this suite.

• [SLOW TEST:10.167 seconds]
[sig-api-machinery] Watchers
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":346,"completed":276,"skipped":5056,"failed":0}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:53:58.739: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul 31 11:53:58.867: INFO: Waiting up to 5m0s for pod "pod-db147824-035e-4ebe-a57d-4a48b8057010" in namespace "emptydir-9055" to be "Succeeded or Failed"
Jul 31 11:53:58.892: INFO: Pod "pod-db147824-035e-4ebe-a57d-4a48b8057010": Phase="Pending", Reason="", readiness=false. Elapsed: 25.179418ms
Jul 31 11:54:00.907: INFO: Pod "pod-db147824-035e-4ebe-a57d-4a48b8057010": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0400858s
Jul 31 11:54:02.920: INFO: Pod "pod-db147824-035e-4ebe-a57d-4a48b8057010": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.053211493s
STEP: Saw pod success
Jul 31 11:54:02.920: INFO: Pod "pod-db147824-035e-4ebe-a57d-4a48b8057010" satisfied condition "Succeeded or Failed"
Jul 31 11:54:02.925: INFO: Trying to get logs from node p1-ashfcfj4pzo6aemt1j3a9ah7ew pod pod-db147824-035e-4ebe-a57d-4a48b8057010 container test-container: <nil>
STEP: delete the pod
Jul 31 11:54:02.981: INFO: Waiting for pod pod-db147824-035e-4ebe-a57d-4a48b8057010 to disappear
Jul 31 11:54:02.986: INFO: Pod pod-db147824-035e-4ebe-a57d-4a48b8057010 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:54:02.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9055" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":277,"skipped":5061,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:54:03.012: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-map-8521dc8a-bd24-4546-a0cb-d973bb3abd44
STEP: Creating a pod to test consume configMaps
Jul 31 11:54:03.140: INFO: Waiting up to 5m0s for pod "pod-configmaps-36250cd5-836e-4fb6-92a5-3ac48e145cc4" in namespace "configmap-3063" to be "Succeeded or Failed"
Jul 31 11:54:03.169: INFO: Pod "pod-configmaps-36250cd5-836e-4fb6-92a5-3ac48e145cc4": Phase="Pending", Reason="", readiness=false. Elapsed: 28.691134ms
Jul 31 11:54:05.181: INFO: Pod "pod-configmaps-36250cd5-836e-4fb6-92a5-3ac48e145cc4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041223532s
Jul 31 11:54:07.191: INFO: Pod "pod-configmaps-36250cd5-836e-4fb6-92a5-3ac48e145cc4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050755699s
STEP: Saw pod success
Jul 31 11:54:07.191: INFO: Pod "pod-configmaps-36250cd5-836e-4fb6-92a5-3ac48e145cc4" satisfied condition "Succeeded or Failed"
Jul 31 11:54:07.195: INFO: Trying to get logs from node p1-nc3cz44465xd41a6poxnmrpeqo pod pod-configmaps-36250cd5-836e-4fb6-92a5-3ac48e145cc4 container agnhost-container: <nil>
STEP: delete the pod
Jul 31 11:54:07.222: INFO: Waiting for pod pod-configmaps-36250cd5-836e-4fb6-92a5-3ac48e145cc4 to disappear
Jul 31 11:54:07.227: INFO: Pod pod-configmaps-36250cd5-836e-4fb6-92a5-3ac48e145cc4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:54:07.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3063" for this suite.
•{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":278,"skipped":5091,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:54:07.248: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:54:20.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8728" for this suite.

• [SLOW TEST:13.206 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":346,"completed":279,"skipped":5176,"failed":0}
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:54:20.454: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating the pod
Jul 31 11:54:20.540: INFO: The status of Pod labelsupdate9116ef05-cbc8-4c2d-8cbc-3a596edd85ee is Pending, waiting for it to be Running (with Ready = true)
Jul 31 11:54:22.547: INFO: The status of Pod labelsupdate9116ef05-cbc8-4c2d-8cbc-3a596edd85ee is Running (Ready = true)
Jul 31 11:54:23.089: INFO: Successfully updated pod "labelsupdate9116ef05-cbc8-4c2d-8cbc-3a596edd85ee"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:54:25.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9428" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":346,"completed":280,"skipped":5179,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:54:25.141: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test substitution in container's args
Jul 31 11:54:25.218: INFO: Waiting up to 5m0s for pod "var-expansion-1dd771df-4dd7-44dc-be53-8c0532814b5e" in namespace "var-expansion-2843" to be "Succeeded or Failed"
Jul 31 11:54:25.222: INFO: Pod "var-expansion-1dd771df-4dd7-44dc-be53-8c0532814b5e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.613712ms
Jul 31 11:54:27.235: INFO: Pod "var-expansion-1dd771df-4dd7-44dc-be53-8c0532814b5e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017053447s
Jul 31 11:54:29.254: INFO: Pod "var-expansion-1dd771df-4dd7-44dc-be53-8c0532814b5e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036102927s
STEP: Saw pod success
Jul 31 11:54:29.254: INFO: Pod "var-expansion-1dd771df-4dd7-44dc-be53-8c0532814b5e" satisfied condition "Succeeded or Failed"
Jul 31 11:54:29.258: INFO: Trying to get logs from node p1-ashfcfj4pzo6aemt1j3a9ah7ew pod var-expansion-1dd771df-4dd7-44dc-be53-8c0532814b5e container dapi-container: <nil>
STEP: delete the pod
Jul 31 11:54:29.284: INFO: Waiting for pod var-expansion-1dd771df-4dd7-44dc-be53-8c0532814b5e to disappear
Jul 31 11:54:29.286: INFO: Pod var-expansion-1dd771df-4dd7-44dc-be53-8c0532814b5e no longer exists
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:54:29.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2843" for this suite.
•{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":346,"completed":281,"skipped":5192,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:54:29.304: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should provide secure master service  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:54:29.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6654" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":346,"completed":282,"skipped":5205,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:54:29.390: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul 31 11:54:29.499: INFO: Waiting up to 5m0s for pod "pod-63c1c69e-375d-4f79-aee1-782631f2b0c2" in namespace "emptydir-3453" to be "Succeeded or Failed"
Jul 31 11:54:29.521: INFO: Pod "pod-63c1c69e-375d-4f79-aee1-782631f2b0c2": Phase="Pending", Reason="", readiness=false. Elapsed: 22.798588ms
Jul 31 11:54:31.536: INFO: Pod "pod-63c1c69e-375d-4f79-aee1-782631f2b0c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037293229s
Jul 31 11:54:33.544: INFO: Pod "pod-63c1c69e-375d-4f79-aee1-782631f2b0c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045617883s
STEP: Saw pod success
Jul 31 11:54:33.544: INFO: Pod "pod-63c1c69e-375d-4f79-aee1-782631f2b0c2" satisfied condition "Succeeded or Failed"
Jul 31 11:54:33.549: INFO: Trying to get logs from node p1-ashfcfj4pzo6aemt1j3a9ah7ew pod pod-63c1c69e-375d-4f79-aee1-782631f2b0c2 container test-container: <nil>
STEP: delete the pod
Jul 31 11:54:33.597: INFO: Waiting for pod pod-63c1c69e-375d-4f79-aee1-782631f2b0c2 to disappear
Jul 31 11:54:33.601: INFO: Pod pod-63c1c69e-375d-4f79-aee1-782631f2b0c2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:54:33.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3453" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":283,"skipped":5220,"failed":0}
SSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:54:33.617: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
Jul 31 11:54:33.669: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:54:38.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1711" for this suite.
•{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":346,"completed":284,"skipped":5228,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:54:38.136: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: set up a multi version CRD
Jul 31 11:54:38.214: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:55:00.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9811" for this suite.

• [SLOW TEST:22.764 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":346,"completed":285,"skipped":5235,"failed":0}
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:55:00.900: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Jul 31 11:55:01.009: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f366da43-a33c-41e2-8776-31ee2d87a3b0" in namespace "downward-api-7028" to be "Succeeded or Failed"
Jul 31 11:55:01.033: INFO: Pod "downwardapi-volume-f366da43-a33c-41e2-8776-31ee2d87a3b0": Phase="Pending", Reason="", readiness=false. Elapsed: 23.888806ms
Jul 31 11:55:03.263: INFO: Pod "downwardapi-volume-f366da43-a33c-41e2-8776-31ee2d87a3b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.253364927s
Jul 31 11:55:05.275: INFO: Pod "downwardapi-volume-f366da43-a33c-41e2-8776-31ee2d87a3b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.265876065s
STEP: Saw pod success
Jul 31 11:55:05.275: INFO: Pod "downwardapi-volume-f366da43-a33c-41e2-8776-31ee2d87a3b0" satisfied condition "Succeeded or Failed"
Jul 31 11:55:05.280: INFO: Trying to get logs from node p1-ashfcfj4pzo6aemt1j3a9ah7ew pod downwardapi-volume-f366da43-a33c-41e2-8776-31ee2d87a3b0 container client-container: <nil>
STEP: delete the pod
Jul 31 11:55:05.339: INFO: Waiting for pod downwardapi-volume-f366da43-a33c-41e2-8776-31ee2d87a3b0 to disappear
Jul 31 11:55:05.346: INFO: Pod downwardapi-volume-f366da43-a33c-41e2-8776-31ee2d87a3b0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:55:05.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7028" for this suite.
•{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":286,"skipped":5240,"failed":0}
SSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:55:05.374: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service multi-endpoint-test in namespace services-5887
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5887 to expose endpoints map[]
Jul 31 11:55:05.493: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Jul 31 11:55:06.519: INFO: successfully validated that service multi-endpoint-test in namespace services-5887 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-5887
Jul 31 11:55:06.567: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jul 31 11:55:08.582: INFO: The status of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jul 31 11:55:10.579: INFO: The status of Pod pod1 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5887 to expose endpoints map[pod1:[100]]
Jul 31 11:55:10.610: INFO: successfully validated that service multi-endpoint-test in namespace services-5887 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-5887
Jul 31 11:55:10.669: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jul 31 11:55:12.682: INFO: The status of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jul 31 11:55:14.688: INFO: The status of Pod pod2 is Running (Ready = true)
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5887 to expose endpoints map[pod1:[100] pod2:[101]]
Jul 31 11:55:14.718: INFO: successfully validated that service multi-endpoint-test in namespace services-5887 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods
Jul 31 11:55:14.718: INFO: Creating new exec pod
Jul 31 11:55:19.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-5887 exec execpodx6jtt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Jul 31 11:55:20.015: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Jul 31 11:55:20.015: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 31 11:55:20.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-5887 exec execpodx6jtt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.106.64.180 80'
Jul 31 11:55:20.273: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.106.64.180 80\nConnection to 10.106.64.180 80 port [tcp/http] succeeded!\n"
Jul 31 11:55:20.273: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 31 11:55:20.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-5887 exec execpodx6jtt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Jul 31 11:55:20.531: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Jul 31 11:55:20.531: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 31 11:55:20.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-5887 exec execpodx6jtt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.106.64.180 81'
Jul 31 11:55:20.752: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.106.64.180 81\nConnection to 10.106.64.180 81 port [tcp/*] succeeded!\n"
Jul 31 11:55:20.752: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-5887
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5887 to expose endpoints map[pod2:[101]]
Jul 31 11:55:20.819: INFO: successfully validated that service multi-endpoint-test in namespace services-5887 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-5887
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5887 to expose endpoints map[]
Jul 31 11:55:20.889: INFO: successfully validated that service multi-endpoint-test in namespace services-5887 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:55:20.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5887" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:15.602 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":346,"completed":287,"skipped":5243,"failed":0}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:55:20.977: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 31 11:55:22.384: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 31 11:55:25.448: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 11:55:25.462: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9840-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:55:28.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4916" for this suite.
STEP: Destroying namespace "webhook-4916-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:7.854 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":346,"completed":288,"skipped":5251,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:55:28.833: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 11:55:28.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-1599 create -f -'
Jul 31 11:55:30.044: INFO: stderr: ""
Jul 31 11:55:30.044: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jul 31 11:55:30.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-1599 create -f -'
Jul 31 11:55:31.086: INFO: stderr: ""
Jul 31 11:55:31.086: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Jul 31 11:55:32.097: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 31 11:55:32.097: INFO: Found 0 / 1
Jul 31 11:55:33.097: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 31 11:55:33.098: INFO: Found 1 / 1
Jul 31 11:55:33.098: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 31 11:55:33.103: INFO: Selector matched 1 pods for map[app:agnhost]
Jul 31 11:55:33.103: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 31 11:55:33.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-1599 describe pod agnhost-primary-bqrgg'
Jul 31 11:55:33.192: INFO: stderr: ""
Jul 31 11:55:33.192: INFO: stdout: "Name:         agnhost-primary-bqrgg\nNamespace:    kubectl-1599\nPriority:     0\nNode:         p1-ashfcfj4pzo6aemt1j3a9ah7ew/172.27.21.73\nStart Time:   Sun, 31 Jul 2022 11:55:30 +0000\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nStatus:       Running\nIP:           172.28.80.1\nIPs:\n  IP:           172.28.80.1\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   docker://19fe50919c0e819ab6b50dedb936b82fd64374e6ed785bb0f61c78ca98ef263a\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.39\n    Image ID:       docker-pullable://k8s.gcr.io/e2e-test-images/agnhost@sha256:7e8bdd271312fd25fc5ff5a8f04727be84044eb3d7d8d03611972a6752e2e11e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sun, 31 Jul 2022 11:55:31 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6fw9b (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-6fw9b:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-1599/agnhost-primary-bqrgg to p1-ashfcfj4pzo6aemt1j3a9ah7ew\n  Normal  Pulling    3s    kubelet            Pulling image \"k8s.gcr.io/e2e-test-images/agnhost:2.39\"\n  Normal  Pulled     2s    kubelet            Successfully pulled image \"k8s.gcr.io/e2e-test-images/agnhost:2.39\" in 295.850833ms\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
Jul 31 11:55:33.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-1599 describe rc agnhost-primary'
Jul 31 11:55:33.291: INFO: stderr: ""
Jul 31 11:55:33.291: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-1599\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.39\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-bqrgg\n"
Jul 31 11:55:33.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-1599 describe service agnhost-primary'
Jul 31 11:55:33.386: INFO: stderr: ""
Jul 31 11:55:33.386: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-1599\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.98.40.222\nIPs:               10.98.40.222\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.28.80.1:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jul 31 11:55:33.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-1599 describe node master-d695o3nuux4onnradaukiwqz8y'
Jul 31 11:55:33.517: INFO: stderr: ""
Jul 31 11:55:33.517: INFO: stdout: "Name:               master-d695o3nuux4onnradaukiwqz8y\nRoles:              control-plane,master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    cloud.ridge.co/managed=true\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=master-d695o3nuux4onnradaukiwqz8y\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node-role.kubernetes.io/master=\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"driver.csi.ridge.com\":\"master-d695o3nuux4onnradaukiwqz8y\"}\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sun, 31 Jul 2022 10:17:30 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  master-d695o3nuux4onnradaukiwqz8y\n  AcquireTime:     <unset>\n  RenewTime:       Sun, 31 Jul 2022 11:55:26 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Sun, 31 Jul 2022 10:18:08 +0000   Sun, 31 Jul 2022 10:18:08 +0000   WeaveIsUp                    Weave pod has set this\n  MemoryPressure       False   Sun, 31 Jul 2022 11:53:21 +0000   Sun, 31 Jul 2022 10:17:28 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Sun, 31 Jul 2022 11:53:21 +0000   Sun, 31 Jul 2022 10:17:28 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Sun, 31 Jul 2022 11:53:21 +0000   Sun, 31 Jul 2022 10:17:28 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Sun, 31 Jul 2022 11:53:21 +0000   Sun, 31 Jul 2022 10:18:12 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  172.27.21.74\n  Hostname:    master-d695o3nuux4onnradaukiwqz8y\nCapacity:\n  cpu:                2\n  ephemeral-storage:  38216108Ki\n  hugepages-2Mi:      0\n  memory:             8150544Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  35219965075\n  hugepages-2Mi:      0\n  memory:             8048144Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 350a230ed345489b9155db7b70920d16\n  System UUID:                350a230e-d345-489b-9155-db7b70920d16\n  Boot ID:                    5d43c77f-8481-47e7-8901-50136266b605\n  Kernel Version:             5.10.69-flatcar\n  OS Image:                   Flatcar Container Linux by Kinvolk 2905.2.5 (Oklo)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://19.3.15\n  Kubelet Version:            v1.23.9\n  Kube-Proxy Version:         v1.23.9\nPodCIDR:                      172.28.2.0/24\nPodCIDRs:                     172.28.2.0/24\nProviderID:                   ridge://d695o3nuux4onnradaukiwqz8y\nNon-terminated Pods:          (10 in total)\n  Namespace                   Name                                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                         ------------  ----------  ---------------  -------------  ---\n  kube-system                 cloud-controller-manager-rt9jv                               0 (0%)        0 (0%)      0 (0%)           0 (0%)         97m\n  kube-system                 csi-ridge-node-j7vc4                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         97m\n  kube-system                 kube-apiserver-master-d695o3nuux4onnradaukiwqz8y             250m (12%)    0 (0%)      0 (0%)           0 (0%)         97m\n  kube-system                 kube-controller-manager-master-d695o3nuux4onnradaukiwqz8y    200m (10%)    0 (0%)      0 (0%)           0 (0%)         97m\n  kube-system                 kube-proxy-hq2nm                                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         97m\n  kube-system                 kube-scheduler-master-d695o3nuux4onnradaukiwqz8y             100m (5%)     0 (0%)      0 (0%)           0 (0%)         97m\n  kube-system                 meta-6mtmz                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         97m\n  kube-system                 ridge-auth-kpdd2                                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         97m\n  kube-system                 weave-net-nxmjk                                              100m (5%)     0 (0%)      200Mi (2%)       0 (0%)         97m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-b467c6d60405477d-fkj6r      0 (0%)        0 (0%)      0 (0%)           0 (0%)         84m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                650m (32%)  0 (0%)\n  memory             200Mi (2%)  0 (0%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              <none>\n"
Jul 31 11:55:33.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-1599 describe namespace kubectl-1599'
Jul 31 11:55:33.600: INFO: stderr: ""
Jul 31 11:55:33.600: INFO: stdout: "Name:         kubectl-1599\nLabels:       e2e-framework=kubectl\n              e2e-run=abf6ed35-6c60-4022-ac82-39dc496d486f\n              kubernetes.io/metadata.name=kubectl-1599\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:55:33.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1599" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":346,"completed":289,"skipped":5338,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:55:33.620: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jul 31 11:55:34.488: INFO: Pod name wrapped-volume-race-53a4be7c-9ec4-4bde-ab6a-35a63bd13d1a: Found 0 pods out of 5
Jul 31 11:55:39.514: INFO: Pod name wrapped-volume-race-53a4be7c-9ec4-4bde-ab6a-35a63bd13d1a: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-53a4be7c-9ec4-4bde-ab6a-35a63bd13d1a in namespace emptydir-wrapper-4104, will wait for the garbage collector to delete the pods
Jul 31 11:55:51.649: INFO: Deleting ReplicationController wrapped-volume-race-53a4be7c-9ec4-4bde-ab6a-35a63bd13d1a took: 12.766252ms
Jul 31 11:55:51.850: INFO: Terminating ReplicationController wrapped-volume-race-53a4be7c-9ec4-4bde-ab6a-35a63bd13d1a pods took: 201.035425ms
STEP: Creating RC which spawns configmap-volume pods
Jul 31 11:55:55.198: INFO: Pod name wrapped-volume-race-5d840e3b-6267-4bb9-8976-87e4c3f1d36e: Found 0 pods out of 5
Jul 31 11:56:00.236: INFO: Pod name wrapped-volume-race-5d840e3b-6267-4bb9-8976-87e4c3f1d36e: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-5d840e3b-6267-4bb9-8976-87e4c3f1d36e in namespace emptydir-wrapper-4104, will wait for the garbage collector to delete the pods
Jul 31 11:56:12.357: INFO: Deleting ReplicationController wrapped-volume-race-5d840e3b-6267-4bb9-8976-87e4c3f1d36e took: 15.731486ms
Jul 31 11:56:12.558: INFO: Terminating ReplicationController wrapped-volume-race-5d840e3b-6267-4bb9-8976-87e4c3f1d36e pods took: 201.042432ms
STEP: Creating RC which spawns configmap-volume pods
Jul 31 11:56:15.526: INFO: Pod name wrapped-volume-race-33d44e27-9085-4cfb-b7ed-a9f5cc336eff: Found 0 pods out of 5
Jul 31 11:56:20.551: INFO: Pod name wrapped-volume-race-33d44e27-9085-4cfb-b7ed-a9f5cc336eff: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-33d44e27-9085-4cfb-b7ed-a9f5cc336eff in namespace emptydir-wrapper-4104, will wait for the garbage collector to delete the pods
Jul 31 11:56:32.674: INFO: Deleting ReplicationController wrapped-volume-race-33d44e27-9085-4cfb-b7ed-a9f5cc336eff took: 27.342917ms
Jul 31 11:56:32.876: INFO: Terminating ReplicationController wrapped-volume-race-33d44e27-9085-4cfb-b7ed-a9f5cc336eff pods took: 201.215082ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:56:37.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4104" for this suite.

• [SLOW TEST:63.493 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":346,"completed":290,"skipped":5367,"failed":0}
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:56:37.115: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-test-volume-5e18f9c6-9b2b-4d88-b149-f799e4157b55
STEP: Creating a pod to test consume configMaps
Jul 31 11:56:37.225: INFO: Waiting up to 5m0s for pod "pod-configmaps-9d0ed929-d4cf-45a3-8820-58aa096a0fe0" in namespace "configmap-3095" to be "Succeeded or Failed"
Jul 31 11:56:37.249: INFO: Pod "pod-configmaps-9d0ed929-d4cf-45a3-8820-58aa096a0fe0": Phase="Pending", Reason="", readiness=false. Elapsed: 24.710221ms
Jul 31 11:56:39.269: INFO: Pod "pod-configmaps-9d0ed929-d4cf-45a3-8820-58aa096a0fe0": Phase="Running", Reason="", readiness=true. Elapsed: 2.044137017s
Jul 31 11:56:41.290: INFO: Pod "pod-configmaps-9d0ed929-d4cf-45a3-8820-58aa096a0fe0": Phase="Running", Reason="", readiness=false. Elapsed: 4.065448879s
Jul 31 11:56:43.301: INFO: Pod "pod-configmaps-9d0ed929-d4cf-45a3-8820-58aa096a0fe0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.076542836s
STEP: Saw pod success
Jul 31 11:56:43.302: INFO: Pod "pod-configmaps-9d0ed929-d4cf-45a3-8820-58aa096a0fe0" satisfied condition "Succeeded or Failed"
Jul 31 11:56:43.520: INFO: Trying to get logs from node p1-nc3cz44465xd41a6poxnmrpeqo pod pod-configmaps-9d0ed929-d4cf-45a3-8820-58aa096a0fe0 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 31 11:56:43.590: INFO: Waiting for pod pod-configmaps-9d0ed929-d4cf-45a3-8820-58aa096a0fe0 to disappear
Jul 31 11:56:43.608: INFO: Pod pod-configmaps-9d0ed929-d4cf-45a3-8820-58aa096a0fe0 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:56:43.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3095" for this suite.

• [SLOW TEST:6.529 seconds]
[sig-storage] ConfigMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":346,"completed":291,"skipped":5372,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:56:43.646: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating projection with configMap that has name projected-configmap-test-upd-e4c42b07-1cbc-409e-ae63-c51ef5d1e727
STEP: Creating the pod
Jul 31 11:56:43.796: INFO: The status of Pod pod-projected-configmaps-1b776aa9-ab8f-4bcc-8acb-e7002848fbd8 is Pending, waiting for it to be Running (with Ready = true)
Jul 31 11:56:45.806: INFO: The status of Pod pod-projected-configmaps-1b776aa9-ab8f-4bcc-8acb-e7002848fbd8 is Running (Ready = true)
STEP: Updating configmap projected-configmap-test-upd-e4c42b07-1cbc-409e-ae63-c51ef5d1e727
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:56:47.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9511" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":346,"completed":292,"skipped":5394,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:56:47.904: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[It] should check is all data is printed  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 11:56:47.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-3397 version'
Jul 31 11:56:48.053: INFO: stderr: ""
Jul 31 11:56:48.053: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.9\", GitCommit:\"c1de2d70269039fe55efb98e737d9a29f9155246\", GitTreeState:\"clean\", BuildDate:\"2022-07-13T14:26:51Z\", GoVersion:\"go1.17.11\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"23\", GitVersion:\"v1.23.9\", GitCommit:\"c1de2d70269039fe55efb98e737d9a29f9155246\", GitTreeState:\"clean\", BuildDate:\"2022-07-13T14:19:57Z\", GoVersion:\"go1.17.11\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 11:56:48.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3397" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":346,"completed":293,"skipped":5396,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 11:56:48.073: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod liveness-5f17dd36-4751-47f4-b640-c6c10a8728b5 in namespace container-probe-1862
Jul 31 11:56:50.201: INFO: Started pod liveness-5f17dd36-4751-47f4-b640-c6c10a8728b5 in namespace container-probe-1862
STEP: checking the pod's current state and verifying that restartCount is present
Jul 31 11:56:50.208: INFO: Initial restart count of pod liveness-5f17dd36-4751-47f4-b640-c6c10a8728b5 is 0
STEP: deleting the pod
[AfterEach] [sig-node] Probing container
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:00:50.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1862" for this suite.

• [SLOW TEST:242.534 seconds]
[sig-node] Probing container
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":346,"completed":294,"skipped":5416,"failed":0}
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:00:50.609: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating pod pod-subpath-test-configmap-v9mh
STEP: Creating a pod to test atomic-volume-subpath
Jul 31 12:00:50.730: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-v9mh" in namespace "subpath-2992" to be "Succeeded or Failed"
Jul 31 12:00:50.755: INFO: Pod "pod-subpath-test-configmap-v9mh": Phase="Pending", Reason="", readiness=false. Elapsed: 24.753183ms
Jul 31 12:00:52.771: INFO: Pod "pod-subpath-test-configmap-v9mh": Phase="Running", Reason="", readiness=true. Elapsed: 2.041136947s
Jul 31 12:00:54.783: INFO: Pod "pod-subpath-test-configmap-v9mh": Phase="Running", Reason="", readiness=true. Elapsed: 4.052629439s
Jul 31 12:00:56.796: INFO: Pod "pod-subpath-test-configmap-v9mh": Phase="Running", Reason="", readiness=true. Elapsed: 6.065617219s
Jul 31 12:00:58.810: INFO: Pod "pod-subpath-test-configmap-v9mh": Phase="Running", Reason="", readiness=true. Elapsed: 8.079462975s
Jul 31 12:01:00.825: INFO: Pod "pod-subpath-test-configmap-v9mh": Phase="Running", Reason="", readiness=true. Elapsed: 10.094825837s
Jul 31 12:01:02.840: INFO: Pod "pod-subpath-test-configmap-v9mh": Phase="Running", Reason="", readiness=true. Elapsed: 12.109932535s
Jul 31 12:01:04.925: INFO: Pod "pod-subpath-test-configmap-v9mh": Phase="Running", Reason="", readiness=true. Elapsed: 14.194830856s
Jul 31 12:01:06.937: INFO: Pod "pod-subpath-test-configmap-v9mh": Phase="Running", Reason="", readiness=true. Elapsed: 16.206918619s
Jul 31 12:01:08.954: INFO: Pod "pod-subpath-test-configmap-v9mh": Phase="Running", Reason="", readiness=true. Elapsed: 18.22417414s
Jul 31 12:01:10.968: INFO: Pod "pod-subpath-test-configmap-v9mh": Phase="Running", Reason="", readiness=true. Elapsed: 20.237829289s
Jul 31 12:01:12.982: INFO: Pod "pod-subpath-test-configmap-v9mh": Phase="Running", Reason="", readiness=false. Elapsed: 22.252235708s
Jul 31 12:01:14.997: INFO: Pod "pod-subpath-test-configmap-v9mh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.266666491s
STEP: Saw pod success
Jul 31 12:01:14.997: INFO: Pod "pod-subpath-test-configmap-v9mh" satisfied condition "Succeeded or Failed"
Jul 31 12:01:15.003: INFO: Trying to get logs from node p1-nc3cz44465xd41a6poxnmrpeqo pod pod-subpath-test-configmap-v9mh container test-container-subpath-configmap-v9mh: <nil>
STEP: delete the pod
Jul 31 12:01:15.064: INFO: Waiting for pod pod-subpath-test-configmap-v9mh to disappear
Jul 31 12:01:15.068: INFO: Pod pod-subpath-test-configmap-v9mh no longer exists
STEP: Deleting pod pod-subpath-test-configmap-v9mh
Jul 31 12:01:15.068: INFO: Deleting pod "pod-subpath-test-configmap-v9mh" in namespace "subpath-2992"
[AfterEach] [sig-storage] Subpath
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:01:15.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2992" for this suite.

• [SLOW TEST:24.497 seconds]
[sig-storage] Subpath
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Excluded:WindowsDocker] [Conformance]","total":346,"completed":295,"skipped":5420,"failed":0}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:01:15.109: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul 31 12:01:15.183: INFO: Waiting up to 5m0s for pod "pod-d595706a-6263-49e0-8fa3-988bd1b3ac1c" in namespace "emptydir-9342" to be "Succeeded or Failed"
Jul 31 12:01:15.220: INFO: Pod "pod-d595706a-6263-49e0-8fa3-988bd1b3ac1c": Phase="Pending", Reason="", readiness=false. Elapsed: 36.714484ms
Jul 31 12:01:17.236: INFO: Pod "pod-d595706a-6263-49e0-8fa3-988bd1b3ac1c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05261679s
Jul 31 12:01:19.250: INFO: Pod "pod-d595706a-6263-49e0-8fa3-988bd1b3ac1c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.066691204s
STEP: Saw pod success
Jul 31 12:01:19.250: INFO: Pod "pod-d595706a-6263-49e0-8fa3-988bd1b3ac1c" satisfied condition "Succeeded or Failed"
Jul 31 12:01:19.255: INFO: Trying to get logs from node p1-eipeq63rfgo6ooocmu3kqk3tcc pod pod-d595706a-6263-49e0-8fa3-988bd1b3ac1c container test-container: <nil>
STEP: delete the pod
Jul 31 12:01:19.315: INFO: Waiting for pod pod-d595706a-6263-49e0-8fa3-988bd1b3ac1c to disappear
Jul 31 12:01:19.320: INFO: Pod pod-d595706a-6263-49e0-8fa3-988bd1b3ac1c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:01:19.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9342" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":296,"skipped":5430,"failed":0}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:01:19.339: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: set up a multi version CRD
Jul 31 12:01:19.395: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:01:39.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1226" for this suite.

• [SLOW TEST:20.333 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":346,"completed":297,"skipped":5442,"failed":0}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:01:39.673: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Jul 31 12:01:40.318: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Jul 31 12:01:43.375: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:01:43.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8383" for this suite.
STEP: Destroying namespace "webhook-8383-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102
•{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":346,"completed":298,"skipped":5449,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:01:43.682: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jul 31 12:01:43.831: INFO: The status of Pod pod-update-activedeadlineseconds-eb90ddcb-a7a1-4dc8-8eda-2e10f7542a21 is Pending, waiting for it to be Running (with Ready = true)
Jul 31 12:01:45.844: INFO: The status of Pod pod-update-activedeadlineseconds-eb90ddcb-a7a1-4dc8-8eda-2e10f7542a21 is Pending, waiting for it to be Running (with Ready = true)
Jul 31 12:01:47.846: INFO: The status of Pod pod-update-activedeadlineseconds-eb90ddcb-a7a1-4dc8-8eda-2e10f7542a21 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul 31 12:01:48.376: INFO: Successfully updated pod "pod-update-activedeadlineseconds-eb90ddcb-a7a1-4dc8-8eda-2e10f7542a21"
Jul 31 12:01:48.376: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-eb90ddcb-a7a1-4dc8-8eda-2e10f7542a21" in namespace "pods-1789" to be "terminated due to deadline exceeded"
Jul 31 12:01:48.399: INFO: Pod "pod-update-activedeadlineseconds-eb90ddcb-a7a1-4dc8-8eda-2e10f7542a21": Phase="Running", Reason="", readiness=true. Elapsed: 22.600849ms
Jul 31 12:01:50.409: INFO: Pod "pod-update-activedeadlineseconds-eb90ddcb-a7a1-4dc8-8eda-2e10f7542a21": Phase="Running", Reason="", readiness=false. Elapsed: 2.033093805s
Jul 31 12:01:52.420: INFO: Pod "pod-update-activedeadlineseconds-eb90ddcb-a7a1-4dc8-8eda-2e10f7542a21": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.044166837s
Jul 31 12:01:52.420: INFO: Pod "pod-update-activedeadlineseconds-eb90ddcb-a7a1-4dc8-8eda-2e10f7542a21" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:01:52.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1789" for this suite.

• [SLOW TEST:8.768 seconds]
[sig-node] Pods
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":346,"completed":299,"skipped":5487,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:01:52.450: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-12617461-e195-4b9b-a97c-5706e9f4f0cf
STEP: Creating a pod to test consume secrets
Jul 31 12:01:52.525: INFO: Waiting up to 5m0s for pod "pod-secrets-c09219c3-d741-4cf9-9ae2-5dc66970ea0e" in namespace "secrets-7813" to be "Succeeded or Failed"
Jul 31 12:01:52.547: INFO: Pod "pod-secrets-c09219c3-d741-4cf9-9ae2-5dc66970ea0e": Phase="Pending", Reason="", readiness=false. Elapsed: 21.30058ms
Jul 31 12:01:54.558: INFO: Pod "pod-secrets-c09219c3-d741-4cf9-9ae2-5dc66970ea0e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033028236s
Jul 31 12:01:56.581: INFO: Pod "pod-secrets-c09219c3-d741-4cf9-9ae2-5dc66970ea0e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.055329247s
STEP: Saw pod success
Jul 31 12:01:56.581: INFO: Pod "pod-secrets-c09219c3-d741-4cf9-9ae2-5dc66970ea0e" satisfied condition "Succeeded or Failed"
Jul 31 12:01:56.585: INFO: Trying to get logs from node p1-nc3cz44465xd41a6poxnmrpeqo pod pod-secrets-c09219c3-d741-4cf9-9ae2-5dc66970ea0e container secret-volume-test: <nil>
STEP: delete the pod
Jul 31 12:01:56.635: INFO: Waiting for pod pod-secrets-c09219c3-d741-4cf9-9ae2-5dc66970ea0e to disappear
Jul 31 12:01:56.641: INFO: Pod pod-secrets-c09219c3-d741-4cf9-9ae2-5dc66970ea0e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:01:56.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7813" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":346,"completed":300,"skipped":5504,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:01:56.660: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1539
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Jul 31 12:01:56.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-7283 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2'
Jul 31 12:01:56.829: INFO: stderr: ""
Jul 31 12:01:56.829: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1543
Jul 31 12:01:56.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-7283 delete pods e2e-test-httpd-pod'
Jul 31 12:01:59.723: INFO: stderr: ""
Jul 31 12:01:59.723: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:01:59.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7283" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":346,"completed":301,"skipped":5529,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:01:59.755: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul 31 12:01:59.875: INFO: Waiting up to 5m0s for pod "pod-2afc0e29-67f1-4a68-8e18-b7a40e20c887" in namespace "emptydir-3138" to be "Succeeded or Failed"
Jul 31 12:01:59.889: INFO: Pod "pod-2afc0e29-67f1-4a68-8e18-b7a40e20c887": Phase="Pending", Reason="", readiness=false. Elapsed: 13.547557ms
Jul 31 12:02:01.902: INFO: Pod "pod-2afc0e29-67f1-4a68-8e18-b7a40e20c887": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026499026s
Jul 31 12:02:03.915: INFO: Pod "pod-2afc0e29-67f1-4a68-8e18-b7a40e20c887": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039917016s
STEP: Saw pod success
Jul 31 12:02:03.915: INFO: Pod "pod-2afc0e29-67f1-4a68-8e18-b7a40e20c887" satisfied condition "Succeeded or Failed"
Jul 31 12:02:03.922: INFO: Trying to get logs from node p1-ashfcfj4pzo6aemt1j3a9ah7ew pod pod-2afc0e29-67f1-4a68-8e18-b7a40e20c887 container test-container: <nil>
STEP: delete the pod
Jul 31 12:02:03.979: INFO: Waiting for pod pod-2afc0e29-67f1-4a68-8e18-b7a40e20c887 to disappear
Jul 31 12:02:03.991: INFO: Pod pod-2afc0e29-67f1-4a68-8e18-b7a40e20c887 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:02:03.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3138" for this suite.
•{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":302,"skipped":5545,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:02:04.018: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Performing setup for networking test in namespace pod-network-test-5388
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 31 12:02:04.089: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jul 31 12:02:04.332: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jul 31 12:02:06.345: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 31 12:02:08.347: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 31 12:02:10.347: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 31 12:02:12.347: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 31 12:02:14.348: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 31 12:02:16.344: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 31 12:02:18.344: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 31 12:02:20.344: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 31 12:02:22.563: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 31 12:02:24.345: INFO: The status of Pod netserver-0 is Running (Ready = false)
Jul 31 12:02:26.342: INFO: The status of Pod netserver-0 is Running (Ready = true)
Jul 31 12:02:26.353: INFO: The status of Pod netserver-1 is Running (Ready = true)
Jul 31 12:02:26.364: INFO: The status of Pod netserver-2 is Running (Ready = true)
Jul 31 12:02:26.375: INFO: The status of Pod netserver-3 is Running (Ready = true)
Jul 31 12:02:26.391: INFO: The status of Pod netserver-4 is Running (Ready = true)
STEP: Creating test pods
Jul 31 12:02:28.467: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
Jul 31 12:02:28.467: INFO: Going to poll 172.28.80.1 on port 8081 at least 0 times, with a maximum of 55 tries before failing
Jul 31 12:02:28.471: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.28.80.1 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5388 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 31 12:02:28.471: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
Jul 31 12:02:28.472: INFO: ExecWithOptions: Clientset creation
Jul 31 12:02:28.472: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5388/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.28.80.1+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Jul 31 12:02:29.629: INFO: Found all 1 expected endpoints: [netserver-0]
Jul 31 12:02:29.629: INFO: Going to poll 172.28.0.2 on port 8081 at least 0 times, with a maximum of 55 tries before failing
Jul 31 12:02:29.641: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.28.0.2 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5388 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 31 12:02:29.641: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
Jul 31 12:02:29.642: INFO: ExecWithOptions: Clientset creation
Jul 31 12:02:29.642: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5388/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.28.0.2+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Jul 31 12:02:30.799: INFO: Found all 1 expected endpoints: [netserver-1]
Jul 31 12:02:30.800: INFO: Going to poll 172.28.176.2 on port 8081 at least 0 times, with a maximum of 55 tries before failing
Jul 31 12:02:30.813: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.28.176.2 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5388 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 31 12:02:30.813: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
Jul 31 12:02:30.814: INFO: ExecWithOptions: Clientset creation
Jul 31 12:02:30.814: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5388/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.28.176.2+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Jul 31 12:02:31.970: INFO: Found all 1 expected endpoints: [netserver-2]
Jul 31 12:02:31.970: INFO: Going to poll 172.28.96.2 on port 8081 at least 0 times, with a maximum of 55 tries before failing
Jul 31 12:02:31.982: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.28.96.2 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5388 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 31 12:02:31.982: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
Jul 31 12:02:31.983: INFO: ExecWithOptions: Clientset creation
Jul 31 12:02:31.983: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5388/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.28.96.2+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Jul 31 12:02:33.142: INFO: Found all 1 expected endpoints: [netserver-3]
Jul 31 12:02:33.142: INFO: Going to poll 172.28.64.1 on port 8081 at least 0 times, with a maximum of 55 tries before failing
Jul 31 12:02:33.152: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.28.64.1 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5388 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jul 31 12:02:33.152: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
Jul 31 12:02:33.153: INFO: ExecWithOptions: Clientset creation
Jul 31 12:02:33.153: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5388/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.28.64.1+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true %!s(MISSING))
Jul 31 12:02:34.310: INFO: Found all 1 expected endpoints: [netserver-4]
[AfterEach] [sig-network] Networking
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:02:34.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5388" for this suite.

• [SLOW TEST:30.321 seconds]
[sig-network] Networking
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/network/networking.go:30
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":303,"skipped":5593,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:02:34.340: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Jul 31 12:02:34.476: INFO: Waiting up to 5m0s for pod "downwardapi-volume-de00cc87-1550-4ede-b701-11c2bd7117ff" in namespace "downward-api-2727" to be "Succeeded or Failed"
Jul 31 12:02:34.497: INFO: Pod "downwardapi-volume-de00cc87-1550-4ede-b701-11c2bd7117ff": Phase="Pending", Reason="", readiness=false. Elapsed: 21.182766ms
Jul 31 12:02:36.506: INFO: Pod "downwardapi-volume-de00cc87-1550-4ede-b701-11c2bd7117ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030395967s
Jul 31 12:02:38.520: INFO: Pod "downwardapi-volume-de00cc87-1550-4ede-b701-11c2bd7117ff": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044446806s
Jul 31 12:02:40.535: INFO: Pod "downwardapi-volume-de00cc87-1550-4ede-b701-11c2bd7117ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.05964657s
STEP: Saw pod success
Jul 31 12:02:40.535: INFO: Pod "downwardapi-volume-de00cc87-1550-4ede-b701-11c2bd7117ff" satisfied condition "Succeeded or Failed"
Jul 31 12:02:40.540: INFO: Trying to get logs from node p1-nc3cz44465xd41a6poxnmrpeqo pod downwardapi-volume-de00cc87-1550-4ede-b701-11c2bd7117ff container client-container: <nil>
STEP: delete the pod
Jul 31 12:02:40.601: INFO: Waiting for pod downwardapi-volume-de00cc87-1550-4ede-b701-11c2bd7117ff to disappear
Jul 31 12:02:40.605: INFO: Pod downwardapi-volume-de00cc87-1550-4ede-b701-11c2bd7117ff no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:02:40.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2727" for this suite.

• [SLOW TEST:6.347 seconds]
[sig-storage] Downward API volume
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":346,"completed":304,"skipped":5644,"failed":0}
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:02:40.688: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Jul 31 12:02:40.785: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a86ed208-f82c-4db5-a032-7fbec648c789" in namespace "projected-1518" to be "Succeeded or Failed"
Jul 31 12:02:40.821: INFO: Pod "downwardapi-volume-a86ed208-f82c-4db5-a032-7fbec648c789": Phase="Pending", Reason="", readiness=false. Elapsed: 35.743084ms
Jul 31 12:02:42.838: INFO: Pod "downwardapi-volume-a86ed208-f82c-4db5-a032-7fbec648c789": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052807217s
Jul 31 12:02:44.852: INFO: Pod "downwardapi-volume-a86ed208-f82c-4db5-a032-7fbec648c789": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.067531436s
STEP: Saw pod success
Jul 31 12:02:44.852: INFO: Pod "downwardapi-volume-a86ed208-f82c-4db5-a032-7fbec648c789" satisfied condition "Succeeded or Failed"
Jul 31 12:02:44.857: INFO: Trying to get logs from node p1-ashfcfj4pzo6aemt1j3a9ah7ew pod downwardapi-volume-a86ed208-f82c-4db5-a032-7fbec648c789 container client-container: <nil>
STEP: delete the pod
Jul 31 12:02:44.930: INFO: Waiting for pod downwardapi-volume-a86ed208-f82c-4db5-a032-7fbec648c789 to disappear
Jul 31 12:02:44.944: INFO: Pod downwardapi-volume-a86ed208-f82c-4db5-a032-7fbec648c789 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:02:44.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1518" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":346,"completed":305,"skipped":5648,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:02:44.980: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating secret with name secret-test-b6f00d59-e430-469a-a1a4-c5cdbed28a63
STEP: Creating a pod to test consume secrets
Jul 31 12:02:45.078: INFO: Waiting up to 5m0s for pod "pod-secrets-b79055ee-e530-4210-9ec6-d77b8f84f1bc" in namespace "secrets-3101" to be "Succeeded or Failed"
Jul 31 12:02:45.097: INFO: Pod "pod-secrets-b79055ee-e530-4210-9ec6-d77b8f84f1bc": Phase="Pending", Reason="", readiness=false. Elapsed: 18.866157ms
Jul 31 12:02:47.110: INFO: Pod "pod-secrets-b79055ee-e530-4210-9ec6-d77b8f84f1bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032448337s
Jul 31 12:02:49.123: INFO: Pod "pod-secrets-b79055ee-e530-4210-9ec6-d77b8f84f1bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044708005s
STEP: Saw pod success
Jul 31 12:02:49.123: INFO: Pod "pod-secrets-b79055ee-e530-4210-9ec6-d77b8f84f1bc" satisfied condition "Succeeded or Failed"
Jul 31 12:02:49.127: INFO: Trying to get logs from node p1-nc3cz44465xd41a6poxnmrpeqo pod pod-secrets-b79055ee-e530-4210-9ec6-d77b8f84f1bc container secret-volume-test: <nil>
STEP: delete the pod
Jul 31 12:02:49.173: INFO: Waiting for pod pod-secrets-b79055ee-e530-4210-9ec6-d77b8f84f1bc to disappear
Jul 31 12:02:49.178: INFO: Pod pod-secrets-b79055ee-e530-4210-9ec6-d77b8f84f1bc no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:02:49.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3101" for this suite.
•{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":306,"skipped":5782,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:02:49.205: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating server pod server in namespace prestop-7323
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-7323
STEP: Deleting pre-stop pod
Jul 31 12:02:58.419: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [sig-node] PreStop
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:02:58.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-7323" for this suite.

• [SLOW TEST:9.307 seconds]
[sig-node] PreStop
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":346,"completed":307,"skipped":5805,"failed":0}
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:02:58.514: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:94
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:109
STEP: Creating service test in namespace statefulset-5956
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating stateful set ss in namespace statefulset-5956
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5956
Jul 31 12:02:58.622: INFO: Found 0 stateful pods, waiting for 1
Jul 31 12:03:08.638: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jul 31 12:03:08.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=statefulset-5956 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 31 12:03:09.077: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 31 12:03:09.077: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 31 12:03:09.077: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 31 12:03:09.091: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul 31 12:03:19.117: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 31 12:03:19.117: INFO: Waiting for statefulset status.replicas updated to 0
Jul 31 12:03:19.148: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Jul 31 12:03:19.148: INFO: ss-0  p1-eipeq63rfgo6ooocmu3kqk3tcc  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-07-31 12:02:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-07-31 12:03:09 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-07-31 12:03:09 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-07-31 12:02:58 +0000 UTC  }]
Jul 31 12:03:19.148: INFO: 
Jul 31 12:03:19.148: INFO: StatefulSet ss has not reached scale 3, at 1
Jul 31 12:03:20.379: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993408053s
Jul 31 12:03:21.393: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.762652164s
Jul 31 12:03:22.407: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.747987391s
Jul 31 12:03:23.422: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.734229209s
Jul 31 12:03:24.432: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.719683047s
Jul 31 12:03:25.443: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.709660406s
Jul 31 12:03:26.454: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.698250173s
Jul 31 12:03:27.470: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.688048642s
Jul 31 12:03:28.482: INFO: Verifying statefulset ss doesn't scale past 3 for another 670.32259ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5956
Jul 31 12:03:29.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=statefulset-5956 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 31 12:03:29.783: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jul 31 12:03:29.783: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 31 12:03:29.783: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 31 12:03:29.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=statefulset-5956 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 31 12:03:30.043: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul 31 12:03:30.043: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 31 12:03:30.043: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 31 12:03:30.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=statefulset-5956 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jul 31 12:03:30.274: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul 31 12:03:30.274: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jul 31 12:03:30.274: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jul 31 12:03:30.285: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 31 12:03:30.285: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 31 12:03:30.285: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jul 31 12:03:30.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=statefulset-5956 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 31 12:03:30.517: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 31 12:03:30.517: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 31 12:03:30.517: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 31 12:03:30.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=statefulset-5956 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 31 12:03:30.747: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 31 12:03:30.747: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 31 12:03:30.747: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 31 12:03:30.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=statefulset-5956 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jul 31 12:03:30.961: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jul 31 12:03:30.961: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jul 31 12:03:30.961: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jul 31 12:03:30.961: INFO: Waiting for statefulset status.replicas updated to 0
Jul 31 12:03:30.968: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jul 31 12:03:40.995: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 31 12:03:40.995: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul 31 12:03:40.995: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul 31 12:03:41.024: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Jul 31 12:03:41.024: INFO: ss-0  p1-eipeq63rfgo6ooocmu3kqk3tcc  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-07-31 12:02:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-07-31 12:03:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-07-31 12:03:30 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-07-31 12:02:58 +0000 UTC  }]
Jul 31 12:03:41.024: INFO: ss-1  p1-ashfcfj4pzo6aemt1j3a9ah7ew  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-07-31 12:03:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-07-31 12:03:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-07-31 12:03:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-07-31 12:03:19 +0000 UTC  }]
Jul 31 12:03:41.024: INFO: ss-2  p1-nc3cz44465xd41a6poxnmrpeqo  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-07-31 12:03:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-07-31 12:03:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-07-31 12:03:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-07-31 12:03:19 +0000 UTC  }]
Jul 31 12:03:41.024: INFO: 
Jul 31 12:03:41.024: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 31 12:03:42.034: INFO: POD   NODE                           PHASE    GRACE  CONDITIONS
Jul 31 12:03:42.034: INFO: ss-2  p1-nc3cz44465xd41a6poxnmrpeqo  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2022-07-31 12:03:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2022-07-31 12:03:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2022-07-31 12:03:31 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2022-07-31 12:03:19 +0000 UTC  }]
Jul 31 12:03:42.034: INFO: 
Jul 31 12:03:42.034: INFO: StatefulSet ss has not reached scale 0, at 1
Jul 31 12:03:43.044: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.984632329s
Jul 31 12:03:44.056: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.973302304s
Jul 31 12:03:45.067: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.96232572s
Jul 31 12:03:46.287: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.950993417s
Jul 31 12:03:47.297: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.731751899s
Jul 31 12:03:48.306: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.72152554s
Jul 31 12:03:49.318: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.7122606s
Jul 31 12:03:50.327: INFO: Verifying statefulset ss doesn't scale past 0 for another 699.942978ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5956
Jul 31 12:03:51.339: INFO: Scaling statefulset ss to 0
Jul 31 12:03:51.358: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:120
Jul 31 12:03:51.362: INFO: Deleting all statefulset in ns statefulset-5956
Jul 31 12:03:51.365: INFO: Scaling statefulset ss to 0
Jul 31 12:03:51.380: INFO: Waiting for statefulset status.replicas updated to 0
Jul 31 12:03:51.385: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:03:51.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5956" for this suite.

• [SLOW TEST:52.921 seconds]
[sig-apps] StatefulSet
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:99
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":346,"completed":308,"skipped":5807,"failed":0}
SSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:03:51.437: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 12:03:51.507: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: creating replication controller svc-latency-rc in namespace svc-latency-3026
I0731 12:03:51.536658      23 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-3026, replica count: 1
I0731 12:03:52.588332      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0731 12:03:53.589057      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 31 12:03:53.738: INFO: Created: latency-svc-8k2nj
Jul 31 12:03:53.748: INFO: Got endpoints: latency-svc-8k2nj [58.502546ms]
Jul 31 12:03:53.787: INFO: Created: latency-svc-h8g7l
Jul 31 12:03:53.798: INFO: Got endpoints: latency-svc-h8g7l [49.318651ms]
Jul 31 12:03:53.832: INFO: Created: latency-svc-9wgfs
Jul 31 12:03:53.861: INFO: Got endpoints: latency-svc-9wgfs [112.811012ms]
Jul 31 12:03:53.868: INFO: Created: latency-svc-cw9qh
Jul 31 12:03:53.889: INFO: Got endpoints: latency-svc-cw9qh [140.312643ms]
Jul 31 12:03:53.908: INFO: Created: latency-svc-9wscb
Jul 31 12:03:53.922: INFO: Got endpoints: latency-svc-9wscb [173.26757ms]
Jul 31 12:03:53.958: INFO: Created: latency-svc-z7x9h
Jul 31 12:03:53.980: INFO: Got endpoints: latency-svc-z7x9h [231.476495ms]
Jul 31 12:03:53.996: INFO: Created: latency-svc-5htnr
Jul 31 12:03:54.003: INFO: Got endpoints: latency-svc-5htnr [254.824171ms]
Jul 31 12:03:54.046: INFO: Created: latency-svc-pmxhq
Jul 31 12:03:54.080: INFO: Got endpoints: latency-svc-pmxhq [331.605046ms]
Jul 31 12:03:54.089: INFO: Created: latency-svc-7xrtx
Jul 31 12:03:54.095: INFO: Got endpoints: latency-svc-7xrtx [345.605508ms]
Jul 31 12:03:54.128: INFO: Created: latency-svc-6fp5p
Jul 31 12:03:54.162: INFO: Got endpoints: latency-svc-6fp5p [413.334425ms]
Jul 31 12:03:54.200: INFO: Created: latency-svc-sjnlc
Jul 31 12:03:54.213: INFO: Got endpoints: latency-svc-sjnlc [463.803253ms]
Jul 31 12:03:54.238: INFO: Created: latency-svc-69vjn
Jul 31 12:03:54.248: INFO: Got endpoints: latency-svc-69vjn [499.243546ms]
Jul 31 12:03:54.296: INFO: Created: latency-svc-kscb8
Jul 31 12:03:54.300: INFO: Got endpoints: latency-svc-kscb8 [551.17448ms]
Jul 31 12:03:54.332: INFO: Created: latency-svc-kcbqz
Jul 31 12:03:54.359: INFO: Got endpoints: latency-svc-kcbqz [610.053495ms]
Jul 31 12:03:54.373: INFO: Created: latency-svc-5sqlh
Jul 31 12:03:54.386: INFO: Got endpoints: latency-svc-5sqlh [637.172347ms]
Jul 31 12:03:54.417: INFO: Created: latency-svc-shkv6
Jul 31 12:03:54.439: INFO: Got endpoints: latency-svc-shkv6 [641.574188ms]
Jul 31 12:03:54.473: INFO: Created: latency-svc-2vqcp
Jul 31 12:03:54.495: INFO: Got endpoints: latency-svc-2vqcp [634.040452ms]
Jul 31 12:03:54.544: INFO: Created: latency-svc-qwrld
Jul 31 12:03:54.557: INFO: Got endpoints: latency-svc-qwrld [667.777034ms]
Jul 31 12:03:54.604: INFO: Created: latency-svc-scdw2
Jul 31 12:03:54.627: INFO: Got endpoints: latency-svc-scdw2 [705.402525ms]
Jul 31 12:03:54.650: INFO: Created: latency-svc-d5v4d
Jul 31 12:03:54.661: INFO: Got endpoints: latency-svc-d5v4d [680.588983ms]
Jul 31 12:03:54.722: INFO: Created: latency-svc-x6xfz
Jul 31 12:03:54.722: INFO: Got endpoints: latency-svc-x6xfz [718.380034ms]
Jul 31 12:03:54.761: INFO: Created: latency-svc-8j59z
Jul 31 12:03:54.798: INFO: Got endpoints: latency-svc-8j59z [717.632088ms]
Jul 31 12:03:54.805: INFO: Created: latency-svc-jbv5s
Jul 31 12:03:54.808: INFO: Got endpoints: latency-svc-jbv5s [713.422776ms]
Jul 31 12:03:54.853: INFO: Created: latency-svc-f9z4t
Jul 31 12:03:54.866: INFO: Created: latency-svc-d8hgd
Jul 31 12:03:54.868: INFO: Got endpoints: latency-svc-d8hgd [705.073499ms]
Jul 31 12:03:54.874: INFO: Got endpoints: latency-svc-f9z4t [1.125264717s]
Jul 31 12:03:54.901: INFO: Created: latency-svc-bzw4d
Jul 31 12:03:54.916: INFO: Got endpoints: latency-svc-bzw4d [703.36663ms]
Jul 31 12:03:54.929: INFO: Created: latency-svc-t4j6g
Jul 31 12:03:54.950: INFO: Got endpoints: latency-svc-t4j6g [701.942293ms]
Jul 31 12:03:54.969: INFO: Created: latency-svc-n84n2
Jul 31 12:03:54.984: INFO: Got endpoints: latency-svc-n84n2 [684.072554ms]
Jul 31 12:03:55.024: INFO: Created: latency-svc-nl5g5
Jul 31 12:03:55.032: INFO: Got endpoints: latency-svc-nl5g5 [672.377537ms]
Jul 31 12:03:55.060: INFO: Created: latency-svc-2glgt
Jul 31 12:03:55.079: INFO: Got endpoints: latency-svc-2glgt [692.542625ms]
Jul 31 12:03:55.115: INFO: Created: latency-svc-2k6nr
Jul 31 12:03:55.128: INFO: Got endpoints: latency-svc-2k6nr [688.487435ms]
Jul 31 12:03:55.175: INFO: Created: latency-svc-5pq7j
Jul 31 12:03:55.193: INFO: Got endpoints: latency-svc-5pq7j [697.381216ms]
Jul 31 12:03:55.220: INFO: Created: latency-svc-zq6lm
Jul 31 12:03:55.234: INFO: Got endpoints: latency-svc-zq6lm [677.115888ms]
Jul 31 12:03:55.281: INFO: Created: latency-svc-c8g4n
Jul 31 12:03:55.293: INFO: Got endpoints: latency-svc-c8g4n [666.016105ms]
Jul 31 12:03:55.330: INFO: Created: latency-svc-28vvp
Jul 31 12:03:55.352: INFO: Got endpoints: latency-svc-28vvp [691.306742ms]
Jul 31 12:03:55.382: INFO: Created: latency-svc-8lzqp
Jul 31 12:03:55.397: INFO: Got endpoints: latency-svc-8lzqp [675.352205ms]
Jul 31 12:03:55.438: INFO: Created: latency-svc-ztqz2
Jul 31 12:03:55.691: INFO: Got endpoints: latency-svc-ztqz2 [892.887552ms]
Jul 31 12:03:55.698: INFO: Created: latency-svc-2btxv
Jul 31 12:03:55.727: INFO: Got endpoints: latency-svc-2btxv [918.41071ms]
Jul 31 12:03:55.761: INFO: Created: latency-svc-drmqq
Jul 31 12:03:55.782: INFO: Got endpoints: latency-svc-drmqq [914.16145ms]
Jul 31 12:03:55.822: INFO: Created: latency-svc-mm4mp
Jul 31 12:03:55.822: INFO: Got endpoints: latency-svc-mm4mp [947.828961ms]
Jul 31 12:03:55.861: INFO: Created: latency-svc-bvs45
Jul 31 12:03:55.869: INFO: Got endpoints: latency-svc-bvs45 [952.638301ms]
Jul 31 12:03:55.893: INFO: Created: latency-svc-pkfrt
Jul 31 12:03:55.902: INFO: Got endpoints: latency-svc-pkfrt [950.884796ms]
Jul 31 12:03:55.978: INFO: Created: latency-svc-l9g5g
Jul 31 12:03:56.001: INFO: Got endpoints: latency-svc-l9g5g [1.01600144s]
Jul 31 12:03:56.011: INFO: Created: latency-svc-ds5jz
Jul 31 12:03:56.013: INFO: Got endpoints: latency-svc-ds5jz [981.477417ms]
Jul 31 12:03:56.041: INFO: Created: latency-svc-wbl9p
Jul 31 12:03:56.108: INFO: Got endpoints: latency-svc-wbl9p [1.028910615s]
Jul 31 12:03:56.125: INFO: Created: latency-svc-5zdv7
Jul 31 12:03:56.153: INFO: Got endpoints: latency-svc-5zdv7 [1.02517583s]
Jul 31 12:03:56.704: INFO: Created: latency-svc-gdm5m
Jul 31 12:03:56.705: INFO: Created: latency-svc-mwc2v
Jul 31 12:03:56.706: INFO: Created: latency-svc-jwv49
Jul 31 12:03:56.706: INFO: Created: latency-svc-m6gmk
Jul 31 12:03:56.748: INFO: Got endpoints: latency-svc-m6gmk [594.633619ms]
Jul 31 12:03:56.765: INFO: Got endpoints: latency-svc-gdm5m [1.038071264s]
Jul 31 12:03:56.765: INFO: Got endpoints: latency-svc-jwv49 [752.037531ms]
Jul 31 12:03:56.766: INFO: Got endpoints: latency-svc-mwc2v [1.572829706s]
Jul 31 12:03:56.770: INFO: Created: latency-svc-mswq2
Jul 31 12:03:56.770: INFO: Created: latency-svc-wdxg6
Jul 31 12:03:56.771: INFO: Created: latency-svc-qrp6s
Jul 31 12:03:56.771: INFO: Created: latency-svc-7k94l
Jul 31 12:03:56.771: INFO: Created: latency-svc-69npd
Jul 31 12:03:56.771: INFO: Created: latency-svc-q4sq9
Jul 31 12:03:56.771: INFO: Created: latency-svc-lvx8d
Jul 31 12:03:56.771: INFO: Created: latency-svc-xfv5x
Jul 31 12:03:56.771: INFO: Created: latency-svc-5xpg6
Jul 31 12:03:56.771: INFO: Got endpoints: latency-svc-5xpg6 [989.416966ms]
Jul 31 12:03:56.772: INFO: Created: latency-svc-4hjhs
Jul 31 12:03:56.772: INFO: Created: latency-svc-77rdn
Jul 31 12:03:56.781: INFO: Got endpoints: latency-svc-4hjhs [1.547586729s]
Jul 31 12:03:56.798: INFO: Got endpoints: latency-svc-q4sq9 [690.500154ms]
Jul 31 12:03:56.801: INFO: Got endpoints: latency-svc-77rdn [979.427107ms]
Jul 31 12:03:56.814: INFO: Created: latency-svc-ckjtz
Jul 31 12:03:56.818: INFO: Got endpoints: latency-svc-lvx8d [1.126728793s]
Jul 31 12:03:56.818: INFO: Got endpoints: latency-svc-69npd [1.466291764s]
Jul 31 12:03:56.819: INFO: Got endpoints: latency-svc-xfv5x [1.525336156s]
Jul 31 12:03:56.821: INFO: Got endpoints: latency-svc-7k94l [951.682918ms]
Jul 31 12:03:56.821: INFO: Got endpoints: latency-svc-qrp6s [919.325279ms]
Jul 31 12:03:56.832: INFO: Got endpoints: latency-svc-wdxg6 [1.434942734s]
Jul 31 12:03:56.868: INFO: Got endpoints: latency-svc-mswq2 [867.590073ms]
Jul 31 12:03:56.868: INFO: Got endpoints: latency-svc-ckjtz [120.440613ms]
Jul 31 12:03:56.879: INFO: Created: latency-svc-hq58j
Jul 31 12:03:56.885: INFO: Got endpoints: latency-svc-hq58j [119.851389ms]
Jul 31 12:03:56.908: INFO: Created: latency-svc-db5wk
Jul 31 12:03:56.955: INFO: Got endpoints: latency-svc-db5wk [183.374911ms]
Jul 31 12:03:56.984: INFO: Created: latency-svc-6vxjz
Jul 31 12:03:56.984: INFO: Got endpoints: latency-svc-6vxjz [218.969668ms]
Jul 31 12:03:57.027: INFO: Created: latency-svc-dzlgt
Jul 31 12:03:57.042: INFO: Got endpoints: latency-svc-dzlgt [276.341088ms]
Jul 31 12:03:57.076: INFO: Created: latency-svc-pk5s2
Jul 31 12:03:57.093: INFO: Got endpoints: latency-svc-pk5s2 [311.839604ms]
Jul 31 12:03:57.137: INFO: Created: latency-svc-694x4
Jul 31 12:03:57.142: INFO: Got endpoints: latency-svc-694x4 [343.515376ms]
Jul 31 12:03:57.172: INFO: Created: latency-svc-rpdtk
Jul 31 12:03:57.191: INFO: Got endpoints: latency-svc-rpdtk [389.467239ms]
Jul 31 12:03:57.204: INFO: Created: latency-svc-bld7k
Jul 31 12:03:57.216: INFO: Got endpoints: latency-svc-bld7k [397.694021ms]
Jul 31 12:03:57.237: INFO: Created: latency-svc-x9lbr
Jul 31 12:03:57.247: INFO: Got endpoints: latency-svc-x9lbr [427.782249ms]
Jul 31 12:03:57.276: INFO: Created: latency-svc-xjtrg
Jul 31 12:03:57.295: INFO: Got endpoints: latency-svc-xjtrg [476.120034ms]
Jul 31 12:03:57.311: INFO: Created: latency-svc-m9j5h
Jul 31 12:03:57.321: INFO: Got endpoints: latency-svc-m9j5h [499.529675ms]
Jul 31 12:03:57.338: INFO: Created: latency-svc-j25jp
Jul 31 12:03:57.339: INFO: Got endpoints: latency-svc-j25jp [517.734316ms]
Jul 31 12:03:57.368: INFO: Created: latency-svc-kchf9
Jul 31 12:03:57.383: INFO: Got endpoints: latency-svc-kchf9 [514.428636ms]
Jul 31 12:03:57.393: INFO: Created: latency-svc-bs7zx
Jul 31 12:03:57.399: INFO: Got endpoints: latency-svc-bs7zx [531.163005ms]
Jul 31 12:03:57.450: INFO: Created: latency-svc-qskgf
Jul 31 12:03:57.468: INFO: Got endpoints: latency-svc-qskgf [583.065037ms]
Jul 31 12:03:57.500: INFO: Created: latency-svc-lk57c
Jul 31 12:03:57.513: INFO: Got endpoints: latency-svc-lk57c [558.242162ms]
Jul 31 12:03:57.566: INFO: Created: latency-svc-ksnpj
Jul 31 12:03:57.599: INFO: Got endpoints: latency-svc-ksnpj [614.819562ms]
Jul 31 12:03:57.612: INFO: Created: latency-svc-p6k9x
Jul 31 12:03:57.719: INFO: Got endpoints: latency-svc-p6k9x [676.522015ms]
Jul 31 12:03:57.739: INFO: Created: latency-svc-fgh2d
Jul 31 12:03:57.757: INFO: Got endpoints: latency-svc-fgh2d [663.372859ms]
Jul 31 12:03:57.784: INFO: Created: latency-svc-bh4m5
Jul 31 12:03:57.865: INFO: Got endpoints: latency-svc-bh4m5 [723.169793ms]
Jul 31 12:03:57.874: INFO: Created: latency-svc-5ls46
Jul 31 12:03:57.897: INFO: Got endpoints: latency-svc-5ls46 [705.496471ms]
Jul 31 12:03:57.907: INFO: Created: latency-svc-kjpns
Jul 31 12:03:57.932: INFO: Got endpoints: latency-svc-kjpns [716.102429ms]
Jul 31 12:03:57.990: INFO: Created: latency-svc-xg67l
Jul 31 12:03:57.990: INFO: Got endpoints: latency-svc-xg67l [743.09955ms]
Jul 31 12:03:58.031: INFO: Created: latency-svc-x5758
Jul 31 12:03:58.051: INFO: Got endpoints: latency-svc-x5758 [756.282692ms]
Jul 31 12:03:58.081: INFO: Created: latency-svc-f6m6s
Jul 31 12:03:58.114: INFO: Got endpoints: latency-svc-f6m6s [793.32001ms]
Jul 31 12:03:58.150: INFO: Created: latency-svc-m5ww8
Jul 31 12:03:58.150: INFO: Created: latency-svc-rz65z
Jul 31 12:03:58.164: INFO: Got endpoints: latency-svc-rz65z [825.020462ms]
Jul 31 12:03:58.166: INFO: Got endpoints: latency-svc-m5ww8 [1.334127792s]
Jul 31 12:03:58.205: INFO: Created: latency-svc-s5gr8
Jul 31 12:03:58.246: INFO: Got endpoints: latency-svc-s5gr8 [862.731297ms]
Jul 31 12:03:58.280: INFO: Created: latency-svc-dpjwv
Jul 31 12:03:58.306: INFO: Got endpoints: latency-svc-dpjwv [906.81774ms]
Jul 31 12:03:58.336: INFO: Created: latency-svc-2hrqm
Jul 31 12:03:58.380: INFO: Got endpoints: latency-svc-2hrqm [912.041626ms]
Jul 31 12:03:58.419: INFO: Created: latency-svc-lsj7p
Jul 31 12:03:58.431: INFO: Got endpoints: latency-svc-lsj7p [917.782944ms]
Jul 31 12:03:58.472: INFO: Created: latency-svc-46s6p
Jul 31 12:03:58.561: INFO: Got endpoints: latency-svc-46s6p [961.701039ms]
Jul 31 12:03:58.583: INFO: Created: latency-svc-6lvb5
Jul 31 12:03:58.590: INFO: Got endpoints: latency-svc-6lvb5 [871.008098ms]
Jul 31 12:03:58.626: INFO: Created: latency-svc-lcx5s
Jul 31 12:03:58.638: INFO: Got endpoints: latency-svc-lcx5s [881.588336ms]
Jul 31 12:03:58.717: INFO: Created: latency-svc-fgqr9
Jul 31 12:03:58.727: INFO: Got endpoints: latency-svc-fgqr9 [861.825441ms]
Jul 31 12:03:58.757: INFO: Created: latency-svc-275rw
Jul 31 12:03:58.768: INFO: Got endpoints: latency-svc-275rw [870.965829ms]
Jul 31 12:03:58.801: INFO: Created: latency-svc-s8l7z
Jul 31 12:03:58.838: INFO: Got endpoints: latency-svc-s8l7z [905.533927ms]
Jul 31 12:03:58.865: INFO: Created: latency-svc-gst85
Jul 31 12:03:58.900: INFO: Got endpoints: latency-svc-gst85 [910.588711ms]
Jul 31 12:03:58.945: INFO: Created: latency-svc-99wc8
Jul 31 12:03:58.978: INFO: Got endpoints: latency-svc-99wc8 [926.792165ms]
Jul 31 12:03:59.010: INFO: Created: latency-svc-bgxbx
Jul 31 12:03:59.024: INFO: Got endpoints: latency-svc-bgxbx [909.60526ms]
Jul 31 12:03:59.060: INFO: Created: latency-svc-gt6wd
Jul 31 12:03:59.096: INFO: Got endpoints: latency-svc-gt6wd [929.912706ms]
Jul 31 12:03:59.120: INFO: Created: latency-svc-tkxx6
Jul 31 12:03:59.155: INFO: Got endpoints: latency-svc-tkxx6 [990.764716ms]
Jul 31 12:03:59.155: INFO: Created: latency-svc-2hk8z
Jul 31 12:03:59.179: INFO: Got endpoints: latency-svc-2hk8z [933.48543ms]
Jul 31 12:03:59.215: INFO: Created: latency-svc-g5xst
Jul 31 12:03:59.235: INFO: Got endpoints: latency-svc-g5xst [928.322204ms]
Jul 31 12:03:59.246: INFO: Created: latency-svc-8t8jt
Jul 31 12:03:59.259: INFO: Got endpoints: latency-svc-8t8jt [878.441068ms]
Jul 31 12:03:59.281: INFO: Created: latency-svc-rn59x
Jul 31 12:03:59.302: INFO: Got endpoints: latency-svc-rn59x [871.00876ms]
Jul 31 12:03:59.363: INFO: Created: latency-svc-2rsz4
Jul 31 12:03:59.363: INFO: Got endpoints: latency-svc-2rsz4 [801.681831ms]
Jul 31 12:03:59.412: INFO: Created: latency-svc-bmxkc
Jul 31 12:03:59.425: INFO: Got endpoints: latency-svc-bmxkc [835.559108ms]
Jul 31 12:03:59.458: INFO: Created: latency-svc-46xgq
Jul 31 12:03:59.507: INFO: Got endpoints: latency-svc-46xgq [868.906038ms]
Jul 31 12:03:59.517: INFO: Created: latency-svc-zhk89
Jul 31 12:03:59.539: INFO: Got endpoints: latency-svc-zhk89 [811.584154ms]
Jul 31 12:03:59.581: INFO: Created: latency-svc-82dk6
Jul 31 12:03:59.663: INFO: Got endpoints: latency-svc-82dk6 [895.293127ms]
Jul 31 12:03:59.681: INFO: Created: latency-svc-hlzfz
Jul 31 12:03:59.693: INFO: Got endpoints: latency-svc-hlzfz [855.170662ms]
Jul 31 12:03:59.718: INFO: Created: latency-svc-5tmhl
Jul 31 12:03:59.739: INFO: Got endpoints: latency-svc-5tmhl [838.680999ms]
Jul 31 12:03:59.807: INFO: Created: latency-svc-rrh5p
Jul 31 12:03:59.813: INFO: Got endpoints: latency-svc-rrh5p [834.702585ms]
Jul 31 12:03:59.856: INFO: Created: latency-svc-99hpm
Jul 31 12:03:59.886: INFO: Got endpoints: latency-svc-99hpm [861.888882ms]
Jul 31 12:03:59.910: INFO: Created: latency-svc-7w49d
Jul 31 12:03:59.911: INFO: Got endpoints: latency-svc-7w49d [814.860445ms]
Jul 31 12:03:59.954: INFO: Created: latency-svc-mx284
Jul 31 12:04:00.001: INFO: Got endpoints: latency-svc-mx284 [845.85869ms]
Jul 31 12:04:00.016: INFO: Created: latency-svc-lq6zm
Jul 31 12:04:00.035: INFO: Got endpoints: latency-svc-lq6zm [855.579859ms]
Jul 31 12:04:00.050: INFO: Created: latency-svc-v9d5m
Jul 31 12:04:00.050: INFO: Got endpoints: latency-svc-v9d5m [815.474119ms]
Jul 31 12:04:00.084: INFO: Created: latency-svc-9r5rq
Jul 31 12:04:00.111: INFO: Got endpoints: latency-svc-9r5rq [852.451863ms]
Jul 31 12:04:00.156: INFO: Created: latency-svc-7zr7w
Jul 31 12:04:00.156: INFO: Got endpoints: latency-svc-7zr7w [853.816267ms]
Jul 31 12:04:00.182: INFO: Created: latency-svc-7wzqk
Jul 31 12:04:00.191: INFO: Got endpoints: latency-svc-7wzqk [827.725549ms]
Jul 31 12:04:00.225: INFO: Created: latency-svc-j99nj
Jul 31 12:04:00.252: INFO: Got endpoints: latency-svc-j99nj [826.15946ms]
Jul 31 12:04:00.288: INFO: Created: latency-svc-z5lhw
Jul 31 12:04:00.301: INFO: Got endpoints: latency-svc-z5lhw [793.328221ms]
Jul 31 12:04:00.329: INFO: Created: latency-svc-t4nvz
Jul 31 12:04:00.339: INFO: Got endpoints: latency-svc-t4nvz [800.191075ms]
Jul 31 12:04:00.386: INFO: Created: latency-svc-7qd8j
Jul 31 12:04:00.399: INFO: Got endpoints: latency-svc-7qd8j [736.519517ms]
Jul 31 12:04:00.449: INFO: Created: latency-svc-d5qwd
Jul 31 12:04:00.461: INFO: Got endpoints: latency-svc-d5qwd [767.834266ms]
Jul 31 12:04:00.518: INFO: Created: latency-svc-4g4l9
Jul 31 12:04:00.518: INFO: Got endpoints: latency-svc-4g4l9 [778.210945ms]
Jul 31 12:04:00.530: INFO: Created: latency-svc-xp8xc
Jul 31 12:04:00.537: INFO: Got endpoints: latency-svc-xp8xc [724.010198ms]
Jul 31 12:04:00.568: INFO: Created: latency-svc-fhxlv
Jul 31 12:04:00.577: INFO: Got endpoints: latency-svc-fhxlv [690.776967ms]
Jul 31 12:04:00.607: INFO: Created: latency-svc-crms8
Jul 31 12:04:00.682: INFO: Got endpoints: latency-svc-crms8 [770.073897ms]
Jul 31 12:04:00.705: INFO: Created: latency-svc-prhq7
Jul 31 12:04:00.732: INFO: Got endpoints: latency-svc-prhq7 [731.538447ms]
Jul 31 12:04:00.776: INFO: Created: latency-svc-v5bhn
Jul 31 12:04:00.856: INFO: Got endpoints: latency-svc-v5bhn [820.706938ms]
Jul 31 12:04:00.865: INFO: Created: latency-svc-hhzw4
Jul 31 12:04:00.866: INFO: Got endpoints: latency-svc-hhzw4 [815.934084ms]
Jul 31 12:04:00.910: INFO: Created: latency-svc-chfn4
Jul 31 12:04:00.933: INFO: Got endpoints: latency-svc-chfn4 [821.572796ms]
Jul 31 12:04:00.948: INFO: Created: latency-svc-fbg82
Jul 31 12:04:00.969: INFO: Got endpoints: latency-svc-fbg82 [812.428392ms]
Jul 31 12:04:01.007: INFO: Created: latency-svc-x2w9s
Jul 31 12:04:01.025: INFO: Got endpoints: latency-svc-x2w9s [834.298225ms]
Jul 31 12:04:01.062: INFO: Created: latency-svc-2686h
Jul 31 12:04:01.104: INFO: Got endpoints: latency-svc-2686h [852.292011ms]
Jul 31 12:04:01.117: INFO: Created: latency-svc-x9b69
Jul 31 12:04:01.123: INFO: Got endpoints: latency-svc-x9b69 [821.923002ms]
Jul 31 12:04:01.195: INFO: Created: latency-svc-lzsq5
Jul 31 12:04:01.272: INFO: Got endpoints: latency-svc-lzsq5 [932.882838ms]
Jul 31 12:04:01.279: INFO: Created: latency-svc-g7t4v
Jul 31 12:04:01.282: INFO: Got endpoints: latency-svc-g7t4v [882.220487ms]
Jul 31 12:04:01.314: INFO: Created: latency-svc-755dk
Jul 31 12:04:01.334: INFO: Got endpoints: latency-svc-755dk [873.573563ms]
Jul 31 12:04:01.371: INFO: Created: latency-svc-226b5
Jul 31 12:04:01.401: INFO: Got endpoints: latency-svc-226b5 [883.086364ms]
Jul 31 12:04:01.442: INFO: Created: latency-svc-67ml6
Jul 31 12:04:01.453: INFO: Got endpoints: latency-svc-67ml6 [916.67222ms]
Jul 31 12:04:01.534: INFO: Created: latency-svc-5r2mv
Jul 31 12:04:01.534: INFO: Got endpoints: latency-svc-5r2mv [957.414652ms]
Jul 31 12:04:01.561: INFO: Created: latency-svc-mqxxd
Jul 31 12:04:01.569: INFO: Got endpoints: latency-svc-mqxxd [886.93464ms]
Jul 31 12:04:01.585: INFO: Created: latency-svc-zg5tq
Jul 31 12:04:01.605: INFO: Got endpoints: latency-svc-zg5tq [872.452526ms]
Jul 31 12:04:01.667: INFO: Created: latency-svc-cdztr
Jul 31 12:04:01.701: INFO: Got endpoints: latency-svc-cdztr [845.318261ms]
Jul 31 12:04:01.721: INFO: Created: latency-svc-bwfmn
Jul 31 12:04:01.721: INFO: Got endpoints: latency-svc-bwfmn [854.785284ms]
Jul 31 12:04:01.821: INFO: Created: latency-svc-ltfnd
Jul 31 12:04:01.834: INFO: Got endpoints: latency-svc-ltfnd [900.724751ms]
Jul 31 12:04:01.864: INFO: Created: latency-svc-m8bhh
Jul 31 12:04:01.892: INFO: Got endpoints: latency-svc-m8bhh [923.370951ms]
Jul 31 12:04:01.909: INFO: Created: latency-svc-f8c7q
Jul 31 12:04:01.946: INFO: Got endpoints: latency-svc-f8c7q [920.583774ms]
Jul 31 12:04:01.960: INFO: Created: latency-svc-4clxf
Jul 31 12:04:01.969: INFO: Got endpoints: latency-svc-4clxf [864.710899ms]
Jul 31 12:04:02.006: INFO: Created: latency-svc-kw8jt
Jul 31 12:04:02.016: INFO: Got endpoints: latency-svc-kw8jt [893.66864ms]
Jul 31 12:04:02.054: INFO: Created: latency-svc-n92jf
Jul 31 12:04:02.054: INFO: Got endpoints: latency-svc-n92jf [781.129536ms]
Jul 31 12:04:02.122: INFO: Created: latency-svc-px4zk
Jul 31 12:04:02.178: INFO: Got endpoints: latency-svc-px4zk [896.203532ms]
Jul 31 12:04:02.185: INFO: Created: latency-svc-2zr2d
Jul 31 12:04:02.191: INFO: Got endpoints: latency-svc-2zr2d [856.381047ms]
Jul 31 12:04:02.228: INFO: Created: latency-svc-x9ckq
Jul 31 12:04:02.262: INFO: Got endpoints: latency-svc-x9ckq [861.511972ms]
Jul 31 12:04:02.312: INFO: Created: latency-svc-2v2kf
Jul 31 12:04:02.312: INFO: Got endpoints: latency-svc-2v2kf [858.824474ms]
Jul 31 12:04:02.358: INFO: Created: latency-svc-xbv6k
Jul 31 12:04:02.359: INFO: Got endpoints: latency-svc-xbv6k [824.780005ms]
Jul 31 12:04:02.485: INFO: Created: latency-svc-w4x54
Jul 31 12:04:02.486: INFO: Got endpoints: latency-svc-w4x54 [916.998304ms]
Jul 31 12:04:02.528: INFO: Created: latency-svc-qgn9g
Jul 31 12:04:02.543: INFO: Got endpoints: latency-svc-qgn9g [938.375889ms]
Jul 31 12:04:02.583: INFO: Created: latency-svc-h49rj
Jul 31 12:04:02.635: INFO: Got endpoints: latency-svc-h49rj [933.872885ms]
Jul 31 12:04:02.646: INFO: Created: latency-svc-ptdkp
Jul 31 12:04:02.662: INFO: Got endpoints: latency-svc-ptdkp [940.241866ms]
Jul 31 12:04:02.704: INFO: Created: latency-svc-5lr2k
Jul 31 12:04:02.737: INFO: Got endpoints: latency-svc-5lr2k [903.157101ms]
Jul 31 12:04:02.775: INFO: Created: latency-svc-g8qjg
Jul 31 12:04:02.784: INFO: Got endpoints: latency-svc-g8qjg [891.842679ms]
Jul 31 12:04:02.833: INFO: Created: latency-svc-7xkbk
Jul 31 12:04:02.834: INFO: Got endpoints: latency-svc-7xkbk [888.70768ms]
Jul 31 12:04:02.874: INFO: Created: latency-svc-7gvk7
Jul 31 12:04:02.908: INFO: Got endpoints: latency-svc-7gvk7 [939.207388ms]
Jul 31 12:04:02.937: INFO: Created: latency-svc-gjx62
Jul 31 12:04:02.947: INFO: Got endpoints: latency-svc-gjx62 [930.181761ms]
Jul 31 12:04:03.001: INFO: Created: latency-svc-g8h9d
Jul 31 12:04:03.041: INFO: Got endpoints: latency-svc-g8h9d [987.734138ms]
Jul 31 12:04:03.060: INFO: Created: latency-svc-pznhq
Jul 31 12:04:03.067: INFO: Got endpoints: latency-svc-pznhq [888.211988ms]
Jul 31 12:04:03.146: INFO: Created: latency-svc-kx7kv
Jul 31 12:04:03.156: INFO: Got endpoints: latency-svc-kx7kv [965.374576ms]
Jul 31 12:04:03.174: INFO: Created: latency-svc-lh4rj
Jul 31 12:04:03.185: INFO: Got endpoints: latency-svc-lh4rj [922.174822ms]
Jul 31 12:04:03.214: INFO: Created: latency-svc-rcmvb
Jul 31 12:04:03.251: INFO: Got endpoints: latency-svc-rcmvb [938.766793ms]
Jul 31 12:04:03.271: INFO: Created: latency-svc-jpphg
Jul 31 12:04:03.279: INFO: Got endpoints: latency-svc-jpphg [919.404409ms]
Jul 31 12:04:03.312: INFO: Created: latency-svc-mjslc
Jul 31 12:04:03.316: INFO: Got endpoints: latency-svc-mjslc [829.791947ms]
Jul 31 12:04:03.350: INFO: Created: latency-svc-r4qjp
Jul 31 12:04:03.375: INFO: Got endpoints: latency-svc-r4qjp [831.604888ms]
Jul 31 12:04:03.403: INFO: Created: latency-svc-496mj
Jul 31 12:04:03.419: INFO: Got endpoints: latency-svc-496mj [783.370961ms]
Jul 31 12:04:03.449: INFO: Created: latency-svc-vxkxn
Jul 31 12:04:03.460: INFO: Got endpoints: latency-svc-vxkxn [798.917807ms]
Jul 31 12:04:03.511: INFO: Created: latency-svc-t9h57
Jul 31 12:04:03.511: INFO: Got endpoints: latency-svc-t9h57 [774.427213ms]
Jul 31 12:04:03.565: INFO: Created: latency-svc-dsj5k
Jul 31 12:04:03.582: INFO: Got endpoints: latency-svc-dsj5k [797.85275ms]
Jul 31 12:04:03.664: INFO: Created: latency-svc-6whvf
Jul 31 12:04:03.666: INFO: Got endpoints: latency-svc-6whvf [831.026445ms]
Jul 31 12:04:03.710: INFO: Created: latency-svc-pslbl
Jul 31 12:04:03.722: INFO: Got endpoints: latency-svc-pslbl [813.661811ms]
Jul 31 12:04:03.743: INFO: Created: latency-svc-9nltj
Jul 31 12:04:03.791: INFO: Got endpoints: latency-svc-9nltj [844.342968ms]
Jul 31 12:04:03.800: INFO: Created: latency-svc-vlc5m
Jul 31 12:04:03.831: INFO: Got endpoints: latency-svc-vlc5m [789.124951ms]
Jul 31 12:04:03.842: INFO: Created: latency-svc-88ddd
Jul 31 12:04:03.855: INFO: Got endpoints: latency-svc-88ddd [788.702291ms]
Jul 31 12:04:03.928: INFO: Created: latency-svc-fq8sw
Jul 31 12:04:03.930: INFO: Got endpoints: latency-svc-fq8sw [773.966293ms]
Jul 31 12:04:03.964: INFO: Created: latency-svc-w9rwk
Jul 31 12:04:03.967: INFO: Got endpoints: latency-svc-w9rwk [782.288028ms]
Jul 31 12:04:04.009: INFO: Created: latency-svc-lmxrn
Jul 31 12:04:04.038: INFO: Got endpoints: latency-svc-lmxrn [787.015625ms]
Jul 31 12:04:04.050: INFO: Created: latency-svc-fdtxk
Jul 31 12:04:04.052: INFO: Got endpoints: latency-svc-fdtxk [773.708541ms]
Jul 31 12:04:04.094: INFO: Created: latency-svc-ff8lc
Jul 31 12:04:04.105: INFO: Got endpoints: latency-svc-ff8lc [788.93804ms]
Jul 31 12:04:04.163: INFO: Created: latency-svc-rg2vj
Jul 31 12:04:04.163: INFO: Got endpoints: latency-svc-rg2vj [787.992147ms]
Jul 31 12:04:04.191: INFO: Created: latency-svc-t426z
Jul 31 12:04:04.205: INFO: Got endpoints: latency-svc-t426z [786.534867ms]
Jul 31 12:04:04.232: INFO: Created: latency-svc-k4ts8
Jul 31 12:04:04.299: INFO: Got endpoints: latency-svc-k4ts8 [838.521328ms]
Jul 31 12:04:04.315: INFO: Created: latency-svc-52bff
Jul 31 12:04:04.326: INFO: Got endpoints: latency-svc-52bff [814.315314ms]
Jul 31 12:04:04.355: INFO: Created: latency-svc-nq5v8
Jul 31 12:04:04.360: INFO: Got endpoints: latency-svc-nq5v8 [777.607308ms]
Jul 31 12:04:04.389: INFO: Created: latency-svc-ldzfs
Jul 31 12:04:04.428: INFO: Got endpoints: latency-svc-ldzfs [762.151716ms]
Jul 31 12:04:04.433: INFO: Created: latency-svc-jmnqn
Jul 31 12:04:04.450: INFO: Got endpoints: latency-svc-jmnqn [728.449541ms]
Jul 31 12:04:04.488: INFO: Created: latency-svc-499lj
Jul 31 12:04:04.496: INFO: Got endpoints: latency-svc-499lj [704.438773ms]
Jul 31 12:04:04.496: INFO: Latencies: [49.318651ms 112.811012ms 119.851389ms 120.440613ms 140.312643ms 173.26757ms 183.374911ms 218.969668ms 231.476495ms 254.824171ms 276.341088ms 311.839604ms 331.605046ms 343.515376ms 345.605508ms 389.467239ms 397.694021ms 413.334425ms 427.782249ms 463.803253ms 476.120034ms 499.243546ms 499.529675ms 514.428636ms 517.734316ms 531.163005ms 551.17448ms 558.242162ms 583.065037ms 594.633619ms 610.053495ms 614.819562ms 634.040452ms 637.172347ms 641.574188ms 663.372859ms 666.016105ms 667.777034ms 672.377537ms 675.352205ms 676.522015ms 677.115888ms 680.588983ms 684.072554ms 688.487435ms 690.500154ms 690.776967ms 691.306742ms 692.542625ms 697.381216ms 701.942293ms 703.36663ms 704.438773ms 705.073499ms 705.402525ms 705.496471ms 713.422776ms 716.102429ms 717.632088ms 718.380034ms 723.169793ms 724.010198ms 728.449541ms 731.538447ms 736.519517ms 743.09955ms 752.037531ms 756.282692ms 762.151716ms 767.834266ms 770.073897ms 773.708541ms 773.966293ms 774.427213ms 777.607308ms 778.210945ms 781.129536ms 782.288028ms 783.370961ms 786.534867ms 787.015625ms 787.992147ms 788.702291ms 788.93804ms 789.124951ms 793.32001ms 793.328221ms 797.85275ms 798.917807ms 800.191075ms 801.681831ms 811.584154ms 812.428392ms 813.661811ms 814.315314ms 814.860445ms 815.474119ms 815.934084ms 820.706938ms 821.572796ms 821.923002ms 824.780005ms 825.020462ms 826.15946ms 827.725549ms 829.791947ms 831.026445ms 831.604888ms 834.298225ms 834.702585ms 835.559108ms 838.521328ms 838.680999ms 844.342968ms 845.318261ms 845.85869ms 852.292011ms 852.451863ms 853.816267ms 854.785284ms 855.170662ms 855.579859ms 856.381047ms 858.824474ms 861.511972ms 861.825441ms 861.888882ms 862.731297ms 864.710899ms 867.590073ms 868.906038ms 870.965829ms 871.008098ms 871.00876ms 872.452526ms 873.573563ms 878.441068ms 881.588336ms 882.220487ms 883.086364ms 886.93464ms 888.211988ms 888.70768ms 891.842679ms 892.887552ms 893.66864ms 895.293127ms 896.203532ms 900.724751ms 903.157101ms 905.533927ms 906.81774ms 909.60526ms 910.588711ms 912.041626ms 914.16145ms 916.67222ms 916.998304ms 917.782944ms 918.41071ms 919.325279ms 919.404409ms 920.583774ms 922.174822ms 923.370951ms 926.792165ms 928.322204ms 929.912706ms 930.181761ms 932.882838ms 933.48543ms 933.872885ms 938.375889ms 938.766793ms 939.207388ms 940.241866ms 947.828961ms 950.884796ms 951.682918ms 952.638301ms 957.414652ms 961.701039ms 965.374576ms 979.427107ms 981.477417ms 987.734138ms 989.416966ms 990.764716ms 1.01600144s 1.02517583s 1.028910615s 1.038071264s 1.125264717s 1.126728793s 1.334127792s 1.434942734s 1.466291764s 1.525336156s 1.547586729s 1.572829706s]
Jul 31 12:04:04.496: INFO: 50 %ile: 821.923002ms
Jul 31 12:04:04.496: INFO: 90 %ile: 957.414652ms
Jul 31 12:04:04.496: INFO: 99 %ile: 1.547586729s
Jul 31 12:04:04.496: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:04:04.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-3026" for this suite.

• [SLOW TEST:13.315 seconds]
[sig-network] Service endpoints latency
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":346,"completed":309,"skipped":5821,"failed":0}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:04:04.753: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:244
[BeforeEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1573
[It] should update a single-container pod's image  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: running the image k8s.gcr.io/e2e-test-images/httpd:2.4.38-2
Jul 31 12:04:04.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-3539 run e2e-test-httpd-pod --image=k8s.gcr.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jul 31 12:04:04.966: INFO: stderr: ""
Jul 31 12:04:04.966: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Jul 31 12:04:10.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-3539 get pod e2e-test-httpd-pod -o json'
Jul 31 12:04:10.113: INFO: stderr: ""
Jul 31 12:04:10.113: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2022-07-31T12:04:04Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-3539\",\n        \"resourceVersion\": \"40359\",\n        \"uid\": \"617bb618-aff5-4542-9987-53e2004b6a4a\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"Always\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-tcflv\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"p1-eipeq63rfgo6ooocmu3kqk3tcc\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-tcflv\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-07-31T12:04:04Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-07-31T12:04:07Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-07-31T12:04:07Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2022-07-31T12:04:04Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://6ac513fcbdac6b92b5ab47f2f15f1e92ed88fb73f4d100dc0ab97b432df8dba6\",\n                \"image\": \"k8s.gcr.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"docker-pullable://k8s.gcr.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2022-07-31T12:04:06Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.27.21.69\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.28.0.2\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.28.0.2\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2022-07-31T12:04:04Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jul 31 12:04:10.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-3539 replace -f -'
Jul 31 12:04:10.873: INFO: stderr: ""
Jul 31 12:04:10.873: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image k8s.gcr.io/e2e-test-images/busybox:1.29-2
[AfterEach] Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1577
Jul 31 12:04:10.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=kubectl-3539 delete pods e2e-test-httpd-pod'
Jul 31 12:04:14.693: INFO: stderr: ""
Jul 31 12:04:14.693: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:04:14.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3539" for this suite.

• [SLOW TEST:10.008 seconds]
[sig-cli] Kubectl client
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1570
    should update a single-container pod's image  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":346,"completed":310,"skipped":5829,"failed":0}
SSSSSSS
------------------------------
[sig-node] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:04:14.762: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating secret secrets-1683/secret-test-2e1ec1f8-df96-4549-bc5f-dc03e45b59e6
STEP: Creating a pod to test consume secrets
Jul 31 12:04:14.973: INFO: Waiting up to 5m0s for pod "pod-configmaps-35020200-d7dd-48f0-b2ec-f1650cdd580e" in namespace "secrets-1683" to be "Succeeded or Failed"
Jul 31 12:04:15.024: INFO: Pod "pod-configmaps-35020200-d7dd-48f0-b2ec-f1650cdd580e": Phase="Pending", Reason="", readiness=false. Elapsed: 50.916868ms
Jul 31 12:04:17.092: INFO: Pod "pod-configmaps-35020200-d7dd-48f0-b2ec-f1650cdd580e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.118528352s
Jul 31 12:04:19.113: INFO: Pod "pod-configmaps-35020200-d7dd-48f0-b2ec-f1650cdd580e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.139394221s
STEP: Saw pod success
Jul 31 12:04:19.113: INFO: Pod "pod-configmaps-35020200-d7dd-48f0-b2ec-f1650cdd580e" satisfied condition "Succeeded or Failed"
Jul 31 12:04:19.139: INFO: Trying to get logs from node p1-ashfcfj4pzo6aemt1j3a9ah7ew pod pod-configmaps-35020200-d7dd-48f0-b2ec-f1650cdd580e container env-test: <nil>
STEP: delete the pod
Jul 31 12:04:19.243: INFO: Waiting for pod pod-configmaps-35020200-d7dd-48f0-b2ec-f1650cdd580e to disappear
Jul 31 12:04:19.250: INFO: Pod pod-configmaps-35020200-d7dd-48f0-b2ec-f1650cdd580e no longer exists
[AfterEach] [sig-node] Secrets
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:04:19.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1683" for this suite.
•{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":346,"completed":311,"skipped":5836,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:04:19.317: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename certificates
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Jul 31 12:04:20.561: INFO: starting watch
STEP: patching
STEP: updating
Jul 31 12:04:20.597: INFO: waiting for watch events with expected annotations
Jul 31 12:04:20.597: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:04:20.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-7676" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":346,"completed":312,"skipped":5866,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces 
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:04:20.935: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename disruption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/disruption.go:69
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:04:21.003: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename disruption-2
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: Waiting for the pdb to be processed
STEP: listing a collection of PDBs across all namespaces
STEP: listing a collection of PDBs in namespace disruption-8909
STEP: deleting a collection of PDBs
STEP: Waiting for the PDB collection to be deleted
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:04:21.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-6134" for this suite.
[AfterEach] [sig-apps] DisruptionController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:04:21.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-8909" for this suite.
•{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","total":346,"completed":313,"skipped":5905,"failed":0}
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:04:21.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 12:04:21.638: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:04:28.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6437" for this suite.

• [SLOW TEST:6.721 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":346,"completed":314,"skipped":5907,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:04:28.257: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward API volume plugin
Jul 31 12:04:28.329: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fd30b95d-5ed2-4d53-94e2-e73388cd49eb" in namespace "projected-3663" to be "Succeeded or Failed"
Jul 31 12:04:28.337: INFO: Pod "downwardapi-volume-fd30b95d-5ed2-4d53-94e2-e73388cd49eb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.975386ms
Jul 31 12:04:30.351: INFO: Pod "downwardapi-volume-fd30b95d-5ed2-4d53-94e2-e73388cd49eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021935606s
Jul 31 12:04:32.367: INFO: Pod "downwardapi-volume-fd30b95d-5ed2-4d53-94e2-e73388cd49eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038327758s
STEP: Saw pod success
Jul 31 12:04:32.367: INFO: Pod "downwardapi-volume-fd30b95d-5ed2-4d53-94e2-e73388cd49eb" satisfied condition "Succeeded or Failed"
Jul 31 12:04:32.373: INFO: Trying to get logs from node p1-ashfcfj4pzo6aemt1j3a9ah7ew pod downwardapi-volume-fd30b95d-5ed2-4d53-94e2-e73388cd49eb container client-container: <nil>
STEP: delete the pod
Jul 31 12:04:32.426: INFO: Waiting for pod downwardapi-volume-fd30b95d-5ed2-4d53-94e2-e73388cd49eb to disappear
Jul 31 12:04:32.431: INFO: Pod downwardapi-volume-fd30b95d-5ed2-4d53-94e2-e73388cd49eb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:04:32.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3663" for this suite.
•{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":315,"skipped":6025,"failed":0}
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:04:32.453: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:04:43.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2617" for this suite.

• [SLOW TEST:11.220 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":346,"completed":316,"skipped":6030,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:04:43.678: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 12:04:43.787: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jul 31 12:04:45.885: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:04:46.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8851" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":346,"completed":317,"skipped":6071,"failed":0}
SSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:04:46.918: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:53
STEP: create the container to handle the HTTPGet hook request.
Jul 31 12:04:47.060: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jul 31 12:04:49.071: INFO: The status of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jul 31 12:04:51.073: INFO: The status of Pod pod-handle-http-request is Running (Ready = true)
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the pod with lifecycle hook
Jul 31 12:04:51.115: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jul 31 12:04:53.126: INFO: The status of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jul 31 12:04:55.129: INFO: The status of Pod pod-with-poststart-exec-hook is Running (Ready = true)
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jul 31 12:04:55.191: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 31 12:04:55.197: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 31 12:04:57.198: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 31 12:04:57.208: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:04:57.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9932" for this suite.

• [SLOW TEST:10.314 seconds]
[sig-node] Container Lifecycle Hook
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/lifecycle_hook.go:44
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":346,"completed":318,"skipped":6077,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:04:57.233: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod with failed condition
STEP: updating the pod
Jul 31 12:06:57.907: INFO: Successfully updated pod "var-expansion-da05dc91-2a1b-423e-8dc9-0b104511c16e"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Jul 31 12:06:59.930: INFO: Deleting pod "var-expansion-da05dc91-2a1b-423e-8dc9-0b104511c16e" in namespace "var-expansion-2431"
Jul 31 12:06:59.941: INFO: Wait up to 5m0s for pod "var-expansion-da05dc91-2a1b-423e-8dc9-0b104511c16e" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:07:33.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2431" for this suite.

• [SLOW TEST:156.752 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","total":346,"completed":319,"skipped":6116,"failed":0}
SSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:07:33.987: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a pod to test downward api env vars
Jul 31 12:07:34.080: INFO: Waiting up to 5m0s for pod "downward-api-9a97ddc7-a19e-43c7-9fef-d6822c11b720" in namespace "downward-api-8278" to be "Succeeded or Failed"
Jul 31 12:07:34.098: INFO: Pod "downward-api-9a97ddc7-a19e-43c7-9fef-d6822c11b720": Phase="Pending", Reason="", readiness=false. Elapsed: 17.462264ms
Jul 31 12:07:36.112: INFO: Pod "downward-api-9a97ddc7-a19e-43c7-9fef-d6822c11b720": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031270193s
Jul 31 12:07:38.130: INFO: Pod "downward-api-9a97ddc7-a19e-43c7-9fef-d6822c11b720": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050037188s
STEP: Saw pod success
Jul 31 12:07:38.131: INFO: Pod "downward-api-9a97ddc7-a19e-43c7-9fef-d6822c11b720" satisfied condition "Succeeded or Failed"
Jul 31 12:07:38.137: INFO: Trying to get logs from node p1-nc3cz44465xd41a6poxnmrpeqo pod downward-api-9a97ddc7-a19e-43c7-9fef-d6822c11b720 container dapi-container: <nil>
STEP: delete the pod
Jul 31 12:07:38.200: INFO: Waiting for pod downward-api-9a97ddc7-a19e-43c7-9fef-d6822c11b720 to disappear
Jul 31 12:07:38.217: INFO: Pod downward-api-9a97ddc7-a19e-43c7-9fef-d6822c11b720 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:07:38.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8278" for this suite.
•{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":346,"completed":320,"skipped":6120,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:07:38.242: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:07:55.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4307" for this suite.

• [SLOW TEST:17.421 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":346,"completed":321,"skipped":6167,"failed":0}
SS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:07:55.663: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jul 31 12:07:55.781: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 31 12:08:55.866: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create pods that use 4/5 of node resources.
Jul 31 12:08:55.928: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jul 31 12:08:55.951: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jul 31 12:08:56.018: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jul 31 12:08:56.046: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jul 31 12:08:56.080: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jul 31 12:08:56.099: INFO: Created pod: pod2-1-sched-preemption-medium-priority
Jul 31 12:08:56.138: INFO: Created pod: pod3-0-sched-preemption-medium-priority
Jul 31 12:08:56.152: INFO: Created pod: pod3-1-sched-preemption-medium-priority
Jul 31 12:08:56.221: INFO: Created pod: pod4-0-sched-preemption-medium-priority
Jul 31 12:08:56.236: INFO: Created pod: pod4-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a high priority pod that has same requirements as that of lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:09:08.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-8587" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:72.883 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","total":346,"completed":322,"skipped":6169,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:09:08.548: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-n6zz2 in namespace proxy-8154
I0731 12:09:08.674707      23 runners.go:193] Created replication controller with name: proxy-service-n6zz2, namespace: proxy-8154, replica count: 1
I0731 12:09:09.725351      23 runners.go:193] proxy-service-n6zz2 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0731 12:09:10.726097      23 runners.go:193] proxy-service-n6zz2 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 31 12:09:10.734: INFO: setup took 2.124507703s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jul 31 12:09:10.748: INFO: (0) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:162/proxy/: bar (200; 13.734439ms)
Jul 31 12:09:10.752: INFO: (0) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:1080/proxy/rewriteme">test<... (200; 17.288105ms)
Jul 31 12:09:10.752: INFO: (0) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:160/proxy/: foo (200; 17.206026ms)
Jul 31 12:09:10.752: INFO: (0) /api/v1/namespaces/proxy-8154/services/http:proxy-service-n6zz2:portname2/proxy/: bar (200; 17.403335ms)
Jul 31 12:09:10.752: INFO: (0) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:160/proxy/: foo (200; 17.765587ms)
Jul 31 12:09:10.758: INFO: (0) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:1080/proxy/rewriteme">... (200; 23.476136ms)
Jul 31 12:09:10.761: INFO: (0) /api/v1/namespaces/proxy-8154/services/proxy-service-n6zz2:portname2/proxy/: bar (200; 26.201878ms)
Jul 31 12:09:10.761: INFO: (0) /api/v1/namespaces/proxy-8154/services/http:proxy-service-n6zz2:portname1/proxy/: foo (200; 26.206711ms)
Jul 31 12:09:10.761: INFO: (0) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:162/proxy/: bar (200; 26.537694ms)
Jul 31 12:09:10.761: INFO: (0) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l/proxy/rewriteme">test</a> (200; 26.474943ms)
Jul 31 12:09:10.762: INFO: (0) /api/v1/namespaces/proxy-8154/services/https:proxy-service-n6zz2:tlsportname2/proxy/: tls qux (200; 26.807416ms)
Jul 31 12:09:10.762: INFO: (0) /api/v1/namespaces/proxy-8154/services/proxy-service-n6zz2:portname1/proxy/: foo (200; 26.691416ms)
Jul 31 12:09:10.762: INFO: (0) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:462/proxy/: tls qux (200; 26.795148ms)
Jul 31 12:09:10.762: INFO: (0) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:443/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:443/proxy/tlsrewritem... (200; 27.164938ms)
Jul 31 12:09:10.763: INFO: (0) /api/v1/namespaces/proxy-8154/services/https:proxy-service-n6zz2:tlsportname1/proxy/: tls baz (200; 28.526546ms)
Jul 31 12:09:10.763: INFO: (0) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:460/proxy/: tls baz (200; 28.837556ms)
Jul 31 12:09:10.773: INFO: (1) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:162/proxy/: bar (200; 9.509491ms)
Jul 31 12:09:10.780: INFO: (1) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:162/proxy/: bar (200; 16.209369ms)
Jul 31 12:09:10.780: INFO: (1) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:1080/proxy/rewriteme">test<... (200; 16.045871ms)
Jul 31 12:09:10.780: INFO: (1) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l/proxy/rewriteme">test</a> (200; 16.578122ms)
Jul 31 12:09:10.782: INFO: (1) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:160/proxy/: foo (200; 17.746462ms)
Jul 31 12:09:10.783: INFO: (1) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:1080/proxy/rewriteme">... (200; 19.031142ms)
Jul 31 12:09:10.783: INFO: (1) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:443/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:443/proxy/tlsrewritem... (200; 18.598971ms)
Jul 31 12:09:10.783: INFO: (1) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:462/proxy/: tls qux (200; 18.878802ms)
Jul 31 12:09:10.783: INFO: (1) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:160/proxy/: foo (200; 18.6587ms)
Jul 31 12:09:10.783: INFO: (1) /api/v1/namespaces/proxy-8154/services/https:proxy-service-n6zz2:tlsportname1/proxy/: tls baz (200; 19.147058ms)
Jul 31 12:09:10.783: INFO: (1) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:460/proxy/: tls baz (200; 18.873805ms)
Jul 31 12:09:10.784: INFO: (1) /api/v1/namespaces/proxy-8154/services/http:proxy-service-n6zz2:portname2/proxy/: bar (200; 19.969354ms)
Jul 31 12:09:10.787: INFO: (1) /api/v1/namespaces/proxy-8154/services/https:proxy-service-n6zz2:tlsportname2/proxy/: tls qux (200; 23.247732ms)
Jul 31 12:09:10.788: INFO: (1) /api/v1/namespaces/proxy-8154/services/proxy-service-n6zz2:portname1/proxy/: foo (200; 24.294566ms)
Jul 31 12:09:10.788: INFO: (1) /api/v1/namespaces/proxy-8154/services/http:proxy-service-n6zz2:portname1/proxy/: foo (200; 24.317762ms)
Jul 31 12:09:10.788: INFO: (1) /api/v1/namespaces/proxy-8154/services/proxy-service-n6zz2:portname2/proxy/: bar (200; 24.311656ms)
Jul 31 12:09:10.796: INFO: (2) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:1080/proxy/rewriteme">... (200; 7.898285ms)
Jul 31 12:09:10.801: INFO: (2) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l/proxy/rewriteme">test</a> (200; 12.272307ms)
Jul 31 12:09:10.803: INFO: (2) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:162/proxy/: bar (200; 14.000567ms)
Jul 31 12:09:10.803: INFO: (2) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:1080/proxy/rewriteme">test<... (200; 13.991371ms)
Jul 31 12:09:10.803: INFO: (2) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:460/proxy/: tls baz (200; 14.112721ms)
Jul 31 12:09:10.804: INFO: (2) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:462/proxy/: tls qux (200; 15.343399ms)
Jul 31 12:09:10.804: INFO: (2) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:162/proxy/: bar (200; 15.335013ms)
Jul 31 12:09:10.804: INFO: (2) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:160/proxy/: foo (200; 15.6251ms)
Jul 31 12:09:10.804: INFO: (2) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:160/proxy/: foo (200; 15.440108ms)
Jul 31 12:09:10.804: INFO: (2) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:443/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:443/proxy/tlsrewritem... (200; 15.597111ms)
Jul 31 12:09:10.805: INFO: (2) /api/v1/namespaces/proxy-8154/services/https:proxy-service-n6zz2:tlsportname1/proxy/: tls baz (200; 16.666279ms)
Jul 31 12:09:10.807: INFO: (2) /api/v1/namespaces/proxy-8154/services/proxy-service-n6zz2:portname1/proxy/: foo (200; 18.955393ms)
Jul 31 12:09:10.807: INFO: (2) /api/v1/namespaces/proxy-8154/services/proxy-service-n6zz2:portname2/proxy/: bar (200; 18.811443ms)
Jul 31 12:09:10.808: INFO: (2) /api/v1/namespaces/proxy-8154/services/http:proxy-service-n6zz2:portname2/proxy/: bar (200; 19.06873ms)
Jul 31 12:09:10.808: INFO: (2) /api/v1/namespaces/proxy-8154/services/https:proxy-service-n6zz2:tlsportname2/proxy/: tls qux (200; 19.052486ms)
Jul 31 12:09:10.808: INFO: (2) /api/v1/namespaces/proxy-8154/services/http:proxy-service-n6zz2:portname1/proxy/: foo (200; 19.020761ms)
Jul 31 12:09:10.815: INFO: (3) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:460/proxy/: tls baz (200; 7.681258ms)
Jul 31 12:09:10.820: INFO: (3) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:1080/proxy/rewriteme">... (200; 11.112478ms)
Jul 31 12:09:10.821: INFO: (3) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:160/proxy/: foo (200; 13.044119ms)
Jul 31 12:09:10.822: INFO: (3) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:162/proxy/: bar (200; 13.460107ms)
Jul 31 12:09:10.822: INFO: (3) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:462/proxy/: tls qux (200; 13.439068ms)
Jul 31 12:09:10.823: INFO: (3) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:160/proxy/: foo (200; 13.866682ms)
Jul 31 12:09:10.823: INFO: (3) /api/v1/namespaces/proxy-8154/services/http:proxy-service-n6zz2:portname2/proxy/: bar (200; 14.298166ms)
Jul 31 12:09:10.823: INFO: (3) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l/proxy/rewriteme">test</a> (200; 14.075322ms)
Jul 31 12:09:10.823: INFO: (3) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:162/proxy/: bar (200; 14.282158ms)
Jul 31 12:09:10.823: INFO: (3) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:1080/proxy/rewriteme">test<... (200; 15.575495ms)
Jul 31 12:09:10.824: INFO: (3) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:443/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:443/proxy/tlsrewritem... (200; 15.326997ms)
Jul 31 12:09:10.827: INFO: (3) /api/v1/namespaces/proxy-8154/services/https:proxy-service-n6zz2:tlsportname2/proxy/: tls qux (200; 18.260569ms)
Jul 31 12:09:10.830: INFO: (3) /api/v1/namespaces/proxy-8154/services/proxy-service-n6zz2:portname2/proxy/: bar (200; 21.412521ms)
Jul 31 12:09:10.830: INFO: (3) /api/v1/namespaces/proxy-8154/services/proxy-service-n6zz2:portname1/proxy/: foo (200; 21.930167ms)
Jul 31 12:09:10.831: INFO: (3) /api/v1/namespaces/proxy-8154/services/https:proxy-service-n6zz2:tlsportname1/proxy/: tls baz (200; 21.887932ms)
Jul 31 12:09:10.831: INFO: (3) /api/v1/namespaces/proxy-8154/services/http:proxy-service-n6zz2:portname1/proxy/: foo (200; 21.969416ms)
Jul 31 12:09:10.844: INFO: (4) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:160/proxy/: foo (200; 13.62787ms)
Jul 31 12:09:10.851: INFO: (4) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l/proxy/rewriteme">test</a> (200; 20.448547ms)
Jul 31 12:09:10.854: INFO: (4) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:1080/proxy/rewriteme">... (200; 22.772824ms)
Jul 31 12:09:10.854: INFO: (4) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:1080/proxy/rewriteme">test<... (200; 22.956983ms)
Jul 31 12:09:10.854: INFO: (4) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:162/proxy/: bar (200; 22.870379ms)
Jul 31 12:09:10.854: INFO: (4) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:162/proxy/: bar (200; 22.904899ms)
Jul 31 12:09:10.855: INFO: (4) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:462/proxy/: tls qux (200; 23.616937ms)
Jul 31 12:09:10.855: INFO: (4) /api/v1/namespaces/proxy-8154/services/https:proxy-service-n6zz2:tlsportname2/proxy/: tls qux (200; 23.834587ms)
Jul 31 12:09:10.855: INFO: (4) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:160/proxy/: foo (200; 24.419921ms)
Jul 31 12:09:10.855: INFO: (4) /api/v1/namespaces/proxy-8154/services/proxy-service-n6zz2:portname1/proxy/: foo (200; 24.380552ms)
Jul 31 12:09:10.856: INFO: (4) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:460/proxy/: tls baz (200; 25.167512ms)
Jul 31 12:09:10.856: INFO: (4) /api/v1/namespaces/proxy-8154/services/http:proxy-service-n6zz2:portname2/proxy/: bar (200; 25.590329ms)
Jul 31 12:09:10.856: INFO: (4) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:443/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:443/proxy/tlsrewritem... (200; 25.650468ms)
Jul 31 12:09:10.857: INFO: (4) /api/v1/namespaces/proxy-8154/services/http:proxy-service-n6zz2:portname1/proxy/: foo (200; 26.150673ms)
Jul 31 12:09:10.857: INFO: (4) /api/v1/namespaces/proxy-8154/services/proxy-service-n6zz2:portname2/proxy/: bar (200; 26.225043ms)
Jul 31 12:09:10.857: INFO: (4) /api/v1/namespaces/proxy-8154/services/https:proxy-service-n6zz2:tlsportname1/proxy/: tls baz (200; 26.409742ms)
Jul 31 12:09:10.865: INFO: (5) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:460/proxy/: tls baz (200; 7.586228ms)
Jul 31 12:09:10.871: INFO: (5) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:160/proxy/: foo (200; 13.007813ms)
Jul 31 12:09:10.871: INFO: (5) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:162/proxy/: bar (200; 12.966535ms)
Jul 31 12:09:10.872: INFO: (5) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:1080/proxy/rewriteme">test<... (200; 14.563904ms)
Jul 31 12:09:10.872: INFO: (5) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:160/proxy/: foo (200; 14.539295ms)
Jul 31 12:09:10.872: INFO: (5) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:162/proxy/: bar (200; 14.504401ms)
Jul 31 12:09:10.873: INFO: (5) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:1080/proxy/rewriteme">... (200; 15.056516ms)
Jul 31 12:09:10.873: INFO: (5) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:443/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:443/proxy/tlsrewritem... (200; 15.243457ms)
Jul 31 12:09:10.873: INFO: (5) /api/v1/namespaces/proxy-8154/services/http:proxy-service-n6zz2:portname1/proxy/: foo (200; 15.682121ms)
Jul 31 12:09:10.874: INFO: (5) /api/v1/namespaces/proxy-8154/services/https:proxy-service-n6zz2:tlsportname2/proxy/: tls qux (200; 16.365586ms)
Jul 31 12:09:10.874: INFO: (5) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:462/proxy/: tls qux (200; 16.323499ms)
Jul 31 12:09:10.875: INFO: (5) /api/v1/namespaces/proxy-8154/services/http:proxy-service-n6zz2:portname2/proxy/: bar (200; 17.455278ms)
Jul 31 12:09:10.876: INFO: (5) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l/proxy/rewriteme">test</a> (200; 18.338078ms)
Jul 31 12:09:10.876: INFO: (5) /api/v1/namespaces/proxy-8154/services/proxy-service-n6zz2:portname2/proxy/: bar (200; 18.704545ms)
Jul 31 12:09:10.876: INFO: (5) /api/v1/namespaces/proxy-8154/services/proxy-service-n6zz2:portname1/proxy/: foo (200; 18.972007ms)
Jul 31 12:09:10.877: INFO: (5) /api/v1/namespaces/proxy-8154/services/https:proxy-service-n6zz2:tlsportname1/proxy/: tls baz (200; 18.984796ms)
Jul 31 12:09:10.900: INFO: (6) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:162/proxy/: bar (200; 23.334787ms)
Jul 31 12:09:10.900: INFO: (6) /api/v1/namespaces/proxy-8154/services/proxy-service-n6zz2:portname2/proxy/: bar (200; 23.280907ms)
Jul 31 12:09:10.900: INFO: (6) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:460/proxy/: tls baz (200; 23.305951ms)
Jul 31 12:09:10.901: INFO: (6) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:1080/proxy/rewriteme">... (200; 23.621809ms)
Jul 31 12:09:10.900: INFO: (6) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:160/proxy/: foo (200; 23.345901ms)
Jul 31 12:09:10.901: INFO: (6) /api/v1/namespaces/proxy-8154/services/proxy-service-n6zz2:portname1/proxy/: foo (200; 23.569333ms)
Jul 31 12:09:10.901: INFO: (6) /api/v1/namespaces/proxy-8154/services/https:proxy-service-n6zz2:tlsportname1/proxy/: tls baz (200; 23.766538ms)
Jul 31 12:09:10.901: INFO: (6) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:1080/proxy/rewriteme">test<... (200; 23.717732ms)
Jul 31 12:09:10.901: INFO: (6) /api/v1/namespaces/proxy-8154/services/http:proxy-service-n6zz2:portname2/proxy/: bar (200; 23.807528ms)
Jul 31 12:09:10.901: INFO: (6) /api/v1/namespaces/proxy-8154/services/http:proxy-service-n6zz2:portname1/proxy/: foo (200; 24.063688ms)
Jul 31 12:09:10.901: INFO: (6) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:160/proxy/: foo (200; 23.825066ms)
Jul 31 12:09:10.901: INFO: (6) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l/proxy/rewriteme">test</a> (200; 23.98829ms)
Jul 31 12:09:10.901: INFO: (6) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:462/proxy/: tls qux (200; 23.975062ms)
Jul 31 12:09:10.901: INFO: (6) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:443/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:443/proxy/tlsrewritem... (200; 24.059066ms)
Jul 31 12:09:10.901: INFO: (6) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:162/proxy/: bar (200; 23.993966ms)
Jul 31 12:09:10.901: INFO: (6) /api/v1/namespaces/proxy-8154/services/https:proxy-service-n6zz2:tlsportname2/proxy/: tls qux (200; 24.138016ms)
Jul 31 12:09:10.907: INFO: (7) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:162/proxy/: bar (200; 5.726459ms)
Jul 31 12:09:10.910: INFO: (7) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l/proxy/rewriteme">test</a> (200; 8.107326ms)
Jul 31 12:09:10.910: INFO: (7) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:443/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:443/proxy/tlsrewritem... (200; 8.44506ms)
Jul 31 12:09:10.913: INFO: (7) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:160/proxy/: foo (200; 10.960953ms)
Jul 31 12:09:10.913: INFO: (7) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:1080/proxy/rewriteme">test<... (200; 11.124506ms)
Jul 31 12:09:10.913: INFO: (7) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:460/proxy/: tls baz (200; 11.470964ms)
Jul 31 12:09:10.914: INFO: (7) /api/v1/namespaces/proxy-8154/services/http:proxy-service-n6zz2:portname1/proxy/: foo (200; 12.050259ms)
Jul 31 12:09:10.914: INFO: (7) /api/v1/namespaces/proxy-8154/services/http:proxy-service-n6zz2:portname2/proxy/: bar (200; 12.30329ms)
Jul 31 12:09:10.914: INFO: (7) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:160/proxy/: foo (200; 12.097361ms)
Jul 31 12:09:10.914: INFO: (7) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:162/proxy/: bar (200; 12.218968ms)
Jul 31 12:09:10.915: INFO: (7) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:462/proxy/: tls qux (200; 12.932636ms)
Jul 31 12:09:10.917: INFO: (7) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:1080/proxy/rewriteme">... (200; 15.21548ms)
Jul 31 12:09:10.918: INFO: (7) /api/v1/namespaces/proxy-8154/services/https:proxy-service-n6zz2:tlsportname2/proxy/: tls qux (200; 16.226226ms)
Jul 31 12:09:10.918: INFO: (7) /api/v1/namespaces/proxy-8154/services/proxy-service-n6zz2:portname1/proxy/: foo (200; 16.543497ms)
Jul 31 12:09:10.920: INFO: (7) /api/v1/namespaces/proxy-8154/services/proxy-service-n6zz2:portname2/proxy/: bar (200; 17.598909ms)
Jul 31 12:09:10.924: INFO: (7) /api/v1/namespaces/proxy-8154/services/https:proxy-service-n6zz2:tlsportname1/proxy/: tls baz (200; 22.335527ms)
Jul 31 12:09:10.942: INFO: (8) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:160/proxy/: foo (200; 18.091208ms)
Jul 31 12:09:10.944: INFO: (8) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:1080/proxy/rewriteme">... (200; 19.331785ms)
Jul 31 12:09:10.944: INFO: (8) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:162/proxy/: bar (200; 19.424008ms)
Jul 31 12:09:10.944: INFO: (8) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l/proxy/rewriteme">test</a> (200; 19.483947ms)
Jul 31 12:09:10.944: INFO: (8) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:460/proxy/: tls baz (200; 19.536384ms)
Jul 31 12:09:10.945: INFO: (8) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:162/proxy/: bar (200; 19.8762ms)
Jul 31 12:09:10.945: INFO: (8) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:160/proxy/: foo (200; 20.396813ms)
Jul 31 12:09:10.946: INFO: (8) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:1080/proxy/rewriteme">test<... (200; 21.089625ms)
Jul 31 12:09:10.946: INFO: (8) /api/v1/namespaces/proxy-8154/services/https:proxy-service-n6zz2:tlsportname1/proxy/: tls baz (200; 21.059019ms)
Jul 31 12:09:10.946: INFO: (8) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:462/proxy/: tls qux (200; 21.06738ms)
Jul 31 12:09:10.946: INFO: (8) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:443/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:443/proxy/tlsrewritem... (200; 20.995111ms)
Jul 31 12:09:10.950: INFO: (8) /api/v1/namespaces/proxy-8154/services/proxy-service-n6zz2:portname1/proxy/: foo (200; 25.202997ms)
Jul 31 12:09:10.953: INFO: (8) /api/v1/namespaces/proxy-8154/services/proxy-service-n6zz2:portname2/proxy/: bar (200; 27.941847ms)
Jul 31 12:09:10.953: INFO: (8) /api/v1/namespaces/proxy-8154/services/http:proxy-service-n6zz2:portname1/proxy/: foo (200; 28.387452ms)
Jul 31 12:09:10.953: INFO: (8) /api/v1/namespaces/proxy-8154/services/http:proxy-service-n6zz2:portname2/proxy/: bar (200; 28.64042ms)
Jul 31 12:09:10.953: INFO: (8) /api/v1/namespaces/proxy-8154/services/https:proxy-service-n6zz2:tlsportname2/proxy/: tls qux (200; 29.091643ms)
Jul 31 12:09:10.963: INFO: (9) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:443/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:443/proxy/tlsrewritem... (200; 8.836308ms)
Jul 31 12:09:10.969: INFO: (9) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:160/proxy/: foo (200; 14.756595ms)
Jul 31 12:09:10.969: INFO: (9) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l/proxy/rewriteme">test</a> (200; 14.835789ms)
Jul 31 12:09:10.969: INFO: (9) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:1080/proxy/rewriteme">test<... (200; 14.799041ms)
Jul 31 12:09:10.970: INFO: (9) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:160/proxy/: foo (200; 15.870664ms)
Jul 31 12:09:10.970: INFO: (9) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:462/proxy/: tls qux (200; 15.992318ms)
Jul 31 12:09:10.971: INFO: (9) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:162/proxy/: bar (200; 16.839183ms)
Jul 31 12:09:10.971: INFO: (9) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:162/proxy/: bar (200; 17.04088ms)
Jul 31 12:09:10.971: INFO: (9) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:1080/proxy/rewriteme">... (200; 17.679496ms)
Jul 31 12:09:10.971: INFO: (9) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:460/proxy/: tls baz (200; 17.814198ms)
Jul 31 12:09:10.977: INFO: (9) /api/v1/namespaces/proxy-8154/services/proxy-service-n6zz2:portname2/proxy/: bar (200; 23.426511ms)
Jul 31 12:09:10.980: INFO: (9) /api/v1/namespaces/proxy-8154/services/https:proxy-service-n6zz2:tlsportname1/proxy/: tls baz (200; 25.71921ms)
Jul 31 12:09:10.980: INFO: (9) /api/v1/namespaces/proxy-8154/services/https:proxy-service-n6zz2:tlsportname2/proxy/: tls qux (200; 26.100631ms)
Jul 31 12:09:10.980: INFO: (9) /api/v1/namespaces/proxy-8154/services/proxy-service-n6zz2:portname1/proxy/: foo (200; 26.292356ms)
Jul 31 12:09:10.980: INFO: (9) /api/v1/namespaces/proxy-8154/services/http:proxy-service-n6zz2:portname1/proxy/: foo (200; 26.275765ms)
Jul 31 12:09:10.980: INFO: (9) /api/v1/namespaces/proxy-8154/services/http:proxy-service-n6zz2:portname2/proxy/: bar (200; 26.568746ms)
Jul 31 12:09:10.989: INFO: (10) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:162/proxy/: bar (200; 8.796808ms)
Jul 31 12:09:10.994: INFO: (10) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:1080/proxy/rewriteme">test<... (200; 13.023734ms)
Jul 31 12:09:10.994: INFO: (10) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:443/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:443/proxy/tlsrewritem... (200; 13.366849ms)
Jul 31 12:09:10.994: INFO: (10) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:160/proxy/: foo (200; 13.468897ms)
Jul 31 12:09:10.994: INFO: (10) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:162/proxy/: bar (200; 13.462822ms)
Jul 31 12:09:10.994: INFO: (10) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:460/proxy/: tls baz (200; 13.366026ms)
Jul 31 12:09:10.994: INFO: (10) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:462/proxy/: tls qux (200; 13.369886ms)
Jul 31 12:09:10.994: INFO: (10) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:1080/proxy/rewriteme">... (200; 13.417979ms)
Jul 31 12:09:10.994: INFO: (10) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l/proxy/rewriteme">test</a> (200; 13.358367ms)
Jul 31 12:09:10.994: INFO: (10) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:160/proxy/: foo (200; 13.411964ms)
Jul 31 12:09:11.005: INFO: (10) /api/v1/namespaces/proxy-8154/services/proxy-service-n6zz2:portname1/proxy/: foo (200; 24.48237ms)
Jul 31 12:09:11.009: INFO: (10) /api/v1/namespaces/proxy-8154/services/https:proxy-service-n6zz2:tlsportname2/proxy/: tls qux (200; 28.033407ms)
Jul 31 12:09:11.009: INFO: (10) /api/v1/namespaces/proxy-8154/services/http:proxy-service-n6zz2:portname1/proxy/: foo (200; 27.935501ms)
Jul 31 12:09:11.009: INFO: (10) /api/v1/namespaces/proxy-8154/services/proxy-service-n6zz2:portname2/proxy/: bar (200; 28.500839ms)
Jul 31 12:09:11.009: INFO: (10) /api/v1/namespaces/proxy-8154/services/https:proxy-service-n6zz2:tlsportname1/proxy/: tls baz (200; 28.422785ms)
Jul 31 12:09:11.010: INFO: (10) /api/v1/namespaces/proxy-8154/services/http:proxy-service-n6zz2:portname2/proxy/: bar (200; 29.501207ms)
Jul 31 12:09:11.023: INFO: (11) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:1080/proxy/rewriteme">... (200; 12.303678ms)
Jul 31 12:09:11.023: INFO: (11) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:160/proxy/: foo (200; 12.484306ms)
Jul 31 12:09:11.023: INFO: (11) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:162/proxy/: bar (200; 12.305402ms)
Jul 31 12:09:11.023: INFO: (11) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:160/proxy/: foo (200; 12.318502ms)
Jul 31 12:09:11.023: INFO: (11) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:1080/proxy/rewriteme">test<... (200; 12.630297ms)
Jul 31 12:09:11.023: INFO: (11) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:162/proxy/: bar (200; 12.852955ms)
Jul 31 12:09:11.023: INFO: (11) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:460/proxy/: tls baz (200; 13.017911ms)
Jul 31 12:09:11.024: INFO: (11) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:462/proxy/: tls qux (200; 13.76138ms)
Jul 31 12:09:11.024: INFO: (11) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l/proxy/rewriteme">test</a> (200; 14.137841ms)
Jul 31 12:09:11.025: INFO: (11) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:443/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:443/proxy/tlsrewritem... (200; 14.448454ms)
Jul 31 12:09:11.025: INFO: (11) /api/v1/namespaces/proxy-8154/services/proxy-service-n6zz2:portname1/proxy/: foo (200; 14.454493ms)
Jul 31 12:09:11.029: INFO: (11) /api/v1/namespaces/proxy-8154/services/http:proxy-service-n6zz2:portname1/proxy/: foo (200; 18.383703ms)
Jul 31 12:09:11.031: INFO: (11) /api/v1/namespaces/proxy-8154/services/http:proxy-service-n6zz2:portname2/proxy/: bar (200; 20.701855ms)
Jul 31 12:09:11.031: INFO: (11) /api/v1/namespaces/proxy-8154/services/proxy-service-n6zz2:portname2/proxy/: bar (200; 21.032666ms)
Jul 31 12:09:11.031: INFO: (11) /api/v1/namespaces/proxy-8154/services/https:proxy-service-n6zz2:tlsportname1/proxy/: tls baz (200; 21.201507ms)
Jul 31 12:09:11.032: INFO: (11) /api/v1/namespaces/proxy-8154/services/https:proxy-service-n6zz2:tlsportname2/proxy/: tls qux (200; 21.464485ms)
Jul 31 12:09:11.039: INFO: (12) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:160/proxy/: foo (200; 6.811566ms)
Jul 31 12:09:11.042: INFO: (12) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:162/proxy/: bar (200; 10.147719ms)
Jul 31 12:09:11.042: INFO: (12) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:460/proxy/: tls baz (200; 10.211153ms)
Jul 31 12:09:11.047: INFO: (12) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:160/proxy/: foo (200; 14.382228ms)
Jul 31 12:09:11.047: INFO: (12) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:162/proxy/: bar (200; 14.483771ms)
Jul 31 12:09:11.047: INFO: (12) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l/proxy/rewriteme">test</a> (200; 15.128706ms)
Jul 31 12:09:11.048: INFO: (12) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:443/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:443/proxy/tlsrewritem... (200; 15.91283ms)
Jul 31 12:09:11.049: INFO: (12) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:462/proxy/: tls qux (200; 16.317542ms)
Jul 31 12:09:11.049: INFO: (12) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:1080/proxy/rewriteme">test<... (200; 16.588066ms)
Jul 31 12:09:11.049: INFO: (12) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:1080/proxy/rewriteme">... (200; 16.90799ms)
Jul 31 12:09:11.052: INFO: (12) /api/v1/namespaces/proxy-8154/services/http:proxy-service-n6zz2:portname1/proxy/: foo (200; 19.354178ms)
Jul 31 12:09:11.052: INFO: (12) /api/v1/namespaces/proxy-8154/services/https:proxy-service-n6zz2:tlsportname1/proxy/: tls baz (200; 19.732529ms)
Jul 31 12:09:11.053: INFO: (12) /api/v1/namespaces/proxy-8154/services/http:proxy-service-n6zz2:portname2/proxy/: bar (200; 20.887411ms)
Jul 31 12:09:11.054: INFO: (12) /api/v1/namespaces/proxy-8154/services/https:proxy-service-n6zz2:tlsportname2/proxy/: tls qux (200; 21.517639ms)
Jul 31 12:09:11.054: INFO: (12) /api/v1/namespaces/proxy-8154/services/proxy-service-n6zz2:portname2/proxy/: bar (200; 21.51633ms)
Jul 31 12:09:11.054: INFO: (12) /api/v1/namespaces/proxy-8154/services/proxy-service-n6zz2:portname1/proxy/: foo (200; 22.171443ms)
Jul 31 12:09:11.061: INFO: (13) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l/proxy/rewriteme">test</a> (200; 6.318821ms)
Jul 31 12:09:11.067: INFO: (13) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:1080/proxy/rewriteme">... (200; 12.05688ms)
Jul 31 12:09:11.067: INFO: (13) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:443/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:443/proxy/tlsrewritem... (200; 12.314077ms)
Jul 31 12:09:11.068: INFO: (13) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:162/proxy/: bar (200; 12.863152ms)
Jul 31 12:09:11.068: INFO: (13) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:160/proxy/: foo (200; 12.814966ms)
Jul 31 12:09:11.068: INFO: (13) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:460/proxy/: tls baz (200; 13.600427ms)
Jul 31 12:09:11.068: INFO: (13) /api/v1/namespaces/proxy-8154/services/proxy-service-n6zz2:portname2/proxy/: bar (200; 13.696348ms)
Jul 31 12:09:11.069: INFO: (13) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:162/proxy/: bar (200; 14.667265ms)
Jul 31 12:09:11.070: INFO: (13) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:160/proxy/: foo (200; 15.017384ms)
Jul 31 12:09:11.070: INFO: (13) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:1080/proxy/rewriteme">test<... (200; 15.203217ms)
Jul 31 12:09:11.070: INFO: (13) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:462/proxy/: tls qux (200; 15.318516ms)
Jul 31 12:09:11.071: INFO: (13) /api/v1/namespaces/proxy-8154/services/http:proxy-service-n6zz2:portname1/proxy/: foo (200; 16.448492ms)
Jul 31 12:09:11.071: INFO: (13) /api/v1/namespaces/proxy-8154/services/proxy-service-n6zz2:portname1/proxy/: foo (200; 16.554307ms)
Jul 31 12:09:11.072: INFO: (13) /api/v1/namespaces/proxy-8154/services/http:proxy-service-n6zz2:portname2/proxy/: bar (200; 17.076392ms)
Jul 31 12:09:11.072: INFO: (13) /api/v1/namespaces/proxy-8154/services/https:proxy-service-n6zz2:tlsportname1/proxy/: tls baz (200; 16.99116ms)
Jul 31 12:09:11.073: INFO: (13) /api/v1/namespaces/proxy-8154/services/https:proxy-service-n6zz2:tlsportname2/proxy/: tls qux (200; 18.082851ms)
Jul 31 12:09:11.084: INFO: (14) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:160/proxy/: foo (200; 11.226521ms)
Jul 31 12:09:11.084: INFO: (14) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:1080/proxy/rewriteme">... (200; 11.192297ms)
Jul 31 12:09:11.085: INFO: (14) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:160/proxy/: foo (200; 11.587233ms)
Jul 31 12:09:11.085: INFO: (14) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:460/proxy/: tls baz (200; 11.531246ms)
Jul 31 12:09:11.085: INFO: (14) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:162/proxy/: bar (200; 11.654349ms)
Jul 31 12:09:11.085: INFO: (14) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:1080/proxy/rewriteme">test<... (200; 12.046703ms)
Jul 31 12:09:11.085: INFO: (14) /api/v1/namespaces/proxy-8154/services/http:proxy-service-n6zz2:portname2/proxy/: bar (200; 12.065003ms)
Jul 31 12:09:11.086: INFO: (14) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l/proxy/rewriteme">test</a> (200; 13.557131ms)
Jul 31 12:09:11.087: INFO: (14) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:462/proxy/: tls qux (200; 14.118919ms)
Jul 31 12:09:11.087: INFO: (14) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:162/proxy/: bar (200; 14.166087ms)
Jul 31 12:09:11.087: INFO: (14) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:443/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:443/proxy/tlsrewritem... (200; 14.288874ms)
Jul 31 12:09:11.088: INFO: (14) /api/v1/namespaces/proxy-8154/services/https:proxy-service-n6zz2:tlsportname2/proxy/: tls qux (200; 14.724383ms)
Jul 31 12:09:11.092: INFO: (14) /api/v1/namespaces/proxy-8154/services/http:proxy-service-n6zz2:portname1/proxy/: foo (200; 19.272753ms)
Jul 31 12:09:11.094: INFO: (14) /api/v1/namespaces/proxy-8154/services/proxy-service-n6zz2:portname2/proxy/: bar (200; 20.603738ms)
Jul 31 12:09:11.094: INFO: (14) /api/v1/namespaces/proxy-8154/services/proxy-service-n6zz2:portname1/proxy/: foo (200; 20.783419ms)
Jul 31 12:09:11.094: INFO: (14) /api/v1/namespaces/proxy-8154/services/https:proxy-service-n6zz2:tlsportname1/proxy/: tls baz (200; 21.084789ms)
Jul 31 12:09:11.103: INFO: (15) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:460/proxy/: tls baz (200; 8.773555ms)
Jul 31 12:09:11.111: INFO: (15) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:162/proxy/: bar (200; 16.482846ms)
Jul 31 12:09:11.112: INFO: (15) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l/proxy/rewriteme">test</a> (200; 17.489378ms)
Jul 31 12:09:11.112: INFO: (15) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:1080/proxy/rewriteme">test<... (200; 18.084132ms)
Jul 31 12:09:11.112: INFO: (15) /api/v1/namespaces/proxy-8154/services/http:proxy-service-n6zz2:portname2/proxy/: bar (200; 17.787907ms)
Jul 31 12:09:11.112: INFO: (15) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:160/proxy/: foo (200; 18.119874ms)
Jul 31 12:09:11.113: INFO: (15) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:462/proxy/: tls qux (200; 17.9283ms)
Jul 31 12:09:11.113: INFO: (15) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:443/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:443/proxy/tlsrewritem... (200; 17.945292ms)
Jul 31 12:09:11.113: INFO: (15) /api/v1/namespaces/proxy-8154/services/https:proxy-service-n6zz2:tlsportname2/proxy/: tls qux (200; 18.119601ms)
Jul 31 12:09:11.113: INFO: (15) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:160/proxy/: foo (200; 18.14741ms)
Jul 31 12:09:11.113: INFO: (15) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:1080/proxy/rewriteme">... (200; 18.251998ms)
Jul 31 12:09:11.113: INFO: (15) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:162/proxy/: bar (200; 18.160949ms)
Jul 31 12:09:11.116: INFO: (15) /api/v1/namespaces/proxy-8154/services/proxy-service-n6zz2:portname1/proxy/: foo (200; 20.987705ms)
Jul 31 12:09:11.116: INFO: (15) /api/v1/namespaces/proxy-8154/services/http:proxy-service-n6zz2:portname1/proxy/: foo (200; 21.145385ms)
Jul 31 12:09:11.116: INFO: (15) /api/v1/namespaces/proxy-8154/services/proxy-service-n6zz2:portname2/proxy/: bar (200; 21.504106ms)
Jul 31 12:09:11.116: INFO: (15) /api/v1/namespaces/proxy-8154/services/https:proxy-service-n6zz2:tlsportname1/proxy/: tls baz (200; 21.493568ms)
Jul 31 12:09:11.126: INFO: (16) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:1080/proxy/rewriteme">... (200; 10.47116ms)
Jul 31 12:09:11.131: INFO: (16) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:162/proxy/: bar (200; 14.576162ms)
Jul 31 12:09:11.131: INFO: (16) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l/proxy/rewriteme">test</a> (200; 14.981233ms)
Jul 31 12:09:11.131: INFO: (16) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:160/proxy/: foo (200; 14.944093ms)
Jul 31 12:09:11.132: INFO: (16) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:443/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:443/proxy/tlsrewritem... (200; 15.459663ms)
Jul 31 12:09:11.132: INFO: (16) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:160/proxy/: foo (200; 15.14937ms)
Jul 31 12:09:11.133: INFO: (16) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:1080/proxy/rewriteme">test<... (200; 16.517236ms)
Jul 31 12:09:11.133: INFO: (16) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:162/proxy/: bar (200; 16.5953ms)
Jul 31 12:09:11.134: INFO: (16) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:460/proxy/: tls baz (200; 17.608647ms)
Jul 31 12:09:11.134: INFO: (16) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:462/proxy/: tls qux (200; 17.657861ms)
Jul 31 12:09:11.136: INFO: (16) /api/v1/namespaces/proxy-8154/services/https:proxy-service-n6zz2:tlsportname2/proxy/: tls qux (200; 19.773717ms)
Jul 31 12:09:11.140: INFO: (16) /api/v1/namespaces/proxy-8154/services/proxy-service-n6zz2:portname1/proxy/: foo (200; 23.341667ms)
Jul 31 12:09:11.140: INFO: (16) /api/v1/namespaces/proxy-8154/services/proxy-service-n6zz2:portname2/proxy/: bar (200; 23.817135ms)
Jul 31 12:09:11.140: INFO: (16) /api/v1/namespaces/proxy-8154/services/https:proxy-service-n6zz2:tlsportname1/proxy/: tls baz (200; 23.364898ms)
Jul 31 12:09:11.140: INFO: (16) /api/v1/namespaces/proxy-8154/services/http:proxy-service-n6zz2:portname1/proxy/: foo (200; 24.027059ms)
Jul 31 12:09:11.140: INFO: (16) /api/v1/namespaces/proxy-8154/services/http:proxy-service-n6zz2:portname2/proxy/: bar (200; 24.146393ms)
Jul 31 12:09:11.148: INFO: (17) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l/proxy/rewriteme">test</a> (200; 7.69229ms)
Jul 31 12:09:11.155: INFO: (17) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:1080/proxy/rewriteme">... (200; 14.130931ms)
Jul 31 12:09:11.158: INFO: (17) /api/v1/namespaces/proxy-8154/services/proxy-service-n6zz2:portname2/proxy/: bar (200; 17.430507ms)
Jul 31 12:09:11.158: INFO: (17) /api/v1/namespaces/proxy-8154/services/http:proxy-service-n6zz2:portname2/proxy/: bar (200; 17.332182ms)
Jul 31 12:09:11.158: INFO: (17) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:1080/proxy/rewriteme">test<... (200; 16.865876ms)
Jul 31 12:09:11.159: INFO: (17) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:160/proxy/: foo (200; 17.689335ms)
Jul 31 12:09:11.160: INFO: (17) /api/v1/namespaces/proxy-8154/services/https:proxy-service-n6zz2:tlsportname1/proxy/: tls baz (200; 19.353416ms)
Jul 31 12:09:11.160: INFO: (17) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:462/proxy/: tls qux (200; 18.883612ms)
Jul 31 12:09:11.161: INFO: (17) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:162/proxy/: bar (200; 19.865781ms)
Jul 31 12:09:11.161: INFO: (17) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:162/proxy/: bar (200; 20.391865ms)
Jul 31 12:09:11.161: INFO: (17) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:160/proxy/: foo (200; 20.046778ms)
Jul 31 12:09:11.161: INFO: (17) /api/v1/namespaces/proxy-8154/services/http:proxy-service-n6zz2:portname1/proxy/: foo (200; 19.829367ms)
Jul 31 12:09:11.162: INFO: (17) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:460/proxy/: tls baz (200; 20.817457ms)
Jul 31 12:09:11.162: INFO: (17) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:443/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:443/proxy/tlsrewritem... (200; 20.769462ms)
Jul 31 12:09:11.165: INFO: (17) /api/v1/namespaces/proxy-8154/services/https:proxy-service-n6zz2:tlsportname2/proxy/: tls qux (200; 24.702989ms)
Jul 31 12:09:11.167: INFO: (17) /api/v1/namespaces/proxy-8154/services/proxy-service-n6zz2:portname1/proxy/: foo (200; 26.11474ms)
Jul 31 12:09:11.182: INFO: (18) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:462/proxy/: tls qux (200; 13.636107ms)
Jul 31 12:09:11.182: INFO: (18) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:160/proxy/: foo (200; 14.210105ms)
Jul 31 12:09:11.182: INFO: (18) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:460/proxy/: tls baz (200; 14.517828ms)
Jul 31 12:09:11.182: INFO: (18) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:1080/proxy/rewriteme">test<... (200; 14.560313ms)
Jul 31 12:09:11.184: INFO: (18) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:162/proxy/: bar (200; 16.400144ms)
Jul 31 12:09:11.185: INFO: (18) /api/v1/namespaces/proxy-8154/services/http:proxy-service-n6zz2:portname1/proxy/: foo (200; 17.268981ms)
Jul 31 12:09:11.185: INFO: (18) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l/proxy/rewriteme">test</a> (200; 17.289272ms)
Jul 31 12:09:11.185: INFO: (18) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:1080/proxy/rewriteme">... (200; 17.106229ms)
Jul 31 12:09:11.185: INFO: (18) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:160/proxy/: foo (200; 17.8202ms)
Jul 31 12:09:11.186: INFO: (18) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:443/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:443/proxy/tlsrewritem... (200; 18.196467ms)
Jul 31 12:09:11.186: INFO: (18) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:162/proxy/: bar (200; 18.459202ms)
Jul 31 12:09:11.191: INFO: (18) /api/v1/namespaces/proxy-8154/services/https:proxy-service-n6zz2:tlsportname1/proxy/: tls baz (200; 22.888897ms)
Jul 31 12:09:11.192: INFO: (18) /api/v1/namespaces/proxy-8154/services/proxy-service-n6zz2:portname2/proxy/: bar (200; 24.383328ms)
Jul 31 12:09:11.192: INFO: (18) /api/v1/namespaces/proxy-8154/services/proxy-service-n6zz2:portname1/proxy/: foo (200; 24.612121ms)
Jul 31 12:09:11.193: INFO: (18) /api/v1/namespaces/proxy-8154/services/https:proxy-service-n6zz2:tlsportname2/proxy/: tls qux (200; 24.966947ms)
Jul 31 12:09:11.193: INFO: (18) /api/v1/namespaces/proxy-8154/services/http:proxy-service-n6zz2:portname2/proxy/: bar (200; 25.196573ms)
Jul 31 12:09:11.205: INFO: (19) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:162/proxy/: bar (200; 12.38326ms)
Jul 31 12:09:11.206: INFO: (19) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:1080/proxy/rewriteme">... (200; 12.408761ms)
Jul 31 12:09:11.206: INFO: (19) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:162/proxy/: bar (200; 12.73349ms)
Jul 31 12:09:11.209: INFO: (19) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l/proxy/rewriteme">test</a> (200; 15.723085ms)
Jul 31 12:09:11.210: INFO: (19) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:1080/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:1080/proxy/rewriteme">test<... (200; 17.464607ms)
Jul 31 12:09:11.210: INFO: (19) /api/v1/namespaces/proxy-8154/services/https:proxy-service-n6zz2:tlsportname2/proxy/: tls qux (200; 17.29372ms)
Jul 31 12:09:11.210: INFO: (19) /api/v1/namespaces/proxy-8154/pods/proxy-service-n6zz2-vcj5l:160/proxy/: foo (200; 17.153606ms)
Jul 31 12:09:11.210: INFO: (19) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:462/proxy/: tls qux (200; 17.125029ms)
Jul 31 12:09:11.210: INFO: (19) /api/v1/namespaces/proxy-8154/pods/http:proxy-service-n6zz2-vcj5l:160/proxy/: foo (200; 17.388793ms)
Jul 31 12:09:11.211: INFO: (19) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:460/proxy/: tls baz (200; 17.737196ms)
Jul 31 12:09:11.211: INFO: (19) /api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:443/proxy/: <a href="/api/v1/namespaces/proxy-8154/pods/https:proxy-service-n6zz2-vcj5l:443/proxy/tlsrewritem... (200; 17.872791ms)
Jul 31 12:09:11.211: INFO: (19) /api/v1/namespaces/proxy-8154/services/proxy-service-n6zz2:portname1/proxy/: foo (200; 18.203598ms)
Jul 31 12:09:11.213: INFO: (19) /api/v1/namespaces/proxy-8154/services/https:proxy-service-n6zz2:tlsportname1/proxy/: tls baz (200; 20.010579ms)
Jul 31 12:09:11.213: INFO: (19) /api/v1/namespaces/proxy-8154/services/http:proxy-service-n6zz2:portname1/proxy/: foo (200; 20.315535ms)
Jul 31 12:09:11.214: INFO: (19) /api/v1/namespaces/proxy-8154/services/proxy-service-n6zz2:portname2/proxy/: bar (200; 20.645811ms)
Jul 31 12:09:11.217: INFO: (19) /api/v1/namespaces/proxy-8154/services/http:proxy-service-n6zz2:portname2/proxy/: bar (200; 23.955375ms)
STEP: deleting ReplicationController proxy-service-n6zz2 in namespace proxy-8154, will wait for the garbage collector to delete the pods
Jul 31 12:09:11.290: INFO: Deleting ReplicationController proxy-service-n6zz2 took: 14.564415ms
Jul 31 12:09:11.391: INFO: Terminating ReplicationController proxy-service-n6zz2 pods took: 100.819884ms
[AfterEach] version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:09:13.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8154" for this suite.

• [SLOW TEST:5.300 seconds]
[sig-network] Proxy
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  version v1
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":346,"completed":323,"skipped":6207,"failed":0}
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:09:13.848: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
Jul 31 12:09:20.289: INFO: 80 pods remaining
Jul 31 12:09:20.289: INFO: 80 pods has nil DeletionTimestamp
Jul 31 12:09:20.289: INFO: 
Jul 31 12:09:21.454: INFO: 70 pods remaining
Jul 31 12:09:21.455: INFO: 67 pods has nil DeletionTimestamp
Jul 31 12:09:21.455: INFO: 
Jul 31 12:09:22.299: INFO: 60 pods remaining
Jul 31 12:09:22.299: INFO: 59 pods has nil DeletionTimestamp
Jul 31 12:09:22.299: INFO: 
Jul 31 12:09:23.309: INFO: 40 pods remaining
Jul 31 12:09:23.309: INFO: 40 pods has nil DeletionTimestamp
Jul 31 12:09:23.309: INFO: 
Jul 31 12:09:24.352: INFO: 31 pods remaining
Jul 31 12:09:24.352: INFO: 30 pods has nil DeletionTimestamp
Jul 31 12:09:24.352: INFO: 
Jul 31 12:09:25.325: INFO: 17 pods remaining
Jul 31 12:09:25.325: INFO: 16 pods has nil DeletionTimestamp
Jul 31 12:09:25.325: INFO: 
STEP: Gathering metrics
Jul 31 12:09:26.322: INFO: The status of Pod kube-controller-manager-master-um8faxf3d4468jbdm49zh4o57y is Running (Ready = true)
Jul 31 12:09:26.487: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:09:26.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4133" for this suite.

• [SLOW TEST:12.743 seconds]
[sig-api-machinery] Garbage collector
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":346,"completed":324,"skipped":6210,"failed":0}
SSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:09:26.592: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:09:26.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6491" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":346,"completed":325,"skipped":6213,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:09:26.958: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:09:27.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8660" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":346,"completed":326,"skipped":6236,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:09:27.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:09:33.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4186" for this suite.
STEP: Destroying namespace "nsdeletetest-5243" for this suite.
Jul 31 12:09:34.054: INFO: Namespace nsdeletetest-5243 was already deleted
STEP: Destroying namespace "nsdeletetest-1139" for this suite.

• [SLOW TEST:6.530 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":346,"completed":327,"skipped":6281,"failed":0}
S
------------------------------
[sig-network] Services 
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:09:34.064: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service in namespace services-2947
Jul 31 12:09:34.194: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jul 31 12:09:36.220: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Jul 31 12:09:36.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-2947 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jul 31 12:09:36.489: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jul 31 12:09:36.489: INFO: stdout: "iptables"
Jul 31 12:09:36.489: INFO: proxyMode: iptables
Jul 31 12:09:36.526: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jul 31 12:09:36.539: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-2947
STEP: creating replication controller affinity-clusterip-timeout in namespace services-2947
I0731 12:09:36.614961      23 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-2947, replica count: 3
I0731 12:09:39.665994      23 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 31 12:09:39.693: INFO: Creating new exec pod
Jul 31 12:09:44.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-2947 exec execpod-affinityk6mgl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Jul 31 12:09:45.015: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Jul 31 12:09:45.015: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 31 12:09:45.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-2947 exec execpod-affinityk6mgl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.108.18.82 80'
Jul 31 12:09:45.234: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.108.18.82 80\nConnection to 10.108.18.82 80 port [tcp/http] succeeded!\n"
Jul 31 12:09:45.234: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 31 12:09:45.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-2947 exec execpod-affinityk6mgl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.108.18.82:80/ ; done'
Jul 31 12:09:45.549: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.18.82:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.18.82:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.18.82:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.18.82:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.18.82:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.18.82:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.18.82:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.18.82:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.18.82:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.18.82:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.18.82:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.18.82:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.18.82:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.18.82:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.18.82:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.18.82:80/\n"
Jul 31 12:09:45.549: INFO: stdout: "\naffinity-clusterip-timeout-bv4s2\naffinity-clusterip-timeout-bv4s2\naffinity-clusterip-timeout-bv4s2\naffinity-clusterip-timeout-bv4s2\naffinity-clusterip-timeout-bv4s2\naffinity-clusterip-timeout-bv4s2\naffinity-clusterip-timeout-bv4s2\naffinity-clusterip-timeout-bv4s2\naffinity-clusterip-timeout-bv4s2\naffinity-clusterip-timeout-bv4s2\naffinity-clusterip-timeout-bv4s2\naffinity-clusterip-timeout-bv4s2\naffinity-clusterip-timeout-bv4s2\naffinity-clusterip-timeout-bv4s2\naffinity-clusterip-timeout-bv4s2\naffinity-clusterip-timeout-bv4s2"
Jul 31 12:09:45.549: INFO: Received response from host: affinity-clusterip-timeout-bv4s2
Jul 31 12:09:45.549: INFO: Received response from host: affinity-clusterip-timeout-bv4s2
Jul 31 12:09:45.549: INFO: Received response from host: affinity-clusterip-timeout-bv4s2
Jul 31 12:09:45.549: INFO: Received response from host: affinity-clusterip-timeout-bv4s2
Jul 31 12:09:45.549: INFO: Received response from host: affinity-clusterip-timeout-bv4s2
Jul 31 12:09:45.549: INFO: Received response from host: affinity-clusterip-timeout-bv4s2
Jul 31 12:09:45.549: INFO: Received response from host: affinity-clusterip-timeout-bv4s2
Jul 31 12:09:45.549: INFO: Received response from host: affinity-clusterip-timeout-bv4s2
Jul 31 12:09:45.549: INFO: Received response from host: affinity-clusterip-timeout-bv4s2
Jul 31 12:09:45.549: INFO: Received response from host: affinity-clusterip-timeout-bv4s2
Jul 31 12:09:45.549: INFO: Received response from host: affinity-clusterip-timeout-bv4s2
Jul 31 12:09:45.549: INFO: Received response from host: affinity-clusterip-timeout-bv4s2
Jul 31 12:09:45.549: INFO: Received response from host: affinity-clusterip-timeout-bv4s2
Jul 31 12:09:45.549: INFO: Received response from host: affinity-clusterip-timeout-bv4s2
Jul 31 12:09:45.549: INFO: Received response from host: affinity-clusterip-timeout-bv4s2
Jul 31 12:09:45.549: INFO: Received response from host: affinity-clusterip-timeout-bv4s2
Jul 31 12:09:45.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-2947 exec execpod-affinityk6mgl -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.108.18.82:80/'
Jul 31 12:09:45.785: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.108.18.82:80/\n"
Jul 31 12:09:45.785: INFO: stdout: "affinity-clusterip-timeout-bv4s2"
Jul 31 12:10:05.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-2947 exec execpod-affinityk6mgl -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.108.18.82:80/'
Jul 31 12:10:06.081: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.108.18.82:80/\n"
Jul 31 12:10:06.081: INFO: stdout: "affinity-clusterip-timeout-8gz7b"
Jul 31 12:10:06.081: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-2947, will wait for the garbage collector to delete the pods
Jul 31 12:10:06.229: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 21.964154ms
Jul 31 12:10:06.430: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 200.76565ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:10:08.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2947" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:34.863 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","total":346,"completed":328,"skipped":6282,"failed":0}
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:10:08.928: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 31 12:10:13.112: INFO: DNS probes using dns-1684/dns-test-79f69581-7652-4822-98fa-aa3c95d70471 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:10:13.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1684" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":346,"completed":329,"skipped":6282,"failed":0}

------------------------------
[sig-apps] Daemon set [Serial] 
  should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:10:13.181: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:143
[It] should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul 31 12:10:13.333: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 12:10:13.333: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 12:10:13.333: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 12:10:13.348: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 31 12:10:13.348: INFO: Node p1-ashfcfj4pzo6aemt1j3a9ah7ew is running 0 daemon pod, expected 1
Jul 31 12:10:14.363: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 12:10:14.364: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 12:10:14.364: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 12:10:14.369: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 31 12:10:14.369: INFO: Node p1-ashfcfj4pzo6aemt1j3a9ah7ew is running 0 daemon pod, expected 1
Jul 31 12:10:15.369: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 12:10:15.369: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 12:10:15.369: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 12:10:15.379: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jul 31 12:10:15.379: INFO: Node p1-ashfcfj4pzo6aemt1j3a9ah7ew is running 0 daemon pod, expected 1
Jul 31 12:10:16.365: INFO: DaemonSet pods can't tolerate node master-d695o3nuux4onnradaukiwqz8y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 12:10:16.365: INFO: DaemonSet pods can't tolerate node master-hmwupoi3qtw4ihx69rj5w4d81e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 12:10:16.365: INFO: DaemonSet pods can't tolerate node master-um8faxf3d4468jbdm49zh4o57y with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 31 12:10:16.371: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Jul 31 12:10:16.371: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
STEP: Getting /status
Jul 31 12:10:16.381: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status
Jul 31 12:10:16.403: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated
Jul 31 12:10:16.406: INFO: Observed &DaemonSet event: ADDED
Jul 31 12:10:16.407: INFO: Observed &DaemonSet event: MODIFIED
Jul 31 12:10:16.407: INFO: Observed &DaemonSet event: MODIFIED
Jul 31 12:10:16.407: INFO: Observed &DaemonSet event: MODIFIED
Jul 31 12:10:16.407: INFO: Observed &DaemonSet event: MODIFIED
Jul 31 12:10:16.408: INFO: Observed &DaemonSet event: MODIFIED
Jul 31 12:10:16.408: INFO: Observed &DaemonSet event: MODIFIED
Jul 31 12:10:16.408: INFO: Found daemon set daemon-set in namespace daemonsets-8939 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jul 31 12:10:16.408: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status
STEP: watching for the daemon set status to be patched
Jul 31 12:10:16.432: INFO: Observed &DaemonSet event: ADDED
Jul 31 12:10:16.433: INFO: Observed &DaemonSet event: MODIFIED
Jul 31 12:10:16.433: INFO: Observed &DaemonSet event: MODIFIED
Jul 31 12:10:16.433: INFO: Observed &DaemonSet event: MODIFIED
Jul 31 12:10:16.434: INFO: Observed &DaemonSet event: MODIFIED
Jul 31 12:10:16.434: INFO: Observed &DaemonSet event: MODIFIED
Jul 31 12:10:16.435: INFO: Observed &DaemonSet event: MODIFIED
Jul 31 12:10:16.435: INFO: Observed daemon set daemon-set in namespace daemonsets-8939 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jul 31 12:10:16.435: INFO: Observed &DaemonSet event: MODIFIED
Jul 31 12:10:16.435: INFO: Found daemon set daemon-set in namespace daemonsets-8939 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Jul 31 12:10:16.435: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:109
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8939, will wait for the garbage collector to delete the pods
Jul 31 12:10:16.511: INFO: Deleting DaemonSet.extensions daemon-set took: 12.93145ms
Jul 31 12:10:16.611: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.1691ms
Jul 31 12:10:19.324: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jul 31 12:10:19.324: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jul 31 12:10:19.329: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"45071"},"items":null}

Jul 31 12:10:19.333: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"45071"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:10:19.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8939" for this suite.

• [SLOW TEST:6.212 seconds]
[sig-apps] Daemon set [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","total":346,"completed":330,"skipped":6282,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:10:19.399: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6473.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-6473.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6473.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-6473.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 31 12:10:23.569: INFO: DNS probes using dns-6473/dns-test-1cf04889-d263-4be2-b9dd-44c5a356632c succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:10:23.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6473" for this suite.
•{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":346,"completed":331,"skipped":6338,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:10:23.701: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 12:10:23.840: INFO: The status of Pod busybox-scheduling-104886a5-76eb-47d2-82c6-0fe94d9e3d8e is Pending, waiting for it to be Running (with Ready = true)
Jul 31 12:10:25.854: INFO: The status of Pod busybox-scheduling-104886a5-76eb-47d2-82c6-0fe94d9e3d8e is Running (Ready = true)
[AfterEach] [sig-node] Kubelet
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:10:25.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8555" for this suite.
•{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":346,"completed":332,"skipped":6361,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:10:25.903: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name configmap-projected-all-test-volume-af55b65f-3d2a-4789-a3e9-390e46e22697
STEP: Creating secret with name secret-projected-all-test-volume-c899382d-ad6a-43b8-a65c-7f6b3cc1239b
STEP: Creating a pod to test Check all projections for projected volume plugin
Jul 31 12:10:26.046: INFO: Waiting up to 5m0s for pod "projected-volume-d766d081-cb34-4e2e-a6dd-bc66947e834a" in namespace "projected-2344" to be "Succeeded or Failed"
Jul 31 12:10:26.066: INFO: Pod "projected-volume-d766d081-cb34-4e2e-a6dd-bc66947e834a": Phase="Pending", Reason="", readiness=false. Elapsed: 19.289105ms
Jul 31 12:10:28.084: INFO: Pod "projected-volume-d766d081-cb34-4e2e-a6dd-bc66947e834a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037129531s
Jul 31 12:10:30.095: INFO: Pod "projected-volume-d766d081-cb34-4e2e-a6dd-bc66947e834a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048530563s
Jul 31 12:10:32.108: INFO: Pod "projected-volume-d766d081-cb34-4e2e-a6dd-bc66947e834a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.061220018s
STEP: Saw pod success
Jul 31 12:10:32.108: INFO: Pod "projected-volume-d766d081-cb34-4e2e-a6dd-bc66947e834a" satisfied condition "Succeeded or Failed"
Jul 31 12:10:32.114: INFO: Trying to get logs from node p1-ashfcfj4pzo6aemt1j3a9ah7ew pod projected-volume-d766d081-cb34-4e2e-a6dd-bc66947e834a container projected-all-volume-test: <nil>
STEP: delete the pod
Jul 31 12:10:32.174: INFO: Waiting for pod projected-volume-d766d081-cb34-4e2e-a6dd-bc66947e834a to disappear
Jul 31 12:10:32.182: INFO: Pod projected-volume-d766d081-cb34-4e2e-a6dd-bc66947e834a no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:10:32.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2344" for this suite.

• [SLOW TEST:6.294 seconds]
[sig-storage] Projected combined
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":346,"completed":333,"skipped":6417,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:10:32.198: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-map-2d888eba-8a12-47af-9699-a04d8b935d9e
STEP: Creating a pod to test consume configMaps
Jul 31 12:10:32.284: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-da291c2e-ab35-442f-85e8-f3086a9e32ad" in namespace "projected-2411" to be "Succeeded or Failed"
Jul 31 12:10:32.290: INFO: Pod "pod-projected-configmaps-da291c2e-ab35-442f-85e8-f3086a9e32ad": Phase="Pending", Reason="", readiness=false. Elapsed: 6.11691ms
Jul 31 12:10:34.304: INFO: Pod "pod-projected-configmaps-da291c2e-ab35-442f-85e8-f3086a9e32ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020463779s
Jul 31 12:10:36.319: INFO: Pod "pod-projected-configmaps-da291c2e-ab35-442f-85e8-f3086a9e32ad": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035074008s
Jul 31 12:10:38.332: INFO: Pod "pod-projected-configmaps-da291c2e-ab35-442f-85e8-f3086a9e32ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.048113228s
STEP: Saw pod success
Jul 31 12:10:38.332: INFO: Pod "pod-projected-configmaps-da291c2e-ab35-442f-85e8-f3086a9e32ad" satisfied condition "Succeeded or Failed"
Jul 31 12:10:38.338: INFO: Trying to get logs from node p1-ashfcfj4pzo6aemt1j3a9ah7ew pod pod-projected-configmaps-da291c2e-ab35-442f-85e8-f3086a9e32ad container agnhost-container: <nil>
STEP: delete the pod
Jul 31 12:10:38.384: INFO: Waiting for pod pod-projected-configmaps-da291c2e-ab35-442f-85e8-f3086a9e32ad to disappear
Jul 31 12:10:38.389: INFO: Pod pod-projected-configmaps-da291c2e-ab35-442f-85e8-f3086a9e32ad no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:10:38.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2411" for this suite.

• [SLOW TEST:6.228 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":346,"completed":334,"skipped":6444,"failed":0}
SSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:10:38.428: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 12:10:38.478: INFO: Got root ca configmap in namespace "svcaccounts-6585"
Jul 31 12:10:38.497: INFO: Deleted root ca configmap in namespace "svcaccounts-6585"
STEP: waiting for a new root ca configmap created
Jul 31 12:10:39.015: INFO: Recreated root ca configmap in namespace "svcaccounts-6585"
Jul 31 12:10:39.024: INFO: Updated root ca configmap in namespace "svcaccounts-6585"
STEP: waiting for the root ca configmap reconciled
Jul 31 12:10:39.534: INFO: Reconciled root ca configmap in namespace "svcaccounts-6585"
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:10:39.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6585" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","total":346,"completed":335,"skipped":6459,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:10:39.559: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-map-1ea884c9-cb82-4ce5-9ca2-e96c479672a0
STEP: Creating a pod to test consume configMaps
Jul 31 12:10:39.650: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5ad9400d-cc93-4f04-987e-81fcc48bbd87" in namespace "projected-398" to be "Succeeded or Failed"
Jul 31 12:10:39.674: INFO: Pod "pod-projected-configmaps-5ad9400d-cc93-4f04-987e-81fcc48bbd87": Phase="Pending", Reason="", readiness=false. Elapsed: 23.889989ms
Jul 31 12:10:41.687: INFO: Pod "pod-projected-configmaps-5ad9400d-cc93-4f04-987e-81fcc48bbd87": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036303873s
Jul 31 12:10:43.703: INFO: Pod "pod-projected-configmaps-5ad9400d-cc93-4f04-987e-81fcc48bbd87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05270407s
STEP: Saw pod success
Jul 31 12:10:43.703: INFO: Pod "pod-projected-configmaps-5ad9400d-cc93-4f04-987e-81fcc48bbd87" satisfied condition "Succeeded or Failed"
Jul 31 12:10:43.708: INFO: Trying to get logs from node p1-eipeq63rfgo6ooocmu3kqk3tcc pod pod-projected-configmaps-5ad9400d-cc93-4f04-987e-81fcc48bbd87 container agnhost-container: <nil>
STEP: delete the pod
Jul 31 12:10:43.784: INFO: Waiting for pod pod-projected-configmaps-5ad9400d-cc93-4f04-987e-81fcc48bbd87 to disappear
Jul 31 12:10:43.793: INFO: Pod pod-projected-configmaps-5ad9400d-cc93-4f04-987e-81fcc48bbd87 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:10:43.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-398" for this suite.
•{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":346,"completed":336,"skipped":6502,"failed":0}
SSSS
------------------------------
[sig-network] Services 
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:10:43.813: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating service in namespace services-1297
Jul 31 12:10:43.924: INFO: The status of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jul 31 12:10:45.938: INFO: The status of Pod kube-proxy-mode-detector is Running (Ready = true)
Jul 31 12:10:45.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-1297 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jul 31 12:10:46.211: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jul 31 12:10:46.211: INFO: stdout: "iptables"
Jul 31 12:10:46.212: INFO: proxyMode: iptables
Jul 31 12:10:46.253: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jul 31 12:10:46.267: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-1297
STEP: creating replication controller affinity-nodeport-timeout in namespace services-1297
I0731 12:10:46.342715      23 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-1297, replica count: 3
I0731 12:10:49.393814      23 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 31 12:10:49.423: INFO: Creating new exec pod
Jul 31 12:10:54.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-1297 exec execpod-affinity9lwxg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Jul 31 12:10:54.738: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Jul 31 12:10:54.738: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 31 12:10:54.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-1297 exec execpod-affinity9lwxg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.109.146.53 80'
Jul 31 12:10:54.972: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.109.146.53 80\nConnection to 10.109.146.53 80 port [tcp/http] succeeded!\n"
Jul 31 12:10:54.972: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 31 12:10:54.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-1297 exec execpod-affinity9lwxg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.27.21.79 30286'
Jul 31 12:10:55.197: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.27.21.79 30286\nConnection to 172.27.21.79 30286 port [tcp/*] succeeded!\n"
Jul 31 12:10:55.197: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 31 12:10:55.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-1297 exec execpod-affinity9lwxg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.27.21.72 30286'
Jul 31 12:10:55.449: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.27.21.72 30286\nConnection to 172.27.21.72 30286 port [tcp/*] succeeded!\n"
Jul 31 12:10:55.449: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jul 31 12:10:55.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-1297 exec execpod-affinity9lwxg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.27.21.73:30286/ ; done'
Jul 31 12:10:55.823: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:30286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:30286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:30286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:30286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:30286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:30286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:30286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:30286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:30286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:30286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:30286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:30286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:30286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:30286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:30286/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.27.21.73:30286/\n"
Jul 31 12:10:55.823: INFO: stdout: "\naffinity-nodeport-timeout-lb5k7\naffinity-nodeport-timeout-lb5k7\naffinity-nodeport-timeout-lb5k7\naffinity-nodeport-timeout-lb5k7\naffinity-nodeport-timeout-lb5k7\naffinity-nodeport-timeout-lb5k7\naffinity-nodeport-timeout-lb5k7\naffinity-nodeport-timeout-lb5k7\naffinity-nodeport-timeout-lb5k7\naffinity-nodeport-timeout-lb5k7\naffinity-nodeport-timeout-lb5k7\naffinity-nodeport-timeout-lb5k7\naffinity-nodeport-timeout-lb5k7\naffinity-nodeport-timeout-lb5k7\naffinity-nodeport-timeout-lb5k7\naffinity-nodeport-timeout-lb5k7"
Jul 31 12:10:55.823: INFO: Received response from host: affinity-nodeport-timeout-lb5k7
Jul 31 12:10:55.823: INFO: Received response from host: affinity-nodeport-timeout-lb5k7
Jul 31 12:10:55.823: INFO: Received response from host: affinity-nodeport-timeout-lb5k7
Jul 31 12:10:55.823: INFO: Received response from host: affinity-nodeport-timeout-lb5k7
Jul 31 12:10:55.823: INFO: Received response from host: affinity-nodeport-timeout-lb5k7
Jul 31 12:10:55.823: INFO: Received response from host: affinity-nodeport-timeout-lb5k7
Jul 31 12:10:55.823: INFO: Received response from host: affinity-nodeport-timeout-lb5k7
Jul 31 12:10:55.823: INFO: Received response from host: affinity-nodeport-timeout-lb5k7
Jul 31 12:10:55.823: INFO: Received response from host: affinity-nodeport-timeout-lb5k7
Jul 31 12:10:55.823: INFO: Received response from host: affinity-nodeport-timeout-lb5k7
Jul 31 12:10:55.823: INFO: Received response from host: affinity-nodeport-timeout-lb5k7
Jul 31 12:10:55.823: INFO: Received response from host: affinity-nodeport-timeout-lb5k7
Jul 31 12:10:55.823: INFO: Received response from host: affinity-nodeport-timeout-lb5k7
Jul 31 12:10:55.823: INFO: Received response from host: affinity-nodeport-timeout-lb5k7
Jul 31 12:10:55.823: INFO: Received response from host: affinity-nodeport-timeout-lb5k7
Jul 31 12:10:55.823: INFO: Received response from host: affinity-nodeport-timeout-lb5k7
Jul 31 12:10:55.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-1297 exec execpod-affinity9lwxg -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.27.21.73:30286/'
Jul 31 12:10:56.047: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.27.21.73:30286/\n"
Jul 31 12:10:56.047: INFO: stdout: "affinity-nodeport-timeout-lb5k7"
Jul 31 12:11:16.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3485299162 --namespace=services-1297 exec execpod-affinity9lwxg -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.27.21.73:30286/'
Jul 31 12:11:16.313: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.27.21.73:30286/\n"
Jul 31 12:11:16.313: INFO: stdout: "affinity-nodeport-timeout-h7l2f"
Jul 31 12:11:16.313: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-1297, will wait for the garbage collector to delete the pods
Jul 31 12:11:16.469: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 23.181272ms
Jul 31 12:11:16.670: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 201.117945ms
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:11:19.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1297" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756

• [SLOW TEST:35.852 seconds]
[sig-network] Services
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","total":346,"completed":337,"skipped":6506,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:11:19.667: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
Jul 31 12:11:21.809: INFO: Deleting pod "var-expansion-da35e83d-08e9-42d6-aeb5-17fa185b2d0a" in namespace "var-expansion-3041"
Jul 31 12:11:21.821: INFO: Wait up to 5m0s for pod "var-expansion-da35e83d-08e9-42d6-aeb5-17fa185b2d0a" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:11:25.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3041" for this suite.

• [SLOW TEST:6.193 seconds]
[sig-node] Variable Expansion
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","total":346,"completed":338,"skipped":6519,"failed":0}
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:11:25.861: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: getting the auto-created API token
Jul 31 12:11:26.495: INFO: created pod pod-service-account-defaultsa
Jul 31 12:11:26.496: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jul 31 12:11:26.520: INFO: created pod pod-service-account-mountsa
Jul 31 12:11:26.520: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jul 31 12:11:26.532: INFO: created pod pod-service-account-nomountsa
Jul 31 12:11:26.532: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jul 31 12:11:26.579: INFO: created pod pod-service-account-defaultsa-mountspec
Jul 31 12:11:26.579: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jul 31 12:11:26.594: INFO: created pod pod-service-account-mountsa-mountspec
Jul 31 12:11:26.594: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jul 31 12:11:26.630: INFO: created pod pod-service-account-nomountsa-mountspec
Jul 31 12:11:26.630: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jul 31 12:11:26.655: INFO: created pod pod-service-account-defaultsa-nomountspec
Jul 31 12:11:26.655: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jul 31 12:11:26.686: INFO: created pod pod-service-account-mountsa-nomountspec
Jul 31 12:11:26.686: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jul 31 12:11:26.736: INFO: created pod pod-service-account-nomountsa-nomountspec
Jul 31 12:11:26.737: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:11:26.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5902" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":346,"completed":339,"skipped":6519,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:11:26.800: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:11:55.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9390" for this suite.

• [SLOW TEST:28.235 seconds]
[sig-api-machinery] ResourceQuota
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":346,"completed":340,"skipped":6546,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:11:55.039: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should test the lifecycle of an Endpoint [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:11:55.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7333" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":346,"completed":341,"skipped":6581,"failed":0}
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:11:55.278: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Creating configMap with name projected-configmap-test-volume-map-7e1aeec2-8646-4a1d-945d-b2e957cf50b6
STEP: Creating a pod to test consume configMaps
Jul 31 12:11:55.372: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-dec5ee20-bf98-4112-8990-239fc10317af" in namespace "projected-5782" to be "Succeeded or Failed"
Jul 31 12:11:55.402: INFO: Pod "pod-projected-configmaps-dec5ee20-bf98-4112-8990-239fc10317af": Phase="Pending", Reason="", readiness=false. Elapsed: 29.698718ms
Jul 31 12:11:57.415: INFO: Pod "pod-projected-configmaps-dec5ee20-bf98-4112-8990-239fc10317af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043585644s
Jul 31 12:11:59.428: INFO: Pod "pod-projected-configmaps-dec5ee20-bf98-4112-8990-239fc10317af": Phase="Pending", Reason="", readiness=false. Elapsed: 4.056134587s
Jul 31 12:12:01.438: INFO: Pod "pod-projected-configmaps-dec5ee20-bf98-4112-8990-239fc10317af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.066293806s
STEP: Saw pod success
Jul 31 12:12:01.438: INFO: Pod "pod-projected-configmaps-dec5ee20-bf98-4112-8990-239fc10317af" satisfied condition "Succeeded or Failed"
Jul 31 12:12:01.444: INFO: Trying to get logs from node p1-ashfcfj4pzo6aemt1j3a9ah7ew pod pod-projected-configmaps-dec5ee20-bf98-4112-8990-239fc10317af container agnhost-container: <nil>
STEP: delete the pod
Jul 31 12:12:01.500: INFO: Waiting for pod pod-projected-configmaps-dec5ee20-bf98-4112-8990-239fc10317af to disappear
Jul 31 12:12:01.506: INFO: Pod pod-projected-configmaps-dec5ee20-bf98-4112-8990-239fc10317af no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:12:01.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5782" for this suite.

• [SLOW TEST:6.253 seconds]
[sig-storage] Projected configMap
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":346,"completed":342,"skipped":6588,"failed":0}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:12:01.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:752
[It] should find a service from listing all namespaces [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: fetching services
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:12:01.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8538" for this suite.
[AfterEach] [sig-network] Services
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:756
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":346,"completed":343,"skipped":6598,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:12:01.661: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/node/pods.go:189
[It] should be updated [NodeConformance] [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: creating the pod
STEP: submitting the pod to kubernetes
Jul 31 12:12:01.753: INFO: The status of Pod pod-update-e1e5c142-8d0d-4d74-b92e-92d68150b7b6 is Pending, waiting for it to be Running (with Ready = true)
Jul 31 12:12:03.765: INFO: The status of Pod pod-update-e1e5c142-8d0d-4d74-b92e-92d68150b7b6 is Running (Ready = true)
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul 31 12:12:04.308: INFO: Successfully updated pod "pod-update-e1e5c142-8d0d-4d74-b92e-92d68150b7b6"
STEP: verifying the updated pod is in kubernetes
Jul 31 12:12:04.331: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:12:04.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6945" for this suite.
•{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","total":346,"completed":344,"skipped":6634,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:12:04.353: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:12:04.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6278" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":346,"completed":345,"skipped":6688,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] 
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:185
STEP: Creating a kubernetes client
Jul 31 12:12:04.494: INFO: >>> kubeConfig: /tmp/kubeconfig-3485299162
STEP: Building a namespace api object, basename sched-preemption
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Jul 31 12:12:04.573: INFO: Waiting up to 1m0s for all nodes to be ready
Jul 31 12:13:04.828: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
STEP: Create pods that use 4/5 of node resources.
Jul 31 12:13:04.863: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jul 31 12:13:04.889: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jul 31 12:13:04.950: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jul 31 12:13:04.984: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jul 31 12:13:05.031: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jul 31 12:13:05.060: INFO: Created pod: pod2-1-sched-preemption-medium-priority
Jul 31 12:13:05.131: INFO: Created pod: pod3-0-sched-preemption-medium-priority
Jul 31 12:13:05.149: INFO: Created pod: pod3-1-sched-preemption-medium-priority
Jul 31 12:13:05.197: INFO: Created pod: pod4-0-sched-preemption-medium-priority
Jul 31 12:13:05.218: INFO: Created pod: pod4-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled.
STEP: Run a critical pod that use same resources as that of a lower priority pod
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:186
Jul 31 12:13:15.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-8954" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:71.197 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  /workspace/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:633
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","total":346,"completed":346,"skipped":6701,"failed":0}
SSSJul 31 12:13:15.691: INFO: Running AfterSuite actions on all nodes
Jul 31 12:13:15.691: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func18.2
Jul 31 12:13:15.691: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func8.2
Jul 31 12:13:15.691: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func7.2
Jul 31 12:13:15.691: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Jul 31 12:13:15.691: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Jul 31 12:13:15.691: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Jul 31 12:13:15.691: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
Jul 31 12:13:15.691: INFO: Running AfterSuite actions on node 1
Jul 31 12:13:15.691: INFO: Skipping dumping logs from cluster

JUnit report was created: /tmp/sonobuoy/results/junit_01.xml
{"msg":"Test Suite completed","total":346,"completed":346,"skipped":6704,"failed":0}

Ran 346 of 7050 Specs in 6117.050 seconds
SUCCESS! -- 346 Passed | 0 Failed | 0 Pending | 6704 Skipped
PASS

Ginkgo ran 1 suite in 1h41m59.998341288s
Test Suite Passed
