I1204 20:33:34.386455      15 test_context.go:405] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-246239386
I1204 20:33:34.386873      15 e2e.go:242] Starting e2e run "56e9ef6b-16d5-11ea-8695-527d496f91de" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1575491612 - Will randomize all specs
Will run 204 of 3586 specs

Dec  4 20:33:34.485: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
Dec  4 20:33:34.487: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Dec  4 20:33:34.504: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Dec  4 20:33:34.548: INFO: 16 / 16 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Dec  4 20:33:34.548: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready.
Dec  4 20:33:34.548: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Dec  4 20:33:34.560: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Dec  4 20:33:34.560: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Dec  4 20:33:34.560: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'nvidia-device-plugin-daemonset' (0 seconds elapsed)
Dec  4 20:33:34.560: INFO: e2e test version: v1.14.8
Dec  4 20:33:34.561: INFO: kube-apiserver version: v1.14.8
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:33:34.561: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename downward-api
Dec  4 20:33:34.594: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Dec  4 20:33:34.607: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5525
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  4 20:33:34.730: INFO: Waiting up to 5m0s for pod "downwardapi-volume-57eccdae-16d5-11ea-8695-527d496f91de" in namespace "downward-api-5525" to be "success or failure"
Dec  4 20:33:34.749: INFO: Pod "downwardapi-volume-57eccdae-16d5-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 18.92278ms
Dec  4 20:33:36.752: INFO: Pod "downwardapi-volume-57eccdae-16d5-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022085727s
Dec  4 20:33:38.755: INFO: Pod "downwardapi-volume-57eccdae-16d5-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025280636s
STEP: Saw pod success
Dec  4 20:33:38.755: INFO: Pod "downwardapi-volume-57eccdae-16d5-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 20:33:38.757: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker6c7da5d4d5 pod downwardapi-volume-57eccdae-16d5-11ea-8695-527d496f91de container client-container: <nil>
STEP: delete the pod
Dec  4 20:33:38.784: INFO: Waiting for pod downwardapi-volume-57eccdae-16d5-11ea-8695-527d496f91de to disappear
Dec  4 20:33:38.787: INFO: Pod downwardapi-volume-57eccdae-16d5-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:33:38.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5525" for this suite.
Dec  4 20:33:44.803: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:33:44.879: INFO: namespace downward-api-5525 deletion completed in 6.087045568s

• [SLOW TEST:10.318 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:33:44.882: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3293
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1256
STEP: creating an rc
Dec  4 20:33:45.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 create -f - --namespace=kubectl-3293'
Dec  4 20:33:45.558: INFO: stderr: ""
Dec  4 20:33:45.558: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Waiting for Redis master to start.
Dec  4 20:33:46.561: INFO: Selector matched 1 pods for map[app:redis]
Dec  4 20:33:46.561: INFO: Found 0 / 1
Dec  4 20:33:47.561: INFO: Selector matched 1 pods for map[app:redis]
Dec  4 20:33:47.561: INFO: Found 0 / 1
Dec  4 20:33:48.561: INFO: Selector matched 1 pods for map[app:redis]
Dec  4 20:33:48.561: INFO: Found 0 / 1
Dec  4 20:33:49.561: INFO: Selector matched 1 pods for map[app:redis]
Dec  4 20:33:49.561: INFO: Found 1 / 1
Dec  4 20:33:49.561: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec  4 20:33:49.564: INFO: Selector matched 1 pods for map[app:redis]
Dec  4 20:33:49.564: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Dec  4 20:33:49.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 logs redis-master-mfx5t redis-master --namespace=kubectl-3293'
Dec  4 20:33:49.676: INFO: stderr: ""
Dec  4 20:33:49.676: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 04 Dec 20:33:47.223 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 04 Dec 20:33:47.223 # Server started, Redis version 3.2.12\n1:M 04 Dec 20:33:47.224 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 04 Dec 20:33:47.224 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Dec  4 20:33:49.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 log redis-master-mfx5t redis-master --namespace=kubectl-3293 --tail=1'
Dec  4 20:33:49.784: INFO: stderr: ""
Dec  4 20:33:49.785: INFO: stdout: "1:M 04 Dec 20:33:47.224 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Dec  4 20:33:49.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 log redis-master-mfx5t redis-master --namespace=kubectl-3293 --limit-bytes=1'
Dec  4 20:33:49.950: INFO: stderr: ""
Dec  4 20:33:49.950: INFO: stdout: " "
STEP: exposing timestamps
Dec  4 20:33:49.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 log redis-master-mfx5t redis-master --namespace=kubectl-3293 --tail=1 --timestamps'
Dec  4 20:33:50.049: INFO: stderr: ""
Dec  4 20:33:50.049: INFO: stdout: "2019-12-04T20:33:47.22413707Z 1:M 04 Dec 20:33:47.224 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Dec  4 20:33:52.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 log redis-master-mfx5t redis-master --namespace=kubectl-3293 --since=1s'
Dec  4 20:33:52.652: INFO: stderr: ""
Dec  4 20:33:52.652: INFO: stdout: ""
Dec  4 20:33:52.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 log redis-master-mfx5t redis-master --namespace=kubectl-3293 --since=24h'
Dec  4 20:33:52.756: INFO: stderr: ""
Dec  4 20:33:52.756: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 04 Dec 20:33:47.223 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 04 Dec 20:33:47.223 # Server started, Redis version 3.2.12\n1:M 04 Dec 20:33:47.224 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 04 Dec 20:33:47.224 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1262
STEP: using delete to clean up resources
Dec  4 20:33:52.757: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 delete --grace-period=0 --force -f - --namespace=kubectl-3293'
Dec  4 20:33:52.865: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  4 20:33:52.865: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Dec  4 20:33:52.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get rc,svc -l name=nginx --no-headers --namespace=kubectl-3293'
Dec  4 20:33:52.948: INFO: stderr: "No resources found.\n"
Dec  4 20:33:52.948: INFO: stdout: ""
Dec  4 20:33:52.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get pods -l name=nginx --namespace=kubectl-3293 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  4 20:33:53.048: INFO: stderr: ""
Dec  4 20:33:53.048: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:33:53.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3293" for this suite.
Dec  4 20:34:15.063: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:34:15.139: INFO: namespace kubectl-3293 deletion completed in 22.087121873s

• [SLOW TEST:30.257 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:34:15.143: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3664
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:34:21.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3664" for this suite.
Dec  4 20:35:07.324: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:35:07.403: INFO: namespace kubelet-test-3664 deletion completed in 46.086805144s

• [SLOW TEST:52.260 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:35:07.407: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5860
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-8f451ed7-16d5-11ea-8695-527d496f91de
STEP: Creating secret with name s-test-opt-upd-8f451f74-16d5-11ea-8695-527d496f91de
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-8f451ed7-16d5-11ea-8695-527d496f91de
STEP: Updating secret s-test-opt-upd-8f451f74-16d5-11ea-8695-527d496f91de
STEP: Creating secret with name s-test-opt-create-8f451fa2-16d5-11ea-8695-527d496f91de
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:36:23.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5860" for this suite.
Dec  4 20:36:45.992: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:36:46.059: INFO: namespace projected-5860 deletion completed in 22.077184044s

• [SLOW TEST:98.653 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:36:46.060: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-4914
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-7203
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-2596
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:37:12.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4914" for this suite.
Dec  4 20:37:18.497: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:37:18.561: INFO: namespace namespaces-4914 deletion completed in 6.073325855s
STEP: Destroying namespace "nsdeletetest-7203" for this suite.
Dec  4 20:37:18.563: INFO: Namespace nsdeletetest-7203 was already deleted
STEP: Destroying namespace "nsdeletetest-2596" for this suite.
Dec  4 20:37:24.571: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:37:24.654: INFO: namespace nsdeletetest-2596 deletion completed in 6.091233302s

• [SLOW TEST:38.594 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:37:24.655: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6068
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  4 20:37:24.801: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Dec  4 20:37:24.817: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec  4 20:37:29.821: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec  4 20:37:29.822: INFO: Creating deployment "test-rolling-update-deployment"
Dec  4 20:37:29.828: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Dec  4 20:37:29.862: INFO: deployment "test-rolling-update-deployment" doesn't have the required revision set
Dec  4 20:37:31.867: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Dec  4 20:37:31.869: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711088649, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711088649, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711088649, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711088649, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-57b6b5bb54\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  4 20:37:33.872: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Dec  4 20:37:33.878: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-6068,SelfLink:/apis/apps/v1/namespaces/deployment-6068/deployments/test-rolling-update-deployment,UID:e40eb544-16d5-11ea-825c-005056950656,ResourceVersion:276832,Generation:1,CreationTimestamp:2019-12-04 20:37:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-12-04 20:37:29 +0000 UTC 2019-12-04 20:37:29 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-12-04 20:37:33 +0000 UTC 2019-12-04 20:37:29 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-57b6b5bb54" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Dec  4 20:37:33.880: INFO: New ReplicaSet "test-rolling-update-deployment-57b6b5bb54" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-57b6b5bb54,GenerateName:,Namespace:deployment-6068,SelfLink:/apis/apps/v1/namespaces/deployment-6068/replicasets/test-rolling-update-deployment-57b6b5bb54,UID:e411dab7-16d5-11ea-825c-005056950656,ResourceVersion:276822,Generation:1,CreationTimestamp:2019-12-04 20:37:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 57b6b5bb54,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment e40eb544-16d5-11ea-825c-005056950656 0xc00213a9e7 0xc00213a9e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 57b6b5bb54,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 57b6b5bb54,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Dec  4 20:37:33.880: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Dec  4 20:37:33.880: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-6068,SelfLink:/apis/apps/v1/namespaces/deployment-6068/replicasets/test-rolling-update-controller,UID:e1109ee6-16d5-11ea-825c-005056950656,ResourceVersion:276831,Generation:2,CreationTimestamp:2019-12-04 20:37:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment e40eb544-16d5-11ea-825c-005056950656 0xc00213a917 0xc00213a918}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  4 20:37:33.883: INFO: Pod "test-rolling-update-deployment-57b6b5bb54-755td" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-57b6b5bb54-755td,GenerateName:test-rolling-update-deployment-57b6b5bb54-,Namespace:deployment-6068,SelfLink:/api/v1/namespaces/deployment-6068/pods/test-rolling-update-deployment-57b6b5bb54-755td,UID:e412e7da-16d5-11ea-825c-005056950656,ResourceVersion:276821,Generation:0,CreationTimestamp:2019-12-04 20:37:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 57b6b5bb54,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.2.22/32,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-57b6b5bb54 e411dab7-16d5-11ea-825c-005056950656 0xc00213b3a7 0xc00213b3a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-6lrwp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6lrwp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-6lrwp true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v2-vsp1-worker6c7da5d4d5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00213b410} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00213b430}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 20:37:34 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 20:37:37 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 20:37:37 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 20:37:29 +0000 UTC  }],Message:,Reason:,HostIP:10.10.128.23,PodIP:192.168.2.22,StartTime:2019-12-04 20:37:34 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-12-04 20:37:36 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://a32951f5bcec04e0909b7ac740063dee4c898e629a66b906b97e8e19dc2c8c96}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:37:33.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6068" for this suite.
Dec  4 20:37:39.896: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:37:39.974: INFO: namespace deployment-6068 deletion completed in 6.086787384s

• [SLOW TEST:15.319 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:37:39.975: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-4502
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
Dec  4 20:37:40.637: INFO: created pod pod-service-account-defaultsa
Dec  4 20:37:40.637: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Dec  4 20:37:40.648: INFO: created pod pod-service-account-mountsa
Dec  4 20:37:40.648: INFO: pod pod-service-account-mountsa service account token volume mount: true
Dec  4 20:37:40.659: INFO: created pod pod-service-account-nomountsa
Dec  4 20:37:40.660: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Dec  4 20:37:40.688: INFO: created pod pod-service-account-defaultsa-mountspec
Dec  4 20:37:40.688: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Dec  4 20:37:40.698: INFO: created pod pod-service-account-mountsa-mountspec
Dec  4 20:37:40.698: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Dec  4 20:37:40.736: INFO: created pod pod-service-account-nomountsa-mountspec
Dec  4 20:37:40.737: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Dec  4 20:37:40.746: INFO: created pod pod-service-account-defaultsa-nomountspec
Dec  4 20:37:40.746: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Dec  4 20:37:40.762: INFO: created pod pod-service-account-mountsa-nomountspec
Dec  4 20:37:40.764: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Dec  4 20:37:40.776: INFO: created pod pod-service-account-nomountsa-nomountspec
Dec  4 20:37:40.776: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:37:40.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4502" for this suite.
Dec  4 20:37:46.798: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:37:46.981: INFO: namespace svcaccounts-4502 deletion completed in 6.194862893s

• [SLOW TEST:7.007 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:37:46.983: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9753
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W1204 20:37:47.689176      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec  4 20:37:47.689: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:37:47.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9753" for this suite.
Dec  4 20:37:53.700: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:37:53.782: INFO: namespace gc-9753 deletion completed in 6.089907529s

• [SLOW TEST:6.800 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:37:53.785: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-78
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  4 20:37:53.938: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f26d5c32-16d5-11ea-8695-527d496f91de" in namespace "downward-api-78" to be "success or failure"
Dec  4 20:37:53.943: INFO: Pod "downwardapi-volume-f26d5c32-16d5-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 4.849752ms
Dec  4 20:37:55.947: INFO: Pod "downwardapi-volume-f26d5c32-16d5-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009192604s
STEP: Saw pod success
Dec  4 20:37:55.947: INFO: Pod "downwardapi-volume-f26d5c32-16d5-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 20:37:55.949: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker6c7da5d4d5 pod downwardapi-volume-f26d5c32-16d5-11ea-8695-527d496f91de container client-container: <nil>
STEP: delete the pod
Dec  4 20:37:55.968: INFO: Waiting for pod downwardapi-volume-f26d5c32-16d5-11ea-8695-527d496f91de to disappear
Dec  4 20:37:55.970: INFO: Pod downwardapi-volume-f26d5c32-16d5-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:37:55.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-78" for this suite.
Dec  4 20:38:01.987: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:38:02.088: INFO: namespace downward-api-78 deletion completed in 6.110545097s

• [SLOW TEST:8.303 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:38:02.088: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3673
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-3673
Dec  4 20:38:06.303: INFO: Started pod liveness-exec in namespace container-probe-3673
STEP: checking the pod's current state and verifying that restartCount is present
Dec  4 20:38:06.305: INFO: Initial restart count of pod liveness-exec is 0
Dec  4 20:38:50.371: INFO: Restart count of pod container-probe-3673/liveness-exec is now 1 (44.065783869s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:38:50.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3673" for this suite.
Dec  4 20:38:56.405: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:38:56.495: INFO: namespace container-probe-3673 deletion completed in 6.099622499s

• [SLOW TEST:54.407 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:38:56.495: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1865
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:163
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  4 20:38:56.632: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:39:00.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1865" for this suite.
Dec  4 20:39:40.695: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:39:40.760: INFO: namespace pods-1865 deletion completed in 40.074627374s

• [SLOW TEST:44.265 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:39:40.762: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-681
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  4 20:39:40.904: INFO: Waiting up to 5m0s for pod "downwardapi-volume-322f0484-16d6-11ea-8695-527d496f91de" in namespace "downward-api-681" to be "success or failure"
Dec  4 20:39:40.909: INFO: Pod "downwardapi-volume-322f0484-16d6-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 4.422078ms
Dec  4 20:39:42.912: INFO: Pod "downwardapi-volume-322f0484-16d6-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007111525s
STEP: Saw pod success
Dec  4 20:39:42.912: INFO: Pod "downwardapi-volume-322f0484-16d6-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 20:39:42.913: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker9f6c17cd60 pod downwardapi-volume-322f0484-16d6-11ea-8695-527d496f91de container client-container: <nil>
STEP: delete the pod
Dec  4 20:39:42.934: INFO: Waiting for pod downwardapi-volume-322f0484-16d6-11ea-8695-527d496f91de to disappear
Dec  4 20:39:42.937: INFO: Pod downwardapi-volume-322f0484-16d6-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:39:42.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-681" for this suite.
Dec  4 20:39:48.952: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:39:49.031: INFO: namespace downward-api-681 deletion completed in 6.088859065s

• [SLOW TEST:8.269 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:39:49.032: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1762
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1762.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1762.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1762.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1762.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1762.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1762.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1762.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1762.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1762.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1762.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1762.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 160.36.111.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.111.36.160_udp@PTR;check="$$(dig +tcp +noall +answer +search 160.36.111.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.111.36.160_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1762.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1762.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1762.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1762.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1762.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1762.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1762.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1762.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1762.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1762.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1762.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 160.36.111.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.111.36.160_udp@PTR;check="$$(dig +tcp +noall +answer +search 160.36.111.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.111.36.160_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  4 20:39:59.220: INFO: Unable to read wheezy_udp@dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-371ff070-16d6-11ea-8695-527d496f91de: the server could not find the requested resource (get pods dns-test-371ff070-16d6-11ea-8695-527d496f91de)
Dec  4 20:39:59.222: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-371ff070-16d6-11ea-8695-527d496f91de: the server could not find the requested resource (get pods dns-test-371ff070-16d6-11ea-8695-527d496f91de)
Dec  4 20:39:59.224: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-371ff070-16d6-11ea-8695-527d496f91de: the server could not find the requested resource (get pods dns-test-371ff070-16d6-11ea-8695-527d496f91de)
Dec  4 20:39:59.226: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-371ff070-16d6-11ea-8695-527d496f91de: the server could not find the requested resource (get pods dns-test-371ff070-16d6-11ea-8695-527d496f91de)
Dec  4 20:39:59.243: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-371ff070-16d6-11ea-8695-527d496f91de: the server could not find the requested resource (get pods dns-test-371ff070-16d6-11ea-8695-527d496f91de)
Dec  4 20:39:59.246: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local from pod dns-1762/dns-test-371ff070-16d6-11ea-8695-527d496f91de: the server could not find the requested resource (get pods dns-test-371ff070-16d6-11ea-8695-527d496f91de)
Dec  4 20:39:59.259: INFO: Lookups using dns-1762/dns-test-371ff070-16d6-11ea-8695-527d496f91de failed for: [wheezy_udp@dns-test-service.dns-1762.svc.cluster.local wheezy_tcp@dns-test-service.dns-1762.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1762.svc.cluster.local]

Dec  4 20:40:04.303: INFO: DNS probes using dns-1762/dns-test-371ff070-16d6-11ea-8695-527d496f91de succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:40:04.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1762" for this suite.
Dec  4 20:40:10.415: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:40:10.478: INFO: namespace dns-1762 deletion completed in 6.075583501s

• [SLOW TEST:21.446 seconds]
[sig-network] DNS
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:40:10.481: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5845
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-43e535ea-16d6-11ea-8695-527d496f91de
STEP: Creating a pod to test consume configMaps
Dec  4 20:40:10.623: INFO: Waiting up to 5m0s for pod "pod-configmaps-43e5d151-16d6-11ea-8695-527d496f91de" in namespace "configmap-5845" to be "success or failure"
Dec  4 20:40:10.632: INFO: Pod "pod-configmaps-43e5d151-16d6-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 8.587138ms
Dec  4 20:40:12.635: INFO: Pod "pod-configmaps-43e5d151-16d6-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011479752s
Dec  4 20:40:14.638: INFO: Pod "pod-configmaps-43e5d151-16d6-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014616756s
STEP: Saw pod success
Dec  4 20:40:14.638: INFO: Pod "pod-configmaps-43e5d151-16d6-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 20:40:14.640: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker9f6c17cd60 pod pod-configmaps-43e5d151-16d6-11ea-8695-527d496f91de container configmap-volume-test: <nil>
STEP: delete the pod
Dec  4 20:40:14.662: INFO: Waiting for pod pod-configmaps-43e5d151-16d6-11ea-8695-527d496f91de to disappear
Dec  4 20:40:14.673: INFO: Pod pod-configmaps-43e5d151-16d6-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:40:14.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5845" for this suite.
Dec  4 20:40:20.686: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:40:20.757: INFO: namespace configmap-5845 deletion completed in 6.079515376s

• [SLOW TEST:10.276 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:40:20.759: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-5452
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-5q68j in namespace proxy-5452
I1204 20:40:20.912180      15 runners.go:184] Created replication controller with name: proxy-service-5q68j, namespace: proxy-5452, replica count: 1
I1204 20:40:21.962896      15 runners.go:184] proxy-service-5q68j Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1204 20:40:22.963067      15 runners.go:184] proxy-service-5q68j Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1204 20:40:23.963246      15 runners.go:184] proxy-service-5q68j Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1204 20:40:24.963417      15 runners.go:184] proxy-service-5q68j Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1204 20:40:25.963587      15 runners.go:184] proxy-service-5q68j Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1204 20:40:26.963824      15 runners.go:184] proxy-service-5q68j Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1204 20:40:27.964055      15 runners.go:184] proxy-service-5q68j Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1204 20:40:28.964253      15 runners.go:184] proxy-service-5q68j Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  4 20:40:28.967: INFO: setup took 8.076661219s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Dec  4 20:40:28.983: INFO: (0) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:162/proxy/: bar (200; 15.426041ms)
Dec  4 20:40:28.983: INFO: (0) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:1080/proxy/rewriteme">... (200; 15.076226ms)
Dec  4 20:40:28.983: INFO: (0) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:162/proxy/: bar (200; 15.83026ms)
Dec  4 20:40:28.984: INFO: (0) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:1080/proxy/rewriteme">test<... (200; 15.223142ms)
Dec  4 20:40:28.984: INFO: (0) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk/proxy/rewriteme">test</a> (200; 15.725774ms)
Dec  4 20:40:28.984: INFO: (0) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:160/proxy/: foo (200; 16.048592ms)
Dec  4 20:40:28.984: INFO: (0) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:160/proxy/: foo (200; 15.523366ms)
Dec  4 20:40:28.990: INFO: (0) /api/v1/namespaces/proxy-5452/services/http:proxy-service-5q68j:portname1/proxy/: foo (200; 21.68806ms)
Dec  4 20:40:28.990: INFO: (0) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:462/proxy/: tls qux (200; 22.229485ms)
Dec  4 20:40:28.990: INFO: (0) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:443/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:443/proxy/tlsrewritem... (200; 22.801087ms)
Dec  4 20:40:28.990: INFO: (0) /api/v1/namespaces/proxy-5452/services/proxy-service-5q68j:portname2/proxy/: bar (200; 21.642386ms)
Dec  4 20:40:28.990: INFO: (0) /api/v1/namespaces/proxy-5452/services/https:proxy-service-5q68j:tlsportname1/proxy/: tls baz (200; 22.730923ms)
Dec  4 20:40:28.990: INFO: (0) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:460/proxy/: tls baz (200; 22.20428ms)
Dec  4 20:40:28.990: INFO: (0) /api/v1/namespaces/proxy-5452/services/http:proxy-service-5q68j:portname2/proxy/: bar (200; 22.004449ms)
Dec  4 20:40:28.990: INFO: (0) /api/v1/namespaces/proxy-5452/services/https:proxy-service-5q68j:tlsportname2/proxy/: tls qux (200; 23.360909ms)
Dec  4 20:40:28.990: INFO: (0) /api/v1/namespaces/proxy-5452/services/proxy-service-5q68j:portname1/proxy/: foo (200; 21.904302ms)
Dec  4 20:40:28.999: INFO: (1) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:1080/proxy/rewriteme">... (200; 5.9886ms)
Dec  4 20:40:28.999: INFO: (1) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:160/proxy/: foo (200; 8.30695ms)
Dec  4 20:40:29.003: INFO: (1) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:160/proxy/: foo (200; 8.571041ms)
Dec  4 20:40:29.004: INFO: (1) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:162/proxy/: bar (200; 9.026102ms)
Dec  4 20:40:29.006: INFO: (1) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:443/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:443/proxy/tlsrewritem... (200; 9.722997ms)
Dec  4 20:40:29.006: INFO: (1) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:462/proxy/: tls qux (200; 10.041798ms)
Dec  4 20:40:29.007: INFO: (1) /api/v1/namespaces/proxy-5452/services/http:proxy-service-5q68j:portname1/proxy/: foo (200; 11.303131ms)
Dec  4 20:40:29.007: INFO: (1) /api/v1/namespaces/proxy-5452/services/proxy-service-5q68j:portname2/proxy/: bar (200; 15.835002ms)
Dec  4 20:40:29.011: INFO: (1) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:1080/proxy/rewriteme">test<... (200; 14.850792ms)
Dec  4 20:40:29.011: INFO: (1) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:162/proxy/: bar (200; 14.965349ms)
Dec  4 20:40:29.011: INFO: (1) /api/v1/namespaces/proxy-5452/services/proxy-service-5q68j:portname1/proxy/: foo (200; 20.078603ms)
Dec  4 20:40:29.011: INFO: (1) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk/proxy/rewriteme">test</a> (200; 15.107898ms)
Dec  4 20:40:29.011: INFO: (1) /api/v1/namespaces/proxy-5452/services/https:proxy-service-5q68j:tlsportname2/proxy/: tls qux (200; 19.190279ms)
Dec  4 20:40:29.011: INFO: (1) /api/v1/namespaces/proxy-5452/services/https:proxy-service-5q68j:tlsportname1/proxy/: tls baz (200; 18.307883ms)
Dec  4 20:40:29.013: INFO: (1) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:460/proxy/: tls baz (200; 16.470113ms)
Dec  4 20:40:29.013: INFO: (1) /api/v1/namespaces/proxy-5452/services/http:proxy-service-5q68j:portname2/proxy/: bar (200; 16.920319ms)
Dec  4 20:40:29.026: INFO: (2) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:462/proxy/: tls qux (200; 8.557191ms)
Dec  4 20:40:29.026: INFO: (2) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:1080/proxy/rewriteme">... (200; 10.722887ms)
Dec  4 20:40:29.026: INFO: (2) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:162/proxy/: bar (200; 9.113021ms)
Dec  4 20:40:29.026: INFO: (2) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:160/proxy/: foo (200; 10.12447ms)
Dec  4 20:40:29.026: INFO: (2) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:460/proxy/: tls baz (200; 8.949602ms)
Dec  4 20:40:29.026: INFO: (2) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:162/proxy/: bar (200; 9.499769ms)
Dec  4 20:40:29.026: INFO: (2) /api/v1/namespaces/proxy-5452/services/http:proxy-service-5q68j:portname2/proxy/: bar (200; 9.104371ms)
Dec  4 20:40:29.027: INFO: (2) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:1080/proxy/rewriteme">test<... (200; 9.182072ms)
Dec  4 20:40:29.027: INFO: (2) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:160/proxy/: foo (200; 11.41417ms)
Dec  4 20:40:29.027: INFO: (2) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk/proxy/rewriteme">test</a> (200; 10.567815ms)
Dec  4 20:40:29.027: INFO: (2) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:443/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:443/proxy/tlsrewritem... (200; 10.848687ms)
Dec  4 20:40:29.030: INFO: (2) /api/v1/namespaces/proxy-5452/services/proxy-service-5q68j:portname1/proxy/: foo (200; 16.323115ms)
Dec  4 20:40:29.030: INFO: (2) /api/v1/namespaces/proxy-5452/services/http:proxy-service-5q68j:portname1/proxy/: foo (200; 13.48979ms)
Dec  4 20:40:29.030: INFO: (2) /api/v1/namespaces/proxy-5452/services/proxy-service-5q68j:portname2/proxy/: bar (200; 15.940406ms)
Dec  4 20:40:29.031: INFO: (2) /api/v1/namespaces/proxy-5452/services/https:proxy-service-5q68j:tlsportname2/proxy/: tls qux (200; 17.388364ms)
Dec  4 20:40:29.031: INFO: (2) /api/v1/namespaces/proxy-5452/services/https:proxy-service-5q68j:tlsportname1/proxy/: tls baz (200; 15.048249ms)
Dec  4 20:40:29.039: INFO: (3) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:1080/proxy/rewriteme">test<... (200; 8.24843ms)
Dec  4 20:40:29.040: INFO: (3) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:162/proxy/: bar (200; 7.362776ms)
Dec  4 20:40:29.040: INFO: (3) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:443/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:443/proxy/tlsrewritem... (200; 7.479379ms)
Dec  4 20:40:29.040: INFO: (3) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:462/proxy/: tls qux (200; 9.248095ms)
Dec  4 20:40:29.040: INFO: (3) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:160/proxy/: foo (200; 8.831763ms)
Dec  4 20:40:29.040: INFO: (3) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:1080/proxy/rewriteme">... (200; 9.280576ms)
Dec  4 20:40:29.040: INFO: (3) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:160/proxy/: foo (200; 7.609614ms)
Dec  4 20:40:29.041: INFO: (3) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk/proxy/rewriteme">test</a> (200; 8.00601ms)
Dec  4 20:40:29.042: INFO: (3) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:460/proxy/: tls baz (200; 9.292078ms)
Dec  4 20:40:29.042: INFO: (3) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:162/proxy/: bar (200; 9.615418ms)
Dec  4 20:40:29.045: INFO: (3) /api/v1/namespaces/proxy-5452/services/proxy-service-5q68j:portname2/proxy/: bar (200; 13.156154ms)
Dec  4 20:40:29.045: INFO: (3) /api/v1/namespaces/proxy-5452/services/http:proxy-service-5q68j:portname2/proxy/: bar (200; 13.70559ms)
Dec  4 20:40:29.045: INFO: (3) /api/v1/namespaces/proxy-5452/services/http:proxy-service-5q68j:portname1/proxy/: foo (200; 13.935961ms)
Dec  4 20:40:29.045: INFO: (3) /api/v1/namespaces/proxy-5452/services/https:proxy-service-5q68j:tlsportname2/proxy/: tls qux (200; 12.57024ms)
Dec  4 20:40:29.047: INFO: (3) /api/v1/namespaces/proxy-5452/services/https:proxy-service-5q68j:tlsportname1/proxy/: tls baz (200; 15.711431ms)
Dec  4 20:40:29.047: INFO: (3) /api/v1/namespaces/proxy-5452/services/proxy-service-5q68j:portname1/proxy/: foo (200; 15.540853ms)
Dec  4 20:40:29.053: INFO: (4) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:160/proxy/: foo (200; 5.877792ms)
Dec  4 20:40:29.053: INFO: (4) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:1080/proxy/rewriteme">test<... (200; 6.07193ms)
Dec  4 20:40:29.053: INFO: (4) /api/v1/namespaces/proxy-5452/services/http:proxy-service-5q68j:portname2/proxy/: bar (200; 6.357258ms)
Dec  4 20:40:29.053: INFO: (4) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:1080/proxy/rewriteme">... (200; 6.088599ms)
Dec  4 20:40:29.055: INFO: (4) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:162/proxy/: bar (200; 7.190944ms)
Dec  4 20:40:29.058: INFO: (4) /api/v1/namespaces/proxy-5452/services/http:proxy-service-5q68j:portname1/proxy/: foo (200; 10.665829ms)
Dec  4 20:40:29.059: INFO: (4) /api/v1/namespaces/proxy-5452/services/https:proxy-service-5q68j:tlsportname1/proxy/: tls baz (200; 10.504469ms)
Dec  4 20:40:29.059: INFO: (4) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:162/proxy/: bar (200; 10.713466ms)
Dec  4 20:40:29.059: INFO: (4) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:160/proxy/: foo (200; 10.459693ms)
Dec  4 20:40:29.059: INFO: (4) /api/v1/namespaces/proxy-5452/services/https:proxy-service-5q68j:tlsportname2/proxy/: tls qux (200; 11.543108ms)
Dec  4 20:40:29.059: INFO: (4) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:462/proxy/: tls qux (200; 10.619756ms)
Dec  4 20:40:29.060: INFO: (4) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk/proxy/rewriteme">test</a> (200; 11.090467ms)
Dec  4 20:40:29.060: INFO: (4) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:460/proxy/: tls baz (200; 11.487667ms)
Dec  4 20:40:29.060: INFO: (4) /api/v1/namespaces/proxy-5452/services/proxy-service-5q68j:portname2/proxy/: bar (200; 11.857552ms)
Dec  4 20:40:29.060: INFO: (4) /api/v1/namespaces/proxy-5452/services/proxy-service-5q68j:portname1/proxy/: foo (200; 12.307282ms)
Dec  4 20:40:29.060: INFO: (4) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:443/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:443/proxy/tlsrewritem... (200; 11.947009ms)
Dec  4 20:40:29.065: INFO: (5) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:1080/proxy/rewriteme">... (200; 4.962924ms)
Dec  4 20:40:29.065: INFO: (5) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:462/proxy/: tls qux (200; 5.021862ms)
Dec  4 20:40:29.067: INFO: (5) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:160/proxy/: foo (200; 6.637989ms)
Dec  4 20:40:29.067: INFO: (5) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:160/proxy/: foo (200; 6.339721ms)
Dec  4 20:40:29.067: INFO: (5) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:162/proxy/: bar (200; 6.626078ms)
Dec  4 20:40:29.067: INFO: (5) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:443/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:443/proxy/tlsrewritem... (200; 6.622819ms)
Dec  4 20:40:29.067: INFO: (5) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk/proxy/rewriteme">test</a> (200; 6.586094ms)
Dec  4 20:40:29.069: INFO: (5) /api/v1/namespaces/proxy-5452/services/http:proxy-service-5q68j:portname2/proxy/: bar (200; 8.010339ms)
Dec  4 20:40:29.069: INFO: (5) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:162/proxy/: bar (200; 8.186515ms)
Dec  4 20:40:29.069: INFO: (5) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:460/proxy/: tls baz (200; 8.155982ms)
Dec  4 20:40:29.069: INFO: (5) /api/v1/namespaces/proxy-5452/services/https:proxy-service-5q68j:tlsportname1/proxy/: tls baz (200; 8.497962ms)
Dec  4 20:40:29.070: INFO: (5) /api/v1/namespaces/proxy-5452/services/proxy-service-5q68j:portname2/proxy/: bar (200; 8.416466ms)
Dec  4 20:40:29.070: INFO: (5) /api/v1/namespaces/proxy-5452/services/proxy-service-5q68j:portname1/proxy/: foo (200; 8.394672ms)
Dec  4 20:40:29.071: INFO: (5) /api/v1/namespaces/proxy-5452/services/https:proxy-service-5q68j:tlsportname2/proxy/: tls qux (200; 9.726175ms)
Dec  4 20:40:29.072: INFO: (5) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:1080/proxy/rewriteme">test<... (200; 10.62598ms)
Dec  4 20:40:29.073: INFO: (5) /api/v1/namespaces/proxy-5452/services/http:proxy-service-5q68j:portname1/proxy/: foo (200; 12.662526ms)
Dec  4 20:40:29.080: INFO: (6) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:460/proxy/: tls baz (200; 6.554962ms)
Dec  4 20:40:29.080: INFO: (6) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:160/proxy/: foo (200; 6.114448ms)
Dec  4 20:40:29.080: INFO: (6) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:1080/proxy/rewriteme">test<... (200; 6.39802ms)
Dec  4 20:40:29.081: INFO: (6) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:462/proxy/: tls qux (200; 7.208721ms)
Dec  4 20:40:29.081: INFO: (6) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk/proxy/rewriteme">test</a> (200; 7.091392ms)
Dec  4 20:40:29.081: INFO: (6) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:162/proxy/: bar (200; 7.316937ms)
Dec  4 20:40:29.082: INFO: (6) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:443/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:443/proxy/tlsrewritem... (200; 7.664027ms)
Dec  4 20:40:29.082: INFO: (6) /api/v1/namespaces/proxy-5452/services/http:proxy-service-5q68j:portname2/proxy/: bar (200; 8.449094ms)
Dec  4 20:40:29.082: INFO: (6) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:1080/proxy/rewriteme">... (200; 7.821449ms)
Dec  4 20:40:29.083: INFO: (6) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:160/proxy/: foo (200; 7.708519ms)
Dec  4 20:40:29.083: INFO: (6) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:162/proxy/: bar (200; 8.015241ms)
Dec  4 20:40:29.085: INFO: (6) /api/v1/namespaces/proxy-5452/services/proxy-service-5q68j:portname1/proxy/: foo (200; 10.156794ms)
Dec  4 20:40:29.085: INFO: (6) /api/v1/namespaces/proxy-5452/services/https:proxy-service-5q68j:tlsportname1/proxy/: tls baz (200; 9.959632ms)
Dec  4 20:40:29.085: INFO: (6) /api/v1/namespaces/proxy-5452/services/https:proxy-service-5q68j:tlsportname2/proxy/: tls qux (200; 10.440131ms)
Dec  4 20:40:29.086: INFO: (6) /api/v1/namespaces/proxy-5452/services/http:proxy-service-5q68j:portname1/proxy/: foo (200; 11.919679ms)
Dec  4 20:40:29.086: INFO: (6) /api/v1/namespaces/proxy-5452/services/proxy-service-5q68j:portname2/proxy/: bar (200; 11.382333ms)
Dec  4 20:40:29.098: INFO: (7) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:162/proxy/: bar (200; 2.309232ms)
Dec  4 20:40:29.099: INFO: (7) /api/v1/namespaces/proxy-5452/services/proxy-service-5q68j:portname1/proxy/: foo (200; 12.698875ms)
Dec  4 20:40:29.099: INFO: (7) /api/v1/namespaces/proxy-5452/services/https:proxy-service-5q68j:tlsportname2/proxy/: tls qux (200; 11.355953ms)
Dec  4 20:40:29.100: INFO: (7) /api/v1/namespaces/proxy-5452/services/https:proxy-service-5q68j:tlsportname1/proxy/: tls baz (200; 10.039895ms)
Dec  4 20:40:29.100: INFO: (7) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:160/proxy/: foo (200; 9.542457ms)
Dec  4 20:40:29.100: INFO: (7) /api/v1/namespaces/proxy-5452/services/proxy-service-5q68j:portname2/proxy/: bar (200; 13.223413ms)
Dec  4 20:40:29.101: INFO: (7) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:162/proxy/: bar (200; 9.207539ms)
Dec  4 20:40:29.101: INFO: (7) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:443/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:443/proxy/tlsrewritem... (200; 14.547611ms)
Dec  4 20:40:29.101: INFO: (7) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:1080/proxy/rewriteme">... (200; 12.380902ms)
Dec  4 20:40:29.101: INFO: (7) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk/proxy/rewriteme">test</a> (200; 5.554775ms)
Dec  4 20:40:29.101: INFO: (7) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:462/proxy/: tls qux (200; 9.297568ms)
Dec  4 20:40:29.101: INFO: (7) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:460/proxy/: tls baz (200; 4.088847ms)
Dec  4 20:40:29.108: INFO: (7) /api/v1/namespaces/proxy-5452/services/http:proxy-service-5q68j:portname1/proxy/: foo (200; 6.131343ms)
Dec  4 20:40:29.108: INFO: (7) /api/v1/namespaces/proxy-5452/services/http:proxy-service-5q68j:portname2/proxy/: bar (200; 6.386746ms)
Dec  4 20:40:29.108: INFO: (7) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:1080/proxy/rewriteme">test<... (200; 5.441707ms)
Dec  4 20:40:29.108: INFO: (7) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:160/proxy/: foo (200; 22.090614ms)
Dec  4 20:40:29.122: INFO: (8) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:462/proxy/: tls qux (200; 13.147783ms)
Dec  4 20:40:29.122: INFO: (8) /api/v1/namespaces/proxy-5452/services/http:proxy-service-5q68j:portname1/proxy/: foo (200; 13.154776ms)
Dec  4 20:40:29.123: INFO: (8) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:160/proxy/: foo (200; 10.749972ms)
Dec  4 20:40:29.123: INFO: (8) /api/v1/namespaces/proxy-5452/services/proxy-service-5q68j:portname1/proxy/: foo (200; 10.640904ms)
Dec  4 20:40:29.123: INFO: (8) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:1080/proxy/rewriteme">test<... (200; 12.062437ms)
Dec  4 20:40:29.123: INFO: (8) /api/v1/namespaces/proxy-5452/services/http:proxy-service-5q68j:portname2/proxy/: bar (200; 12.890151ms)
Dec  4 20:40:29.123: INFO: (8) /api/v1/namespaces/proxy-5452/services/https:proxy-service-5q68j:tlsportname2/proxy/: tls qux (200; 8.07183ms)
Dec  4 20:40:29.123: INFO: (8) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:1080/proxy/rewriteme">... (200; 7.182889ms)
Dec  4 20:40:29.124: INFO: (8) /api/v1/namespaces/proxy-5452/services/proxy-service-5q68j:portname2/proxy/: bar (200; 10.194941ms)
Dec  4 20:40:29.125: INFO: (8) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:162/proxy/: bar (200; 5.947143ms)
Dec  4 20:40:29.125: INFO: (8) /api/v1/namespaces/proxy-5452/services/https:proxy-service-5q68j:tlsportname1/proxy/: tls baz (200; 7.30852ms)
Dec  4 20:40:29.125: INFO: (8) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk/proxy/rewriteme">test</a> (200; 6.101695ms)
Dec  4 20:40:29.127: INFO: (8) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:160/proxy/: foo (200; 7.812093ms)
Dec  4 20:40:29.127: INFO: (8) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:460/proxy/: tls baz (200; 7.90676ms)
Dec  4 20:40:29.127: INFO: (8) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:443/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:443/proxy/tlsrewritem... (200; 8.473052ms)
Dec  4 20:40:29.128: INFO: (8) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:162/proxy/: bar (200; 8.802467ms)
Dec  4 20:40:29.132: INFO: (9) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:160/proxy/: foo (200; 3.355159ms)
Dec  4 20:40:29.138: INFO: (9) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:162/proxy/: bar (200; 8.411888ms)
Dec  4 20:40:29.138: INFO: (9) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:462/proxy/: tls qux (200; 8.35207ms)
Dec  4 20:40:29.138: INFO: (9) /api/v1/namespaces/proxy-5452/services/proxy-service-5q68j:portname1/proxy/: foo (200; 7.578128ms)
Dec  4 20:40:29.138: INFO: (9) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk/proxy/rewriteme">test</a> (200; 8.786755ms)
Dec  4 20:40:29.140: INFO: (9) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:162/proxy/: bar (200; 9.306688ms)
Dec  4 20:40:29.141: INFO: (9) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:443/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:443/proxy/tlsrewritem... (200; 12.18116ms)
Dec  4 20:40:29.141: INFO: (9) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:1080/proxy/rewriteme">... (200; 10.449881ms)
Dec  4 20:40:29.141: INFO: (9) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:460/proxy/: tls baz (200; 11.839564ms)
Dec  4 20:40:29.142: INFO: (9) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:160/proxy/: foo (200; 10.829906ms)
Dec  4 20:40:29.142: INFO: (9) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:1080/proxy/rewriteme">test<... (200; 12.445949ms)
Dec  4 20:40:29.146: INFO: (9) /api/v1/namespaces/proxy-5452/services/proxy-service-5q68j:portname2/proxy/: bar (200; 15.065206ms)
Dec  4 20:40:29.146: INFO: (9) /api/v1/namespaces/proxy-5452/services/https:proxy-service-5q68j:tlsportname2/proxy/: tls qux (200; 15.002659ms)
Dec  4 20:40:29.146: INFO: (9) /api/v1/namespaces/proxy-5452/services/http:proxy-service-5q68j:portname2/proxy/: bar (200; 16.00126ms)
Dec  4 20:40:29.146: INFO: (9) /api/v1/namespaces/proxy-5452/services/https:proxy-service-5q68j:tlsportname1/proxy/: tls baz (200; 15.077735ms)
Dec  4 20:40:29.147: INFO: (9) /api/v1/namespaces/proxy-5452/services/http:proxy-service-5q68j:portname1/proxy/: foo (200; 16.981551ms)
Dec  4 20:40:29.150: INFO: (10) /api/v1/namespaces/proxy-5452/services/https:proxy-service-5q68j:tlsportname1/proxy/: tls baz (200; 3.29527ms)
Dec  4 20:40:29.154: INFO: (10) /api/v1/namespaces/proxy-5452/services/proxy-service-5q68j:portname2/proxy/: bar (200; 6.720076ms)
Dec  4 20:40:29.160: INFO: (10) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:1080/proxy/rewriteme">test<... (200; 10.443703ms)
Dec  4 20:40:29.160: INFO: (10) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:162/proxy/: bar (200; 10.52059ms)
Dec  4 20:40:29.161: INFO: (10) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:162/proxy/: bar (200; 13.035291ms)
Dec  4 20:40:29.161: INFO: (10) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:443/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:443/proxy/tlsrewritem... (200; 11.468068ms)
Dec  4 20:40:29.161: INFO: (10) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:160/proxy/: foo (200; 11.650009ms)
Dec  4 20:40:29.161: INFO: (10) /api/v1/namespaces/proxy-5452/services/http:proxy-service-5q68j:portname1/proxy/: foo (200; 14.352495ms)
Dec  4 20:40:29.161: INFO: (10) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:462/proxy/: tls qux (200; 11.936983ms)
Dec  4 20:40:29.161: INFO: (10) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:460/proxy/: tls baz (200; 13.630714ms)
Dec  4 20:40:29.161: INFO: (10) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:160/proxy/: foo (200; 11.700527ms)
Dec  4 20:40:29.161: INFO: (10) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk/proxy/rewriteme">test</a> (200; 11.893832ms)
Dec  4 20:40:29.161: INFO: (10) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:1080/proxy/rewriteme">... (200; 11.792016ms)
Dec  4 20:40:29.162: INFO: (10) /api/v1/namespaces/proxy-5452/services/http:proxy-service-5q68j:portname2/proxy/: bar (200; 11.954631ms)
Dec  4 20:40:29.164: INFO: (10) /api/v1/namespaces/proxy-5452/services/proxy-service-5q68j:portname1/proxy/: foo (200; 13.969587ms)
Dec  4 20:40:29.164: INFO: (10) /api/v1/namespaces/proxy-5452/services/https:proxy-service-5q68j:tlsportname2/proxy/: tls qux (200; 14.873139ms)
Dec  4 20:40:29.182: INFO: (11) /api/v1/namespaces/proxy-5452/services/proxy-service-5q68j:portname1/proxy/: foo (200; 16.757525ms)
Dec  4 20:40:29.182: INFO: (11) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:162/proxy/: bar (200; 16.591901ms)
Dec  4 20:40:29.182: INFO: (11) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:1080/proxy/rewriteme">... (200; 17.316756ms)
Dec  4 20:40:29.182: INFO: (11) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:160/proxy/: foo (200; 17.161482ms)
Dec  4 20:40:29.182: INFO: (11) /api/v1/namespaces/proxy-5452/services/https:proxy-service-5q68j:tlsportname2/proxy/: tls qux (200; 16.791973ms)
Dec  4 20:40:29.182: INFO: (11) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:460/proxy/: tls baz (200; 16.506089ms)
Dec  4 20:40:29.182: INFO: (11) /api/v1/namespaces/proxy-5452/services/https:proxy-service-5q68j:tlsportname1/proxy/: tls baz (200; 17.279318ms)
Dec  4 20:40:29.182: INFO: (11) /api/v1/namespaces/proxy-5452/services/http:proxy-service-5q68j:portname1/proxy/: foo (200; 17.234056ms)
Dec  4 20:40:29.182: INFO: (11) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk/proxy/rewriteme">test</a> (200; 17.070137ms)
Dec  4 20:40:29.182: INFO: (11) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:443/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:443/proxy/tlsrewritem... (200; 17.042837ms)
Dec  4 20:40:29.182: INFO: (11) /api/v1/namespaces/proxy-5452/services/proxy-service-5q68j:portname2/proxy/: bar (200; 17.286303ms)
Dec  4 20:40:29.184: INFO: (11) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:160/proxy/: foo (200; 18.824805ms)
Dec  4 20:40:29.184: INFO: (11) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:462/proxy/: tls qux (200; 19.339972ms)
Dec  4 20:40:29.184: INFO: (11) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:162/proxy/: bar (200; 18.738147ms)
Dec  4 20:40:29.184: INFO: (11) /api/v1/namespaces/proxy-5452/services/http:proxy-service-5q68j:portname2/proxy/: bar (200; 19.335752ms)
Dec  4 20:40:29.184: INFO: (11) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:1080/proxy/rewriteme">test<... (200; 19.209906ms)
Dec  4 20:40:29.196: INFO: (12) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:160/proxy/: foo (200; 11.510734ms)
Dec  4 20:40:29.197: INFO: (12) /api/v1/namespaces/proxy-5452/services/https:proxy-service-5q68j:tlsportname1/proxy/: tls baz (200; 12.300727ms)
Dec  4 20:40:29.197: INFO: (12) /api/v1/namespaces/proxy-5452/services/proxy-service-5q68j:portname1/proxy/: foo (200; 12.39785ms)
Dec  4 20:40:29.197: INFO: (12) /api/v1/namespaces/proxy-5452/services/https:proxy-service-5q68j:tlsportname2/proxy/: tls qux (200; 12.687526ms)
Dec  4 20:40:29.197: INFO: (12) /api/v1/namespaces/proxy-5452/services/proxy-service-5q68j:portname2/proxy/: bar (200; 12.845025ms)
Dec  4 20:40:29.198: INFO: (12) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:162/proxy/: bar (200; 13.014633ms)
Dec  4 20:40:29.198: INFO: (12) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:1080/proxy/rewriteme">... (200; 13.155736ms)
Dec  4 20:40:29.204: INFO: (12) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:460/proxy/: tls baz (200; 18.982348ms)
Dec  4 20:40:29.204: INFO: (12) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:160/proxy/: foo (200; 19.272451ms)
Dec  4 20:40:29.204: INFO: (12) /api/v1/namespaces/proxy-5452/services/http:proxy-service-5q68j:portname1/proxy/: foo (200; 19.180179ms)
Dec  4 20:40:29.204: INFO: (12) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:462/proxy/: tls qux (200; 19.1205ms)
Dec  4 20:40:29.204: INFO: (12) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk/proxy/rewriteme">test</a> (200; 19.245519ms)
Dec  4 20:40:29.204: INFO: (12) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:1080/proxy/rewriteme">test<... (200; 19.084017ms)
Dec  4 20:40:29.204: INFO: (12) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:162/proxy/: bar (200; 19.367449ms)
Dec  4 20:40:29.204: INFO: (12) /api/v1/namespaces/proxy-5452/services/http:proxy-service-5q68j:portname2/proxy/: bar (200; 19.192008ms)
Dec  4 20:40:29.204: INFO: (12) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:443/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:443/proxy/tlsrewritem... (200; 19.373508ms)
Dec  4 20:40:29.220: INFO: (13) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:460/proxy/: tls baz (200; 14.086405ms)
Dec  4 20:40:29.220: INFO: (13) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:462/proxy/: tls qux (200; 15.156621ms)
Dec  4 20:40:29.220: INFO: (13) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:443/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:443/proxy/tlsrewritem... (200; 14.586023ms)
Dec  4 20:40:29.220: INFO: (13) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk/proxy/rewriteme">test</a> (200; 14.473876ms)
Dec  4 20:40:29.220: INFO: (13) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:162/proxy/: bar (200; 14.464377ms)
Dec  4 20:40:29.220: INFO: (13) /api/v1/namespaces/proxy-5452/services/http:proxy-service-5q68j:portname1/proxy/: foo (200; 15.126882ms)
Dec  4 20:40:29.220: INFO: (13) /api/v1/namespaces/proxy-5452/services/http:proxy-service-5q68j:portname2/proxy/: bar (200; 14.902249ms)
Dec  4 20:40:29.220: INFO: (13) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:160/proxy/: foo (200; 14.406056ms)
Dec  4 20:40:29.220: INFO: (13) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:1080/proxy/rewriteme">test<... (200; 14.645533ms)
Dec  4 20:40:29.220: INFO: (13) /api/v1/namespaces/proxy-5452/services/https:proxy-service-5q68j:tlsportname2/proxy/: tls qux (200; 14.13469ms)
Dec  4 20:40:29.220: INFO: (13) /api/v1/namespaces/proxy-5452/services/https:proxy-service-5q68j:tlsportname1/proxy/: tls baz (200; 15.066535ms)
Dec  4 20:40:29.220: INFO: (13) /api/v1/namespaces/proxy-5452/services/proxy-service-5q68j:portname2/proxy/: bar (200; 14.219843ms)
Dec  4 20:40:29.220: INFO: (13) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:160/proxy/: foo (200; 14.918628ms)
Dec  4 20:40:29.220: INFO: (13) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:1080/proxy/rewriteme">... (200; 15.302012ms)
Dec  4 20:40:29.220: INFO: (13) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:162/proxy/: bar (200; 14.868916ms)
Dec  4 20:40:29.220: INFO: (13) /api/v1/namespaces/proxy-5452/services/proxy-service-5q68j:portname1/proxy/: foo (200; 14.309282ms)
Dec  4 20:40:29.225: INFO: (14) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:160/proxy/: foo (200; 2.993841ms)
Dec  4 20:40:29.225: INFO: (14) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:443/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:443/proxy/tlsrewritem... (200; 3.16376ms)
Dec  4 20:40:29.233: INFO: (14) /api/v1/namespaces/proxy-5452/services/proxy-service-5q68j:portname1/proxy/: foo (200; 8.821722ms)
Dec  4 20:40:29.233: INFO: (14) /api/v1/namespaces/proxy-5452/services/https:proxy-service-5q68j:tlsportname1/proxy/: tls baz (200; 11.076118ms)
Dec  4 20:40:29.233: INFO: (14) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:1080/proxy/rewriteme">... (200; 10.97205ms)
Dec  4 20:40:29.233: INFO: (14) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:162/proxy/: bar (200; 8.793769ms)
Dec  4 20:40:29.233: INFO: (14) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk/proxy/rewriteme">test</a> (200; 8.947901ms)
Dec  4 20:40:29.233: INFO: (14) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:162/proxy/: bar (200; 9.07249ms)
Dec  4 20:40:29.233: INFO: (14) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:462/proxy/: tls qux (200; 9.105227ms)
Dec  4 20:40:29.233: INFO: (14) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:460/proxy/: tls baz (200; 9.014556ms)
Dec  4 20:40:29.233: INFO: (14) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:160/proxy/: foo (200; 9.101324ms)
Dec  4 20:40:29.233: INFO: (14) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:1080/proxy/rewriteme">test<... (200; 11.62409ms)
Dec  4 20:40:29.233: INFO: (14) /api/v1/namespaces/proxy-5452/services/http:proxy-service-5q68j:portname1/proxy/: foo (200; 9.169618ms)
Dec  4 20:40:29.235: INFO: (14) /api/v1/namespaces/proxy-5452/services/https:proxy-service-5q68j:tlsportname2/proxy/: tls qux (200; 11.099068ms)
Dec  4 20:40:29.235: INFO: (14) /api/v1/namespaces/proxy-5452/services/proxy-service-5q68j:portname2/proxy/: bar (200; 11.143254ms)
Dec  4 20:40:29.235: INFO: (14) /api/v1/namespaces/proxy-5452/services/http:proxy-service-5q68j:portname2/proxy/: bar (200; 11.305393ms)
Dec  4 20:40:29.242: INFO: (15) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:1080/proxy/rewriteme">... (200; 6.598776ms)
Dec  4 20:40:29.250: INFO: (15) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:443/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:443/proxy/tlsrewritem... (200; 13.185938ms)
Dec  4 20:40:29.250: INFO: (15) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:160/proxy/: foo (200; 14.071759ms)
Dec  4 20:40:29.250: INFO: (15) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:160/proxy/: foo (200; 13.211424ms)
Dec  4 20:40:29.253: INFO: (15) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:162/proxy/: bar (200; 16.675946ms)
Dec  4 20:40:29.253: INFO: (15) /api/v1/namespaces/proxy-5452/services/proxy-service-5q68j:portname2/proxy/: bar (200; 16.552035ms)
Dec  4 20:40:29.253: INFO: (15) /api/v1/namespaces/proxy-5452/services/https:proxy-service-5q68j:tlsportname2/proxy/: tls qux (200; 16.567219ms)
Dec  4 20:40:29.253: INFO: (15) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk/proxy/rewriteme">test</a> (200; 16.645774ms)
Dec  4 20:40:29.253: INFO: (15) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:462/proxy/: tls qux (200; 17.123875ms)
Dec  4 20:40:29.254: INFO: (15) /api/v1/namespaces/proxy-5452/services/https:proxy-service-5q68j:tlsportname1/proxy/: tls baz (200; 17.999064ms)
Dec  4 20:40:29.254: INFO: (15) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:1080/proxy/rewriteme">test<... (200; 17.30212ms)
Dec  4 20:40:29.254: INFO: (15) /api/v1/namespaces/proxy-5452/services/proxy-service-5q68j:portname1/proxy/: foo (200; 17.267084ms)
Dec  4 20:40:29.254: INFO: (15) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:162/proxy/: bar (200; 17.245427ms)
Dec  4 20:40:29.254: INFO: (15) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:460/proxy/: tls baz (200; 18.327708ms)
Dec  4 20:40:29.255: INFO: (15) /api/v1/namespaces/proxy-5452/services/http:proxy-service-5q68j:portname1/proxy/: foo (200; 18.759449ms)
Dec  4 20:40:29.255: INFO: (15) /api/v1/namespaces/proxy-5452/services/http:proxy-service-5q68j:portname2/proxy/: bar (200; 19.056362ms)
Dec  4 20:40:29.263: INFO: (16) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk/proxy/rewriteme">test</a> (200; 7.053242ms)
Dec  4 20:40:29.263: INFO: (16) /api/v1/namespaces/proxy-5452/services/https:proxy-service-5q68j:tlsportname2/proxy/: tls qux (200; 7.221162ms)
Dec  4 20:40:29.264: INFO: (16) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:443/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:443/proxy/tlsrewritem... (200; 8.296599ms)
Dec  4 20:40:29.274: INFO: (16) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:162/proxy/: bar (200; 17.845825ms)
Dec  4 20:40:29.274: INFO: (16) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:1080/proxy/rewriteme">... (200; 18.018231ms)
Dec  4 20:40:29.274: INFO: (16) /api/v1/namespaces/proxy-5452/services/https:proxy-service-5q68j:tlsportname1/proxy/: tls baz (200; 18.550498ms)
Dec  4 20:40:29.275: INFO: (16) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:1080/proxy/rewriteme">test<... (200; 18.459121ms)
Dec  4 20:40:29.275: INFO: (16) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:460/proxy/: tls baz (200; 18.679216ms)
Dec  4 20:40:29.275: INFO: (16) /api/v1/namespaces/proxy-5452/services/proxy-service-5q68j:portname2/proxy/: bar (200; 18.67946ms)
Dec  4 20:40:29.275: INFO: (16) /api/v1/namespaces/proxy-5452/services/proxy-service-5q68j:portname1/proxy/: foo (200; 18.866212ms)
Dec  4 20:40:29.275: INFO: (16) /api/v1/namespaces/proxy-5452/services/http:proxy-service-5q68j:portname1/proxy/: foo (200; 19.095704ms)
Dec  4 20:40:29.275: INFO: (16) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:160/proxy/: foo (200; 19.159ms)
Dec  4 20:40:29.275: INFO: (16) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:160/proxy/: foo (200; 19.339429ms)
Dec  4 20:40:29.275: INFO: (16) /api/v1/namespaces/proxy-5452/services/http:proxy-service-5q68j:portname2/proxy/: bar (200; 19.315601ms)
Dec  4 20:40:29.275: INFO: (16) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:162/proxy/: bar (200; 19.385373ms)
Dec  4 20:40:29.275: INFO: (16) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:462/proxy/: tls qux (200; 19.373089ms)
Dec  4 20:40:29.282: INFO: (17) /api/v1/namespaces/proxy-5452/services/http:proxy-service-5q68j:portname1/proxy/: foo (200; 5.471953ms)
Dec  4 20:40:29.283: INFO: (17) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:1080/proxy/rewriteme">... (200; 6.894537ms)
Dec  4 20:40:29.283: INFO: (17) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:160/proxy/: foo (200; 6.98545ms)
Dec  4 20:40:29.283: INFO: (17) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:1080/proxy/rewriteme">test<... (200; 7.029633ms)
Dec  4 20:40:29.288: INFO: (17) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:443/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:443/proxy/tlsrewritem... (200; 11.593006ms)
Dec  4 20:40:29.289: INFO: (17) /api/v1/namespaces/proxy-5452/services/http:proxy-service-5q68j:portname2/proxy/: bar (200; 12.412145ms)
Dec  4 20:40:29.295: INFO: (17) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:162/proxy/: bar (200; 18.348096ms)
Dec  4 20:40:29.295: INFO: (17) /api/v1/namespaces/proxy-5452/services/https:proxy-service-5q68j:tlsportname2/proxy/: tls qux (200; 18.685682ms)
Dec  4 20:40:29.295: INFO: (17) /api/v1/namespaces/proxy-5452/services/proxy-service-5q68j:portname2/proxy/: bar (200; 18.766435ms)
Dec  4 20:40:29.295: INFO: (17) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk/proxy/rewriteme">test</a> (200; 18.763709ms)
Dec  4 20:40:29.295: INFO: (17) /api/v1/namespaces/proxy-5452/services/https:proxy-service-5q68j:tlsportname1/proxy/: tls baz (200; 18.859504ms)
Dec  4 20:40:29.295: INFO: (17) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:460/proxy/: tls baz (200; 18.799493ms)
Dec  4 20:40:29.295: INFO: (17) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:160/proxy/: foo (200; 19.072343ms)
Dec  4 20:40:29.296: INFO: (17) /api/v1/namespaces/proxy-5452/services/proxy-service-5q68j:portname1/proxy/: foo (200; 19.250949ms)
Dec  4 20:40:29.296: INFO: (17) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:462/proxy/: tls qux (200; 19.727332ms)
Dec  4 20:40:29.296: INFO: (17) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:162/proxy/: bar (200; 19.256393ms)
Dec  4 20:40:29.302: INFO: (18) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk/proxy/rewriteme">test</a> (200; 6.528173ms)
Dec  4 20:40:29.303: INFO: (18) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:160/proxy/: foo (200; 6.777256ms)
Dec  4 20:40:29.303: INFO: (18) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:162/proxy/: bar (200; 5.696907ms)
Dec  4 20:40:29.307: INFO: (18) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:1080/proxy/rewriteme">... (200; 9.06568ms)
Dec  4 20:40:29.307: INFO: (18) /api/v1/namespaces/proxy-5452/services/proxy-service-5q68j:portname1/proxy/: foo (200; 10.159375ms)
Dec  4 20:40:29.308: INFO: (18) /api/v1/namespaces/proxy-5452/services/https:proxy-service-5q68j:tlsportname1/proxy/: tls baz (200; 10.142497ms)
Dec  4 20:40:29.310: INFO: (18) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:162/proxy/: bar (200; 12.126571ms)
Dec  4 20:40:29.310: INFO: (18) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:1080/proxy/rewriteme">test<... (200; 13.235127ms)
Dec  4 20:40:29.311: INFO: (18) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:443/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:443/proxy/tlsrewritem... (200; 13.467885ms)
Dec  4 20:40:29.311: INFO: (18) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:160/proxy/: foo (200; 12.613606ms)
Dec  4 20:40:29.311: INFO: (18) /api/v1/namespaces/proxy-5452/services/proxy-service-5q68j:portname2/proxy/: bar (200; 12.90242ms)
Dec  4 20:40:29.311: INFO: (18) /api/v1/namespaces/proxy-5452/services/http:proxy-service-5q68j:portname2/proxy/: bar (200; 13.788816ms)
Dec  4 20:40:29.311: INFO: (18) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:460/proxy/: tls baz (200; 14.080538ms)
Dec  4 20:40:29.311: INFO: (18) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:462/proxy/: tls qux (200; 14.052329ms)
Dec  4 20:40:29.311: INFO: (18) /api/v1/namespaces/proxy-5452/services/http:proxy-service-5q68j:portname1/proxy/: foo (200; 14.045458ms)
Dec  4 20:40:29.312: INFO: (18) /api/v1/namespaces/proxy-5452/services/https:proxy-service-5q68j:tlsportname2/proxy/: tls qux (200; 13.831125ms)
Dec  4 20:40:29.327: INFO: (19) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:162/proxy/: bar (200; 14.608053ms)
Dec  4 20:40:29.328: INFO: (19) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:1080/proxy/rewriteme">test<... (200; 15.329566ms)
Dec  4 20:40:29.330: INFO: (19) /api/v1/namespaces/proxy-5452/services/http:proxy-service-5q68j:portname1/proxy/: foo (200; 17.09707ms)
Dec  4 20:40:29.330: INFO: (19) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:160/proxy/: foo (200; 17.003022ms)
Dec  4 20:40:29.330: INFO: (19) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:462/proxy/: tls qux (200; 17.429101ms)
Dec  4 20:40:29.330: INFO: (19) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk/proxy/rewriteme">test</a> (200; 17.404392ms)
Dec  4 20:40:29.330: INFO: (19) /api/v1/namespaces/proxy-5452/services/http:proxy-service-5q68j:portname2/proxy/: bar (200; 17.252961ms)
Dec  4 20:40:29.330: INFO: (19) /api/v1/namespaces/proxy-5452/services/https:proxy-service-5q68j:tlsportname2/proxy/: tls qux (200; 17.263293ms)
Dec  4 20:40:29.330: INFO: (19) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:1080/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:1080/proxy/rewriteme">... (200; 17.237022ms)
Dec  4 20:40:29.330: INFO: (19) /api/v1/namespaces/proxy-5452/services/proxy-service-5q68j:portname1/proxy/: foo (200; 17.227408ms)
Dec  4 20:40:29.330: INFO: (19) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:443/proxy/: <a href="/api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:443/proxy/tlsrewritem... (200; 17.136346ms)
Dec  4 20:40:29.330: INFO: (19) /api/v1/namespaces/proxy-5452/services/proxy-service-5q68j:portname2/proxy/: bar (200; 17.217433ms)
Dec  4 20:40:29.330: INFO: (19) /api/v1/namespaces/proxy-5452/pods/proxy-service-5q68j-hgtpk:162/proxy/: bar (200; 17.605545ms)
Dec  4 20:40:29.330: INFO: (19) /api/v1/namespaces/proxy-5452/pods/https:proxy-service-5q68j-hgtpk:460/proxy/: tls baz (200; 17.619887ms)
Dec  4 20:40:29.330: INFO: (19) /api/v1/namespaces/proxy-5452/pods/http:proxy-service-5q68j-hgtpk:160/proxy/: foo (200; 17.612223ms)
Dec  4 20:40:29.330: INFO: (19) /api/v1/namespaces/proxy-5452/services/https:proxy-service-5q68j:tlsportname1/proxy/: tls baz (200; 17.394885ms)
STEP: deleting ReplicationController proxy-service-5q68j in namespace proxy-5452, will wait for the garbage collector to delete the pods
Dec  4 20:40:29.465: INFO: Deleting ReplicationController proxy-service-5q68j took: 27.873965ms
Dec  4 20:40:29.768: INFO: Terminating ReplicationController proxy-service-5q68j pods took: 302.989597ms
[AfterEach] version v1
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:40:32.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5452" for this suite.
Dec  4 20:40:38.194: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:40:38.283: INFO: namespace proxy-5452 deletion completed in 6.104059294s

• [SLOW TEST:17.525 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:40:38.288: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-8791
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec  4 20:40:38.501: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 20:40:38.523: INFO: Number of nodes with available pods: 0
Dec  4 20:40:38.523: INFO: Node alex-slot1-v2-vsp1-worker6c7da5d4d5 is running more than one daemon pod
Dec  4 20:40:39.526: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 20:40:39.528: INFO: Number of nodes with available pods: 0
Dec  4 20:40:39.528: INFO: Node alex-slot1-v2-vsp1-worker6c7da5d4d5 is running more than one daemon pod
Dec  4 20:40:40.527: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 20:40:40.530: INFO: Number of nodes with available pods: 1
Dec  4 20:40:40.530: INFO: Node alex-slot1-v2-vsp1-worker6c7da5d4d5 is running more than one daemon pod
Dec  4 20:40:41.527: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 20:40:41.530: INFO: Number of nodes with available pods: 1
Dec  4 20:40:41.530: INFO: Node alex-slot1-v2-vsp1-worker6c7da5d4d5 is running more than one daemon pod
Dec  4 20:40:42.529: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 20:40:42.533: INFO: Number of nodes with available pods: 2
Dec  4 20:40:42.533: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Dec  4 20:40:42.551: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 20:40:42.553: INFO: Number of nodes with available pods: 1
Dec  4 20:40:42.553: INFO: Node alex-slot1-v2-vsp1-worker9f6c17cd60 is running more than one daemon pod
Dec  4 20:40:43.562: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 20:40:43.564: INFO: Number of nodes with available pods: 1
Dec  4 20:40:43.564: INFO: Node alex-slot1-v2-vsp1-worker9f6c17cd60 is running more than one daemon pod
Dec  4 20:40:44.558: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 20:40:44.562: INFO: Number of nodes with available pods: 1
Dec  4 20:40:44.562: INFO: Node alex-slot1-v2-vsp1-worker9f6c17cd60 is running more than one daemon pod
Dec  4 20:40:45.557: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 20:40:45.560: INFO: Number of nodes with available pods: 1
Dec  4 20:40:45.560: INFO: Node alex-slot1-v2-vsp1-worker9f6c17cd60 is running more than one daemon pod
Dec  4 20:40:46.557: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 20:40:46.560: INFO: Number of nodes with available pods: 1
Dec  4 20:40:46.560: INFO: Node alex-slot1-v2-vsp1-worker9f6c17cd60 is running more than one daemon pod
Dec  4 20:40:47.558: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 20:40:47.560: INFO: Number of nodes with available pods: 2
Dec  4 20:40:47.560: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8791, will wait for the garbage collector to delete the pods
Dec  4 20:40:47.626: INFO: Deleting DaemonSet.extensions daemon-set took: 11.472459ms
Dec  4 20:40:48.029: INFO: Terminating DaemonSet.extensions daemon-set pods took: 403.734304ms
Dec  4 20:40:59.834: INFO: Number of nodes with available pods: 0
Dec  4 20:40:59.834: INFO: Number of running nodes: 0, number of available pods: 0
Dec  4 20:40:59.838: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8791/daemonsets","resourceVersion":"277683"},"items":null}

Dec  4 20:40:59.846: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8791/pods","resourceVersion":"277683"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:40:59.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8791" for this suite.
Dec  4 20:41:05.870: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:41:05.945: INFO: namespace daemonsets-8791 deletion completed in 6.082390105s

• [SLOW TEST:27.657 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:41:05.945: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-4214
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Dec  4 20:41:16.125: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4214 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  4 20:41:16.125: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
Dec  4 20:41:16.292: INFO: Exec stderr: ""
Dec  4 20:41:16.293: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4214 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  4 20:41:16.293: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
Dec  4 20:41:16.431: INFO: Exec stderr: ""
Dec  4 20:41:16.431: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4214 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  4 20:41:16.431: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
Dec  4 20:41:16.574: INFO: Exec stderr: ""
Dec  4 20:41:16.574: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4214 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  4 20:41:16.574: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
Dec  4 20:41:16.757: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Dec  4 20:41:16.757: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4214 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  4 20:41:16.757: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
Dec  4 20:41:16.891: INFO: Exec stderr: ""
Dec  4 20:41:16.891: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4214 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  4 20:41:16.891: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
Dec  4 20:41:17.033: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Dec  4 20:41:17.033: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4214 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  4 20:41:17.033: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
Dec  4 20:41:17.118: INFO: Exec stderr: ""
Dec  4 20:41:17.118: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4214 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  4 20:41:17.118: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
Dec  4 20:41:17.198: INFO: Exec stderr: ""
Dec  4 20:41:17.199: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-4214 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  4 20:41:17.199: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
Dec  4 20:41:17.287: INFO: Exec stderr: ""
Dec  4 20:41:17.288: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-4214 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  4 20:41:17.288: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
Dec  4 20:41:17.376: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:41:17.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-4214" for this suite.
Dec  4 20:41:55.390: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:41:55.450: INFO: namespace e2e-kubelet-etc-hosts-4214 deletion completed in 38.069905939s

• [SLOW TEST:49.505 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:41:55.453: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3299
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-827e788d-16d6-11ea-8695-527d496f91de
STEP: Creating a pod to test consume configMaps
Dec  4 20:41:55.646: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-827ef945-16d6-11ea-8695-527d496f91de" in namespace "projected-3299" to be "success or failure"
Dec  4 20:41:55.651: INFO: Pod "pod-projected-configmaps-827ef945-16d6-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 5.596259ms
Dec  4 20:41:57.654: INFO: Pod "pod-projected-configmaps-827ef945-16d6-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008222224s
STEP: Saw pod success
Dec  4 20:41:57.654: INFO: Pod "pod-projected-configmaps-827ef945-16d6-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 20:41:57.656: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker9f6c17cd60 pod pod-projected-configmaps-827ef945-16d6-11ea-8695-527d496f91de container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  4 20:41:57.681: INFO: Waiting for pod pod-projected-configmaps-827ef945-16d6-11ea-8695-527d496f91de to disappear
Dec  4 20:41:57.683: INFO: Pod pod-projected-configmaps-827ef945-16d6-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:41:57.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3299" for this suite.
Dec  4 20:42:03.698: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:42:03.780: INFO: namespace projected-3299 deletion completed in 6.092397625s

• [SLOW TEST:8.327 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:42:03.782: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2463
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with configMap that has name projected-configmap-test-upd-876e3142-16d6-11ea-8695-527d496f91de
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-876e3142-16d6-11ea-8695-527d496f91de
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:42:07.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2463" for this suite.
Dec  4 20:42:29.982: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:42:30.065: INFO: namespace projected-2463 deletion completed in 22.092687611s

• [SLOW TEST:26.284 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:42:30.067: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9303
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating secret secrets-9303/secret-test-97184eb5-16d6-11ea-8695-527d496f91de
STEP: Creating a pod to test consume secrets
Dec  4 20:42:30.210: INFO: Waiting up to 5m0s for pod "pod-configmaps-9718ea33-16d6-11ea-8695-527d496f91de" in namespace "secrets-9303" to be "success or failure"
Dec  4 20:42:30.217: INFO: Pod "pod-configmaps-9718ea33-16d6-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 7.269827ms
Dec  4 20:42:32.220: INFO: Pod "pod-configmaps-9718ea33-16d6-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00983637s
STEP: Saw pod success
Dec  4 20:42:32.220: INFO: Pod "pod-configmaps-9718ea33-16d6-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 20:42:32.223: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker6c7da5d4d5 pod pod-configmaps-9718ea33-16d6-11ea-8695-527d496f91de container env-test: <nil>
STEP: delete the pod
Dec  4 20:42:32.254: INFO: Waiting for pod pod-configmaps-9718ea33-16d6-11ea-8695-527d496f91de to disappear
Dec  4 20:42:32.261: INFO: Pod pod-configmaps-9718ea33-16d6-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:42:32.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9303" for this suite.
Dec  4 20:42:38.303: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:42:38.370: INFO: namespace secrets-9303 deletion completed in 6.101484536s

• [SLOW TEST:8.303 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:42:38.371: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2526
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1576
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  4 20:42:38.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-2526'
Dec  4 20:42:38.620: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec  4 20:42:38.620: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1581
Dec  4 20:42:38.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 delete jobs e2e-test-nginx-job --namespace=kubectl-2526'
Dec  4 20:42:38.802: INFO: stderr: ""
Dec  4 20:42:38.802: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:42:38.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2526" for this suite.
Dec  4 20:42:44.822: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:42:44.901: INFO: namespace kubectl-2526 deletion completed in 6.090681772s

• [SLOW TEST:6.531 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:42:44.903: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-1650
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec  4 20:42:51.117: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  4 20:42:51.120: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  4 20:42:53.120: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  4 20:42:53.123: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  4 20:42:55.120: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  4 20:42:55.123: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  4 20:42:57.120: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  4 20:42:57.123: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  4 20:42:59.120: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  4 20:42:59.123: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  4 20:43:01.120: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  4 20:43:01.123: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  4 20:43:03.120: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  4 20:43:03.123: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  4 20:43:05.120: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  4 20:43:05.124: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  4 20:43:07.120: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  4 20:43:07.123: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  4 20:43:09.120: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  4 20:43:09.123: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  4 20:43:11.120: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  4 20:43:11.124: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  4 20:43:13.120: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  4 20:43:13.123: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  4 20:43:15.120: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  4 20:43:15.123: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  4 20:43:17.121: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  4 20:43:17.123: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  4 20:43:19.122: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  4 20:43:19.125: INFO: Pod pod-with-prestop-exec-hook still exists
Dec  4 20:43:21.120: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec  4 20:43:21.123: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:43:21.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1650" for this suite.
Dec  4 20:43:43.145: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:43:43.217: INFO: namespace container-lifecycle-hook-1650 deletion completed in 22.081663143s

• [SLOW TEST:58.314 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:43:43.220: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9183
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Dec  4 20:43:45.886: INFO: Successfully updated pod "labelsupdatec2b25d52-16d6-11ea-8695-527d496f91de"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:43:47.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9183" for this suite.
Dec  4 20:44:09.939: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:44:10.016: INFO: namespace projected-9183 deletion completed in 22.085833158s

• [SLOW TEST:26.797 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:44:10.018: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8140
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Dec  4 20:44:14.691: INFO: Successfully updated pod "annotationupdated2ac0a3f-16d6-11ea-8695-527d496f91de"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:44:16.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8140" for this suite.
Dec  4 20:44:38.717: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:44:38.821: INFO: namespace projected-8140 deletion completed in 22.11563077s

• [SLOW TEST:28.803 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:44:38.822: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9268
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating api versions
Dec  4 20:44:38.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 api-versions'
Dec  4 20:44:39.083: INFO: stderr: ""
Dec  4 20:44:39.083: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncertmanager.k8s.io/v1alpha1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:44:39.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9268" for this suite.
Dec  4 20:44:45.097: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:44:45.174: INFO: namespace kubectl-9268 deletion completed in 6.086972481s

• [SLOW TEST:6.352 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:44:45.175: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6356
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:266
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Dec  4 20:44:45.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 create -f - --namespace=kubectl-6356'
Dec  4 20:44:45.809: INFO: stderr: ""
Dec  4 20:44:45.809: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  4 20:44:45.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6356'
Dec  4 20:44:45.908: INFO: stderr: ""
Dec  4 20:44:45.908: INFO: stdout: "update-demo-nautilus-6ddzw update-demo-nautilus-csbng "
Dec  4 20:44:45.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get pods update-demo-nautilus-6ddzw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6356'
Dec  4 20:44:46.006: INFO: stderr: ""
Dec  4 20:44:46.006: INFO: stdout: ""
Dec  4 20:44:46.006: INFO: update-demo-nautilus-6ddzw is created but not running
Dec  4 20:44:51.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6356'
Dec  4 20:44:51.103: INFO: stderr: ""
Dec  4 20:44:51.103: INFO: stdout: "update-demo-nautilus-6ddzw update-demo-nautilus-csbng "
Dec  4 20:44:51.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get pods update-demo-nautilus-6ddzw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6356'
Dec  4 20:44:51.194: INFO: stderr: ""
Dec  4 20:44:51.194: INFO: stdout: "true"
Dec  4 20:44:51.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get pods update-demo-nautilus-6ddzw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6356'
Dec  4 20:44:51.278: INFO: stderr: ""
Dec  4 20:44:51.278: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  4 20:44:51.278: INFO: validating pod update-demo-nautilus-6ddzw
Dec  4 20:44:51.281: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  4 20:44:51.281: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  4 20:44:51.281: INFO: update-demo-nautilus-6ddzw is verified up and running
Dec  4 20:44:51.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get pods update-demo-nautilus-csbng -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6356'
Dec  4 20:44:51.380: INFO: stderr: ""
Dec  4 20:44:51.380: INFO: stdout: "true"
Dec  4 20:44:51.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get pods update-demo-nautilus-csbng -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6356'
Dec  4 20:44:51.458: INFO: stderr: ""
Dec  4 20:44:51.458: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  4 20:44:51.458: INFO: validating pod update-demo-nautilus-csbng
Dec  4 20:44:51.461: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  4 20:44:51.461: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  4 20:44:51.461: INFO: update-demo-nautilus-csbng is verified up and running
STEP: using delete to clean up resources
Dec  4 20:44:51.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 delete --grace-period=0 --force -f - --namespace=kubectl-6356'
Dec  4 20:44:51.609: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  4 20:44:51.609: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec  4 20:44:51.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6356'
Dec  4 20:44:51.695: INFO: stderr: "No resources found.\n"
Dec  4 20:44:51.695: INFO: stdout: ""
Dec  4 20:44:51.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get pods -l name=update-demo --namespace=kubectl-6356 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  4 20:44:51.787: INFO: stderr: ""
Dec  4 20:44:51.787: INFO: stdout: "update-demo-nautilus-6ddzw\nupdate-demo-nautilus-csbng\n"
Dec  4 20:44:52.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6356'
Dec  4 20:44:52.378: INFO: stderr: "No resources found.\n"
Dec  4 20:44:52.378: INFO: stdout: ""
Dec  4 20:44:52.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get pods -l name=update-demo --namespace=kubectl-6356 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  4 20:44:52.459: INFO: stderr: ""
Dec  4 20:44:52.459: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:44:52.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6356" for this suite.
Dec  4 20:45:14.480: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:45:14.549: INFO: namespace kubectl-6356 deletion completed in 22.085464617s

• [SLOW TEST:29.374 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:45:14.552: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1085
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:45:16.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1085" for this suite.
Dec  4 20:46:06.738: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:46:06.803: INFO: namespace kubelet-test-1085 deletion completed in 50.072067748s

• [SLOW TEST:52.251 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command in a pod
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:46:06.807: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5936
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-18487e76-16d7-11ea-8695-527d496f91de
STEP: Creating a pod to test consume configMaps
Dec  4 20:46:06.949: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1848f508-16d7-11ea-8695-527d496f91de" in namespace "projected-5936" to be "success or failure"
Dec  4 20:46:06.954: INFO: Pod "pod-projected-configmaps-1848f508-16d7-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 4.450294ms
Dec  4 20:46:08.957: INFO: Pod "pod-projected-configmaps-1848f508-16d7-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00757562s
Dec  4 20:46:10.960: INFO: Pod "pod-projected-configmaps-1848f508-16d7-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010484966s
STEP: Saw pod success
Dec  4 20:46:10.960: INFO: Pod "pod-projected-configmaps-1848f508-16d7-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 20:46:10.962: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker9f6c17cd60 pod pod-projected-configmaps-1848f508-16d7-11ea-8695-527d496f91de container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  4 20:46:10.978: INFO: Waiting for pod pod-projected-configmaps-1848f508-16d7-11ea-8695-527d496f91de to disappear
Dec  4 20:46:10.980: INFO: Pod pod-projected-configmaps-1848f508-16d7-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:46:10.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5936" for this suite.
Dec  4 20:46:16.994: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:46:17.078: INFO: namespace projected-5936 deletion completed in 6.094213711s

• [SLOW TEST:10.272 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:46:17.078: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-983
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-1e696d65-16d7-11ea-8695-527d496f91de
STEP: Creating a pod to test consume secrets
Dec  4 20:46:17.233: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1e69f6bc-16d7-11ea-8695-527d496f91de" in namespace "projected-983" to be "success or failure"
Dec  4 20:46:17.238: INFO: Pod "pod-projected-secrets-1e69f6bc-16d7-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 4.167985ms
Dec  4 20:46:19.241: INFO: Pod "pod-projected-secrets-1e69f6bc-16d7-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006643381s
STEP: Saw pod success
Dec  4 20:46:19.241: INFO: Pod "pod-projected-secrets-1e69f6bc-16d7-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 20:46:19.242: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker9f6c17cd60 pod pod-projected-secrets-1e69f6bc-16d7-11ea-8695-527d496f91de container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  4 20:46:19.259: INFO: Waiting for pod pod-projected-secrets-1e69f6bc-16d7-11ea-8695-527d496f91de to disappear
Dec  4 20:46:19.261: INFO: Pod pod-projected-secrets-1e69f6bc-16d7-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:46:19.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-983" for this suite.
Dec  4 20:46:25.273: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:46:25.347: INFO: namespace projected-983 deletion completed in 6.082598837s

• [SLOW TEST:8.269 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:46:25.347: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1471
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec  4 20:46:25.488: INFO: Waiting up to 5m0s for pod "pod-2355bcd2-16d7-11ea-8695-527d496f91de" in namespace "emptydir-1471" to be "success or failure"
Dec  4 20:46:25.497: INFO: Pod "pod-2355bcd2-16d7-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 8.709604ms
Dec  4 20:46:27.501: INFO: Pod "pod-2355bcd2-16d7-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012271299s
STEP: Saw pod success
Dec  4 20:46:27.501: INFO: Pod "pod-2355bcd2-16d7-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 20:46:27.504: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker6c7da5d4d5 pod pod-2355bcd2-16d7-11ea-8695-527d496f91de container test-container: <nil>
STEP: delete the pod
Dec  4 20:46:27.526: INFO: Waiting for pod pod-2355bcd2-16d7-11ea-8695-527d496f91de to disappear
Dec  4 20:46:27.527: INFO: Pod pod-2355bcd2-16d7-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:46:27.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1471" for this suite.
Dec  4 20:46:33.539: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:46:33.607: INFO: namespace emptydir-1471 deletion completed in 6.076384636s

• [SLOW TEST:8.260 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:46:33.608: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7433
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap that has name configmap-test-emptyKey-28427350-16d7-11ea-8695-527d496f91de
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:46:33.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7433" for this suite.
Dec  4 20:46:39.776: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:46:39.868: INFO: namespace configmap-7433 deletion completed in 6.117178569s

• [SLOW TEST:6.261 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:46:39.874: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6859
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:163
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  4 20:46:44.061: INFO: Waiting up to 5m0s for pod "client-envvars-2e678cbd-16d7-11ea-8695-527d496f91de" in namespace "pods-6859" to be "success or failure"
Dec  4 20:46:44.068: INFO: Pod "client-envvars-2e678cbd-16d7-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 6.332493ms
Dec  4 20:46:46.071: INFO: Pod "client-envvars-2e678cbd-16d7-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009894036s
STEP: Saw pod success
Dec  4 20:46:46.071: INFO: Pod "client-envvars-2e678cbd-16d7-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 20:46:46.074: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker6c7da5d4d5 pod client-envvars-2e678cbd-16d7-11ea-8695-527d496f91de container env3cont: <nil>
STEP: delete the pod
Dec  4 20:46:46.093: INFO: Waiting for pod client-envvars-2e678cbd-16d7-11ea-8695-527d496f91de to disappear
Dec  4 20:46:46.096: INFO: Pod client-envvars-2e678cbd-16d7-11ea-8695-527d496f91de no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:46:46.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6859" for this suite.
Dec  4 20:47:36.112: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:47:36.194: INFO: namespace pods-6859 deletion completed in 50.09417268s

• [SLOW TEST:56.321 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:47:36.195: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5689
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  4 20:47:36.338: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4d8fd456-16d7-11ea-8695-527d496f91de" in namespace "downward-api-5689" to be "success or failure"
Dec  4 20:47:36.344: INFO: Pod "downwardapi-volume-4d8fd456-16d7-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019405ms
Dec  4 20:47:38.347: INFO: Pod "downwardapi-volume-4d8fd456-16d7-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008871285s
STEP: Saw pod success
Dec  4 20:47:38.347: INFO: Pod "downwardapi-volume-4d8fd456-16d7-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 20:47:38.349: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker9f6c17cd60 pod downwardapi-volume-4d8fd456-16d7-11ea-8695-527d496f91de container client-container: <nil>
STEP: delete the pod
Dec  4 20:47:38.366: INFO: Waiting for pod downwardapi-volume-4d8fd456-16d7-11ea-8695-527d496f91de to disappear
Dec  4 20:47:38.367: INFO: Pod downwardapi-volume-4d8fd456-16d7-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:47:38.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5689" for this suite.
Dec  4 20:47:44.389: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:47:44.459: INFO: namespace downward-api-5689 deletion completed in 6.089318648s

• [SLOW TEST:8.264 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:47:44.459: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4594
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-527d7f23-16d7-11ea-8695-527d496f91de
STEP: Creating a pod to test consume configMaps
Dec  4 20:47:44.607: INFO: Waiting up to 5m0s for pod "pod-configmaps-527e190b-16d7-11ea-8695-527d496f91de" in namespace "configmap-4594" to be "success or failure"
Dec  4 20:47:44.612: INFO: Pod "pod-configmaps-527e190b-16d7-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 4.662077ms
Dec  4 20:47:46.615: INFO: Pod "pod-configmaps-527e190b-16d7-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00745474s
Dec  4 20:47:48.618: INFO: Pod "pod-configmaps-527e190b-16d7-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010967953s
STEP: Saw pod success
Dec  4 20:47:48.618: INFO: Pod "pod-configmaps-527e190b-16d7-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 20:47:48.620: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker6c7da5d4d5 pod pod-configmaps-527e190b-16d7-11ea-8695-527d496f91de container configmap-volume-test: <nil>
STEP: delete the pod
Dec  4 20:47:48.635: INFO: Waiting for pod pod-configmaps-527e190b-16d7-11ea-8695-527d496f91de to disappear
Dec  4 20:47:48.638: INFO: Pod pod-configmaps-527e190b-16d7-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:47:48.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4594" for this suite.
Dec  4 20:47:54.653: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:47:54.733: INFO: namespace configmap-4594 deletion completed in 6.086883409s

• [SLOW TEST:10.274 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:47:54.734: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-150
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-589c99f5-16d7-11ea-8695-527d496f91de
STEP: Creating a pod to test consume configMaps
Dec  4 20:47:54.875: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-589d04f9-16d7-11ea-8695-527d496f91de" in namespace "projected-150" to be "success or failure"
Dec  4 20:47:54.877: INFO: Pod "pod-projected-configmaps-589d04f9-16d7-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.191387ms
Dec  4 20:47:56.881: INFO: Pod "pod-projected-configmaps-589d04f9-16d7-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005802835s
Dec  4 20:47:58.884: INFO: Pod "pod-projected-configmaps-589d04f9-16d7-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009071711s
STEP: Saw pod success
Dec  4 20:47:58.884: INFO: Pod "pod-projected-configmaps-589d04f9-16d7-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 20:47:58.886: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker9f6c17cd60 pod pod-projected-configmaps-589d04f9-16d7-11ea-8695-527d496f91de container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  4 20:47:58.916: INFO: Waiting for pod pod-projected-configmaps-589d04f9-16d7-11ea-8695-527d496f91de to disappear
Dec  4 20:47:58.918: INFO: Pod pod-projected-configmaps-589d04f9-16d7-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:47:58.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-150" for this suite.
Dec  4 20:48:04.930: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:48:05.001: INFO: namespace projected-150 deletion completed in 6.079944918s

• [SLOW TEST:10.268 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:48:05.002: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4572
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec  4 20:48:05.147: INFO: Waiting up to 5m0s for pod "pod-5ebbf8bc-16d7-11ea-8695-527d496f91de" in namespace "emptydir-4572" to be "success or failure"
Dec  4 20:48:05.159: INFO: Pod "pod-5ebbf8bc-16d7-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 12.350058ms
Dec  4 20:48:07.166: INFO: Pod "pod-5ebbf8bc-16d7-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018446147s
Dec  4 20:48:09.168: INFO: Pod "pod-5ebbf8bc-16d7-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020934795s
STEP: Saw pod success
Dec  4 20:48:09.168: INFO: Pod "pod-5ebbf8bc-16d7-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 20:48:09.170: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker6c7da5d4d5 pod pod-5ebbf8bc-16d7-11ea-8695-527d496f91de container test-container: <nil>
STEP: delete the pod
Dec  4 20:48:09.186: INFO: Waiting for pod pod-5ebbf8bc-16d7-11ea-8695-527d496f91de to disappear
Dec  4 20:48:09.189: INFO: Pod pod-5ebbf8bc-16d7-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:48:09.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4572" for this suite.
Dec  4 20:48:15.200: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:48:15.285: INFO: namespace emptydir-4572 deletion completed in 6.093243363s

• [SLOW TEST:10.283 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:48:15.291: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2569
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-64df8e00-16d7-11ea-8695-527d496f91de
STEP: Creating a pod to test consume configMaps
Dec  4 20:48:15.446: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-64dff9f0-16d7-11ea-8695-527d496f91de" in namespace "projected-2569" to be "success or failure"
Dec  4 20:48:15.453: INFO: Pod "pod-projected-configmaps-64dff9f0-16d7-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 7.336295ms
Dec  4 20:48:17.456: INFO: Pod "pod-projected-configmaps-64dff9f0-16d7-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010516911s
Dec  4 20:48:19.460: INFO: Pod "pod-projected-configmaps-64dff9f0-16d7-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013638779s
STEP: Saw pod success
Dec  4 20:48:19.460: INFO: Pod "pod-projected-configmaps-64dff9f0-16d7-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 20:48:19.462: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker9f6c17cd60 pod pod-projected-configmaps-64dff9f0-16d7-11ea-8695-527d496f91de container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  4 20:48:19.481: INFO: Waiting for pod pod-projected-configmaps-64dff9f0-16d7-11ea-8695-527d496f91de to disappear
Dec  4 20:48:19.483: INFO: Pod pod-projected-configmaps-64dff9f0-16d7-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:48:19.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2569" for this suite.
Dec  4 20:48:25.496: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:48:25.571: INFO: namespace projected-2569 deletion completed in 6.084800277s

• [SLOW TEST:10.280 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:48:25.573: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2999
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on node default medium
Dec  4 20:48:25.717: INFO: Waiting up to 5m0s for pod "pod-6afec530-16d7-11ea-8695-527d496f91de" in namespace "emptydir-2999" to be "success or failure"
Dec  4 20:48:25.730: INFO: Pod "pod-6afec530-16d7-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 12.874207ms
Dec  4 20:48:27.734: INFO: Pod "pod-6afec530-16d7-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016756895s
STEP: Saw pod success
Dec  4 20:48:27.734: INFO: Pod "pod-6afec530-16d7-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 20:48:27.736: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker6c7da5d4d5 pod pod-6afec530-16d7-11ea-8695-527d496f91de container test-container: <nil>
STEP: delete the pod
Dec  4 20:48:27.761: INFO: Waiting for pod pod-6afec530-16d7-11ea-8695-527d496f91de to disappear
Dec  4 20:48:27.763: INFO: Pod pod-6afec530-16d7-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:48:27.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2999" for this suite.
Dec  4 20:48:33.786: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:48:33.860: INFO: namespace emptydir-2999 deletion completed in 6.09376504s

• [SLOW TEST:8.288 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:48:33.862: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-5231
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Dec  4 20:48:33.999: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:48:37.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5231" for this suite.
Dec  4 20:48:53.941: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:48:54.016: INFO: namespace init-container-5231 deletion completed in 16.08330426s

• [SLOW TEST:20.154 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:48:54.016: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-6636
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Dec  4 20:48:54.157: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6636,SelfLink:/api/v1/namespaces/watch-6636/configmaps/e2e-watch-test-configmap-a,UID:7bf355e6-16d7-11ea-825c-005056950656,ResourceVersion:279253,Generation:0,CreationTimestamp:2019-12-04 20:48:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec  4 20:48:54.157: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6636,SelfLink:/api/v1/namespaces/watch-6636/configmaps/e2e-watch-test-configmap-a,UID:7bf355e6-16d7-11ea-825c-005056950656,ResourceVersion:279253,Generation:0,CreationTimestamp:2019-12-04 20:48:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Dec  4 20:49:04.163: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6636,SelfLink:/api/v1/namespaces/watch-6636/configmaps/e2e-watch-test-configmap-a,UID:7bf355e6-16d7-11ea-825c-005056950656,ResourceVersion:279269,Generation:0,CreationTimestamp:2019-12-04 20:48:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Dec  4 20:49:04.164: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6636,SelfLink:/api/v1/namespaces/watch-6636/configmaps/e2e-watch-test-configmap-a,UID:7bf355e6-16d7-11ea-825c-005056950656,ResourceVersion:279269,Generation:0,CreationTimestamp:2019-12-04 20:48:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Dec  4 20:49:14.170: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6636,SelfLink:/api/v1/namespaces/watch-6636/configmaps/e2e-watch-test-configmap-a,UID:7bf355e6-16d7-11ea-825c-005056950656,ResourceVersion:279288,Generation:0,CreationTimestamp:2019-12-04 20:48:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec  4 20:49:14.171: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6636,SelfLink:/api/v1/namespaces/watch-6636/configmaps/e2e-watch-test-configmap-a,UID:7bf355e6-16d7-11ea-825c-005056950656,ResourceVersion:279288,Generation:0,CreationTimestamp:2019-12-04 20:48:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Dec  4 20:49:24.177: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6636,SelfLink:/api/v1/namespaces/watch-6636/configmaps/e2e-watch-test-configmap-a,UID:7bf355e6-16d7-11ea-825c-005056950656,ResourceVersion:279305,Generation:0,CreationTimestamp:2019-12-04 20:48:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec  4 20:49:24.177: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6636,SelfLink:/api/v1/namespaces/watch-6636/configmaps/e2e-watch-test-configmap-a,UID:7bf355e6-16d7-11ea-825c-005056950656,ResourceVersion:279305,Generation:0,CreationTimestamp:2019-12-04 20:48:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Dec  4 20:49:34.182: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-6636,SelfLink:/api/v1/namespaces/watch-6636/configmaps/e2e-watch-test-configmap-b,UID:93ce916d-16d7-11ea-825c-005056950656,ResourceVersion:279321,Generation:0,CreationTimestamp:2019-12-04 20:49:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec  4 20:49:34.182: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-6636,SelfLink:/api/v1/namespaces/watch-6636/configmaps/e2e-watch-test-configmap-b,UID:93ce916d-16d7-11ea-825c-005056950656,ResourceVersion:279321,Generation:0,CreationTimestamp:2019-12-04 20:49:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Dec  4 20:49:44.187: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-6636,SelfLink:/api/v1/namespaces/watch-6636/configmaps/e2e-watch-test-configmap-b,UID:93ce916d-16d7-11ea-825c-005056950656,ResourceVersion:279340,Generation:0,CreationTimestamp:2019-12-04 20:49:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec  4 20:49:44.188: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-6636,SelfLink:/api/v1/namespaces/watch-6636/configmaps/e2e-watch-test-configmap-b,UID:93ce916d-16d7-11ea-825c-005056950656,ResourceVersion:279340,Generation:0,CreationTimestamp:2019-12-04 20:49:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:49:54.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6636" for this suite.
Dec  4 20:50:00.200: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:50:00.271: INFO: namespace watch-6636 deletion completed in 6.079191175s

• [SLOW TEST:66.255 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:50:00.271: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8440
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-a370baad-16d7-11ea-8695-527d496f91de
STEP: Creating configMap with name cm-test-opt-upd-a370bb12-16d7-11ea-8695-527d496f91de
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-a370baad-16d7-11ea-8695-527d496f91de
STEP: Updating configmap cm-test-opt-upd-a370bb12-16d7-11ea-8695-527d496f91de
STEP: Creating configMap with name cm-test-opt-create-a370bb27-16d7-11ea-8695-527d496f91de
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:51:36.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8440" for this suite.
Dec  4 20:51:58.880: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:51:58.960: INFO: namespace configmap-8440 deletion completed in 22.089838625s

• [SLOW TEST:118.689 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:51:58.964: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7401
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-ea2fe169-16d7-11ea-8695-527d496f91de
STEP: Creating a pod to test consume configMaps
Dec  4 20:51:59.111: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ea304f06-16d7-11ea-8695-527d496f91de" in namespace "projected-7401" to be "success or failure"
Dec  4 20:51:59.118: INFO: Pod "pod-projected-configmaps-ea304f06-16d7-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 7.683116ms
Dec  4 20:52:01.121: INFO: Pod "pod-projected-configmaps-ea304f06-16d7-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010261206s
Dec  4 20:52:03.124: INFO: Pod "pod-projected-configmaps-ea304f06-16d7-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012857724s
STEP: Saw pod success
Dec  4 20:52:03.124: INFO: Pod "pod-projected-configmaps-ea304f06-16d7-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 20:52:03.126: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker9f6c17cd60 pod pod-projected-configmaps-ea304f06-16d7-11ea-8695-527d496f91de container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  4 20:52:03.143: INFO: Waiting for pod pod-projected-configmaps-ea304f06-16d7-11ea-8695-527d496f91de to disappear
Dec  4 20:52:03.145: INFO: Pod pod-projected-configmaps-ea304f06-16d7-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:52:03.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7401" for this suite.
Dec  4 20:52:09.157: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:52:09.221: INFO: namespace projected-7401 deletion completed in 6.072118229s

• [SLOW TEST:10.256 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:52:09.221: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2734
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:52:09.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2734" for this suite.
Dec  4 20:52:31.394: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:52:31.461: INFO: namespace kubelet-test-2734 deletion completed in 22.077654477s

• [SLOW TEST:22.240 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:52:31.462: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4635
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec  4 20:52:31.605: INFO: Waiting up to 5m0s for pod "pod-fd8e68f2-16d7-11ea-8695-527d496f91de" in namespace "emptydir-4635" to be "success or failure"
Dec  4 20:52:31.611: INFO: Pod "pod-fd8e68f2-16d7-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 5.394232ms
Dec  4 20:52:33.614: INFO: Pod "pod-fd8e68f2-16d7-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008452592s
STEP: Saw pod success
Dec  4 20:52:33.614: INFO: Pod "pod-fd8e68f2-16d7-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 20:52:33.616: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker6c7da5d4d5 pod pod-fd8e68f2-16d7-11ea-8695-527d496f91de container test-container: <nil>
STEP: delete the pod
Dec  4 20:52:33.633: INFO: Waiting for pod pod-fd8e68f2-16d7-11ea-8695-527d496f91de to disappear
Dec  4 20:52:33.642: INFO: Pod pod-fd8e68f2-16d7-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:52:33.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4635" for this suite.
Dec  4 20:52:39.655: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:52:39.735: INFO: namespace emptydir-4635 deletion completed in 6.088754442s

• [SLOW TEST:8.273 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:52:39.736: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4077
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Dec  4 20:52:39.887: INFO: Waiting up to 5m0s for pod "downward-api-027d0575-16d8-11ea-8695-527d496f91de" in namespace "downward-api-4077" to be "success or failure"
Dec  4 20:52:39.894: INFO: Pod "downward-api-027d0575-16d8-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 7.591279ms
Dec  4 20:52:41.897: INFO: Pod "downward-api-027d0575-16d8-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010126095s
STEP: Saw pod success
Dec  4 20:52:41.897: INFO: Pod "downward-api-027d0575-16d8-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 20:52:41.899: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker6c7da5d4d5 pod downward-api-027d0575-16d8-11ea-8695-527d496f91de container dapi-container: <nil>
STEP: delete the pod
Dec  4 20:52:41.926: INFO: Waiting for pod downward-api-027d0575-16d8-11ea-8695-527d496f91de to disappear
Dec  4 20:52:41.929: INFO: Pod downward-api-027d0575-16d8-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:52:41.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4077" for this suite.
Dec  4 20:52:47.943: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:52:48.017: INFO: namespace downward-api-4077 deletion completed in 6.083901165s

• [SLOW TEST:8.281 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:52:48.018: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-8619
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec  4 20:52:56.213: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  4 20:52:56.216: INFO: Pod pod-with-poststart-http-hook still exists
Dec  4 20:52:58.216: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  4 20:52:58.219: INFO: Pod pod-with-poststart-http-hook still exists
Dec  4 20:53:00.216: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  4 20:53:00.218: INFO: Pod pod-with-poststart-http-hook still exists
Dec  4 20:53:02.216: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  4 20:53:02.219: INFO: Pod pod-with-poststart-http-hook still exists
Dec  4 20:53:04.216: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  4 20:53:04.219: INFO: Pod pod-with-poststart-http-hook still exists
Dec  4 20:53:06.216: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  4 20:53:06.219: INFO: Pod pod-with-poststart-http-hook still exists
Dec  4 20:53:08.216: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  4 20:53:08.219: INFO: Pod pod-with-poststart-http-hook still exists
Dec  4 20:53:10.216: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec  4 20:53:10.219: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:53:10.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8619" for this suite.
Dec  4 20:53:32.234: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:53:32.352: INFO: namespace container-lifecycle-hook-8619 deletion completed in 22.128822164s

• [SLOW TEST:44.334 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:53:32.353: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6766
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-6766
Dec  4 20:53:36.587: INFO: Started pod liveness-http in namespace container-probe-6766
STEP: checking the pod's current state and verifying that restartCount is present
Dec  4 20:53:36.590: INFO: Initial restart count of pod liveness-http is 0
Dec  4 20:53:52.616: INFO: Restart count of pod container-probe-6766/liveness-http is now 1 (16.026269502s elapsed)
Dec  4 20:54:10.647: INFO: Restart count of pod container-probe-6766/liveness-http is now 2 (34.057432427s elapsed)
Dec  4 20:54:32.723: INFO: Restart count of pod container-probe-6766/liveness-http is now 3 (56.132902539s elapsed)
Dec  4 20:54:50.756: INFO: Restart count of pod container-probe-6766/liveness-http is now 4 (1m14.166423231s elapsed)
Dec  4 20:56:04.868: INFO: Restart count of pod container-probe-6766/liveness-http is now 5 (2m28.27837815s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:56:04.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6766" for this suite.
Dec  4 20:56:10.904: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:56:10.976: INFO: namespace container-probe-6766 deletion completed in 6.092945432s

• [SLOW TEST:158.623 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:56:10.977: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-199
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1420
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  4 20:56:11.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-199'
Dec  4 20:56:11.522: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec  4 20:56:11.522: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Dec  4 20:56:11.530: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-64nh9]
Dec  4 20:56:11.530: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-64nh9" in namespace "kubectl-199" to be "running and ready"
Dec  4 20:56:11.534: INFO: Pod "e2e-test-nginx-rc-64nh9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.718734ms
Dec  4 20:56:13.537: INFO: Pod "e2e-test-nginx-rc-64nh9": Phase="Running", Reason="", readiness=true. Elapsed: 2.006349033s
Dec  4 20:56:13.537: INFO: Pod "e2e-test-nginx-rc-64nh9" satisfied condition "running and ready"
Dec  4 20:56:13.537: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-64nh9]
Dec  4 20:56:13.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 logs rc/e2e-test-nginx-rc --namespace=kubectl-199'
Dec  4 20:56:13.639: INFO: stderr: ""
Dec  4 20:56:13.639: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1425
Dec  4 20:56:13.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 delete rc e2e-test-nginx-rc --namespace=kubectl-199'
Dec  4 20:56:13.749: INFO: stderr: ""
Dec  4 20:56:13.749: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:56:13.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-199" for this suite.
Dec  4 20:56:35.776: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:56:35.844: INFO: namespace kubectl-199 deletion completed in 22.079354699s

• [SLOW TEST:24.867 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:56:35.844: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-71
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:163
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating pod
Dec  4 20:56:38.075: INFO: Pod pod-hostip-8f40f365-16d8-11ea-8695-527d496f91de has hostIP: 10.10.128.22
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:56:38.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-71" for this suite.
Dec  4 20:57:00.094: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:57:00.161: INFO: namespace pods-71 deletion completed in 22.077485586s

• [SLOW TEST:24.317 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:57:00.162: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2721
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:177
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:57:00.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2721" for this suite.
Dec  4 20:57:22.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:57:22.398: INFO: namespace pods-2721 deletion completed in 22.081846435s

• [SLOW TEST:22.236 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:57:22.400: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3730
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Dec  4 20:57:22.537: INFO: Waiting up to 5m0s for pod "downward-api-aaf72b86-16d8-11ea-8695-527d496f91de" in namespace "downward-api-3730" to be "success or failure"
Dec  4 20:57:22.545: INFO: Pod "downward-api-aaf72b86-16d8-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 7.649351ms
Dec  4 20:57:24.548: INFO: Pod "downward-api-aaf72b86-16d8-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010709296s
Dec  4 20:57:26.593: INFO: Pod "downward-api-aaf72b86-16d8-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.056121522s
STEP: Saw pod success
Dec  4 20:57:26.626: INFO: Pod "downward-api-aaf72b86-16d8-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 20:57:26.640: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker9f6c17cd60 pod downward-api-aaf72b86-16d8-11ea-8695-527d496f91de container dapi-container: <nil>
STEP: delete the pod
Dec  4 20:57:26.706: INFO: Waiting for pod downward-api-aaf72b86-16d8-11ea-8695-527d496f91de to disappear
Dec  4 20:57:26.708: INFO: Pod downward-api-aaf72b86-16d8-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:57:26.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3730" for this suite.
Dec  4 20:57:32.720: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:57:32.802: INFO: namespace downward-api-3730 deletion completed in 6.089961243s

• [SLOW TEST:10.403 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:57:32.806: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4045
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service multi-endpoint-test in namespace services-4045
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4045 to expose endpoints map[]
Dec  4 20:57:32.961: INFO: successfully validated that service multi-endpoint-test in namespace services-4045 exposes endpoints map[] (4.619076ms elapsed)
STEP: Creating pod pod1 in namespace services-4045
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4045 to expose endpoints map[pod1:[100]]
Dec  4 20:57:34.997: INFO: successfully validated that service multi-endpoint-test in namespace services-4045 exposes endpoints map[pod1:[100]] (2.027077855s elapsed)
STEP: Creating pod pod2 in namespace services-4045
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4045 to expose endpoints map[pod1:[100] pod2:[101]]
Dec  4 20:57:38.061: INFO: successfully validated that service multi-endpoint-test in namespace services-4045 exposes endpoints map[pod1:[100] pod2:[101]] (3.05470966s elapsed)
STEP: Deleting pod pod1 in namespace services-4045
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4045 to expose endpoints map[pod2:[101]]
Dec  4 20:57:38.078: INFO: successfully validated that service multi-endpoint-test in namespace services-4045 exposes endpoints map[pod2:[101]] (10.101834ms elapsed)
STEP: Deleting pod pod2 in namespace services-4045
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4045 to expose endpoints map[]
Dec  4 20:57:38.093: INFO: successfully validated that service multi-endpoint-test in namespace services-4045 exposes endpoints map[] (7.524874ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:57:38.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4045" for this suite.
Dec  4 20:58:00.137: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:58:00.205: INFO: namespace services-4045 deletion completed in 22.085569924s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:27.400 seconds]
[sig-network] Services
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:58:00.208: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2913
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-q5bc
STEP: Creating a pod to test atomic-volume-subpath
Dec  4 20:58:00.358: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-q5bc" in namespace "subpath-2913" to be "success or failure"
Dec  4 20:58:00.362: INFO: Pod "pod-subpath-test-configmap-q5bc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.155107ms
Dec  4 20:58:02.364: INFO: Pod "pod-subpath-test-configmap-q5bc": Phase="Running", Reason="", readiness=true. Elapsed: 2.006464857s
Dec  4 20:58:04.367: INFO: Pod "pod-subpath-test-configmap-q5bc": Phase="Running", Reason="", readiness=true. Elapsed: 4.0093101s
Dec  4 20:58:06.370: INFO: Pod "pod-subpath-test-configmap-q5bc": Phase="Running", Reason="", readiness=true. Elapsed: 6.012410267s
Dec  4 20:58:08.372: INFO: Pod "pod-subpath-test-configmap-q5bc": Phase="Running", Reason="", readiness=true. Elapsed: 8.01478796s
Dec  4 20:58:10.375: INFO: Pod "pod-subpath-test-configmap-q5bc": Phase="Running", Reason="", readiness=true. Elapsed: 10.017254951s
Dec  4 20:58:12.378: INFO: Pod "pod-subpath-test-configmap-q5bc": Phase="Running", Reason="", readiness=true. Elapsed: 12.020382459s
Dec  4 20:58:14.383: INFO: Pod "pod-subpath-test-configmap-q5bc": Phase="Running", Reason="", readiness=true. Elapsed: 14.02523601s
Dec  4 20:58:16.386: INFO: Pod "pod-subpath-test-configmap-q5bc": Phase="Running", Reason="", readiness=true. Elapsed: 16.028135706s
Dec  4 20:58:18.389: INFO: Pod "pod-subpath-test-configmap-q5bc": Phase="Running", Reason="", readiness=true. Elapsed: 18.030994034s
Dec  4 20:58:20.391: INFO: Pod "pod-subpath-test-configmap-q5bc": Phase="Running", Reason="", readiness=true. Elapsed: 20.033560352s
Dec  4 20:58:22.394: INFO: Pod "pod-subpath-test-configmap-q5bc": Phase="Running", Reason="", readiness=true. Elapsed: 22.036319891s
Dec  4 20:58:24.398: INFO: Pod "pod-subpath-test-configmap-q5bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.039993301s
STEP: Saw pod success
Dec  4 20:58:24.398: INFO: Pod "pod-subpath-test-configmap-q5bc" satisfied condition "success or failure"
Dec  4 20:58:24.400: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker6c7da5d4d5 pod pod-subpath-test-configmap-q5bc container test-container-subpath-configmap-q5bc: <nil>
STEP: delete the pod
Dec  4 20:58:24.425: INFO: Waiting for pod pod-subpath-test-configmap-q5bc to disappear
Dec  4 20:58:24.427: INFO: Pod pod-subpath-test-configmap-q5bc no longer exists
STEP: Deleting pod pod-subpath-test-configmap-q5bc
Dec  4 20:58:24.427: INFO: Deleting pod "pod-subpath-test-configmap-q5bc" in namespace "subpath-2913"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:58:24.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2913" for this suite.
Dec  4 20:58:30.442: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:58:30.524: INFO: namespace subpath-2913 deletion completed in 6.091297496s

• [SLOW TEST:30.317 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:58:30.526: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-2438
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-2438
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec  4 20:58:30.673: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec  4 20:58:54.748: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.1.54:8080/dial?request=hostName&protocol=udp&host=192.168.2.51&port=8081&tries=1'] Namespace:pod-network-test-2438 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  4 20:58:54.749: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
Dec  4 20:58:54.854: INFO: Waiting for endpoints: map[]
Dec  4 20:58:54.856: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.1.54:8080/dial?request=hostName&protocol=udp&host=192.168.1.53&port=8081&tries=1'] Namespace:pod-network-test-2438 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  4 20:58:54.856: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
Dec  4 20:58:54.946: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:58:54.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2438" for this suite.
Dec  4 20:59:16.959: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:59:17.024: INFO: namespace pod-network-test-2438 deletion completed in 22.073495977s

• [SLOW TEST:46.498 seconds]
[sig-network] Networking
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:59:17.026: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-9431
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-9660
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-3841
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:59:23.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9431" for this suite.
Dec  4 20:59:29.558: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:59:29.630: INFO: namespace namespaces-9431 deletion completed in 6.08240902s
STEP: Destroying namespace "nsdeletetest-9660" for this suite.
Dec  4 20:59:29.632: INFO: Namespace nsdeletetest-9660 was already deleted
STEP: Destroying namespace "nsdeletetest-3841" for this suite.
Dec  4 20:59:35.641: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:59:35.721: INFO: namespace nsdeletetest-3841 deletion completed in 6.089364553s

• [SLOW TEST:18.696 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:59:35.724: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5594
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-fa6f72fd-16d8-11ea-8695-527d496f91de
STEP: Creating a pod to test consume configMaps
Dec  4 20:59:35.913: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fa751a0b-16d8-11ea-8695-527d496f91de" in namespace "projected-5594" to be "success or failure"
Dec  4 20:59:35.920: INFO: Pod "pod-projected-configmaps-fa751a0b-16d8-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 6.942505ms
Dec  4 20:59:37.923: INFO: Pod "pod-projected-configmaps-fa751a0b-16d8-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00997208s
STEP: Saw pod success
Dec  4 20:59:37.923: INFO: Pod "pod-projected-configmaps-fa751a0b-16d8-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 20:59:37.925: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker9f6c17cd60 pod pod-projected-configmaps-fa751a0b-16d8-11ea-8695-527d496f91de container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  4 20:59:37.947: INFO: Waiting for pod pod-projected-configmaps-fa751a0b-16d8-11ea-8695-527d496f91de to disappear
Dec  4 20:59:37.949: INFO: Pod pod-projected-configmaps-fa751a0b-16d8-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:59:37.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5594" for this suite.
Dec  4 20:59:43.961: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 20:59:44.033: INFO: namespace projected-5594 deletion completed in 6.080559841s

• [SLOW TEST:8.309 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 20:59:44.035: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8463
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:163
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec  4 20:59:46.695: INFO: Successfully updated pod "pod-update-ff6342ed-16d8-11ea-8695-527d496f91de"
STEP: verifying the updated pod is in kubernetes
Dec  4 20:59:46.715: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 20:59:46.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8463" for this suite.
Dec  4 21:00:08.727: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:00:08.815: INFO: namespace pods-8463 deletion completed in 22.096096966s

• [SLOW TEST:24.780 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:00:08.816: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9466
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-0e29af77-16d9-11ea-8695-527d496f91de
STEP: Creating a pod to test consume configMaps
Dec  4 21:00:08.967: INFO: Waiting up to 5m0s for pod "pod-configmaps-0e2a57bc-16d9-11ea-8695-527d496f91de" in namespace "configmap-9466" to be "success or failure"
Dec  4 21:00:08.979: INFO: Pod "pod-configmaps-0e2a57bc-16d9-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 9.407808ms
Dec  4 21:00:10.982: INFO: Pod "pod-configmaps-0e2a57bc-16d9-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012420974s
Dec  4 21:00:12.985: INFO: Pod "pod-configmaps-0e2a57bc-16d9-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01556914s
STEP: Saw pod success
Dec  4 21:00:12.985: INFO: Pod "pod-configmaps-0e2a57bc-16d9-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 21:00:12.988: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker9f6c17cd60 pod pod-configmaps-0e2a57bc-16d9-11ea-8695-527d496f91de container configmap-volume-test: <nil>
STEP: delete the pod
Dec  4 21:00:13.008: INFO: Waiting for pod pod-configmaps-0e2a57bc-16d9-11ea-8695-527d496f91de to disappear
Dec  4 21:00:13.010: INFO: Pod pod-configmaps-0e2a57bc-16d9-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:00:13.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9466" for this suite.
Dec  4 21:00:19.026: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:00:19.094: INFO: namespace configmap-9466 deletion completed in 6.080042237s

• [SLOW TEST:10.278 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:00:19.094: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8703
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-1449256f-16d9-11ea-8695-527d496f91de
STEP: Creating a pod to test consume configMaps
Dec  4 21:00:19.236: INFO: Waiting up to 5m0s for pod "pod-configmaps-1449a714-16d9-11ea-8695-527d496f91de" in namespace "configmap-8703" to be "success or failure"
Dec  4 21:00:19.241: INFO: Pod "pod-configmaps-1449a714-16d9-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 5.317897ms
Dec  4 21:00:21.244: INFO: Pod "pod-configmaps-1449a714-16d9-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008159775s
Dec  4 21:00:23.247: INFO: Pod "pod-configmaps-1449a714-16d9-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010800476s
STEP: Saw pod success
Dec  4 21:00:23.247: INFO: Pod "pod-configmaps-1449a714-16d9-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 21:00:23.249: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker6c7da5d4d5 pod pod-configmaps-1449a714-16d9-11ea-8695-527d496f91de container configmap-volume-test: <nil>
STEP: delete the pod
Dec  4 21:00:23.264: INFO: Waiting for pod pod-configmaps-1449a714-16d9-11ea-8695-527d496f91de to disappear
Dec  4 21:00:23.266: INFO: Pod pod-configmaps-1449a714-16d9-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:00:23.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8703" for this suite.
Dec  4 21:00:29.277: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:00:29.369: INFO: namespace configmap-8703 deletion completed in 6.099593289s

• [SLOW TEST:10.274 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:00:29.369: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-3230
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-secret-8jt2
STEP: Creating a pod to test atomic-volume-subpath
Dec  4 21:00:29.517: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-8jt2" in namespace "subpath-3230" to be "success or failure"
Dec  4 21:00:29.525: INFO: Pod "pod-subpath-test-secret-8jt2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.339774ms
Dec  4 21:00:31.528: INFO: Pod "pod-subpath-test-secret-8jt2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008882363s
Dec  4 21:00:33.531: INFO: Pod "pod-subpath-test-secret-8jt2": Phase="Running", Reason="", readiness=true. Elapsed: 4.011952428s
Dec  4 21:00:35.535: INFO: Pod "pod-subpath-test-secret-8jt2": Phase="Running", Reason="", readiness=true. Elapsed: 6.015177835s
Dec  4 21:00:37.538: INFO: Pod "pod-subpath-test-secret-8jt2": Phase="Running", Reason="", readiness=true. Elapsed: 8.018267605s
Dec  4 21:00:39.540: INFO: Pod "pod-subpath-test-secret-8jt2": Phase="Running", Reason="", readiness=true. Elapsed: 10.020750181s
Dec  4 21:00:41.543: INFO: Pod "pod-subpath-test-secret-8jt2": Phase="Running", Reason="", readiness=true. Elapsed: 12.023479546s
Dec  4 21:00:43.546: INFO: Pod "pod-subpath-test-secret-8jt2": Phase="Running", Reason="", readiness=true. Elapsed: 14.026411276s
Dec  4 21:00:45.549: INFO: Pod "pod-subpath-test-secret-8jt2": Phase="Running", Reason="", readiness=true. Elapsed: 16.029618935s
Dec  4 21:00:47.552: INFO: Pod "pod-subpath-test-secret-8jt2": Phase="Running", Reason="", readiness=true. Elapsed: 18.032501076s
Dec  4 21:00:49.555: INFO: Pod "pod-subpath-test-secret-8jt2": Phase="Running", Reason="", readiness=true. Elapsed: 20.035647849s
Dec  4 21:00:51.558: INFO: Pod "pod-subpath-test-secret-8jt2": Phase="Running", Reason="", readiness=true. Elapsed: 22.038727865s
Dec  4 21:00:53.561: INFO: Pod "pod-subpath-test-secret-8jt2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.041739602s
STEP: Saw pod success
Dec  4 21:00:53.561: INFO: Pod "pod-subpath-test-secret-8jt2" satisfied condition "success or failure"
Dec  4 21:00:53.564: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker9f6c17cd60 pod pod-subpath-test-secret-8jt2 container test-container-subpath-secret-8jt2: <nil>
STEP: delete the pod
Dec  4 21:00:53.586: INFO: Waiting for pod pod-subpath-test-secret-8jt2 to disappear
Dec  4 21:00:53.588: INFO: Pod pod-subpath-test-secret-8jt2 no longer exists
STEP: Deleting pod pod-subpath-test-secret-8jt2
Dec  4 21:00:53.588: INFO: Deleting pod "pod-subpath-test-secret-8jt2" in namespace "subpath-3230"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:00:53.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3230" for this suite.
Dec  4 21:00:59.614: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:00:59.712: INFO: namespace subpath-3230 deletion completed in 6.114632462s

• [SLOW TEST:30.343 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:00:59.713: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-1688
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:01:03.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1688" for this suite.
Dec  4 21:01:09.926: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:01:10.014: INFO: namespace emptydir-wrapper-1688 deletion completed in 6.09658885s

• [SLOW TEST:10.301 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:01:10.014: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5338
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-32a2896d-16d9-11ea-8695-527d496f91de
STEP: Creating a pod to test consume configMaps
Dec  4 21:01:10.155: INFO: Waiting up to 5m0s for pod "pod-configmaps-32a2f8ef-16d9-11ea-8695-527d496f91de" in namespace "configmap-5338" to be "success or failure"
Dec  4 21:01:10.171: INFO: Pod "pod-configmaps-32a2f8ef-16d9-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 15.795387ms
Dec  4 21:01:12.177: INFO: Pod "pod-configmaps-32a2f8ef-16d9-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021954575s
Dec  4 21:01:14.182: INFO: Pod "pod-configmaps-32a2f8ef-16d9-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027399668s
STEP: Saw pod success
Dec  4 21:01:14.183: INFO: Pod "pod-configmaps-32a2f8ef-16d9-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 21:01:14.190: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker9f6c17cd60 pod pod-configmaps-32a2f8ef-16d9-11ea-8695-527d496f91de container configmap-volume-test: <nil>
STEP: delete the pod
Dec  4 21:01:14.209: INFO: Waiting for pod pod-configmaps-32a2f8ef-16d9-11ea-8695-527d496f91de to disappear
Dec  4 21:01:14.213: INFO: Pod pod-configmaps-32a2f8ef-16d9-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:01:14.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5338" for this suite.
Dec  4 21:01:20.227: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:01:20.340: INFO: namespace configmap-5338 deletion completed in 6.122253924s

• [SLOW TEST:10.326 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:01:20.342: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8197
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W1204 21:01:50.521863      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec  4 21:01:50.521: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:01:50.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8197" for this suite.
Dec  4 21:01:56.533: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:01:56.619: INFO: namespace gc-8197 deletion completed in 6.095238101s

• [SLOW TEST:36.277 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:01:56.622: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-4998
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Dec  4 21:01:56.776: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-4998,SelfLink:/api/v1/namespaces/watch-4998/configmaps/e2e-watch-test-resource-version,UID:4e6a9df8-16d9-11ea-825c-005056950656,ResourceVersion:281407,Generation:0,CreationTimestamp:2019-12-04 21:01:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec  4 21:01:56.776: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-4998,SelfLink:/api/v1/namespaces/watch-4998/configmaps/e2e-watch-test-resource-version,UID:4e6a9df8-16d9-11ea-825c-005056950656,ResourceVersion:281408,Generation:0,CreationTimestamp:2019-12-04 21:01:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:01:56.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4998" for this suite.
Dec  4 21:02:02.787: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:02:02.859: INFO: namespace watch-4998 deletion completed in 6.07927432s

• [SLOW TEST:6.237 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:02:02.859: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-8413
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Dec  4 21:02:02.989: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec  4 21:02:02.996: INFO: Waiting for terminating namespaces to be deleted...
Dec  4 21:02:02.998: INFO: 
Logging pods the kubelet thinks is on node alex-slot1-v2-vsp1-worker6c7da5d4d5 before test
Dec  4 21:02:03.009: INFO: kube-proxy-2krtb from kube-system started at 2019-12-02 22:28:54 +0000 UTC (1 container statuses recorded)
Dec  4 21:02:03.009: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  4 21:02:03.009: INFO: calico-node-27hrx from kube-system started at 2019-12-02 22:28:54 +0000 UTC (1 container statuses recorded)
Dec  4 21:02:03.009: INFO: 	Container calico-node ready: true, restart count 0
Dec  4 21:02:03.009: INFO: coredns-69cb7f6694-qph4l from kube-system started at 2019-12-02 22:29:13 +0000 UTC (1 container statuses recorded)
Dec  4 21:02:03.009: INFO: 	Container coredns ready: true, restart count 0
Dec  4 21:02:03.009: INFO: nginx-ingress-controller-nsxf4 from ccp started at 2019-12-02 22:34:24 +0000 UTC (1 container statuses recorded)
Dec  4 21:02:03.009: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec  4 21:02:03.009: INFO: sonobuoy from sonobuoy started at 2019-12-04 20:32:21 +0000 UTC (1 container statuses recorded)
Dec  4 21:02:03.009: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec  4 21:02:03.009: INFO: nvidia-device-plugin-daemonset-vpx26 from kube-system started at 2019-12-02 22:29:04 +0000 UTC (1 container statuses recorded)
Dec  4 21:02:03.009: INFO: 	Container nvidia-device-plugin-ctr ready: true, restart count 0
Dec  4 21:02:03.009: INFO: ccp-monitor-prometheus-node-exporter-d6xhl from ccp started at 2019-12-02 22:34:32 +0000 UTC (1 container statuses recorded)
Dec  4 21:02:03.009: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Dec  4 21:02:03.009: INFO: ccp-efk-kibana-56866fb888-sx6dh from ccp started at 2019-12-02 22:34:33 +0000 UTC (1 container statuses recorded)
Dec  4 21:02:03.009: INFO: 	Container kibana ready: true, restart count 0
Dec  4 21:02:03.009: INFO: ccp-harbor-harbor-database-0 from default started at 2019-12-02 22:34:38 +0000 UTC (1 container statuses recorded)
Dec  4 21:02:03.010: INFO: 	Container database ready: true, restart count 0
Dec  4 21:02:03.010: INFO: ccp-harbor-harbor-adminserver-5c6f5d5b8c-w2p9p from default started at 2019-12-02 22:34:37 +0000 UTC (1 container statuses recorded)
Dec  4 21:02:03.010: INFO: 	Container adminserver ready: true, restart count 1
Dec  4 21:02:03.010: INFO: ccp-harbor-harbor-disable-self-reg-job-98zz9 from default started at 2019-12-02 22:34:37 +0000 UTC (1 container statuses recorded)
Dec  4 21:02:03.010: INFO: 	Container self-registration ready: false, restart count 0
Dec  4 21:02:03.010: INFO: ccp-efk-elasticsearch-curator-1575334800-f92rq from ccp started at 2019-12-03 01:00:04 +0000 UTC (1 container statuses recorded)
Dec  4 21:02:03.010: INFO: 	Container elasticsearch-curator ready: false, restart count 0
Dec  4 21:02:03.010: INFO: sonobuoy-systemd-logs-daemon-set-f8654487368d4bd0-qjhgr from sonobuoy started at 2019-12-04 20:32:52 +0000 UTC (2 container statuses recorded)
Dec  4 21:02:03.010: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  4 21:02:03.010: INFO: 	Container systemd-logs ready: true, restart count 0
Dec  4 21:02:03.010: INFO: metallb-controller-577b6fb44-cxzgk from ccp started at 2019-12-02 22:34:24 +0000 UTC (1 container statuses recorded)
Dec  4 21:02:03.010: INFO: 	Container controller ready: true, restart count 0
Dec  4 21:02:03.010: INFO: ccp-monitor-prometheus-alertmanager-69f5f8bfbb-wrqkt from ccp started at 2019-12-02 22:34:32 +0000 UTC (2 container statuses recorded)
Dec  4 21:02:03.010: INFO: 	Container prometheus-alertmanager ready: true, restart count 0
Dec  4 21:02:03.010: INFO: 	Container prometheus-alertmanager-configmap-reload ready: true, restart count 0
Dec  4 21:02:03.010: INFO: ccp-monitor-prometheus-kube-state-metrics-dc8476fb4-h4mhf from ccp started at 2019-12-02 22:34:32 +0000 UTC (1 container statuses recorded)
Dec  4 21:02:03.010: INFO: 	Container prometheus-kube-state-metrics ready: true, restart count 0
Dec  4 21:02:03.010: INFO: ccp-monitor-grafana-cf94d4698-582qk from ccp started at 2019-12-02 22:34:33 +0000 UTC (1 container statuses recorded)
Dec  4 21:02:03.010: INFO: 	Container grafana ready: true, restart count 0
Dec  4 21:02:03.010: INFO: ccp-harbor-harbor-core-6c5d44857d-cnzn8 from default started at 2019-12-02 22:34:37 +0000 UTC (1 container statuses recorded)
Dec  4 21:02:03.011: INFO: 	Container core ready: true, restart count 0
Dec  4 21:02:03.011: INFO: cert-manager-799695b5bc-8gkw4 from ccp started at 2019-12-02 22:29:09 +0000 UTC (1 container statuses recorded)
Dec  4 21:02:03.011: INFO: 	Container cert-manager ready: true, restart count 0
Dec  4 21:02:03.011: INFO: metallb-speaker-np9rf from ccp started at 2019-12-02 22:34:24 +0000 UTC (1 container statuses recorded)
Dec  4 21:02:03.011: INFO: 	Container speaker ready: true, restart count 0
Dec  4 21:02:03.011: INFO: ccp-monitor-prometheus-port-update-1a9rs-qj4gx from ccp started at 2019-12-02 22:34:32 +0000 UTC (1 container statuses recorded)
Dec  4 21:02:03.011: INFO: 	Container ccp-monitor-prometheus-port-update ready: false, restart count 0
Dec  4 21:02:03.011: INFO: fluentd-es-v2.0.2-99h8x from ccp started at 2019-12-02 22:34:33 +0000 UTC (1 container statuses recorded)
Dec  4 21:02:03.011: INFO: 	Container fluentd-es ready: true, restart count 0
Dec  4 21:02:03.011: INFO: 
Logging pods the kubelet thinks is on node alex-slot1-v2-vsp1-worker9f6c17cd60 before test
Dec  4 21:02:03.020: INFO: elasticsearch-logging-0 from ccp started at 2019-12-02 22:34:56 +0000 UTC (1 container statuses recorded)
Dec  4 21:02:03.021: INFO: 	Container elasticsearch-logging ready: true, restart count 0
Dec  4 21:02:03.021: INFO: calico-node-p5tcj from kube-system started at 2019-12-02 22:28:53 +0000 UTC (1 container statuses recorded)
Dec  4 21:02:03.021: INFO: 	Container calico-node ready: true, restart count 0
Dec  4 21:02:03.021: INFO: coredns-69cb7f6694-m5vmc from kube-system started at 2019-12-02 22:29:13 +0000 UTC (1 container statuses recorded)
Dec  4 21:02:03.021: INFO: 	Container coredns ready: true, restart count 0
Dec  4 21:02:03.021: INFO: fluentd-es-v2.0.2-f4j7n from ccp started at 2019-12-02 22:34:33 +0000 UTC (1 container statuses recorded)
Dec  4 21:02:03.021: INFO: 	Container fluentd-es ready: true, restart count 0
Dec  4 21:02:03.021: INFO: ccp-harbor-harbor-jobservice-755d997849-vvz4g from default started at 2019-12-02 22:34:37 +0000 UTC (1 container statuses recorded)
Dec  4 21:02:03.021: INFO: 	Container jobservice ready: true, restart count 0
Dec  4 21:02:03.021: INFO: ccp-harbor-harbor-registry-77f88c7594-xd2nm from default started at 2019-12-02 22:34:38 +0000 UTC (2 container statuses recorded)
Dec  4 21:02:03.021: INFO: 	Container registry ready: true, restart count 0
Dec  4 21:02:03.021: INFO: 	Container registryctl ready: true, restart count 0
Dec  4 21:02:03.021: INFO: ccp-harbor-harbor-redis-0 from default started at 2019-12-02 22:34:38 +0000 UTC (1 container statuses recorded)
Dec  4 21:02:03.021: INFO: 	Container redis ready: true, restart count 0
Dec  4 21:02:03.021: INFO: sonobuoy-systemd-logs-daemon-set-f8654487368d4bd0-j78pc from sonobuoy started at 2019-12-04 20:32:46 +0000 UTC (2 container statuses recorded)
Dec  4 21:02:03.021: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec  4 21:02:03.021: INFO: 	Container systemd-logs ready: true, restart count 0
Dec  4 21:02:03.022: INFO: kube-proxy-xxz5s from kube-system started at 2019-12-02 22:28:53 +0000 UTC (1 container statuses recorded)
Dec  4 21:02:03.022: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  4 21:02:03.022: INFO: ccp-monitor-prometheus-pass-job-no1bx-7bmdq from ccp started at 2019-12-02 22:34:31 +0000 UTC (1 container statuses recorded)
Dec  4 21:02:03.022: INFO: 	Container ccp-monitor-prometheus-pass-container ready: false, restart count 0
Dec  4 21:02:03.022: INFO: ccp-monitor-prometheus-server-7745ccd984-jw4hf from ccp started at 2019-12-02 22:34:33 +0000 UTC (3 container statuses recorded)
Dec  4 21:02:03.022: INFO: 	Container nginx-proxy ready: true, restart count 0
Dec  4 21:02:03.022: INFO: 	Container prometheus-server ready: true, restart count 0
Dec  4 21:02:03.022: INFO: 	Container prometheus-server-configmap-reload ready: true, restart count 0
Dec  4 21:02:03.022: INFO: ccp-monitor-prometheus-node-exporter-mzgmm from ccp started at 2019-12-02 22:34:31 +0000 UTC (1 container statuses recorded)
Dec  4 21:02:03.022: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Dec  4 21:02:03.022: INFO: ccp-monitor-prometheus-pushgateway-7dcdb9d494-pzvxm from ccp started at 2019-12-02 22:34:31 +0000 UTC (1 container statuses recorded)
Dec  4 21:02:03.022: INFO: 	Container prometheus-pushgateway ready: true, restart count 0
Dec  4 21:02:03.022: INFO: ccp-harbor-harbor-portal-7445674c74-x2xqp from default started at 2019-12-02 22:34:36 +0000 UTC (1 container statuses recorded)
Dec  4 21:02:03.022: INFO: 	Container portal ready: true, restart count 0
Dec  4 21:02:03.022: INFO: ccp-efk-elasticsearch-curator-1575421200-wwgv5 from ccp started at 2019-12-04 01:00:03 +0000 UTC (1 container statuses recorded)
Dec  4 21:02:03.022: INFO: 	Container elasticsearch-curator ready: false, restart count 0
Dec  4 21:02:03.022: INFO: nvidia-device-plugin-daemonset-km64c from kube-system started at 2019-12-02 22:29:03 +0000 UTC (1 container statuses recorded)
Dec  4 21:02:03.022: INFO: 	Container nvidia-device-plugin-ctr ready: true, restart count 0
Dec  4 21:02:03.022: INFO: nginx-ingress-controller-dbfcd from ccp started at 2019-12-02 22:34:23 +0000 UTC (1 container statuses recorded)
Dec  4 21:02:03.022: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec  4 21:02:03.022: INFO: nginx-ingress-default-backend-dc5596844-xss9h from ccp started at 2019-12-02 22:34:23 +0000 UTC (1 container statuses recorded)
Dec  4 21:02:03.022: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Dec  4 21:02:03.022: INFO: metallb-speaker-xm9d9 from ccp started at 2019-12-02 22:34:24 +0000 UTC (1 container statuses recorded)
Dec  4 21:02:03.022: INFO: 	Container speaker ready: true, restart count 0
Dec  4 21:02:03.023: INFO: kubernetes-dashboard-67d74b6485-wv99t from ccp started at 2019-12-02 22:34:25 +0000 UTC (1 container statuses recorded)
Dec  4 21:02:03.023: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Dec  4 21:02:03.023: INFO: ccp-monitor-grafana-set-datasource-zkb8j from ccp started at 2019-12-02 22:34:31 +0000 UTC (1 container statuses recorded)
Dec  4 21:02:03.023: INFO: 	Container ccp-monitor-grafana-set-datasource ready: false, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15dd46d479e44089], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:02:04.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8413" for this suite.
Dec  4 21:02:10.062: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:02:10.133: INFO: namespace sched-pred-8413 deletion completed in 6.080213791s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:7.273 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:02:10.133: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-930
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-930
Dec  4 21:02:12.280: INFO: Started pod liveness-exec in namespace container-probe-930
STEP: checking the pod's current state and verifying that restartCount is present
Dec  4 21:02:12.282: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:06:12.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-930" for this suite.
Dec  4 21:06:18.715: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:06:18.817: INFO: namespace container-probe-930 deletion completed in 6.110544885s

• [SLOW TEST:248.685 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:06:18.818: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2065
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec  4 21:06:18.964: INFO: Waiting up to 5m0s for pod "pod-eab3344e-16d9-11ea-8695-527d496f91de" in namespace "emptydir-2065" to be "success or failure"
Dec  4 21:06:18.972: INFO: Pod "pod-eab3344e-16d9-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 7.800663ms
Dec  4 21:06:20.975: INFO: Pod "pod-eab3344e-16d9-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010575445s
STEP: Saw pod success
Dec  4 21:06:20.975: INFO: Pod "pod-eab3344e-16d9-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 21:06:20.977: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker9f6c17cd60 pod pod-eab3344e-16d9-11ea-8695-527d496f91de container test-container: <nil>
STEP: delete the pod
Dec  4 21:06:21.002: INFO: Waiting for pod pod-eab3344e-16d9-11ea-8695-527d496f91de to disappear
Dec  4 21:06:21.004: INFO: Pod pod-eab3344e-16d9-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:06:21.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2065" for this suite.
Dec  4 21:06:27.024: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:06:27.089: INFO: namespace emptydir-2065 deletion completed in 6.080077515s

• [SLOW TEST:8.271 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:06:27.090: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-5942
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's args
Dec  4 21:06:27.226: INFO: Waiting up to 5m0s for pod "var-expansion-efa02e3f-16d9-11ea-8695-527d496f91de" in namespace "var-expansion-5942" to be "success or failure"
Dec  4 21:06:27.233: INFO: Pod "var-expansion-efa02e3f-16d9-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 6.976701ms
Dec  4 21:06:29.236: INFO: Pod "var-expansion-efa02e3f-16d9-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009905043s
STEP: Saw pod success
Dec  4 21:06:29.236: INFO: Pod "var-expansion-efa02e3f-16d9-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 21:06:29.239: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker6c7da5d4d5 pod var-expansion-efa02e3f-16d9-11ea-8695-527d496f91de container dapi-container: <nil>
STEP: delete the pod
Dec  4 21:06:29.258: INFO: Waiting for pod var-expansion-efa02e3f-16d9-11ea-8695-527d496f91de to disappear
Dec  4 21:06:29.260: INFO: Pod var-expansion-efa02e3f-16d9-11ea-8695-527d496f91de no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:06:29.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5942" for this suite.
Dec  4 21:06:35.273: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:06:35.355: INFO: namespace var-expansion-5942 deletion completed in 6.090762028s

• [SLOW TEST:8.265 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:06:35.355: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-502
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating replication controller svc-latency-rc in namespace svc-latency-502
I1204 21:06:35.496418      15 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-502, replica count: 1
I1204 21:06:36.546796      15 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1204 21:06:37.547269      15 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1204 21:06:38.547442      15 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec  4 21:06:38.668: INFO: Created: latency-svc-s7w7t
Dec  4 21:06:38.685: INFO: Got endpoints: latency-svc-s7w7t [37.407768ms]
Dec  4 21:06:38.732: INFO: Created: latency-svc-5wl7g
Dec  4 21:06:38.742: INFO: Got endpoints: latency-svc-5wl7g [56.724988ms]
Dec  4 21:06:38.762: INFO: Created: latency-svc-bt4wv
Dec  4 21:06:38.762: INFO: Got endpoints: latency-svc-bt4wv [72.817761ms]
Dec  4 21:06:38.776: INFO: Created: latency-svc-t664g
Dec  4 21:06:38.790: INFO: Got endpoints: latency-svc-t664g [97.764394ms]
Dec  4 21:06:38.814: INFO: Created: latency-svc-jxwm4
Dec  4 21:06:38.821: INFO: Created: latency-svc-88966
Dec  4 21:06:38.821: INFO: Got endpoints: latency-svc-jxwm4 [127.318489ms]
Dec  4 21:06:38.826: INFO: Got endpoints: latency-svc-88966 [131.441879ms]
Dec  4 21:06:38.856: INFO: Created: latency-svc-7cqkg
Dec  4 21:06:38.868: INFO: Got endpoints: latency-svc-7cqkg [172.748838ms]
Dec  4 21:06:38.896: INFO: Created: latency-svc-fhlt5
Dec  4 21:06:38.903: INFO: Got endpoints: latency-svc-fhlt5 [207.384819ms]
Dec  4 21:06:38.918: INFO: Created: latency-svc-wtgh7
Dec  4 21:06:38.921: INFO: Got endpoints: latency-svc-wtgh7 [225.121515ms]
Dec  4 21:06:38.930: INFO: Created: latency-svc-mjjhm
Dec  4 21:06:38.934: INFO: Got endpoints: latency-svc-mjjhm [238.301006ms]
Dec  4 21:06:38.964: INFO: Created: latency-svc-b262t
Dec  4 21:06:38.965: INFO: Got endpoints: latency-svc-b262t [268.664687ms]
Dec  4 21:06:38.982: INFO: Created: latency-svc-hm8vz
Dec  4 21:06:38.994: INFO: Got endpoints: latency-svc-hm8vz [297.217568ms]
Dec  4 21:06:38.995: INFO: Created: latency-svc-zrj7q
Dec  4 21:06:39.008: INFO: Got endpoints: latency-svc-zrj7q [305.331187ms]
Dec  4 21:06:39.014: INFO: Created: latency-svc-9wsf6
Dec  4 21:06:39.023: INFO: Got endpoints: latency-svc-9wsf6 [319.353543ms]
Dec  4 21:06:39.035: INFO: Created: latency-svc-zb789
Dec  4 21:06:39.038: INFO: Got endpoints: latency-svc-zb789 [333.582372ms]
Dec  4 21:06:39.051: INFO: Created: latency-svc-xz7b6
Dec  4 21:06:39.054: INFO: Got endpoints: latency-svc-xz7b6 [340.847765ms]
Dec  4 21:06:39.078: INFO: Created: latency-svc-rx556
Dec  4 21:06:39.092: INFO: Created: latency-svc-nqm6d
Dec  4 21:06:39.092: INFO: Got endpoints: latency-svc-rx556 [350.642706ms]
Dec  4 21:06:39.098: INFO: Got endpoints: latency-svc-nqm6d [336.260533ms]
Dec  4 21:06:39.115: INFO: Created: latency-svc-w6q6f
Dec  4 21:06:39.126: INFO: Created: latency-svc-h4r28
Dec  4 21:06:39.129: INFO: Got endpoints: latency-svc-w6q6f [337.095958ms]
Dec  4 21:06:39.134: INFO: Got endpoints: latency-svc-h4r28 [313.04223ms]
Dec  4 21:06:39.141: INFO: Created: latency-svc-nng2g
Dec  4 21:06:39.155: INFO: Got endpoints: latency-svc-nng2g [328.760198ms]
Dec  4 21:06:39.157: INFO: Created: latency-svc-8mxhw
Dec  4 21:06:39.159: INFO: Got endpoints: latency-svc-8mxhw [291.264981ms]
Dec  4 21:06:39.182: INFO: Created: latency-svc-76bv7
Dec  4 21:06:39.189: INFO: Created: latency-svc-swr5v
Dec  4 21:06:39.189: INFO: Got endpoints: latency-svc-76bv7 [286.185245ms]
Dec  4 21:06:39.194: INFO: Got endpoints: latency-svc-swr5v [273.672172ms]
Dec  4 21:06:39.226: INFO: Created: latency-svc-vfmb7
Dec  4 21:06:39.232: INFO: Created: latency-svc-22b69
Dec  4 21:06:39.237: INFO: Got endpoints: latency-svc-22b69 [272.771985ms]
Dec  4 21:06:39.238: INFO: Got endpoints: latency-svc-vfmb7 [303.52361ms]
Dec  4 21:06:39.256: INFO: Created: latency-svc-klv2x
Dec  4 21:06:39.257: INFO: Got endpoints: latency-svc-klv2x [261.937006ms]
Dec  4 21:06:39.275: INFO: Created: latency-svc-km42r
Dec  4 21:06:39.289: INFO: Got endpoints: latency-svc-km42r [280.764013ms]
Dec  4 21:06:39.308: INFO: Created: latency-svc-dj5xr
Dec  4 21:06:39.314: INFO: Got endpoints: latency-svc-dj5xr [291.011532ms]
Dec  4 21:06:39.327: INFO: Created: latency-svc-5thbr
Dec  4 21:06:39.328: INFO: Got endpoints: latency-svc-5thbr [290.119199ms]
Dec  4 21:06:39.356: INFO: Created: latency-svc-cp68r
Dec  4 21:06:39.358: INFO: Got endpoints: latency-svc-cp68r [303.476218ms]
Dec  4 21:06:39.381: INFO: Created: latency-svc-x6gqn
Dec  4 21:06:39.396: INFO: Got endpoints: latency-svc-x6gqn [303.346154ms]
Dec  4 21:06:39.403: INFO: Created: latency-svc-8676z
Dec  4 21:06:39.410: INFO: Got endpoints: latency-svc-8676z [311.501645ms]
Dec  4 21:06:39.418: INFO: Created: latency-svc-qxbvb
Dec  4 21:06:39.431: INFO: Got endpoints: latency-svc-qxbvb [302.581198ms]
Dec  4 21:06:39.436: INFO: Created: latency-svc-scskq
Dec  4 21:06:39.442: INFO: Got endpoints: latency-svc-scskq [307.503405ms]
Dec  4 21:06:39.462: INFO: Created: latency-svc-2q69h
Dec  4 21:06:39.462: INFO: Got endpoints: latency-svc-2q69h [307.520678ms]
Dec  4 21:06:39.473: INFO: Created: latency-svc-skhr6
Dec  4 21:06:39.473: INFO: Got endpoints: latency-svc-skhr6 [313.883633ms]
Dec  4 21:06:39.486: INFO: Created: latency-svc-57dx9
Dec  4 21:06:39.490: INFO: Got endpoints: latency-svc-57dx9 [299.803532ms]
Dec  4 21:06:39.513: INFO: Created: latency-svc-gmdhz
Dec  4 21:06:39.519: INFO: Got endpoints: latency-svc-gmdhz [324.081743ms]
Dec  4 21:06:39.524: INFO: Created: latency-svc-r9m7f
Dec  4 21:06:39.533: INFO: Got endpoints: latency-svc-r9m7f [295.577213ms]
Dec  4 21:06:39.542: INFO: Created: latency-svc-j2brc
Dec  4 21:06:39.558: INFO: Got endpoints: latency-svc-j2brc [319.802044ms]
Dec  4 21:06:39.568: INFO: Created: latency-svc-qqvw7
Dec  4 21:06:39.568: INFO: Got endpoints: latency-svc-qqvw7 [311.148437ms]
Dec  4 21:06:39.584: INFO: Created: latency-svc-7mh9z
Dec  4 21:06:39.586: INFO: Got endpoints: latency-svc-7mh9z [297.398636ms]
Dec  4 21:06:39.598: INFO: Created: latency-svc-h685v
Dec  4 21:06:39.606: INFO: Got endpoints: latency-svc-h685v [291.395865ms]
Dec  4 21:06:39.628: INFO: Created: latency-svc-4ndcc
Dec  4 21:06:39.631: INFO: Got endpoints: latency-svc-4ndcc [303.225854ms]
Dec  4 21:06:39.642: INFO: Created: latency-svc-qrtg2
Dec  4 21:06:39.662: INFO: Got endpoints: latency-svc-qrtg2 [304.280758ms]
Dec  4 21:06:39.663: INFO: Created: latency-svc-gtvp4
Dec  4 21:06:39.685: INFO: Got endpoints: latency-svc-gtvp4 [288.900013ms]
Dec  4 21:06:39.705: INFO: Created: latency-svc-rtwd9
Dec  4 21:06:39.705: INFO: Got endpoints: latency-svc-rtwd9 [295.121582ms]
Dec  4 21:06:39.720: INFO: Created: latency-svc-phjfs
Dec  4 21:06:39.736: INFO: Got endpoints: latency-svc-phjfs [304.573206ms]
Dec  4 21:06:39.750: INFO: Created: latency-svc-ccrmk
Dec  4 21:06:39.759: INFO: Got endpoints: latency-svc-ccrmk [316.723703ms]
Dec  4 21:06:39.762: INFO: Created: latency-svc-xxnjr
Dec  4 21:06:39.767: INFO: Got endpoints: latency-svc-xxnjr [30.543696ms]
Dec  4 21:06:39.776: INFO: Created: latency-svc-khhl9
Dec  4 21:06:39.789: INFO: Created: latency-svc-hqnzm
Dec  4 21:06:39.792: INFO: Got endpoints: latency-svc-khhl9 [329.143711ms]
Dec  4 21:06:39.806: INFO: Created: latency-svc-zzvbw
Dec  4 21:06:39.825: INFO: Created: latency-svc-wkmlh
Dec  4 21:06:39.842: INFO: Created: latency-svc-8x5bm
Dec  4 21:06:39.860: INFO: Got endpoints: latency-svc-hqnzm [386.593161ms]
Dec  4 21:06:39.876: INFO: Created: latency-svc-64nmx
Dec  4 21:06:39.906: INFO: Created: latency-svc-9td9g
Dec  4 21:06:39.913: INFO: Got endpoints: latency-svc-zzvbw [422.991099ms]
Dec  4 21:06:39.933: INFO: Created: latency-svc-xvksv
Dec  4 21:06:39.943: INFO: Got endpoints: latency-svc-wkmlh [423.780061ms]
Dec  4 21:06:39.946: INFO: Created: latency-svc-mhblx
Dec  4 21:06:39.975: INFO: Created: latency-svc-4snmm
Dec  4 21:06:39.993: INFO: Got endpoints: latency-svc-8x5bm [460.372972ms]
Dec  4 21:06:39.995: INFO: Created: latency-svc-trrvd
Dec  4 21:06:40.003: INFO: Created: latency-svc-9hn4t
Dec  4 21:06:40.024: INFO: Created: latency-svc-9qlmc
Dec  4 21:06:40.033: INFO: Got endpoints: latency-svc-64nmx [475.569412ms]
Dec  4 21:06:40.048: INFO: Created: latency-svc-kwvhc
Dec  4 21:06:40.081: INFO: Created: latency-svc-q948d
Dec  4 21:06:40.210: INFO: Created: latency-svc-62twq
Dec  4 21:06:40.211: INFO: Got endpoints: latency-svc-mhblx [604.796203ms]
Dec  4 21:06:40.211: INFO: Got endpoints: latency-svc-9td9g [642.794082ms]
Dec  4 21:06:40.211: INFO: Got endpoints: latency-svc-xvksv [624.097772ms]
Dec  4 21:06:40.236: INFO: Created: latency-svc-lt7b9
Dec  4 21:06:40.240: INFO: Got endpoints: latency-svc-4snmm [608.09173ms]
Dec  4 21:06:40.266: INFO: Created: latency-svc-9cw5v
Dec  4 21:06:40.288: INFO: Got endpoints: latency-svc-trrvd [625.268277ms]
Dec  4 21:06:40.291: INFO: Created: latency-svc-272nk
Dec  4 21:06:40.310: INFO: Created: latency-svc-sc77t
Dec  4 21:06:40.326: INFO: Created: latency-svc-wc6tz
Dec  4 21:06:40.342: INFO: Got endpoints: latency-svc-9hn4t [656.926383ms]
Dec  4 21:06:40.342: INFO: Created: latency-svc-2p98z
Dec  4 21:06:40.350: INFO: Created: latency-svc-n6wrp
Dec  4 21:06:40.367: INFO: Created: latency-svc-2pbzv
Dec  4 21:06:40.377: INFO: Created: latency-svc-p5bgd
Dec  4 21:06:40.386: INFO: Got endpoints: latency-svc-9qlmc [680.788157ms]
Dec  4 21:06:40.387: INFO: Created: latency-svc-9sncg
Dec  4 21:06:40.408: INFO: Created: latency-svc-2gc8g
Dec  4 21:06:40.426: INFO: Created: latency-svc-vwvmw
Dec  4 21:06:40.434: INFO: Got endpoints: latency-svc-kwvhc [675.361589ms]
Dec  4 21:06:40.452: INFO: Created: latency-svc-6khz5
Dec  4 21:06:40.486: INFO: Got endpoints: latency-svc-q948d [719.41029ms]
Dec  4 21:06:40.505: INFO: Created: latency-svc-jcmvh
Dec  4 21:06:40.535: INFO: Got endpoints: latency-svc-62twq [743.100784ms]
Dec  4 21:06:40.561: INFO: Created: latency-svc-z97v5
Dec  4 21:06:40.587: INFO: Got endpoints: latency-svc-lt7b9 [727.506336ms]
Dec  4 21:06:40.608: INFO: Created: latency-svc-tsv9c
Dec  4 21:06:40.641: INFO: Got endpoints: latency-svc-9cw5v [728.349983ms]
Dec  4 21:06:40.658: INFO: Created: latency-svc-m92gt
Dec  4 21:06:40.686: INFO: Got endpoints: latency-svc-272nk [743.751086ms]
Dec  4 21:06:40.703: INFO: Created: latency-svc-tmgx7
Dec  4 21:06:40.734: INFO: Got endpoints: latency-svc-sc77t [739.548452ms]
Dec  4 21:06:40.758: INFO: Created: latency-svc-f8gg2
Dec  4 21:06:40.801: INFO: Got endpoints: latency-svc-wc6tz [767.589501ms]
Dec  4 21:06:40.816: INFO: Created: latency-svc-ldkpc
Dec  4 21:06:40.837: INFO: Got endpoints: latency-svc-2p98z [626.731495ms]
Dec  4 21:06:40.872: INFO: Created: latency-svc-2vhxz
Dec  4 21:06:40.884: INFO: Got endpoints: latency-svc-n6wrp [672.389085ms]
Dec  4 21:06:40.906: INFO: Created: latency-svc-zg86h
Dec  4 21:06:40.934: INFO: Got endpoints: latency-svc-2pbzv [722.737475ms]
Dec  4 21:06:40.957: INFO: Created: latency-svc-wvt9x
Dec  4 21:06:40.987: INFO: Got endpoints: latency-svc-p5bgd [747.377897ms]
Dec  4 21:06:41.027: INFO: Created: latency-svc-4nh5f
Dec  4 21:06:41.035: INFO: Got endpoints: latency-svc-9sncg [747.240391ms]
Dec  4 21:06:41.063: INFO: Created: latency-svc-jpl6f
Dec  4 21:06:41.086: INFO: Got endpoints: latency-svc-2gc8g [743.663778ms]
Dec  4 21:06:41.101: INFO: Created: latency-svc-gs5tm
Dec  4 21:06:41.135: INFO: Got endpoints: latency-svc-vwvmw [748.62706ms]
Dec  4 21:06:41.149: INFO: Created: latency-svc-plqcx
Dec  4 21:06:41.183: INFO: Got endpoints: latency-svc-6khz5 [748.70554ms]
Dec  4 21:06:41.199: INFO: Created: latency-svc-pjlsh
Dec  4 21:06:41.236: INFO: Got endpoints: latency-svc-jcmvh [749.97187ms]
Dec  4 21:06:41.252: INFO: Created: latency-svc-khf4m
Dec  4 21:06:41.293: INFO: Got endpoints: latency-svc-z97v5 [758.014169ms]
Dec  4 21:06:41.313: INFO: Created: latency-svc-lrgtn
Dec  4 21:06:41.335: INFO: Got endpoints: latency-svc-tsv9c [747.597937ms]
Dec  4 21:06:41.352: INFO: Created: latency-svc-l95g2
Dec  4 21:06:41.384: INFO: Got endpoints: latency-svc-m92gt [742.782856ms]
Dec  4 21:06:41.406: INFO: Created: latency-svc-pbm7g
Dec  4 21:06:41.434: INFO: Got endpoints: latency-svc-tmgx7 [747.602131ms]
Dec  4 21:06:41.447: INFO: Created: latency-svc-drj2v
Dec  4 21:06:41.489: INFO: Got endpoints: latency-svc-f8gg2 [754.330863ms]
Dec  4 21:06:41.543: INFO: Created: latency-svc-sn2dn
Dec  4 21:06:41.548: INFO: Got endpoints: latency-svc-ldkpc [746.742784ms]
Dec  4 21:06:41.573: INFO: Created: latency-svc-psrlq
Dec  4 21:06:41.587: INFO: Got endpoints: latency-svc-2vhxz [749.655289ms]
Dec  4 21:06:41.607: INFO: Created: latency-svc-fl6hl
Dec  4 21:06:41.646: INFO: Got endpoints: latency-svc-zg86h [762.633285ms]
Dec  4 21:06:41.668: INFO: Created: latency-svc-bg8ds
Dec  4 21:06:41.685: INFO: Got endpoints: latency-svc-wvt9x [750.728322ms]
Dec  4 21:06:41.700: INFO: Created: latency-svc-8bhd5
Dec  4 21:06:41.736: INFO: Got endpoints: latency-svc-4nh5f [748.529283ms]
Dec  4 21:06:41.760: INFO: Created: latency-svc-8fsvw
Dec  4 21:06:41.788: INFO: Got endpoints: latency-svc-jpl6f [752.974857ms]
Dec  4 21:06:41.807: INFO: Created: latency-svc-2p2x7
Dec  4 21:06:41.837: INFO: Got endpoints: latency-svc-gs5tm [751.077981ms]
Dec  4 21:06:41.851: INFO: Created: latency-svc-mf22l
Dec  4 21:06:41.887: INFO: Got endpoints: latency-svc-plqcx [751.920958ms]
Dec  4 21:06:41.904: INFO: Created: latency-svc-kxvbr
Dec  4 21:06:41.937: INFO: Got endpoints: latency-svc-pjlsh [753.870201ms]
Dec  4 21:06:41.955: INFO: Created: latency-svc-n59qj
Dec  4 21:06:41.986: INFO: Got endpoints: latency-svc-khf4m [749.724025ms]
Dec  4 21:06:42.003: INFO: Created: latency-svc-knw6n
Dec  4 21:06:42.036: INFO: Got endpoints: latency-svc-lrgtn [742.8171ms]
Dec  4 21:06:42.065: INFO: Created: latency-svc-66zt2
Dec  4 21:06:42.085: INFO: Got endpoints: latency-svc-l95g2 [749.385113ms]
Dec  4 21:06:42.114: INFO: Created: latency-svc-hzrr2
Dec  4 21:06:42.146: INFO: Got endpoints: latency-svc-pbm7g [762.270544ms]
Dec  4 21:06:42.168: INFO: Created: latency-svc-wc4ld
Dec  4 21:06:42.195: INFO: Got endpoints: latency-svc-drj2v [760.504162ms]
Dec  4 21:06:42.232: INFO: Created: latency-svc-6lbp4
Dec  4 21:06:42.235: INFO: Got endpoints: latency-svc-sn2dn [745.70184ms]
Dec  4 21:06:42.264: INFO: Created: latency-svc-sx459
Dec  4 21:06:42.287: INFO: Got endpoints: latency-svc-psrlq [739.390418ms]
Dec  4 21:06:42.318: INFO: Created: latency-svc-v6nbg
Dec  4 21:06:42.338: INFO: Got endpoints: latency-svc-fl6hl [750.390905ms]
Dec  4 21:06:42.354: INFO: Created: latency-svc-wzh7b
Dec  4 21:06:42.385: INFO: Got endpoints: latency-svc-bg8ds [738.158013ms]
Dec  4 21:06:42.413: INFO: Created: latency-svc-8fvxd
Dec  4 21:06:42.436: INFO: Got endpoints: latency-svc-8bhd5 [750.301844ms]
Dec  4 21:06:42.454: INFO: Created: latency-svc-2gnwm
Dec  4 21:06:42.485: INFO: Got endpoints: latency-svc-8fsvw [749.434121ms]
Dec  4 21:06:42.500: INFO: Created: latency-svc-ft8rp
Dec  4 21:06:42.535: INFO: Got endpoints: latency-svc-2p2x7 [746.193325ms]
Dec  4 21:06:42.551: INFO: Created: latency-svc-tb8hf
Dec  4 21:06:42.584: INFO: Got endpoints: latency-svc-mf22l [747.55355ms]
Dec  4 21:06:42.604: INFO: Created: latency-svc-4f5zd
Dec  4 21:06:42.635: INFO: Got endpoints: latency-svc-kxvbr [747.339756ms]
Dec  4 21:06:42.651: INFO: Created: latency-svc-fc489
Dec  4 21:06:42.685: INFO: Got endpoints: latency-svc-n59qj [748.144448ms]
Dec  4 21:06:42.703: INFO: Created: latency-svc-27pcf
Dec  4 21:06:42.736: INFO: Got endpoints: latency-svc-knw6n [750.463332ms]
Dec  4 21:06:42.755: INFO: Created: latency-svc-2gj8m
Dec  4 21:06:42.786: INFO: Got endpoints: latency-svc-66zt2 [749.34113ms]
Dec  4 21:06:42.800: INFO: Created: latency-svc-qj9sj
Dec  4 21:06:42.840: INFO: Got endpoints: latency-svc-hzrr2 [755.285248ms]
Dec  4 21:06:42.857: INFO: Created: latency-svc-72mkw
Dec  4 21:06:42.887: INFO: Got endpoints: latency-svc-wc4ld [740.431087ms]
Dec  4 21:06:42.904: INFO: Created: latency-svc-ld77p
Dec  4 21:06:42.937: INFO: Got endpoints: latency-svc-6lbp4 [742.337553ms]
Dec  4 21:06:42.960: INFO: Created: latency-svc-xskqj
Dec  4 21:06:42.987: INFO: Got endpoints: latency-svc-sx459 [752.341312ms]
Dec  4 21:06:43.003: INFO: Created: latency-svc-nkxlz
Dec  4 21:06:43.035: INFO: Got endpoints: latency-svc-v6nbg [748.206853ms]
Dec  4 21:06:43.057: INFO: Created: latency-svc-pxz98
Dec  4 21:06:43.091: INFO: Got endpoints: latency-svc-wzh7b [753.648456ms]
Dec  4 21:06:43.111: INFO: Created: latency-svc-42fcs
Dec  4 21:06:43.135: INFO: Got endpoints: latency-svc-8fvxd [749.909597ms]
Dec  4 21:06:43.152: INFO: Created: latency-svc-zl2nm
Dec  4 21:06:43.187: INFO: Got endpoints: latency-svc-2gnwm [751.004644ms]
Dec  4 21:06:43.224: INFO: Created: latency-svc-wqppx
Dec  4 21:06:43.235: INFO: Got endpoints: latency-svc-ft8rp [749.54244ms]
Dec  4 21:06:43.251: INFO: Created: latency-svc-j9wvh
Dec  4 21:06:43.288: INFO: Got endpoints: latency-svc-tb8hf [752.747678ms]
Dec  4 21:06:43.311: INFO: Created: latency-svc-bpxmg
Dec  4 21:06:43.338: INFO: Got endpoints: latency-svc-4f5zd [753.278652ms]
Dec  4 21:06:43.382: INFO: Created: latency-svc-qs52r
Dec  4 21:06:43.398: INFO: Got endpoints: latency-svc-fc489 [762.513701ms]
Dec  4 21:06:43.428: INFO: Created: latency-svc-4wzh4
Dec  4 21:06:43.438: INFO: Got endpoints: latency-svc-27pcf [752.580621ms]
Dec  4 21:06:43.458: INFO: Created: latency-svc-szkw9
Dec  4 21:06:43.485: INFO: Got endpoints: latency-svc-2gj8m [748.744587ms]
Dec  4 21:06:43.507: INFO: Created: latency-svc-7jpbn
Dec  4 21:06:43.536: INFO: Got endpoints: latency-svc-qj9sj [750.250942ms]
Dec  4 21:06:43.554: INFO: Created: latency-svc-jr5gr
Dec  4 21:06:43.586: INFO: Got endpoints: latency-svc-72mkw [745.874075ms]
Dec  4 21:06:43.602: INFO: Created: latency-svc-4bfdp
Dec  4 21:06:43.635: INFO: Got endpoints: latency-svc-ld77p [748.334366ms]
Dec  4 21:06:43.657: INFO: Created: latency-svc-b9dj4
Dec  4 21:06:43.685: INFO: Got endpoints: latency-svc-xskqj [746.13643ms]
Dec  4 21:06:43.701: INFO: Created: latency-svc-j9j9j
Dec  4 21:06:43.738: INFO: Got endpoints: latency-svc-nkxlz [750.80552ms]
Dec  4 21:06:43.760: INFO: Created: latency-svc-78kgx
Dec  4 21:06:43.787: INFO: Got endpoints: latency-svc-pxz98 [751.191223ms]
Dec  4 21:06:43.804: INFO: Created: latency-svc-d8bpc
Dec  4 21:06:43.836: INFO: Got endpoints: latency-svc-42fcs [744.124694ms]
Dec  4 21:06:43.851: INFO: Created: latency-svc-t6hv7
Dec  4 21:06:43.886: INFO: Got endpoints: latency-svc-zl2nm [751.182306ms]
Dec  4 21:06:43.902: INFO: Created: latency-svc-wgvs8
Dec  4 21:06:44.015: INFO: Got endpoints: latency-svc-j9wvh [780.088156ms]
Dec  4 21:06:44.016: INFO: Got endpoints: latency-svc-wqppx [828.744083ms]
Dec  4 21:06:44.038: INFO: Created: latency-svc-jlh4z
Dec  4 21:06:44.043: INFO: Got endpoints: latency-svc-bpxmg [755.009104ms]
Dec  4 21:06:44.076: INFO: Created: latency-svc-5qd6n
Dec  4 21:06:44.110: INFO: Got endpoints: latency-svc-qs52r [772.363136ms]
Dec  4 21:06:44.130: INFO: Created: latency-svc-h4whp
Dec  4 21:06:44.137: INFO: Got endpoints: latency-svc-4wzh4 [739.274967ms]
Dec  4 21:06:44.151: INFO: Created: latency-svc-v759k
Dec  4 21:06:44.161: INFO: Created: latency-svc-n7p5v
Dec  4 21:06:44.185: INFO: Got endpoints: latency-svc-szkw9 [747.149691ms]
Dec  4 21:06:44.201: INFO: Created: latency-svc-pxbt2
Dec  4 21:06:44.236: INFO: Got endpoints: latency-svc-7jpbn [749.994689ms]
Dec  4 21:06:44.255: INFO: Created: latency-svc-zxf8p
Dec  4 21:06:44.287: INFO: Got endpoints: latency-svc-jr5gr [750.391242ms]
Dec  4 21:06:44.305: INFO: Created: latency-svc-jcwp7
Dec  4 21:06:44.336: INFO: Got endpoints: latency-svc-4bfdp [749.881056ms]
Dec  4 21:06:44.353: INFO: Created: latency-svc-mjfsz
Dec  4 21:06:44.389: INFO: Got endpoints: latency-svc-b9dj4 [753.569062ms]
Dec  4 21:06:44.406: INFO: Created: latency-svc-b8ljk
Dec  4 21:06:44.441: INFO: Got endpoints: latency-svc-j9j9j [756.217547ms]
Dec  4 21:06:44.464: INFO: Created: latency-svc-ddv4r
Dec  4 21:06:44.484: INFO: Got endpoints: latency-svc-78kgx [745.538761ms]
Dec  4 21:06:44.509: INFO: Created: latency-svc-gk9f4
Dec  4 21:06:44.535: INFO: Got endpoints: latency-svc-d8bpc [747.952855ms]
Dec  4 21:06:44.559: INFO: Created: latency-svc-9crv4
Dec  4 21:06:44.586: INFO: Got endpoints: latency-svc-t6hv7 [749.753392ms]
Dec  4 21:06:44.601: INFO: Created: latency-svc-h58ml
Dec  4 21:06:44.634: INFO: Got endpoints: latency-svc-wgvs8 [748.120547ms]
Dec  4 21:06:44.656: INFO: Created: latency-svc-xxpb4
Dec  4 21:06:44.684: INFO: Got endpoints: latency-svc-jlh4z [669.013187ms]
Dec  4 21:06:44.702: INFO: Created: latency-svc-674hq
Dec  4 21:06:44.735: INFO: Got endpoints: latency-svc-5qd6n [718.99341ms]
Dec  4 21:06:44.752: INFO: Created: latency-svc-624dt
Dec  4 21:06:44.784: INFO: Got endpoints: latency-svc-h4whp [741.02569ms]
Dec  4 21:06:44.823: INFO: Created: latency-svc-kxh6q
Dec  4 21:06:44.836: INFO: Got endpoints: latency-svc-v759k [725.124346ms]
Dec  4 21:06:44.851: INFO: Created: latency-svc-6mclq
Dec  4 21:06:44.886: INFO: Got endpoints: latency-svc-n7p5v [748.847879ms]
Dec  4 21:06:44.899: INFO: Created: latency-svc-spv94
Dec  4 21:06:44.935: INFO: Got endpoints: latency-svc-pxbt2 [750.02842ms]
Dec  4 21:06:44.952: INFO: Created: latency-svc-8nv9t
Dec  4 21:06:44.985: INFO: Got endpoints: latency-svc-zxf8p [748.412413ms]
Dec  4 21:06:45.001: INFO: Created: latency-svc-kmfmw
Dec  4 21:06:45.034: INFO: Got endpoints: latency-svc-jcwp7 [747.18702ms]
Dec  4 21:06:45.054: INFO: Created: latency-svc-tv5xq
Dec  4 21:06:45.087: INFO: Got endpoints: latency-svc-mjfsz [750.446166ms]
Dec  4 21:06:45.116: INFO: Created: latency-svc-sfbjg
Dec  4 21:06:45.134: INFO: Got endpoints: latency-svc-b8ljk [745.03885ms]
Dec  4 21:06:45.150: INFO: Created: latency-svc-7fpwt
Dec  4 21:06:45.185: INFO: Got endpoints: latency-svc-ddv4r [743.398548ms]
Dec  4 21:06:45.217: INFO: Created: latency-svc-m2625
Dec  4 21:06:45.239: INFO: Got endpoints: latency-svc-gk9f4 [754.386685ms]
Dec  4 21:06:45.257: INFO: Created: latency-svc-svcmt
Dec  4 21:06:45.284: INFO: Got endpoints: latency-svc-9crv4 [748.905067ms]
Dec  4 21:06:45.304: INFO: Created: latency-svc-66zkp
Dec  4 21:06:45.336: INFO: Got endpoints: latency-svc-h58ml [750.183604ms]
Dec  4 21:06:45.353: INFO: Created: latency-svc-8mgm8
Dec  4 21:06:45.389: INFO: Got endpoints: latency-svc-xxpb4 [754.979083ms]
Dec  4 21:06:45.403: INFO: Created: latency-svc-8s2vh
Dec  4 21:06:45.437: INFO: Got endpoints: latency-svc-674hq [752.708903ms]
Dec  4 21:06:45.457: INFO: Created: latency-svc-jthgm
Dec  4 21:06:45.484: INFO: Got endpoints: latency-svc-624dt [749.499786ms]
Dec  4 21:06:45.501: INFO: Created: latency-svc-796zm
Dec  4 21:06:45.537: INFO: Got endpoints: latency-svc-kxh6q [752.635831ms]
Dec  4 21:06:45.553: INFO: Created: latency-svc-hgf4d
Dec  4 21:06:45.584: INFO: Got endpoints: latency-svc-6mclq [748.38726ms]
Dec  4 21:06:45.600: INFO: Created: latency-svc-49bvn
Dec  4 21:06:45.634: INFO: Got endpoints: latency-svc-spv94 [747.991782ms]
Dec  4 21:06:45.665: INFO: Created: latency-svc-65g48
Dec  4 21:06:45.683: INFO: Got endpoints: latency-svc-8nv9t [747.888788ms]
Dec  4 21:06:45.702: INFO: Created: latency-svc-fblst
Dec  4 21:06:45.736: INFO: Got endpoints: latency-svc-kmfmw [750.683111ms]
Dec  4 21:06:45.760: INFO: Created: latency-svc-d98sn
Dec  4 21:06:45.784: INFO: Got endpoints: latency-svc-tv5xq [749.734985ms]
Dec  4 21:06:45.802: INFO: Created: latency-svc-b8bjs
Dec  4 21:06:45.836: INFO: Got endpoints: latency-svc-sfbjg [748.109831ms]
Dec  4 21:06:45.849: INFO: Created: latency-svc-2plbx
Dec  4 21:06:45.887: INFO: Got endpoints: latency-svc-7fpwt [752.409043ms]
Dec  4 21:06:45.904: INFO: Created: latency-svc-lbzkt
Dec  4 21:06:45.934: INFO: Got endpoints: latency-svc-m2625 [749.282704ms]
Dec  4 21:06:45.956: INFO: Created: latency-svc-s8jvc
Dec  4 21:06:45.989: INFO: Got endpoints: latency-svc-svcmt [750.838581ms]
Dec  4 21:06:46.005: INFO: Created: latency-svc-hpp5b
Dec  4 21:06:46.036: INFO: Got endpoints: latency-svc-66zkp [752.219177ms]
Dec  4 21:06:46.054: INFO: Created: latency-svc-t9jtn
Dec  4 21:06:46.086: INFO: Got endpoints: latency-svc-8mgm8 [749.38456ms]
Dec  4 21:06:46.103: INFO: Created: latency-svc-wc466
Dec  4 21:06:46.135: INFO: Got endpoints: latency-svc-8s2vh [745.364981ms]
Dec  4 21:06:46.154: INFO: Created: latency-svc-pwwfk
Dec  4 21:06:46.186: INFO: Got endpoints: latency-svc-jthgm [748.359635ms]
Dec  4 21:06:46.210: INFO: Created: latency-svc-7qkp2
Dec  4 21:06:46.236: INFO: Got endpoints: latency-svc-796zm [751.61913ms]
Dec  4 21:06:46.260: INFO: Created: latency-svc-wpklj
Dec  4 21:06:46.284: INFO: Got endpoints: latency-svc-hgf4d [747.185034ms]
Dec  4 21:06:46.301: INFO: Created: latency-svc-mpg4d
Dec  4 21:06:46.335: INFO: Got endpoints: latency-svc-49bvn [750.255105ms]
Dec  4 21:06:46.353: INFO: Created: latency-svc-54qw8
Dec  4 21:06:46.386: INFO: Got endpoints: latency-svc-65g48 [751.244782ms]
Dec  4 21:06:46.432: INFO: Created: latency-svc-78fz5
Dec  4 21:06:46.437: INFO: Got endpoints: latency-svc-fblst [753.892922ms]
Dec  4 21:06:46.453: INFO: Created: latency-svc-rmqbv
Dec  4 21:06:46.489: INFO: Got endpoints: latency-svc-d98sn [753.106109ms]
Dec  4 21:06:46.509: INFO: Created: latency-svc-2jqhc
Dec  4 21:06:46.537: INFO: Got endpoints: latency-svc-b8bjs [752.571201ms]
Dec  4 21:06:46.585: INFO: Got endpoints: latency-svc-2plbx [748.96688ms]
Dec  4 21:06:46.636: INFO: Got endpoints: latency-svc-lbzkt [749.33172ms]
Dec  4 21:06:46.699: INFO: Got endpoints: latency-svc-s8jvc [764.628109ms]
Dec  4 21:06:46.735: INFO: Got endpoints: latency-svc-hpp5b [745.779133ms]
Dec  4 21:06:46.784: INFO: Got endpoints: latency-svc-t9jtn [747.643749ms]
Dec  4 21:06:46.835: INFO: Got endpoints: latency-svc-wc466 [749.639902ms]
Dec  4 21:06:46.885: INFO: Got endpoints: latency-svc-pwwfk [749.845186ms]
Dec  4 21:06:46.935: INFO: Got endpoints: latency-svc-7qkp2 [748.95551ms]
Dec  4 21:06:46.988: INFO: Got endpoints: latency-svc-wpklj [751.798681ms]
Dec  4 21:06:47.042: INFO: Got endpoints: latency-svc-mpg4d [757.843494ms]
Dec  4 21:06:47.087: INFO: Got endpoints: latency-svc-54qw8 [752.703462ms]
Dec  4 21:06:47.133: INFO: Got endpoints: latency-svc-78fz5 [747.418861ms]
Dec  4 21:06:47.189: INFO: Got endpoints: latency-svc-rmqbv [751.27163ms]
Dec  4 21:06:47.236: INFO: Got endpoints: latency-svc-2jqhc [747.349404ms]
Dec  4 21:06:47.236: INFO: Latencies: [30.543696ms 56.724988ms 72.817761ms 97.764394ms 127.318489ms 131.441879ms 172.748838ms 207.384819ms 225.121515ms 238.301006ms 261.937006ms 268.664687ms 272.771985ms 273.672172ms 280.764013ms 286.185245ms 288.900013ms 290.119199ms 291.011532ms 291.264981ms 291.395865ms 295.121582ms 295.577213ms 297.217568ms 297.398636ms 299.803532ms 302.581198ms 303.225854ms 303.346154ms 303.476218ms 303.52361ms 304.280758ms 304.573206ms 305.331187ms 307.503405ms 307.520678ms 311.148437ms 311.501645ms 313.04223ms 313.883633ms 316.723703ms 319.353543ms 319.802044ms 324.081743ms 328.760198ms 329.143711ms 333.582372ms 336.260533ms 337.095958ms 340.847765ms 350.642706ms 386.593161ms 422.991099ms 423.780061ms 460.372972ms 475.569412ms 604.796203ms 608.09173ms 624.097772ms 625.268277ms 626.731495ms 642.794082ms 656.926383ms 669.013187ms 672.389085ms 675.361589ms 680.788157ms 718.99341ms 719.41029ms 722.737475ms 725.124346ms 727.506336ms 728.349983ms 738.158013ms 739.274967ms 739.390418ms 739.548452ms 740.431087ms 741.02569ms 742.337553ms 742.782856ms 742.8171ms 743.100784ms 743.398548ms 743.663778ms 743.751086ms 744.124694ms 745.03885ms 745.364981ms 745.538761ms 745.70184ms 745.779133ms 745.874075ms 746.13643ms 746.193325ms 746.742784ms 747.149691ms 747.185034ms 747.18702ms 747.240391ms 747.339756ms 747.349404ms 747.377897ms 747.418861ms 747.55355ms 747.597937ms 747.602131ms 747.643749ms 747.888788ms 747.952855ms 747.991782ms 748.109831ms 748.120547ms 748.144448ms 748.206853ms 748.334366ms 748.359635ms 748.38726ms 748.412413ms 748.529283ms 748.62706ms 748.70554ms 748.744587ms 748.847879ms 748.905067ms 748.95551ms 748.96688ms 749.282704ms 749.33172ms 749.34113ms 749.38456ms 749.385113ms 749.434121ms 749.499786ms 749.54244ms 749.639902ms 749.655289ms 749.724025ms 749.734985ms 749.753392ms 749.845186ms 749.881056ms 749.909597ms 749.97187ms 749.994689ms 750.02842ms 750.183604ms 750.250942ms 750.255105ms 750.301844ms 750.390905ms 750.391242ms 750.446166ms 750.463332ms 750.683111ms 750.728322ms 750.80552ms 750.838581ms 751.004644ms 751.077981ms 751.182306ms 751.191223ms 751.244782ms 751.27163ms 751.61913ms 751.798681ms 751.920958ms 752.219177ms 752.341312ms 752.409043ms 752.571201ms 752.580621ms 752.635831ms 752.703462ms 752.708903ms 752.747678ms 752.974857ms 753.106109ms 753.278652ms 753.569062ms 753.648456ms 753.870201ms 753.892922ms 754.330863ms 754.386685ms 754.979083ms 755.009104ms 755.285248ms 756.217547ms 757.843494ms 758.014169ms 760.504162ms 762.270544ms 762.513701ms 762.633285ms 764.628109ms 767.589501ms 772.363136ms 780.088156ms 828.744083ms]
Dec  4 21:06:47.236: INFO: 50 %ile: 747.339756ms
Dec  4 21:06:47.236: INFO: 90 %ile: 753.648456ms
Dec  4 21:06:47.236: INFO: 99 %ile: 780.088156ms
Dec  4 21:06:47.236: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:06:47.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-502" for this suite.
Dec  4 21:06:59.255: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:06:59.335: INFO: namespace svc-latency-502 deletion completed in 12.095103538s

• [SLOW TEST:23.980 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:06:59.337: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8694
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:266
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the initial replication controller
Dec  4 21:06:59.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 create -f - --namespace=kubectl-8694'
Dec  4 21:07:00.058: INFO: stderr: ""
Dec  4 21:07:00.058: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  4 21:07:00.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8694'
Dec  4 21:07:00.172: INFO: stderr: ""
Dec  4 21:07:00.172: INFO: stdout: "update-demo-nautilus-6c6b8 update-demo-nautilus-w2dxh "
Dec  4 21:07:00.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get pods update-demo-nautilus-6c6b8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8694'
Dec  4 21:07:00.272: INFO: stderr: ""
Dec  4 21:07:00.272: INFO: stdout: ""
Dec  4 21:07:00.272: INFO: update-demo-nautilus-6c6b8 is created but not running
Dec  4 21:07:05.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8694'
Dec  4 21:07:05.366: INFO: stderr: ""
Dec  4 21:07:05.366: INFO: stdout: "update-demo-nautilus-6c6b8 update-demo-nautilus-w2dxh "
Dec  4 21:07:05.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get pods update-demo-nautilus-6c6b8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8694'
Dec  4 21:07:05.450: INFO: stderr: ""
Dec  4 21:07:05.450: INFO: stdout: "true"
Dec  4 21:07:05.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get pods update-demo-nautilus-6c6b8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8694'
Dec  4 21:07:05.538: INFO: stderr: ""
Dec  4 21:07:05.538: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  4 21:07:05.538: INFO: validating pod update-demo-nautilus-6c6b8
Dec  4 21:07:05.542: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  4 21:07:05.542: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  4 21:07:05.542: INFO: update-demo-nautilus-6c6b8 is verified up and running
Dec  4 21:07:05.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get pods update-demo-nautilus-w2dxh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8694'
Dec  4 21:07:05.629: INFO: stderr: ""
Dec  4 21:07:05.629: INFO: stdout: "true"
Dec  4 21:07:05.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get pods update-demo-nautilus-w2dxh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8694'
Dec  4 21:07:05.726: INFO: stderr: ""
Dec  4 21:07:05.726: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  4 21:07:05.726: INFO: validating pod update-demo-nautilus-w2dxh
Dec  4 21:07:05.729: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  4 21:07:05.730: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  4 21:07:05.730: INFO: update-demo-nautilus-w2dxh is verified up and running
STEP: rolling-update to new replication controller
Dec  4 21:07:05.732: INFO: scanned /root for discovery docs: <nil>
Dec  4 21:07:05.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-8694'
Dec  4 21:07:28.190: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Dec  4 21:07:28.191: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  4 21:07:28.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8694'
Dec  4 21:07:28.295: INFO: stderr: ""
Dec  4 21:07:28.295: INFO: stdout: "update-demo-kitten-42mdg update-demo-kitten-wrh68 "
Dec  4 21:07:28.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get pods update-demo-kitten-42mdg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8694'
Dec  4 21:07:28.376: INFO: stderr: ""
Dec  4 21:07:28.376: INFO: stdout: "true"
Dec  4 21:07:28.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get pods update-demo-kitten-42mdg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8694'
Dec  4 21:07:28.462: INFO: stderr: ""
Dec  4 21:07:28.462: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Dec  4 21:07:28.462: INFO: validating pod update-demo-kitten-42mdg
Dec  4 21:07:28.469: INFO: got data: {
  "image": "kitten.jpg"
}

Dec  4 21:07:28.469: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Dec  4 21:07:28.469: INFO: update-demo-kitten-42mdg is verified up and running
Dec  4 21:07:28.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get pods update-demo-kitten-wrh68 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8694'
Dec  4 21:07:28.569: INFO: stderr: ""
Dec  4 21:07:28.569: INFO: stdout: "true"
Dec  4 21:07:28.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get pods update-demo-kitten-wrh68 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8694'
Dec  4 21:07:28.696: INFO: stderr: ""
Dec  4 21:07:28.696: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Dec  4 21:07:28.696: INFO: validating pod update-demo-kitten-wrh68
Dec  4 21:07:28.701: INFO: got data: {
  "image": "kitten.jpg"
}

Dec  4 21:07:28.701: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Dec  4 21:07:28.701: INFO: update-demo-kitten-wrh68 is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:07:28.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8694" for this suite.
Dec  4 21:07:50.736: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:07:50.812: INFO: namespace kubectl-8694 deletion completed in 22.09458789s

• [SLOW TEST:51.475 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:07:50.814: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1741
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  4 21:07:50.953: INFO: Waiting up to 5m0s for pod "downwardapi-volume-21878b65-16da-11ea-8695-527d496f91de" in namespace "projected-1741" to be "success or failure"
Dec  4 21:07:50.957: INFO: Pod "downwardapi-volume-21878b65-16da-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 4.240311ms
Dec  4 21:07:52.960: INFO: Pod "downwardapi-volume-21878b65-16da-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006855667s
STEP: Saw pod success
Dec  4 21:07:52.960: INFO: Pod "downwardapi-volume-21878b65-16da-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 21:07:52.962: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker6c7da5d4d5 pod downwardapi-volume-21878b65-16da-11ea-8695-527d496f91de container client-container: <nil>
STEP: delete the pod
Dec  4 21:07:52.982: INFO: Waiting for pod downwardapi-volume-21878b65-16da-11ea-8695-527d496f91de to disappear
Dec  4 21:07:52.984: INFO: Pod downwardapi-volume-21878b65-16da-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:07:52.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1741" for this suite.
Dec  4 21:07:58.998: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:07:59.071: INFO: namespace projected-1741 deletion completed in 6.083612653s

• [SLOW TEST:8.257 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:07:59.071: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3553
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1204 21:08:05.233121      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec  4 21:08:05.233: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:08:05.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3553" for this suite.
Dec  4 21:08:11.249: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:08:11.343: INFO: namespace gc-3553 deletion completed in 6.105432673s

• [SLOW TEST:12.272 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:08:11.346: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9655
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-2dc4c5e2-16da-11ea-8695-527d496f91de
STEP: Creating a pod to test consume secrets
Dec  4 21:08:11.496: INFO: Waiting up to 5m0s for pod "pod-secrets-2dc5b09f-16da-11ea-8695-527d496f91de" in namespace "secrets-9655" to be "success or failure"
Dec  4 21:08:11.503: INFO: Pod "pod-secrets-2dc5b09f-16da-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 6.280571ms
Dec  4 21:08:13.507: INFO: Pod "pod-secrets-2dc5b09f-16da-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010566991s
Dec  4 21:08:15.511: INFO: Pod "pod-secrets-2dc5b09f-16da-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014171406s
STEP: Saw pod success
Dec  4 21:08:15.511: INFO: Pod "pod-secrets-2dc5b09f-16da-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 21:08:15.513: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker9f6c17cd60 pod pod-secrets-2dc5b09f-16da-11ea-8695-527d496f91de container secret-volume-test: <nil>
STEP: delete the pod
Dec  4 21:08:15.540: INFO: Waiting for pod pod-secrets-2dc5b09f-16da-11ea-8695-527d496f91de to disappear
Dec  4 21:08:15.541: INFO: Pod pod-secrets-2dc5b09f-16da-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:08:15.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9655" for this suite.
Dec  4 21:08:21.554: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:08:21.647: INFO: namespace secrets-9655 deletion completed in 6.101261463s

• [SLOW TEST:10.301 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:08:21.648: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-1222
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  4 21:08:21.801: INFO: Create a RollingUpdate DaemonSet
Dec  4 21:08:21.805: INFO: Check that daemon pods launch on every node of the cluster
Dec  4 21:08:21.810: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 21:08:21.813: INFO: Number of nodes with available pods: 0
Dec  4 21:08:21.813: INFO: Node alex-slot1-v2-vsp1-worker6c7da5d4d5 is running more than one daemon pod
Dec  4 21:08:22.817: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 21:08:22.819: INFO: Number of nodes with available pods: 0
Dec  4 21:08:22.819: INFO: Node alex-slot1-v2-vsp1-worker6c7da5d4d5 is running more than one daemon pod
Dec  4 21:08:23.818: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 21:08:23.821: INFO: Number of nodes with available pods: 1
Dec  4 21:08:23.821: INFO: Node alex-slot1-v2-vsp1-worker9f6c17cd60 is running more than one daemon pod
Dec  4 21:08:24.817: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 21:08:24.820: INFO: Number of nodes with available pods: 2
Dec  4 21:08:24.820: INFO: Number of running nodes: 2, number of available pods: 2
Dec  4 21:08:24.820: INFO: Update the DaemonSet to trigger a rollout
Dec  4 21:08:24.827: INFO: Updating DaemonSet daemon-set
Dec  4 21:08:34.838: INFO: Roll back the DaemonSet before rollout is complete
Dec  4 21:08:34.845: INFO: Updating DaemonSet daemon-set
Dec  4 21:08:34.845: INFO: Make sure DaemonSet rollback is complete
Dec  4 21:08:34.852: INFO: Wrong image for pod: daemon-set-xmz7z. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Dec  4 21:08:34.852: INFO: Pod daemon-set-xmz7z is not available
Dec  4 21:08:34.864: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 21:08:35.867: INFO: Wrong image for pod: daemon-set-xmz7z. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Dec  4 21:08:35.867: INFO: Pod daemon-set-xmz7z is not available
Dec  4 21:08:35.906: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 21:08:36.868: INFO: Wrong image for pod: daemon-set-xmz7z. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Dec  4 21:08:36.868: INFO: Pod daemon-set-xmz7z is not available
Dec  4 21:08:36.871: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 21:08:37.868: INFO: Pod daemon-set-kmrtw is not available
Dec  4 21:08:37.872: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1222, will wait for the garbage collector to delete the pods
Dec  4 21:08:37.947: INFO: Deleting DaemonSet.extensions daemon-set took: 17.654069ms
Dec  4 21:08:38.348: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.226715ms
Dec  4 21:08:39.951: INFO: Number of nodes with available pods: 0
Dec  4 21:08:39.951: INFO: Number of running nodes: 0, number of available pods: 0
Dec  4 21:08:39.952: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1222/daemonsets","resourceVersion":"283998"},"items":null}

Dec  4 21:08:39.954: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1222/pods","resourceVersion":"283998"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:08:39.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1222" for this suite.
Dec  4 21:08:45.978: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:08:46.057: INFO: namespace daemonsets-1222 deletion completed in 6.087998424s

• [SLOW TEST:24.409 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:08:46.059: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3670
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-9279
STEP: Creating secret with name secret-test-427617b8-16da-11ea-8695-527d496f91de
STEP: Creating a pod to test consume secrets
Dec  4 21:08:46.340: INFO: Waiting up to 5m0s for pod "pod-secrets-428abd43-16da-11ea-8695-527d496f91de" in namespace "secrets-3670" to be "success or failure"
Dec  4 21:08:46.349: INFO: Pod "pod-secrets-428abd43-16da-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 8.830652ms
Dec  4 21:08:48.352: INFO: Pod "pod-secrets-428abd43-16da-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0119633s
Dec  4 21:08:50.358: INFO: Pod "pod-secrets-428abd43-16da-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017918503s
STEP: Saw pod success
Dec  4 21:08:50.358: INFO: Pod "pod-secrets-428abd43-16da-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 21:08:50.360: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker6c7da5d4d5 pod pod-secrets-428abd43-16da-11ea-8695-527d496f91de container secret-volume-test: <nil>
STEP: delete the pod
Dec  4 21:08:50.375: INFO: Waiting for pod pod-secrets-428abd43-16da-11ea-8695-527d496f91de to disappear
Dec  4 21:08:50.380: INFO: Pod pod-secrets-428abd43-16da-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:08:50.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3670" for this suite.
Dec  4 21:08:56.395: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:08:56.471: INFO: namespace secrets-3670 deletion completed in 6.086364972s
STEP: Destroying namespace "secret-namespace-9279" for this suite.
Dec  4 21:09:02.480: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:09:02.552: INFO: namespace secret-namespace-9279 deletion completed in 6.081220723s

• [SLOW TEST:16.493 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:09:02.554: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3571
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting the proxy server
Dec  4 21:09:02.696: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-246239386 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:09:02.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3571" for this suite.
Dec  4 21:09:08.785: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:09:08.880: INFO: namespace kubectl-3571 deletion completed in 6.106876875s

• [SLOW TEST:6.326 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:09:08.880: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5229
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  4 21:09:09.024: INFO: Waiting up to 5m0s for pod "downwardapi-volume-50105d2f-16da-11ea-8695-527d496f91de" in namespace "downward-api-5229" to be "success or failure"
Dec  4 21:09:09.025: INFO: Pod "downwardapi-volume-50105d2f-16da-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 1.468086ms
Dec  4 21:09:11.028: INFO: Pod "downwardapi-volume-50105d2f-16da-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004092748s
Dec  4 21:09:13.031: INFO: Pod "downwardapi-volume-50105d2f-16da-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006843349s
STEP: Saw pod success
Dec  4 21:09:13.031: INFO: Pod "downwardapi-volume-50105d2f-16da-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 21:09:13.033: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker9f6c17cd60 pod downwardapi-volume-50105d2f-16da-11ea-8695-527d496f91de container client-container: <nil>
STEP: delete the pod
Dec  4 21:09:13.053: INFO: Waiting for pod downwardapi-volume-50105d2f-16da-11ea-8695-527d496f91de to disappear
Dec  4 21:09:13.057: INFO: Pod downwardapi-volume-50105d2f-16da-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:09:13.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5229" for this suite.
Dec  4 21:09:19.069: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:09:19.137: INFO: namespace downward-api-5229 deletion completed in 6.075900241s

• [SLOW TEST:10.257 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:09:19.139: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9122
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  4 21:09:19.277: INFO: Waiting up to 5m0s for pod "downwardapi-volume-562da436-16da-11ea-8695-527d496f91de" in namespace "projected-9122" to be "success or failure"
Dec  4 21:09:19.283: INFO: Pod "downwardapi-volume-562da436-16da-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 5.14267ms
Dec  4 21:09:21.286: INFO: Pod "downwardapi-volume-562da436-16da-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008151041s
STEP: Saw pod success
Dec  4 21:09:21.286: INFO: Pod "downwardapi-volume-562da436-16da-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 21:09:21.287: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker6c7da5d4d5 pod downwardapi-volume-562da436-16da-11ea-8695-527d496f91de container client-container: <nil>
STEP: delete the pod
Dec  4 21:09:21.307: INFO: Waiting for pod downwardapi-volume-562da436-16da-11ea-8695-527d496f91de to disappear
Dec  4 21:09:21.309: INFO: Pod downwardapi-volume-562da436-16da-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:09:21.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9122" for this suite.
Dec  4 21:09:27.320: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:09:27.394: INFO: namespace projected-9122 deletion completed in 6.081653955s

• [SLOW TEST:8.255 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:09:27.394: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-853
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-downwardapi-kn6v
STEP: Creating a pod to test atomic-volume-subpath
Dec  4 21:09:27.538: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-kn6v" in namespace "subpath-853" to be "success or failure"
Dec  4 21:09:27.544: INFO: Pod "pod-subpath-test-downwardapi-kn6v": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012915ms
Dec  4 21:09:29.546: INFO: Pod "pod-subpath-test-downwardapi-kn6v": Phase="Running", Reason="", readiness=true. Elapsed: 2.008443607s
Dec  4 21:09:31.552: INFO: Pod "pod-subpath-test-downwardapi-kn6v": Phase="Running", Reason="", readiness=true. Elapsed: 4.014163408s
Dec  4 21:09:33.555: INFO: Pod "pod-subpath-test-downwardapi-kn6v": Phase="Running", Reason="", readiness=true. Elapsed: 6.017032225s
Dec  4 21:09:35.558: INFO: Pod "pod-subpath-test-downwardapi-kn6v": Phase="Running", Reason="", readiness=true. Elapsed: 8.020270544s
Dec  4 21:09:37.561: INFO: Pod "pod-subpath-test-downwardapi-kn6v": Phase="Running", Reason="", readiness=true. Elapsed: 10.022913044s
Dec  4 21:09:39.564: INFO: Pod "pod-subpath-test-downwardapi-kn6v": Phase="Running", Reason="", readiness=true. Elapsed: 12.025570618s
Dec  4 21:09:41.566: INFO: Pod "pod-subpath-test-downwardapi-kn6v": Phase="Running", Reason="", readiness=true. Elapsed: 14.028269736s
Dec  4 21:09:43.569: INFO: Pod "pod-subpath-test-downwardapi-kn6v": Phase="Running", Reason="", readiness=true. Elapsed: 16.031094357s
Dec  4 21:09:45.572: INFO: Pod "pod-subpath-test-downwardapi-kn6v": Phase="Running", Reason="", readiness=true. Elapsed: 18.033813907s
Dec  4 21:09:47.575: INFO: Pod "pod-subpath-test-downwardapi-kn6v": Phase="Running", Reason="", readiness=true. Elapsed: 20.037406218s
Dec  4 21:09:49.578: INFO: Pod "pod-subpath-test-downwardapi-kn6v": Phase="Running", Reason="", readiness=true. Elapsed: 22.039901841s
Dec  4 21:09:51.581: INFO: Pod "pod-subpath-test-downwardapi-kn6v": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.042758147s
STEP: Saw pod success
Dec  4 21:09:51.581: INFO: Pod "pod-subpath-test-downwardapi-kn6v" satisfied condition "success or failure"
Dec  4 21:09:51.583: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker9f6c17cd60 pod pod-subpath-test-downwardapi-kn6v container test-container-subpath-downwardapi-kn6v: <nil>
STEP: delete the pod
Dec  4 21:09:51.602: INFO: Waiting for pod pod-subpath-test-downwardapi-kn6v to disappear
Dec  4 21:09:51.605: INFO: Pod pod-subpath-test-downwardapi-kn6v no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-kn6v
Dec  4 21:09:51.605: INFO: Deleting pod "pod-subpath-test-downwardapi-kn6v" in namespace "subpath-853"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:09:51.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-853" for this suite.
Dec  4 21:09:57.620: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:09:57.698: INFO: namespace subpath-853 deletion completed in 6.086978942s

• [SLOW TEST:30.304 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:09:57.702: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8401
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  4 21:09:57.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 create -f - --namespace=kubectl-8401'
Dec  4 21:09:58.040: INFO: stderr: ""
Dec  4 21:09:58.040: INFO: stdout: "replicationcontroller/redis-master created\n"
Dec  4 21:09:58.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 create -f - --namespace=kubectl-8401'
Dec  4 21:09:58.249: INFO: stderr: ""
Dec  4 21:09:58.249: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec  4 21:09:59.252: INFO: Selector matched 1 pods for map[app:redis]
Dec  4 21:09:59.252: INFO: Found 0 / 1
Dec  4 21:10:00.252: INFO: Selector matched 1 pods for map[app:redis]
Dec  4 21:10:00.252: INFO: Found 1 / 1
Dec  4 21:10:00.252: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec  4 21:10:00.254: INFO: Selector matched 1 pods for map[app:redis]
Dec  4 21:10:00.254: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec  4 21:10:00.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 describe pod redis-master-q85nv --namespace=kubectl-8401'
Dec  4 21:10:00.349: INFO: stderr: ""
Dec  4 21:10:00.349: INFO: stdout: "Name:               redis-master-q85nv\nNamespace:          kubectl-8401\nPriority:           0\nPriorityClassName:  <none>\nNode:               alex-slot1-v2-vsp1-worker6c7da5d4d5/10.10.128.23\nStart Time:         Wed, 04 Dec 2019 21:10:02 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        cni.projectcalico.org/podIP: 192.168.2.68/32\nStatus:             Running\nIP:                 192.168.2.68\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://e41a46e5a993f74d8bb87814c53a0f645c7b42ae7678810b043852433a584492\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 04 Dec 2019 21:10:03 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-xphff (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-xphff:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-xphff\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age        From                                          Message\n  ----    ------     ----       ----                                          -------\n  Normal  Scheduled  2s         default-scheduler                             Successfully assigned kubectl-8401/redis-master-q85nv to alex-slot1-v2-vsp1-worker6c7da5d4d5\n  Normal  Pulled     <invalid>  kubelet, alex-slot1-v2-vsp1-worker6c7da5d4d5  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    <invalid>  kubelet, alex-slot1-v2-vsp1-worker6c7da5d4d5  Created container redis-master\n  Normal  Started    <invalid>  kubelet, alex-slot1-v2-vsp1-worker6c7da5d4d5  Started container redis-master\n"
Dec  4 21:10:00.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 describe rc redis-master --namespace=kubectl-8401'
Dec  4 21:10:00.449: INFO: stderr: ""
Dec  4 21:10:00.449: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-8401\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: redis-master-q85nv\n"
Dec  4 21:10:00.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 describe service redis-master --namespace=kubectl-8401'
Dec  4 21:10:00.547: INFO: stderr: ""
Dec  4 21:10:00.547: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-8401\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.108.162.25\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         192.168.2.68:6379\nSession Affinity:  None\nEvents:            <none>\n"
Dec  4 21:10:00.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 describe node alex-slot1-v2-vsp1-masterb17a9fb4bd'
Dec  4 21:10:00.660: INFO: stderr: ""
Dec  4 21:10:00.660: INFO: stdout: "Name:               alex-slot1-v2-vsp1-masterb17a9fb4bd\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=alex-slot1-v2-vsp1-masterb17a9fb4bd\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.10.128.21/22\n                    projectcalico.org/IPv4IPIPTunnelAddr: 192.168.47.0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 02 Dec 2019 22:28:27 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 02 Dec 2019 22:28:50 +0000   Mon, 02 Dec 2019 22:28:50 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Wed, 04 Dec 2019 21:09:44 +0000   Mon, 02 Dec 2019 22:28:23 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 04 Dec 2019 21:09:44 +0000   Mon, 02 Dec 2019 22:28:23 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 04 Dec 2019 21:09:44 +0000   Mon, 02 Dec 2019 22:28:23 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 04 Dec 2019 21:09:44 +0000   Mon, 02 Dec 2019 22:28:58 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  ExternalIP:  10.10.128.24\n  InternalIP:  10.10.128.24\n  Hostname:    alex-slot1-v2-vsp1-masterb17a9fb4bd\nCapacity:\n cpu:                2\n ephemeral-storage:  40470732Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             16426416Ki\n pods:               110\nAllocatable:\n cpu:                2\n ephemeral-storage:  37297826550\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             16324016Ki\n pods:               110\nSystem Info:\n Machine ID:                 0edbc2bc2f22478182eb509cb93c5eb7\n System UUID:                4215B317-53D3-892D-E3BE-30801687454A\n Boot ID:                    162c02d8-f7b5-44d2-8b50-7ea61c7e8281\n Kernel Version:             4.15.0-64-generic\n OS Image:                   Ubuntu 18.04.3 LTS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.9.9\n Kubelet Version:            v1.14.8\n Kube-Proxy Version:         v1.14.8\nPodCIDR:                     192.168.0.0/24\nProviderID:                  vsphere://4215b317-53d3-892d-e3be-30801687454a\nNon-terminated Pods:         (10 in total)\n  Namespace                  Name                                                           CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                           ------------  ----------  ---------------  -------------  ---\n  kube-system                calico-kube-controllers-65fcb54bcb-gg957                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         46h\n  kube-system                calico-node-rr5c9                                              250m (12%)    0 (0%)      0 (0%)           0 (0%)         46h\n  kube-system                etcd-alex-slot1-v2-vsp1-masterb17a9fb4bd                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         46h\n  kube-system                kube-apiserver-alex-slot1-v2-vsp1-masterb17a9fb4bd             250m (12%)    0 (0%)      0 (0%)           0 (0%)         46h\n  kube-system                kube-controller-manager-alex-slot1-v2-vsp1-masterb17a9fb4bd    200m (10%)    0 (0%)      0 (0%)           0 (0%)         46h\n  kube-system                kube-proxy-crtq2                                               0 (0%)        0 (0%)      0 (0%)           0 (0%)         46h\n  kube-system                kube-scheduler-alex-slot1-v2-vsp1-masterb17a9fb4bd             100m (5%)     0 (0%)      0 (0%)           0 (0%)         46h\n  kube-system                tiller-deploy-6c98d5b756-dncz4                                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         46h\n  sonobuoy                   sonobuoy-e2e-job-67652fdf7b7b4c3b                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         37m\n  sonobuoy                   sonobuoy-systemd-logs-daemon-set-f8654487368d4bd0-2nq5q        0 (0%)        0 (0%)      0 (0%)           0 (0%)         37m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                800m (40%)  0 (0%)\n  memory             0 (0%)      0 (0%)\n  ephemeral-storage  0 (0%)      0 (0%)\nEvents:              <none>\n"
Dec  4 21:10:00.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 describe namespace kubectl-8401'
Dec  4 21:10:00.748: INFO: stderr: ""
Dec  4 21:10:00.748: INFO: stdout: "Name:         kubectl-8401\nLabels:       e2e-framework=kubectl\n              e2e-run=56e9ef6b-16d5-11ea-8695-527d496f91de\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:10:00.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8401" for this suite.
Dec  4 21:10:22.762: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:10:22.839: INFO: namespace kubectl-8401 deletion completed in 22.086780794s

• [SLOW TEST:25.138 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:10:22.841: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-8123
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec  4 21:10:29.027: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  4 21:10:29.030: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  4 21:10:31.031: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  4 21:10:31.033: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  4 21:10:33.031: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  4 21:10:33.034: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  4 21:10:35.031: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  4 21:10:35.034: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  4 21:10:37.031: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  4 21:10:37.034: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  4 21:10:39.031: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  4 21:10:39.033: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  4 21:10:41.032: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  4 21:10:41.036: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  4 21:10:43.031: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  4 21:10:43.034: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  4 21:10:45.031: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  4 21:10:45.034: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  4 21:10:47.031: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  4 21:10:47.034: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  4 21:10:49.031: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  4 21:10:49.034: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  4 21:10:51.031: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  4 21:10:51.034: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  4 21:10:53.031: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  4 21:10:53.034: INFO: Pod pod-with-poststart-exec-hook still exists
Dec  4 21:10:55.031: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec  4 21:10:55.034: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:10:55.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8123" for this suite.
Dec  4 21:11:17.045: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:11:17.132: INFO: namespace container-lifecycle-hook-8123 deletion completed in 22.094780361s

• [SLOW TEST:54.291 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:11:17.134: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1997
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  4 21:11:17.276: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9c822a7e-16da-11ea-8695-527d496f91de" in namespace "downward-api-1997" to be "success or failure"
Dec  4 21:11:17.281: INFO: Pod "downwardapi-volume-9c822a7e-16da-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 4.744891ms
Dec  4 21:11:19.287: INFO: Pod "downwardapi-volume-9c822a7e-16da-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010677332s
STEP: Saw pod success
Dec  4 21:11:19.287: INFO: Pod "downwardapi-volume-9c822a7e-16da-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 21:11:19.289: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker9f6c17cd60 pod downwardapi-volume-9c822a7e-16da-11ea-8695-527d496f91de container client-container: <nil>
STEP: delete the pod
Dec  4 21:11:19.307: INFO: Waiting for pod downwardapi-volume-9c822a7e-16da-11ea-8695-527d496f91de to disappear
Dec  4 21:11:19.309: INFO: Pod downwardapi-volume-9c822a7e-16da-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:11:19.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1997" for this suite.
Dec  4 21:11:25.321: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:11:25.393: INFO: namespace downward-api-1997 deletion completed in 6.080896027s

• [SLOW TEST:8.260 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:11:25.395: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2454
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-a16ed633-16da-11ea-8695-527d496f91de
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:11:27.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2454" for this suite.
Dec  4 21:11:49.570: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:11:49.641: INFO: namespace configmap-2454 deletion completed in 22.078850711s

• [SLOW TEST:24.247 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:11:49.643: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6412
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-6412/configmap-test-afe2625c-16da-11ea-8695-527d496f91de
STEP: Creating a pod to test consume configMaps
Dec  4 21:11:49.788: INFO: Waiting up to 5m0s for pod "pod-configmaps-afe310b0-16da-11ea-8695-527d496f91de" in namespace "configmap-6412" to be "success or failure"
Dec  4 21:11:49.796: INFO: Pod "pod-configmaps-afe310b0-16da-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 8.367475ms
Dec  4 21:11:51.800: INFO: Pod "pod-configmaps-afe310b0-16da-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011954595s
Dec  4 21:11:53.803: INFO: Pod "pod-configmaps-afe310b0-16da-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015059045s
STEP: Saw pod success
Dec  4 21:11:53.803: INFO: Pod "pod-configmaps-afe310b0-16da-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 21:11:53.805: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker9f6c17cd60 pod pod-configmaps-afe310b0-16da-11ea-8695-527d496f91de container env-test: <nil>
STEP: delete the pod
Dec  4 21:11:53.824: INFO: Waiting for pod pod-configmaps-afe310b0-16da-11ea-8695-527d496f91de to disappear
Dec  4 21:11:53.826: INFO: Pod pod-configmaps-afe310b0-16da-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:11:53.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6412" for this suite.
Dec  4 21:11:59.840: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:11:59.932: INFO: namespace configmap-6412 deletion completed in 6.102523315s

• [SLOW TEST:10.289 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:11:59.932: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-6584
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating server pod server in namespace prestop-6584
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-6584
STEP: Deleting pre-stop pod
Dec  4 21:12:13.121: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:12:13.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-6584" for this suite.
Dec  4 21:12:51.143: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:12:51.215: INFO: namespace prestop-6584 deletion completed in 38.083363345s

• [SLOW TEST:51.283 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:12:51.223: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1173
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  4 21:12:51.418: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d49f2375-16da-11ea-8695-527d496f91de" in namespace "projected-1173" to be "success or failure"
Dec  4 21:12:51.426: INFO: Pod "downwardapi-volume-d49f2375-16da-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 8.193783ms
Dec  4 21:12:53.429: INFO: Pod "downwardapi-volume-d49f2375-16da-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011047813s
Dec  4 21:12:55.432: INFO: Pod "downwardapi-volume-d49f2375-16da-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014368042s
STEP: Saw pod success
Dec  4 21:12:55.433: INFO: Pod "downwardapi-volume-d49f2375-16da-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 21:12:55.435: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker6c7da5d4d5 pod downwardapi-volume-d49f2375-16da-11ea-8695-527d496f91de container client-container: <nil>
STEP: delete the pod
Dec  4 21:12:55.459: INFO: Waiting for pod downwardapi-volume-d49f2375-16da-11ea-8695-527d496f91de to disappear
Dec  4 21:12:55.461: INFO: Pod downwardapi-volume-d49f2375-16da-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:12:55.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1173" for this suite.
Dec  4 21:13:01.480: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:13:01.605: INFO: namespace projected-1173 deletion completed in 6.139878727s

• [SLOW TEST:10.382 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:13:01.605: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9018
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating cluster-info
Dec  4 21:13:01.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 cluster-info'
Dec  4 21:13:01.827: INFO: stderr: ""
Dec  4 21:13:01.828: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:13:01.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9018" for this suite.
Dec  4 21:13:07.841: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:13:07.909: INFO: namespace kubectl-9018 deletion completed in 6.076523554s

• [SLOW TEST:6.303 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:13:07.910: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3645
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:163
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Dec  4 21:13:08.042: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:13:24.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3645" for this suite.
Dec  4 21:13:30.507: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:13:30.577: INFO: namespace pods-3645 deletion completed in 6.078810098s

• [SLOW TEST:22.667 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:13:30.579: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-450
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  4 21:13:30.714: INFO: Creating deployment "nginx-deployment"
Dec  4 21:13:30.720: INFO: Waiting for observed generation 1
Dec  4 21:13:32.730: INFO: Waiting for all required pods to come up
Dec  4 21:13:32.738: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Dec  4 21:13:34.757: INFO: Waiting for deployment "nginx-deployment" to complete
Dec  4 21:13:34.762: INFO: Updating deployment "nginx-deployment" with a non-existent image
Dec  4 21:13:34.770: INFO: Updating deployment nginx-deployment
Dec  4 21:13:34.770: INFO: Waiting for observed generation 2
Dec  4 21:13:36.776: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Dec  4 21:13:36.778: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Dec  4 21:13:36.779: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Dec  4 21:13:36.790: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Dec  4 21:13:36.790: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Dec  4 21:13:36.792: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Dec  4 21:13:36.796: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Dec  4 21:13:36.796: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Dec  4 21:13:36.804: INFO: Updating deployment nginx-deployment
Dec  4 21:13:36.805: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Dec  4 21:13:36.826: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Dec  4 21:13:38.858: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Dec  4 21:13:38.862: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-450,SelfLink:/apis/apps/v1/namespaces/deployment-450/deployments/nginx-deployment,UID:ec0cbe62-16da-11ea-825c-005056950656,ResourceVersion:285235,Generation:3,CreationTimestamp:2019-12-04 21:13:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-12-04 21:13:36 +0000 UTC 2019-12-04 21:13:36 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-12-04 21:13:37 +0000 UTC 2019-12-04 21:13:30 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-b79c9d74d" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Dec  4 21:13:38.864: INFO: New ReplicaSet "nginx-deployment-b79c9d74d" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d,GenerateName:,Namespace:deployment-450,SelfLink:/apis/apps/v1/namespaces/deployment-450/replicasets/nginx-deployment-b79c9d74d,UID:ee77371d-16da-11ea-825c-005056950656,ResourceVersion:285229,Generation:3,CreationTimestamp:2019-12-04 21:13:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment ec0cbe62-16da-11ea-825c-005056950656 0xc0018894f7 0xc0018894f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  4 21:13:38.864: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Dec  4 21:13:38.864: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5,GenerateName:,Namespace:deployment-450,SelfLink:/apis/apps/v1/namespaces/deployment-450/replicasets/nginx-deployment-85db8c99c5,UID:ec0d4cb9-16da-11ea-825c-005056950656,ResourceVersion:285223,Generation:3,CreationTimestamp:2019-12-04 21:13:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment ec0cbe62-16da-11ea-825c-005056950656 0xc001889427 0xc001889428}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Dec  4 21:13:38.869: INFO: Pod "nginx-deployment-85db8c99c5-2twjl" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-2twjl,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-450,SelfLink:/api/v1/namespaces/deployment-450/pods/nginx-deployment-85db8c99c5-2twjl,UID:ec0feb5c-16da-11ea-825c-005056950656,ResourceVersion:285084,Generation:0,CreationTimestamp:2019-12-04 21:13:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.1.83/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 ec0d4cb9-16da-11ea-825c-005056950656 0xc001889e87 0xc001889e88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qqctn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qqctn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qqctn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v2-vsp1-worker9f6c17cd60,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001889f00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001889f20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:29 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:32 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:32 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:30 +0000 UTC  }],Message:,Reason:,HostIP:10.10.128.22,PodIP:192.168.1.83,StartTime:2019-12-04 21:13:29 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-04 21:13:31 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://2d820c61e67ac575405464cb9811bafb2b72259a186d0215e17706ced03090e1}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  4 21:13:38.869: INFO: Pod "nginx-deployment-85db8c99c5-7c4qm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-7c4qm,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-450,SelfLink:/api/v1/namespaces/deployment-450/pods/nginx-deployment-85db8c99c5-7c4qm,UID:efb16573-16da-11ea-825c-005056950656,ResourceVersion:285294,Generation:0,CreationTimestamp:2019-12-04 21:13:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.2.83/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 ec0d4cb9-16da-11ea-825c-005056950656 0xc000754017 0xc000754018}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qqctn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qqctn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qqctn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v2-vsp1-worker6c7da5d4d5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000754080} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0007540a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:36 +0000 UTC  }],Message:,Reason:,HostIP:10.10.128.23,PodIP:,StartTime:2019-12-04 21:13:41 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  4 21:13:38.870: INFO: Pod "nginx-deployment-85db8c99c5-7pszv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-7pszv,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-450,SelfLink:/api/v1/namespaces/deployment-450/pods/nginx-deployment-85db8c99c5-7pszv,UID:efbbed7b-16da-11ea-825c-005056950656,ResourceVersion:285259,Generation:0,CreationTimestamp:2019-12-04 21:13:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 ec0d4cb9-16da-11ea-825c-005056950656 0xc000754167 0xc000754168}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qqctn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qqctn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qqctn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v2-vsp1-worker6c7da5d4d5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0007541d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0007541f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:36 +0000 UTC  }],Message:,Reason:,HostIP:10.10.128.23,PodIP:,StartTime:2019-12-04 21:13:41 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  4 21:13:38.870: INFO: Pod "nginx-deployment-85db8c99c5-87b25" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-87b25,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-450,SelfLink:/api/v1/namespaces/deployment-450/pods/nginx-deployment-85db8c99c5-87b25,UID:efb149f9-16da-11ea-825c-005056950656,ResourceVersion:285285,Generation:0,CreationTimestamp:2019-12-04 21:13:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.2.81/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 ec0d4cb9-16da-11ea-825c-005056950656 0xc0007542e7 0xc0007542e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qqctn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qqctn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qqctn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v2-vsp1-worker6c7da5d4d5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000754360} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0007543d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:36 +0000 UTC  }],Message:,Reason:,HostIP:10.10.128.23,PodIP:,StartTime:2019-12-04 21:13:41 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  4 21:13:38.870: INFO: Pod "nginx-deployment-85db8c99c5-8jwll" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-8jwll,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-450,SelfLink:/api/v1/namespaces/deployment-450/pods/nginx-deployment-85db8c99c5-8jwll,UID:ec121e10-16da-11ea-825c-005056950656,ResourceVersion:285087,Generation:0,CreationTimestamp:2019-12-04 21:13:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.1.85/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 ec0d4cb9-16da-11ea-825c-005056950656 0xc0007545f7 0xc0007545f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qqctn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qqctn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qqctn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v2-vsp1-worker9f6c17cd60,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000754720} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000754740}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:29 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:32 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:32 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:30 +0000 UTC  }],Message:,Reason:,HostIP:10.10.128.22,PodIP:192.168.1.85,StartTime:2019-12-04 21:13:29 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-04 21:13:32 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://ed9f47f17993cbed1e324e79e0228dbbf0d539dba2d7738bd60db35ef1e7bdb4}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  4 21:13:38.870: INFO: Pod "nginx-deployment-85db8c99c5-bfgjz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-bfgjz,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-450,SelfLink:/api/v1/namespaces/deployment-450/pods/nginx-deployment-85db8c99c5-bfgjz,UID:efbbcfd7-16da-11ea-825c-005056950656,ResourceVersion:285273,Generation:0,CreationTimestamp:2019-12-04 21:13:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 ec0d4cb9-16da-11ea-825c-005056950656 0xc000754987 0xc000754988}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qqctn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qqctn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qqctn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v2-vsp1-worker9f6c17cd60,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000754b40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000754b60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:36 +0000 UTC  }],Message:,Reason:,HostIP:10.10.128.22,PodIP:,StartTime:2019-12-04 21:13:36 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  4 21:13:38.871: INFO: Pod "nginx-deployment-85db8c99c5-cj2kp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-cj2kp,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-450,SelfLink:/api/v1/namespaces/deployment-450/pods/nginx-deployment-85db8c99c5-cj2kp,UID:efbbf6f4-16da-11ea-825c-005056950656,ResourceVersion:285256,Generation:0,CreationTimestamp:2019-12-04 21:13:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 ec0d4cb9-16da-11ea-825c-005056950656 0xc000754cb7 0xc000754cb8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qqctn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qqctn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qqctn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v2-vsp1-worker9f6c17cd60,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000754d20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000754d40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:36 +0000 UTC  }],Message:,Reason:,HostIP:10.10.128.22,PodIP:,StartTime:2019-12-04 21:13:35 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  4 21:13:38.872: INFO: Pod "nginx-deployment-85db8c99c5-gdjxm" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-gdjxm,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-450,SelfLink:/api/v1/namespaces/deployment-450/pods/nginx-deployment-85db8c99c5-gdjxm,UID:ec16e700-16da-11ea-825c-005056950656,ResourceVersion:285081,Generation:0,CreationTimestamp:2019-12-04 21:13:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.1.84/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 ec0d4cb9-16da-11ea-825c-005056950656 0xc000754e27 0xc000754e28}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qqctn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qqctn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qqctn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v2-vsp1-worker9f6c17cd60,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000754eb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000754ed0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:29 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:32 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:32 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:30 +0000 UTC  }],Message:,Reason:,HostIP:10.10.128.22,PodIP:192.168.1.84,StartTime:2019-12-04 21:13:29 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-04 21:13:32 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://255b7fe74c120e72dc869212288d3c7bc004434a2b9ca960f7daf290ac772e69}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  4 21:13:38.872: INFO: Pod "nginx-deployment-85db8c99c5-ggf5g" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-ggf5g,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-450,SelfLink:/api/v1/namespaces/deployment-450/pods/nginx-deployment-85db8c99c5-ggf5g,UID:ec1249a7-16da-11ea-825c-005056950656,ResourceVersion:285049,Generation:0,CreationTimestamp:2019-12-04 21:13:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.1.82/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 ec0d4cb9-16da-11ea-825c-005056950656 0xc000754fc7 0xc000754fc8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qqctn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qqctn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qqctn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v2-vsp1-worker9f6c17cd60,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000755030} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000755050}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:29 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:31 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:31 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:30 +0000 UTC  }],Message:,Reason:,HostIP:10.10.128.22,PodIP:192.168.1.82,StartTime:2019-12-04 21:13:29 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-04 21:13:31 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://4dab1a0d08f3cf01ea49c1286f87bee2ea17912de0bae48f4ff0225880d4d12f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  4 21:13:38.873: INFO: Pod "nginx-deployment-85db8c99c5-jrkbl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-jrkbl,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-450,SelfLink:/api/v1/namespaces/deployment-450/pods/nginx-deployment-85db8c99c5-jrkbl,UID:efb7c09a-16da-11ea-825c-005056950656,ResourceVersion:285302,Generation:0,CreationTimestamp:2019-12-04 21:13:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 ec0d4cb9-16da-11ea-825c-005056950656 0xc000755177 0xc000755178}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qqctn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qqctn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qqctn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v2-vsp1-worker6c7da5d4d5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000755200} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000755220}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:36 +0000 UTC  }],Message:,Reason:,HostIP:10.10.128.23,PodIP:,StartTime:2019-12-04 21:13:41 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  4 21:13:38.873: INFO: Pod "nginx-deployment-85db8c99c5-lr8tj" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-lr8tj,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-450,SelfLink:/api/v1/namespaces/deployment-450/pods/nginx-deployment-85db8c99c5-lr8tj,UID:ec0e93f3-16da-11ea-825c-005056950656,ResourceVersion:285036,Generation:0,CreationTimestamp:2019-12-04 21:13:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.2.72/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 ec0d4cb9-16da-11ea-825c-005056950656 0xc0007552f7 0xc0007552f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qqctn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qqctn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qqctn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v2-vsp1-worker6c7da5d4d5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000755360} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000755380}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:35 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:36 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:36 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:30 +0000 UTC  }],Message:,Reason:,HostIP:10.10.128.23,PodIP:192.168.2.72,StartTime:2019-12-04 21:13:35 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-04 21:13:36 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://f63c08e8a5a18bdd6eb821a36ab78eb2cadb13080d3a1d80c714abb3aeddf0dd}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  4 21:13:38.873: INFO: Pod "nginx-deployment-85db8c99c5-mqwj2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-mqwj2,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-450,SelfLink:/api/v1/namespaces/deployment-450/pods/nginx-deployment-85db8c99c5-mqwj2,UID:efb7d244-16da-11ea-825c-005056950656,ResourceVersion:285252,Generation:0,CreationTimestamp:2019-12-04 21:13:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 ec0d4cb9-16da-11ea-825c-005056950656 0xc000755457 0xc000755458}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qqctn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qqctn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qqctn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v2-vsp1-worker9f6c17cd60,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0007554c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0007554e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:36 +0000 UTC  }],Message:,Reason:,HostIP:10.10.128.22,PodIP:,StartTime:2019-12-04 21:13:35 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  4 21:13:38.874: INFO: Pod "nginx-deployment-85db8c99c5-ngrhx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-ngrhx,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-450,SelfLink:/api/v1/namespaces/deployment-450/pods/nginx-deployment-85db8c99c5-ngrhx,UID:efae6626-16da-11ea-825c-005056950656,ResourceVersion:285284,Generation:0,CreationTimestamp:2019-12-04 21:13:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.2.80/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 ec0d4cb9-16da-11ea-825c-005056950656 0xc0007555b7 0xc0007555b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qqctn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qqctn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qqctn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v2-vsp1-worker6c7da5d4d5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000755630} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000755650}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:36 +0000 UTC  }],Message:,Reason:,HostIP:10.10.128.23,PodIP:,StartTime:2019-12-04 21:13:41 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  4 21:13:38.878: INFO: Pod "nginx-deployment-85db8c99c5-q7jbv" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-q7jbv,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-450,SelfLink:/api/v1/namespaces/deployment-450/pods/nginx-deployment-85db8c99c5-q7jbv,UID:ec16f4fd-16da-11ea-825c-005056950656,ResourceVersion:285090,Generation:0,CreationTimestamp:2019-12-04 21:13:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.1.86/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 ec0d4cb9-16da-11ea-825c-005056950656 0xc000755737 0xc000755738}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qqctn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qqctn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qqctn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v2-vsp1-worker9f6c17cd60,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0007557a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0007557c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:29 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:32 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:32 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:30 +0000 UTC  }],Message:,Reason:,HostIP:10.10.128.22,PodIP:192.168.1.86,StartTime:2019-12-04 21:13:29 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-04 21:13:32 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://8bb6ab81b6e7939d8c3abdef3e1880a6dc86be154e37cc799d46a1e80339e0fb}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  4 21:13:38.878: INFO: Pod "nginx-deployment-85db8c99c5-rbhgl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-rbhgl,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-450,SelfLink:/api/v1/namespaces/deployment-450/pods/nginx-deployment-85db8c99c5-rbhgl,UID:efbbf813-16da-11ea-825c-005056950656,ResourceVersion:285291,Generation:0,CreationTimestamp:2019-12-04 21:13:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 ec0d4cb9-16da-11ea-825c-005056950656 0xc000755897 0xc000755898}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qqctn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qqctn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qqctn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v2-vsp1-worker6c7da5d4d5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000755900} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000755920}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:36 +0000 UTC  }],Message:,Reason:,HostIP:10.10.128.23,PodIP:,StartTime:2019-12-04 21:13:41 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  4 21:13:38.879: INFO: Pod "nginx-deployment-85db8c99c5-s9h8h" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-s9h8h,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-450,SelfLink:/api/v1/namespaces/deployment-450/pods/nginx-deployment-85db8c99c5-s9h8h,UID:efb6e800-16da-11ea-825c-005056950656,ResourceVersion:285283,Generation:0,CreationTimestamp:2019-12-04 21:13:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 ec0d4cb9-16da-11ea-825c-005056950656 0xc0007559e7 0xc0007559e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qqctn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qqctn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qqctn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v2-vsp1-worker6c7da5d4d5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000755a70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000755a90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:36 +0000 UTC  }],Message:,Reason:,HostIP:10.10.128.23,PodIP:,StartTime:2019-12-04 21:13:41 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  4 21:13:38.879: INFO: Pod "nginx-deployment-85db8c99c5-sx8n7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-sx8n7,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-450,SelfLink:/api/v1/namespaces/deployment-450/pods/nginx-deployment-85db8c99c5-sx8n7,UID:efbbe566-16da-11ea-825c-005056950656,ResourceVersion:285292,Generation:0,CreationTimestamp:2019-12-04 21:13:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 ec0d4cb9-16da-11ea-825c-005056950656 0xc000755b67 0xc000755b68}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qqctn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qqctn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qqctn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v2-vsp1-worker9f6c17cd60,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000755bd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000755bf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:36 +0000 UTC  }],Message:,Reason:,HostIP:10.10.128.22,PodIP:,StartTime:2019-12-04 21:13:36 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  4 21:13:38.879: INFO: Pod "nginx-deployment-85db8c99c5-vw6hk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-vw6hk,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-450,SelfLink:/api/v1/namespaces/deployment-450/pods/nginx-deployment-85db8c99c5-vw6hk,UID:efb6fdfc-16da-11ea-825c-005056950656,ResourceVersion:285264,Generation:0,CreationTimestamp:2019-12-04 21:13:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 ec0d4cb9-16da-11ea-825c-005056950656 0xc000755cd7 0xc000755cd8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qqctn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qqctn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qqctn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v2-vsp1-worker9f6c17cd60,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000755d60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000755d90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:36 +0000 UTC  }],Message:,Reason:,HostIP:10.10.128.22,PodIP:,StartTime:2019-12-04 21:13:36 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  4 21:13:38.879: INFO: Pod "nginx-deployment-85db8c99c5-w4bht" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-w4bht,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-450,SelfLink:/api/v1/namespaces/deployment-450/pods/nginx-deployment-85db8c99c5-w4bht,UID:ec1253b2-16da-11ea-825c-005056950656,ResourceVersion:285075,Generation:0,CreationTimestamp:2019-12-04 21:13:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.2.76/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 ec0d4cb9-16da-11ea-825c-005056950656 0xc000755eb7 0xc000755eb8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qqctn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qqctn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qqctn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v2-vsp1-worker6c7da5d4d5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000755f20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000755f40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:35 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:37 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:37 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:30 +0000 UTC  }],Message:,Reason:,HostIP:10.10.128.23,PodIP:192.168.2.76,StartTime:2019-12-04 21:13:35 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-04 21:13:37 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://759a3e8d4b55b7e4535b16f55baf6b66dcb9ced35d2ab83990e1e8b2eeea69ee}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  4 21:13:38.880: INFO: Pod "nginx-deployment-85db8c99c5-wkrbg" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-85db8c99c5-wkrbg,GenerateName:nginx-deployment-85db8c99c5-,Namespace:deployment-450,SelfLink:/api/v1/namespaces/deployment-450/pods/nginx-deployment-85db8c99c5-wkrbg,UID:ec1004e9-16da-11ea-825c-005056950656,ResourceVersion:285069,Generation:0,CreationTimestamp:2019-12-04 21:13:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 85db8c99c5,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.2.73/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-85db8c99c5 ec0d4cb9-16da-11ea-825c-005056950656 0xc00240e107 0xc00240e108}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qqctn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qqctn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qqctn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v2-vsp1-worker6c7da5d4d5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00240e170} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00240e190}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:35 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:37 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:37 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:30 +0000 UTC  }],Message:,Reason:,HostIP:10.10.128.23,PodIP:192.168.2.73,StartTime:2019-12-04 21:13:35 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-04 21:13:36 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://c66981b8ad518a5faa34656b59b1fd30790e224db2d8569f2eb545e65e51ad49}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  4 21:13:38.880: INFO: Pod "nginx-deployment-b79c9d74d-5rjll" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-5rjll,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-450,SelfLink:/api/v1/namespaces/deployment-450/pods/nginx-deployment-b79c9d74d-5rjll,UID:efb65d27-16da-11ea-825c-005056950656,ResourceVersion:285251,Generation:0,CreationTimestamp:2019-12-04 21:13:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d ee77371d-16da-11ea-825c-005056950656 0xc00240e277 0xc00240e278}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qqctn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qqctn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qqctn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v2-vsp1-worker6c7da5d4d5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00240e2e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00240e300}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:36 +0000 UTC  }],Message:,Reason:,HostIP:10.10.128.23,PodIP:,StartTime:2019-12-04 21:13:41 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  4 21:13:38.880: INFO: Pod "nginx-deployment-b79c9d74d-65xp2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-65xp2,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-450,SelfLink:/api/v1/namespaces/deployment-450/pods/nginx-deployment-b79c9d74d-65xp2,UID:efb0ee5e-16da-11ea-825c-005056950656,ResourceVersion:285290,Generation:0,CreationTimestamp:2019-12-04 21:13:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.2.82/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d ee77371d-16da-11ea-825c-005056950656 0xc00240e3e0 0xc00240e3e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qqctn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qqctn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qqctn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v2-vsp1-worker6c7da5d4d5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00240e450} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00240e480}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:36 +0000 UTC  }],Message:,Reason:,HostIP:10.10.128.23,PodIP:,StartTime:2019-12-04 21:13:41 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  4 21:13:38.881: INFO: Pod "nginx-deployment-b79c9d74d-8jnhl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-8jnhl,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-450,SelfLink:/api/v1/namespaces/deployment-450/pods/nginx-deployment-b79c9d74d-8jnhl,UID:ee7831dd-16da-11ea-825c-005056950656,ResourceVersion:285147,Generation:0,CreationTimestamp:2019-12-04 21:13:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.2.77/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d ee77371d-16da-11ea-825c-005056950656 0xc00240e560 0xc00240e561}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qqctn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qqctn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qqctn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v2-vsp1-worker6c7da5d4d5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00240e5d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00240e5f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:34 +0000 UTC  }],Message:,Reason:,HostIP:10.10.128.23,PodIP:,StartTime:2019-12-04 21:13:39 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  4 21:13:38.881: INFO: Pod "nginx-deployment-b79c9d74d-9qx2n" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-9qx2n,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-450,SelfLink:/api/v1/namespaces/deployment-450/pods/nginx-deployment-b79c9d74d-9qx2n,UID:efb0c09d-16da-11ea-825c-005056950656,ResourceVersion:285227,Generation:0,CreationTimestamp:2019-12-04 21:13:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d ee77371d-16da-11ea-825c-005056950656 0xc00240e6c0 0xc00240e6c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qqctn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qqctn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qqctn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v2-vsp1-worker9f6c17cd60,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00240e730} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00240e750}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:36 +0000 UTC  }],Message:,Reason:,HostIP:10.10.128.22,PodIP:,StartTime:2019-12-04 21:13:35 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  4 21:13:38.881: INFO: Pod "nginx-deployment-b79c9d74d-9t2z8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-9t2z8,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-450,SelfLink:/api/v1/namespaces/deployment-450/pods/nginx-deployment-b79c9d74d-9t2z8,UID:ee795006-16da-11ea-825c-005056950656,ResourceVersion:285149,Generation:0,CreationTimestamp:2019-12-04 21:13:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.1.87/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d ee77371d-16da-11ea-825c-005056950656 0xc00240e830 0xc00240e831}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qqctn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qqctn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qqctn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v2-vsp1-worker9f6c17cd60,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00240e8a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00240e8c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:34 +0000 UTC  }],Message:,Reason:,HostIP:10.10.128.22,PodIP:,StartTime:2019-12-04 21:13:33 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  4 21:13:38.882: INFO: Pod "nginx-deployment-b79c9d74d-fqgcg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-fqgcg,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-450,SelfLink:/api/v1/namespaces/deployment-450/pods/nginx-deployment-b79c9d74d-fqgcg,UID:efb68810-16da-11ea-825c-005056950656,ResourceVersion:285282,Generation:0,CreationTimestamp:2019-12-04 21:13:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d ee77371d-16da-11ea-825c-005056950656 0xc00240e990 0xc00240e991}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qqctn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qqctn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qqctn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v2-vsp1-worker9f6c17cd60,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00240ea00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00240ea20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:36 +0000 UTC  }],Message:,Reason:,HostIP:10.10.128.22,PodIP:,StartTime:2019-12-04 21:13:36 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  4 21:13:38.882: INFO: Pod "nginx-deployment-b79c9d74d-k5nmf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-k5nmf,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-450,SelfLink:/api/v1/namespaces/deployment-450/pods/nginx-deployment-b79c9d74d-k5nmf,UID:ee793eb5-16da-11ea-825c-005056950656,ResourceVersion:285151,Generation:0,CreationTimestamp:2019-12-04 21:13:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.2.78/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d ee77371d-16da-11ea-825c-005056950656 0xc00240eb00 0xc00240eb01}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qqctn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qqctn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qqctn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v2-vsp1-worker6c7da5d4d5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00240eb70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00240eb90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:34 +0000 UTC  }],Message:,Reason:,HostIP:10.10.128.23,PodIP:,StartTime:2019-12-04 21:13:39 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  4 21:13:38.882: INFO: Pod "nginx-deployment-b79c9d74d-lxb9g" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-lxb9g,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-450,SelfLink:/api/v1/namespaces/deployment-450/pods/nginx-deployment-b79c9d74d-lxb9g,UID:ee841a09-16da-11ea-825c-005056950656,ResourceVersion:285153,Generation:0,CreationTimestamp:2019-12-04 21:13:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.1.88/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d ee77371d-16da-11ea-825c-005056950656 0xc00240ec70 0xc00240ec71}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qqctn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qqctn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qqctn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v2-vsp1-worker9f6c17cd60,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00240ece0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00240ed00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:34 +0000 UTC  }],Message:,Reason:,HostIP:10.10.128.22,PodIP:,StartTime:2019-12-04 21:13:33 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  4 21:13:38.882: INFO: Pod "nginx-deployment-b79c9d74d-q4skp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-q4skp,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-450,SelfLink:/api/v1/namespaces/deployment-450/pods/nginx-deployment-b79c9d74d-q4skp,UID:ee8780d3-16da-11ea-825c-005056950656,ResourceVersion:285154,Generation:0,CreationTimestamp:2019-12-04 21:13:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.2.79/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d ee77371d-16da-11ea-825c-005056950656 0xc00240ede0 0xc00240ede1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qqctn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qqctn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qqctn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v2-vsp1-worker6c7da5d4d5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00240ee50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00240ee70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:34 +0000 UTC  }],Message:,Reason:,HostIP:10.10.128.23,PodIP:,StartTime:2019-12-04 21:13:39 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  4 21:13:38.883: INFO: Pod "nginx-deployment-b79c9d74d-qwt77" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-qwt77,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-450,SelfLink:/api/v1/namespaces/deployment-450/pods/nginx-deployment-b79c9d74d-qwt77,UID:efb6af64-16da-11ea-825c-005056950656,ResourceVersion:285261,Generation:0,CreationTimestamp:2019-12-04 21:13:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d ee77371d-16da-11ea-825c-005056950656 0xc00240ef40 0xc00240ef41}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qqctn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qqctn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qqctn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v2-vsp1-worker9f6c17cd60,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00240efb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00240efd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:36 +0000 UTC  }],Message:,Reason:,HostIP:10.10.128.22,PodIP:,StartTime:2019-12-04 21:13:35 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  4 21:13:38.883: INFO: Pod "nginx-deployment-b79c9d74d-t98ll" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-t98ll,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-450,SelfLink:/api/v1/namespaces/deployment-450/pods/nginx-deployment-b79c9d74d-t98ll,UID:efb6c333-16da-11ea-825c-005056950656,ResourceVersion:285300,Generation:0,CreationTimestamp:2019-12-04 21:13:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.2.84/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d ee77371d-16da-11ea-825c-005056950656 0xc00240f0b0 0xc00240f0b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qqctn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qqctn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qqctn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v2-vsp1-worker6c7da5d4d5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00240f120} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00240f140}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:36 +0000 UTC  }],Message:,Reason:,HostIP:10.10.128.23,PodIP:,StartTime:2019-12-04 21:13:41 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  4 21:13:38.883: INFO: Pod "nginx-deployment-b79c9d74d-wdzx4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-wdzx4,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-450,SelfLink:/api/v1/namespaces/deployment-450/pods/nginx-deployment-b79c9d74d-wdzx4,UID:efaf6153-16da-11ea-825c-005056950656,ResourceVersion:285202,Generation:0,CreationTimestamp:2019-12-04 21:13:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d ee77371d-16da-11ea-825c-005056950656 0xc00240f210 0xc00240f211}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qqctn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qqctn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qqctn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v2-vsp1-worker9f6c17cd60,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00240f2a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00240f2c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:36 +0000 UTC  }],Message:,Reason:,HostIP:10.10.128.22,PodIP:,StartTime:2019-12-04 21:13:35 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec  4 21:13:38.883: INFO: Pod "nginx-deployment-b79c9d74d-ztc2c" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-b79c9d74d-ztc2c,GenerateName:nginx-deployment-b79c9d74d-,Namespace:deployment-450,SelfLink:/api/v1/namespaces/deployment-450/pods/nginx-deployment-b79c9d74d-ztc2c,UID:efbbe216-16da-11ea-825c-005056950656,ResourceVersion:285301,Generation:0,CreationTimestamp:2019-12-04 21:13:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: b79c9d74d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-b79c9d74d ee77371d-16da-11ea-825c-005056950656 0xc00240f3a0 0xc00240f3a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qqctn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qqctn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qqctn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v2-vsp1-worker9f6c17cd60,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00240f420} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00240f440}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:13:36 +0000 UTC  }],Message:,Reason:,HostIP:10.10.128.22,PodIP:,StartTime:2019-12-04 21:13:36 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:13:38.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-450" for this suite.
Dec  4 21:13:46.900: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:13:46.974: INFO: namespace deployment-450 deletion completed in 8.085037919s

• [SLOW TEST:16.395 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:13:46.974: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9889
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W1204 21:14:27.171216      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec  4 21:14:27.171: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:14:27.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9889" for this suite.
Dec  4 21:14:33.182: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:14:33.255: INFO: namespace gc-9889 deletion completed in 6.080035974s

• [SLOW TEST:46.281 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:14:33.258: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6188
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Dec  4 21:14:33.399: INFO: Waiting up to 5m0s for pod "downward-api-1168373d-16db-11ea-8695-527d496f91de" in namespace "downward-api-6188" to be "success or failure"
Dec  4 21:14:33.415: INFO: Pod "downward-api-1168373d-16db-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 15.954848ms
Dec  4 21:14:35.419: INFO: Pod "downward-api-1168373d-16db-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019517663s
Dec  4 21:14:37.421: INFO: Pod "downward-api-1168373d-16db-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02236713s
STEP: Saw pod success
Dec  4 21:14:37.422: INFO: Pod "downward-api-1168373d-16db-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 21:14:37.424: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker9f6c17cd60 pod downward-api-1168373d-16db-11ea-8695-527d496f91de container dapi-container: <nil>
STEP: delete the pod
Dec  4 21:14:37.441: INFO: Waiting for pod downward-api-1168373d-16db-11ea-8695-527d496f91de to disappear
Dec  4 21:14:37.444: INFO: Pod downward-api-1168373d-16db-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:14:37.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6188" for this suite.
Dec  4 21:14:43.458: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:14:43.543: INFO: namespace downward-api-6188 deletion completed in 6.095808338s

• [SLOW TEST:10.286 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:14:43.544: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7629
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1685
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  4 21:14:43.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-7629'
Dec  4 21:14:43.842: INFO: stderr: ""
Dec  4 21:14:43.842: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Dec  4 21:14:48.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get pod e2e-test-nginx-pod --namespace=kubectl-7629 -o json'
Dec  4 21:14:48.994: INFO: stderr: ""
Dec  4 21:14:48.994: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"192.168.2.95/32\"\n        },\n        \"creationTimestamp\": \"2019-12-04T21:14:43Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-7629\",\n        \"resourceVersion\": \"285948\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-7629/pods/e2e-test-nginx-pod\",\n        \"uid\": \"179ee146-16db-11ea-825c-005056950656\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-7ckrg\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"alex-slot1-v2-vsp1-worker6c7da5d4d5\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-7ckrg\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-7ckrg\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-04T21:14:48Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-04T21:14:49Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-04T21:14:49Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-04T21:14:43Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://e4e652db0bcb6d59bd2601a3ee6fd181743b8340d4f33aa4d317b65787e52432\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-12-04T21:14:49Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.10.128.23\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.2.95\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-12-04T21:14:48Z\"\n    }\n}\n"
STEP: replace the image in the pod
Dec  4 21:14:48.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 replace -f - --namespace=kubectl-7629'
Dec  4 21:14:49.200: INFO: stderr: ""
Dec  4 21:14:49.200: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1690
Dec  4 21:14:49.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 delete pods e2e-test-nginx-pod --namespace=kubectl-7629'
Dec  4 21:14:59.730: INFO: stderr: ""
Dec  4 21:14:59.731: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:14:59.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7629" for this suite.
Dec  4 21:15:05.745: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:15:05.818: INFO: namespace kubectl-7629 deletion completed in 6.081612923s

• [SLOW TEST:22.274 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:15:05.823: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-5058
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override all
Dec  4 21:15:05.967: INFO: Waiting up to 5m0s for pod "client-containers-24d0c7d3-16db-11ea-8695-527d496f91de" in namespace "containers-5058" to be "success or failure"
Dec  4 21:15:05.971: INFO: Pod "client-containers-24d0c7d3-16db-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 4.425065ms
Dec  4 21:15:07.980: INFO: Pod "client-containers-24d0c7d3-16db-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012765166s
Dec  4 21:15:09.983: INFO: Pod "client-containers-24d0c7d3-16db-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016069413s
STEP: Saw pod success
Dec  4 21:15:09.983: INFO: Pod "client-containers-24d0c7d3-16db-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 21:15:09.985: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker9f6c17cd60 pod client-containers-24d0c7d3-16db-11ea-8695-527d496f91de container test-container: <nil>
STEP: delete the pod
Dec  4 21:15:10.005: INFO: Waiting for pod client-containers-24d0c7d3-16db-11ea-8695-527d496f91de to disappear
Dec  4 21:15:10.007: INFO: Pod client-containers-24d0c7d3-16db-11ea-8695-527d496f91de no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:15:10.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5058" for this suite.
Dec  4 21:15:16.020: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:15:16.116: INFO: namespace containers-5058 deletion completed in 6.106178447s

• [SLOW TEST:10.294 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:15:16.118: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9246
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec  4 21:15:16.260: INFO: Waiting up to 5m0s for pod "pod-2af44f01-16db-11ea-8695-527d496f91de" in namespace "emptydir-9246" to be "success or failure"
Dec  4 21:15:16.264: INFO: Pod "pod-2af44f01-16db-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 3.709409ms
Dec  4 21:15:18.266: INFO: Pod "pod-2af44f01-16db-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00652586s
STEP: Saw pod success
Dec  4 21:15:18.266: INFO: Pod "pod-2af44f01-16db-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 21:15:18.269: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker6c7da5d4d5 pod pod-2af44f01-16db-11ea-8695-527d496f91de container test-container: <nil>
STEP: delete the pod
Dec  4 21:15:18.287: INFO: Waiting for pod pod-2af44f01-16db-11ea-8695-527d496f91de to disappear
Dec  4 21:15:18.289: INFO: Pod pod-2af44f01-16db-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:15:18.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9246" for this suite.
Dec  4 21:15:24.312: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:15:24.389: INFO: namespace emptydir-9246 deletion completed in 6.097765618s

• [SLOW TEST:8.272 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:15:24.391: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5422
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-5422
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Dec  4 21:15:24.549: INFO: Found 0 stateful pods, waiting for 3
Dec  4 21:15:34.552: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  4 21:15:34.552: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  4 21:15:34.552: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Dec  4 21:15:34.577: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Dec  4 21:15:44.614: INFO: Updating stateful set ss2
Dec  4 21:15:44.645: INFO: Waiting for Pod statefulset-5422/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Restoring Pods to the correct revision when they are deleted
Dec  4 21:15:54.752: INFO: Found 2 stateful pods, waiting for 3
Dec  4 21:16:04.758: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  4 21:16:04.758: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  4 21:16:04.758: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Dec  4 21:16:04.781: INFO: Updating stateful set ss2
Dec  4 21:16:04.801: INFO: Waiting for Pod statefulset-5422/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec  4 21:16:14.827: INFO: Updating stateful set ss2
Dec  4 21:16:14.839: INFO: Waiting for StatefulSet statefulset-5422/ss2 to complete update
Dec  4 21:16:14.839: INFO: Waiting for Pod statefulset-5422/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Dec  4 21:16:24.845: INFO: Deleting all statefulset in ns statefulset-5422
Dec  4 21:16:24.847: INFO: Scaling statefulset ss2 to 0
Dec  4 21:16:44.860: INFO: Waiting for statefulset status.replicas updated to 0
Dec  4 21:16:44.862: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:16:44.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5422" for this suite.
Dec  4 21:16:50.904: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:16:50.997: INFO: namespace statefulset-5422 deletion completed in 6.102164689s

• [SLOW TEST:86.607 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:16:50.999: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-259
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:163
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec  4 21:16:53.665: INFO: Successfully updated pod "pod-update-activedeadlineseconds-63820853-16db-11ea-8695-527d496f91de"
Dec  4 21:16:53.665: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-63820853-16db-11ea-8695-527d496f91de" in namespace "pods-259" to be "terminated due to deadline exceeded"
Dec  4 21:16:53.671: INFO: Pod "pod-update-activedeadlineseconds-63820853-16db-11ea-8695-527d496f91de": Phase="Running", Reason="", readiness=true. Elapsed: 5.151734ms
Dec  4 21:16:55.673: INFO: Pod "pod-update-activedeadlineseconds-63820853-16db-11ea-8695-527d496f91de": Phase="Running", Reason="", readiness=true. Elapsed: 2.007910794s
Dec  4 21:16:57.677: INFO: Pod "pod-update-activedeadlineseconds-63820853-16db-11ea-8695-527d496f91de": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.011812675s
Dec  4 21:16:57.677: INFO: Pod "pod-update-activedeadlineseconds-63820853-16db-11ea-8695-527d496f91de" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:16:57.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-259" for this suite.
Dec  4 21:17:03.691: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:17:03.777: INFO: namespace pods-259 deletion completed in 6.095997795s

• [SLOW TEST:12.778 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:17:03.777: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4774
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:17:03.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4774" for this suite.
Dec  4 21:17:09.937: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:17:10.038: INFO: namespace services-4774 deletion completed in 6.110390243s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:6.262 seconds]
[sig-network] Services
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:17:10.042: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7096
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Dec  4 21:17:10.188: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7096,SelfLink:/api/v1/namespaces/watch-7096/configmaps/e2e-watch-test-label-changed,UID:6edba5cb-16db-11ea-825c-005056950656,ResourceVersion:286579,Generation:0,CreationTimestamp:2019-12-04 21:17:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec  4 21:17:10.188: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7096,SelfLink:/api/v1/namespaces/watch-7096/configmaps/e2e-watch-test-label-changed,UID:6edba5cb-16db-11ea-825c-005056950656,ResourceVersion:286580,Generation:0,CreationTimestamp:2019-12-04 21:17:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Dec  4 21:17:10.188: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7096,SelfLink:/api/v1/namespaces/watch-7096/configmaps/e2e-watch-test-label-changed,UID:6edba5cb-16db-11ea-825c-005056950656,ResourceVersion:286581,Generation:0,CreationTimestamp:2019-12-04 21:17:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Dec  4 21:17:20.214: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7096,SelfLink:/api/v1/namespaces/watch-7096/configmaps/e2e-watch-test-label-changed,UID:6edba5cb-16db-11ea-825c-005056950656,ResourceVersion:286598,Generation:0,CreationTimestamp:2019-12-04 21:17:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec  4 21:17:20.214: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7096,SelfLink:/api/v1/namespaces/watch-7096/configmaps/e2e-watch-test-label-changed,UID:6edba5cb-16db-11ea-825c-005056950656,ResourceVersion:286599,Generation:0,CreationTimestamp:2019-12-04 21:17:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Dec  4 21:17:20.214: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7096,SelfLink:/api/v1/namespaces/watch-7096/configmaps/e2e-watch-test-label-changed,UID:6edba5cb-16db-11ea-825c-005056950656,ResourceVersion:286600,Generation:0,CreationTimestamp:2019-12-04 21:17:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:17:20.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7096" for this suite.
Dec  4 21:17:26.234: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:17:26.304: INFO: namespace watch-7096 deletion completed in 6.086779497s

• [SLOW TEST:16.263 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:17:26.308: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-7023
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  4 21:17:26.456: INFO: (0) /api/v1/nodes/alex-slot1-v2-vsp1-worker6c7da5d4d5:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 5.42143ms)
Dec  4 21:17:26.460: INFO: (1) /api/v1/nodes/alex-slot1-v2-vsp1-worker6c7da5d4d5:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 4.091027ms)
Dec  4 21:17:26.464: INFO: (2) /api/v1/nodes/alex-slot1-v2-vsp1-worker6c7da5d4d5:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 3.028881ms)
Dec  4 21:17:26.467: INFO: (3) /api/v1/nodes/alex-slot1-v2-vsp1-worker6c7da5d4d5:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 3.05546ms)
Dec  4 21:17:26.470: INFO: (4) /api/v1/nodes/alex-slot1-v2-vsp1-worker6c7da5d4d5:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.610905ms)
Dec  4 21:17:26.472: INFO: (5) /api/v1/nodes/alex-slot1-v2-vsp1-worker6c7da5d4d5:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.248732ms)
Dec  4 21:17:26.475: INFO: (6) /api/v1/nodes/alex-slot1-v2-vsp1-worker6c7da5d4d5:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.536855ms)
Dec  4 21:17:26.478: INFO: (7) /api/v1/nodes/alex-slot1-v2-vsp1-worker6c7da5d4d5:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 3.342096ms)
Dec  4 21:17:26.481: INFO: (8) /api/v1/nodes/alex-slot1-v2-vsp1-worker6c7da5d4d5:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.405722ms)
Dec  4 21:17:26.483: INFO: (9) /api/v1/nodes/alex-slot1-v2-vsp1-worker6c7da5d4d5:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 1.984793ms)
Dec  4 21:17:26.485: INFO: (10) /api/v1/nodes/alex-slot1-v2-vsp1-worker6c7da5d4d5:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 1.897096ms)
Dec  4 21:17:26.488: INFO: (11) /api/v1/nodes/alex-slot1-v2-vsp1-worker6c7da5d4d5:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.09422ms)
Dec  4 21:17:26.490: INFO: (12) /api/v1/nodes/alex-slot1-v2-vsp1-worker6c7da5d4d5:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.171981ms)
Dec  4 21:17:26.492: INFO: (13) /api/v1/nodes/alex-slot1-v2-vsp1-worker6c7da5d4d5:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.131713ms)
Dec  4 21:17:26.495: INFO: (14) /api/v1/nodes/alex-slot1-v2-vsp1-worker6c7da5d4d5:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.247052ms)
Dec  4 21:17:26.497: INFO: (15) /api/v1/nodes/alex-slot1-v2-vsp1-worker6c7da5d4d5:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.139282ms)
Dec  4 21:17:26.500: INFO: (16) /api/v1/nodes/alex-slot1-v2-vsp1-worker6c7da5d4d5:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.613524ms)
Dec  4 21:17:26.503: INFO: (17) /api/v1/nodes/alex-slot1-v2-vsp1-worker6c7da5d4d5:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.810168ms)
Dec  4 21:17:26.506: INFO: (18) /api/v1/nodes/alex-slot1-v2-vsp1-worker6c7da5d4d5:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 3.184033ms)
Dec  4 21:17:26.509: INFO: (19) /api/v1/nodes/alex-slot1-v2-vsp1-worker6c7da5d4d5:10250/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.727577ms)
[AfterEach] version v1
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:17:26.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7023" for this suite.
Dec  4 21:17:32.521: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:17:32.606: INFO: namespace proxy-7023 deletion completed in 6.092542066s

• [SLOW TEST:6.297 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:17:32.606: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-477
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override arguments
Dec  4 21:17:32.750: INFO: Waiting up to 5m0s for pod "client-containers-7c4ed4e3-16db-11ea-8695-527d496f91de" in namespace "containers-477" to be "success or failure"
Dec  4 21:17:32.757: INFO: Pod "client-containers-7c4ed4e3-16db-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 6.760714ms
Dec  4 21:17:34.759: INFO: Pod "client-containers-7c4ed4e3-16db-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009399967s
Dec  4 21:17:36.762: INFO: Pod "client-containers-7c4ed4e3-16db-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012086966s
STEP: Saw pod success
Dec  4 21:17:36.762: INFO: Pod "client-containers-7c4ed4e3-16db-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 21:17:36.764: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker6c7da5d4d5 pod client-containers-7c4ed4e3-16db-11ea-8695-527d496f91de container test-container: <nil>
STEP: delete the pod
Dec  4 21:17:36.782: INFO: Waiting for pod client-containers-7c4ed4e3-16db-11ea-8695-527d496f91de to disappear
Dec  4 21:17:36.784: INFO: Pod client-containers-7c4ed4e3-16db-11ea-8695-527d496f91de no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:17:36.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-477" for this suite.
Dec  4 21:17:42.795: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:17:42.862: INFO: namespace containers-477 deletion completed in 6.0754189s

• [SLOW TEST:10.256 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:17:42.864: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1501
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:266
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Dec  4 21:17:42.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 create -f - --namespace=kubectl-1501'
Dec  4 21:17:43.492: INFO: stderr: ""
Dec  4 21:17:43.492: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  4 21:17:43.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1501'
Dec  4 21:17:43.614: INFO: stderr: ""
Dec  4 21:17:43.614: INFO: stdout: "update-demo-nautilus-5cslh update-demo-nautilus-7k5df "
Dec  4 21:17:43.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get pods update-demo-nautilus-5cslh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1501'
Dec  4 21:17:43.700: INFO: stderr: ""
Dec  4 21:17:43.700: INFO: stdout: ""
Dec  4 21:17:43.700: INFO: update-demo-nautilus-5cslh is created but not running
Dec  4 21:17:48.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1501'
Dec  4 21:17:48.830: INFO: stderr: ""
Dec  4 21:17:48.830: INFO: stdout: "update-demo-nautilus-5cslh update-demo-nautilus-7k5df "
Dec  4 21:17:48.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get pods update-demo-nautilus-5cslh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1501'
Dec  4 21:17:48.928: INFO: stderr: ""
Dec  4 21:17:48.929: INFO: stdout: "true"
Dec  4 21:17:48.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get pods update-demo-nautilus-5cslh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1501'
Dec  4 21:17:49.022: INFO: stderr: ""
Dec  4 21:17:49.022: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  4 21:17:49.022: INFO: validating pod update-demo-nautilus-5cslh
Dec  4 21:17:49.025: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  4 21:17:49.025: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  4 21:17:49.025: INFO: update-demo-nautilus-5cslh is verified up and running
Dec  4 21:17:49.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get pods update-demo-nautilus-7k5df -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1501'
Dec  4 21:17:49.106: INFO: stderr: ""
Dec  4 21:17:49.106: INFO: stdout: "true"
Dec  4 21:17:49.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get pods update-demo-nautilus-7k5df -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1501'
Dec  4 21:17:49.192: INFO: stderr: ""
Dec  4 21:17:49.192: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  4 21:17:49.192: INFO: validating pod update-demo-nautilus-7k5df
Dec  4 21:17:49.196: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  4 21:17:49.196: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  4 21:17:49.196: INFO: update-demo-nautilus-7k5df is verified up and running
STEP: scaling down the replication controller
Dec  4 21:17:49.198: INFO: scanned /root for discovery docs: <nil>
Dec  4 21:17:49.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-1501'
Dec  4 21:17:49.315: INFO: stderr: ""
Dec  4 21:17:49.315: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  4 21:17:49.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1501'
Dec  4 21:17:49.397: INFO: stderr: ""
Dec  4 21:17:49.397: INFO: stdout: "update-demo-nautilus-5cslh update-demo-nautilus-7k5df "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec  4 21:17:54.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1501'
Dec  4 21:17:54.496: INFO: stderr: ""
Dec  4 21:17:54.496: INFO: stdout: "update-demo-nautilus-5cslh update-demo-nautilus-7k5df "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec  4 21:17:59.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1501'
Dec  4 21:17:59.589: INFO: stderr: ""
Dec  4 21:17:59.589: INFO: stdout: "update-demo-nautilus-5cslh update-demo-nautilus-7k5df "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec  4 21:18:04.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1501'
Dec  4 21:18:04.676: INFO: stderr: ""
Dec  4 21:18:04.677: INFO: stdout: "update-demo-nautilus-7k5df "
Dec  4 21:18:04.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get pods update-demo-nautilus-7k5df -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1501'
Dec  4 21:18:04.770: INFO: stderr: ""
Dec  4 21:18:04.770: INFO: stdout: "true"
Dec  4 21:18:04.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get pods update-demo-nautilus-7k5df -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1501'
Dec  4 21:18:04.866: INFO: stderr: ""
Dec  4 21:18:04.866: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  4 21:18:04.866: INFO: validating pod update-demo-nautilus-7k5df
Dec  4 21:18:04.869: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  4 21:18:04.869: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  4 21:18:04.869: INFO: update-demo-nautilus-7k5df is verified up and running
STEP: scaling up the replication controller
Dec  4 21:18:04.871: INFO: scanned /root for discovery docs: <nil>
Dec  4 21:18:04.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-1501'
Dec  4 21:18:05.984: INFO: stderr: ""
Dec  4 21:18:05.984: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec  4 21:18:05.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1501'
Dec  4 21:18:06.087: INFO: stderr: ""
Dec  4 21:18:06.087: INFO: stdout: "update-demo-nautilus-4ns75 update-demo-nautilus-7k5df "
Dec  4 21:18:06.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get pods update-demo-nautilus-4ns75 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1501'
Dec  4 21:18:06.168: INFO: stderr: ""
Dec  4 21:18:06.168: INFO: stdout: ""
Dec  4 21:18:06.168: INFO: update-demo-nautilus-4ns75 is created but not running
Dec  4 21:18:11.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1501'
Dec  4 21:18:11.246: INFO: stderr: ""
Dec  4 21:18:11.246: INFO: stdout: "update-demo-nautilus-4ns75 update-demo-nautilus-7k5df "
Dec  4 21:18:11.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get pods update-demo-nautilus-4ns75 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1501'
Dec  4 21:18:11.323: INFO: stderr: ""
Dec  4 21:18:11.323: INFO: stdout: "true"
Dec  4 21:18:11.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get pods update-demo-nautilus-4ns75 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1501'
Dec  4 21:18:11.417: INFO: stderr: ""
Dec  4 21:18:11.417: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  4 21:18:11.417: INFO: validating pod update-demo-nautilus-4ns75
Dec  4 21:18:11.420: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  4 21:18:11.420: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  4 21:18:11.420: INFO: update-demo-nautilus-4ns75 is verified up and running
Dec  4 21:18:11.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get pods update-demo-nautilus-7k5df -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1501'
Dec  4 21:18:11.516: INFO: stderr: ""
Dec  4 21:18:11.516: INFO: stdout: "true"
Dec  4 21:18:11.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get pods update-demo-nautilus-7k5df -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1501'
Dec  4 21:18:11.630: INFO: stderr: ""
Dec  4 21:18:11.630: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec  4 21:18:11.630: INFO: validating pod update-demo-nautilus-7k5df
Dec  4 21:18:11.634: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec  4 21:18:11.634: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec  4 21:18:11.634: INFO: update-demo-nautilus-7k5df is verified up and running
STEP: using delete to clean up resources
Dec  4 21:18:11.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 delete --grace-period=0 --force -f - --namespace=kubectl-1501'
Dec  4 21:18:11.754: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  4 21:18:11.754: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec  4 21:18:11.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1501'
Dec  4 21:18:11.862: INFO: stderr: "No resources found.\n"
Dec  4 21:18:11.862: INFO: stdout: ""
Dec  4 21:18:11.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get pods -l name=update-demo --namespace=kubectl-1501 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  4 21:18:11.964: INFO: stderr: ""
Dec  4 21:18:11.964: INFO: stdout: "update-demo-nautilus-4ns75\nupdate-demo-nautilus-7k5df\n"
Dec  4 21:18:12.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1501'
Dec  4 21:18:12.560: INFO: stderr: "No resources found.\n"
Dec  4 21:18:12.560: INFO: stdout: ""
Dec  4 21:18:12.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get pods -l name=update-demo --namespace=kubectl-1501 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  4 21:18:12.652: INFO: stderr: ""
Dec  4 21:18:12.652: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:18:12.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1501" for this suite.
Dec  4 21:18:34.669: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:18:34.745: INFO: namespace kubectl-1501 deletion completed in 22.089613901s

• [SLOW TEST:51.882 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:18:34.745: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-7204
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test use defaults
Dec  4 21:18:34.885: INFO: Waiting up to 5m0s for pod "client-containers-a1584d61-16db-11ea-8695-527d496f91de" in namespace "containers-7204" to be "success or failure"
Dec  4 21:18:34.890: INFO: Pod "client-containers-a1584d61-16db-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 4.97042ms
Dec  4 21:18:36.893: INFO: Pod "client-containers-a1584d61-16db-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007615992s
STEP: Saw pod success
Dec  4 21:18:36.893: INFO: Pod "client-containers-a1584d61-16db-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 21:18:36.895: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker6c7da5d4d5 pod client-containers-a1584d61-16db-11ea-8695-527d496f91de container test-container: <nil>
STEP: delete the pod
Dec  4 21:18:36.914: INFO: Waiting for pod client-containers-a1584d61-16db-11ea-8695-527d496f91de to disappear
Dec  4 21:18:36.916: INFO: Pod client-containers-a1584d61-16db-11ea-8695-527d496f91de no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:18:36.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7204" for this suite.
Dec  4 21:18:42.928: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:18:43.020: INFO: namespace containers-7204 deletion completed in 6.100989701s

• [SLOW TEST:8.275 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:18:43.024: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3878
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-3878
Dec  4 21:18:45.183: INFO: Started pod liveness-http in namespace container-probe-3878
STEP: checking the pod's current state and verifying that restartCount is present
Dec  4 21:18:45.186: INFO: Initial restart count of pod liveness-http is 0
Dec  4 21:19:01.212: INFO: Restart count of pod container-probe-3878/liveness-http is now 1 (16.026075974s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:19:01.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3878" for this suite.
Dec  4 21:19:07.239: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:19:07.318: INFO: namespace container-probe-3878 deletion completed in 6.088057017s

• [SLOW TEST:24.294 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:19:07.319: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4726
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-4726
Dec  4 21:19:09.465: INFO: Started pod liveness-http in namespace container-probe-4726
STEP: checking the pod's current state and verifying that restartCount is present
Dec  4 21:19:09.467: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:23:09.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4726" for this suite.
Dec  4 21:23:15.878: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:23:15.949: INFO: namespace container-probe-4726 deletion completed in 6.083205375s

• [SLOW TEST:248.630 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:23:15.953: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9421
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-projected-728x
STEP: Creating a pod to test atomic-volume-subpath
Dec  4 21:23:16.104: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-728x" in namespace "subpath-9421" to be "success or failure"
Dec  4 21:23:16.111: INFO: Pod "pod-subpath-test-projected-728x": Phase="Pending", Reason="", readiness=false. Elapsed: 4.575306ms
Dec  4 21:23:18.114: INFO: Pod "pod-subpath-test-projected-728x": Phase="Running", Reason="", readiness=true. Elapsed: 2.007904797s
Dec  4 21:23:20.118: INFO: Pod "pod-subpath-test-projected-728x": Phase="Running", Reason="", readiness=true. Elapsed: 4.011685276s
Dec  4 21:23:22.121: INFO: Pod "pod-subpath-test-projected-728x": Phase="Running", Reason="", readiness=true. Elapsed: 6.014665048s
Dec  4 21:23:24.124: INFO: Pod "pod-subpath-test-projected-728x": Phase="Running", Reason="", readiness=true. Elapsed: 8.017467081s
Dec  4 21:23:26.127: INFO: Pod "pod-subpath-test-projected-728x": Phase="Running", Reason="", readiness=true. Elapsed: 10.020920358s
Dec  4 21:23:28.131: INFO: Pod "pod-subpath-test-projected-728x": Phase="Running", Reason="", readiness=true. Elapsed: 12.024662844s
Dec  4 21:23:30.134: INFO: Pod "pod-subpath-test-projected-728x": Phase="Running", Reason="", readiness=true. Elapsed: 14.027659679s
Dec  4 21:23:32.137: INFO: Pod "pod-subpath-test-projected-728x": Phase="Running", Reason="", readiness=true. Elapsed: 16.03072864s
Dec  4 21:23:34.140: INFO: Pod "pod-subpath-test-projected-728x": Phase="Running", Reason="", readiness=true. Elapsed: 18.033749141s
Dec  4 21:23:36.144: INFO: Pod "pod-subpath-test-projected-728x": Phase="Running", Reason="", readiness=true. Elapsed: 20.037183829s
Dec  4 21:23:38.147: INFO: Pod "pod-subpath-test-projected-728x": Phase="Running", Reason="", readiness=true. Elapsed: 22.040262974s
Dec  4 21:23:40.150: INFO: Pod "pod-subpath-test-projected-728x": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.043319207s
STEP: Saw pod success
Dec  4 21:23:40.150: INFO: Pod "pod-subpath-test-projected-728x" satisfied condition "success or failure"
Dec  4 21:23:40.152: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker9f6c17cd60 pod pod-subpath-test-projected-728x container test-container-subpath-projected-728x: <nil>
STEP: delete the pod
Dec  4 21:23:40.175: INFO: Waiting for pod pod-subpath-test-projected-728x to disappear
Dec  4 21:23:40.176: INFO: Pod pod-subpath-test-projected-728x no longer exists
STEP: Deleting pod pod-subpath-test-projected-728x
Dec  4 21:23:40.176: INFO: Deleting pod "pod-subpath-test-projected-728x" in namespace "subpath-9421"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:23:40.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9421" for this suite.
Dec  4 21:23:46.190: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:23:46.269: INFO: namespace subpath-9421 deletion completed in 6.088130221s

• [SLOW TEST:30.317 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:23:46.272: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5000
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5000.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5000.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5000.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5000.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5000.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5000.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  4 21:23:48.440: INFO: Unable to read jessie_hosts@dns-querier-1 from pod dns-5000/dns-test-5b07b032-16dc-11ea-8695-527d496f91de: the server could not find the requested resource (get pods dns-test-5b07b032-16dc-11ea-8695-527d496f91de)
Dec  4 21:23:48.442: INFO: Unable to read jessie_udp@PodARecord from pod dns-5000/dns-test-5b07b032-16dc-11ea-8695-527d496f91de: the server could not find the requested resource (get pods dns-test-5b07b032-16dc-11ea-8695-527d496f91de)
Dec  4 21:23:48.444: INFO: Unable to read jessie_tcp@PodARecord from pod dns-5000/dns-test-5b07b032-16dc-11ea-8695-527d496f91de: the server could not find the requested resource (get pods dns-test-5b07b032-16dc-11ea-8695-527d496f91de)
Dec  4 21:23:48.444: INFO: Lookups using dns-5000/dns-test-5b07b032-16dc-11ea-8695-527d496f91de failed for: [jessie_hosts@dns-querier-1 jessie_udp@PodARecord jessie_tcp@PodARecord]

Dec  4 21:23:53.455: INFO: Unable to read jessie_hosts@dns-querier-1 from pod dns-5000/dns-test-5b07b032-16dc-11ea-8695-527d496f91de: the server could not find the requested resource (get pods dns-test-5b07b032-16dc-11ea-8695-527d496f91de)
Dec  4 21:23:53.457: INFO: Unable to read jessie_udp@PodARecord from pod dns-5000/dns-test-5b07b032-16dc-11ea-8695-527d496f91de: the server could not find the requested resource (get pods dns-test-5b07b032-16dc-11ea-8695-527d496f91de)
Dec  4 21:23:53.459: INFO: Unable to read jessie_tcp@PodARecord from pod dns-5000/dns-test-5b07b032-16dc-11ea-8695-527d496f91de: the server could not find the requested resource (get pods dns-test-5b07b032-16dc-11ea-8695-527d496f91de)
Dec  4 21:23:53.459: INFO: Lookups using dns-5000/dns-test-5b07b032-16dc-11ea-8695-527d496f91de failed for: [jessie_hosts@dns-querier-1 jessie_udp@PodARecord jessie_tcp@PodARecord]

Dec  4 21:23:58.461: INFO: DNS probes using dns-5000/dns-test-5b07b032-16dc-11ea-8695-527d496f91de succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:23:58.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5000" for this suite.
Dec  4 21:24:04.488: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:24:04.571: INFO: namespace dns-5000 deletion completed in 6.091749875s

• [SLOW TEST:18.299 seconds]
[sig-network] DNS
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:24:04.573: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-3250
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Dec  4 21:24:08.724: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-65efb706-16dc-11ea-8695-527d496f91de,GenerateName:,Namespace:events-3250,SelfLink:/api/v1/namespaces/events-3250/pods/send-events-65efb706-16dc-11ea-8695-527d496f91de,UID:65f01f94-16dc-11ea-825c-005056950656,ResourceVersion:287589,Generation:0,CreationTimestamp:2019-12-04 21:24:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 704776326,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.1.113/32,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-rp2j9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rp2j9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-rp2j9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v2-vsp1-worker9f6c17cd60,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00254d370} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00254d390}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:24:03 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:24:06 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:24:06 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:24:04 +0000 UTC  }],Message:,Reason:,HostIP:10.10.128.22,PodIP:192.168.1.113,StartTime:2019-12-04 21:24:03 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-12-04 21:24:05 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://22b94d878a79cdd39fdad5ff88ad9e72b52cbe756a98123a8aaea73defe26721}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Dec  4 21:24:10.728: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Dec  4 21:24:12.730: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:24:12.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3250" for this suite.
Dec  4 21:24:50.754: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:24:50.826: INFO: namespace events-3250 deletion completed in 38.079859207s

• [SLOW TEST:46.253 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:24:50.827: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-122
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec  4 21:24:50.971: INFO: Waiting up to 5m0s for pod "pod-8181ed79-16dc-11ea-8695-527d496f91de" in namespace "emptydir-122" to be "success or failure"
Dec  4 21:24:50.976: INFO: Pod "pod-8181ed79-16dc-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 4.636456ms
Dec  4 21:24:52.979: INFO: Pod "pod-8181ed79-16dc-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007758747s
Dec  4 21:24:54.982: INFO: Pod "pod-8181ed79-16dc-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010731518s
STEP: Saw pod success
Dec  4 21:24:54.982: INFO: Pod "pod-8181ed79-16dc-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 21:24:54.984: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker6c7da5d4d5 pod pod-8181ed79-16dc-11ea-8695-527d496f91de container test-container: <nil>
STEP: delete the pod
Dec  4 21:24:55.003: INFO: Waiting for pod pod-8181ed79-16dc-11ea-8695-527d496f91de to disappear
Dec  4 21:24:55.005: INFO: Pod pod-8181ed79-16dc-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:24:55.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-122" for this suite.
Dec  4 21:25:01.015: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:25:01.089: INFO: namespace emptydir-122 deletion completed in 6.081621517s

• [SLOW TEST:10.262 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:25:01.090: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4741
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-87a0942c-16dc-11ea-8695-527d496f91de
STEP: Creating a pod to test consume secrets
Dec  4 21:25:01.241: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-87a14208-16dc-11ea-8695-527d496f91de" in namespace "projected-4741" to be "success or failure"
Dec  4 21:25:01.247: INFO: Pod "pod-projected-secrets-87a14208-16dc-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 5.76926ms
Dec  4 21:25:03.251: INFO: Pod "pod-projected-secrets-87a14208-16dc-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009648881s
STEP: Saw pod success
Dec  4 21:25:03.251: INFO: Pod "pod-projected-secrets-87a14208-16dc-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 21:25:03.252: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker9f6c17cd60 pod pod-projected-secrets-87a14208-16dc-11ea-8695-527d496f91de container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  4 21:25:03.270: INFO: Waiting for pod pod-projected-secrets-87a14208-16dc-11ea-8695-527d496f91de to disappear
Dec  4 21:25:03.273: INFO: Pod pod-projected-secrets-87a14208-16dc-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:25:03.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4741" for this suite.
Dec  4 21:25:09.283: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:25:09.369: INFO: namespace projected-4741 deletion completed in 6.092866927s

• [SLOW TEST:8.279 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:25:09.375: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-3476
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Dec  4 21:25:09.506: INFO: PodSpec: initContainers in spec.initContainers
Dec  4 21:25:55.010: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-8c8faa41-16dc-11ea-8695-527d496f91de", GenerateName:"", Namespace:"init-container-3476", SelfLink:"/api/v1/namespaces/init-container-3476/pods/pod-init-8c8faa41-16dc-11ea-8695-527d496f91de", UID:"8c903ba6-16dc-11ea-825c-005056950656", ResourceVersion:"287871", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63711091509, loc:(*time.Location)(0x882f100)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"506353770"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"192.168.2.109/32"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-q2fbg", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0027f0ac0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-q2fbg", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-q2fbg", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-q2fbg", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00213a098), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"alex-slot1-v2-vsp1-worker6c7da5d4d5", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002510300), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00213a110)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00213a130)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00213a138), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00213a13c)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711091513, loc:(*time.Location)(0x882f100)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711091513, loc:(*time.Location)(0x882f100)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711091513, loc:(*time.Location)(0x882f100)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711091509, loc:(*time.Location)(0x882f100)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.10.128.23", PodIP:"192.168.2.109", StartTime:(*v1.Time)(0xc001cebd00), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002919030)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0029190a0)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://15eeaf2ba0ea1ac0ddb9644f610e07121373ca9b623df7774aa4081b74a4cc33"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001cebda0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001cebd80), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:25:55.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3476" for this suite.
Dec  4 21:26:17.024: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:26:17.100: INFO: namespace init-container-3476 deletion completed in 22.084762375s

• [SLOW TEST:67.725 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:26:17.102: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-8791
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:26:21.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8791" for this suite.
Dec  4 21:26:27.258: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:26:27.482: INFO: namespace kubelet-test-8791 deletion completed in 6.23218334s

• [SLOW TEST:10.381 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:26:27.485: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-537
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec  4 21:26:27.653: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 21:26:27.681: INFO: Number of nodes with available pods: 0
Dec  4 21:26:27.681: INFO: Node alex-slot1-v2-vsp1-worker6c7da5d4d5 is running more than one daemon pod
Dec  4 21:26:28.693: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 21:26:28.701: INFO: Number of nodes with available pods: 0
Dec  4 21:26:28.701: INFO: Node alex-slot1-v2-vsp1-worker6c7da5d4d5 is running more than one daemon pod
Dec  4 21:26:29.685: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 21:26:29.688: INFO: Number of nodes with available pods: 1
Dec  4 21:26:29.688: INFO: Node alex-slot1-v2-vsp1-worker9f6c17cd60 is running more than one daemon pod
Dec  4 21:26:30.685: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 21:26:30.687: INFO: Number of nodes with available pods: 2
Dec  4 21:26:30.687: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Dec  4 21:26:30.703: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 21:26:30.717: INFO: Number of nodes with available pods: 1
Dec  4 21:26:30.717: INFO: Node alex-slot1-v2-vsp1-worker9f6c17cd60 is running more than one daemon pod
Dec  4 21:26:31.722: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 21:26:31.726: INFO: Number of nodes with available pods: 1
Dec  4 21:26:31.727: INFO: Node alex-slot1-v2-vsp1-worker9f6c17cd60 is running more than one daemon pod
Dec  4 21:26:32.722: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 21:26:32.725: INFO: Number of nodes with available pods: 1
Dec  4 21:26:32.725: INFO: Node alex-slot1-v2-vsp1-worker9f6c17cd60 is running more than one daemon pod
Dec  4 21:26:33.721: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 21:26:33.723: INFO: Number of nodes with available pods: 2
Dec  4 21:26:33.723: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-537, will wait for the garbage collector to delete the pods
Dec  4 21:26:33.794: INFO: Deleting DaemonSet.extensions daemon-set took: 13.181587ms
Dec  4 21:26:34.195: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.24471ms
Dec  4 21:26:39.797: INFO: Number of nodes with available pods: 0
Dec  4 21:26:39.797: INFO: Number of running nodes: 0, number of available pods: 0
Dec  4 21:26:39.799: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-537/daemonsets","resourceVersion":"288047"},"items":null}

Dec  4 21:26:39.801: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-537/pods","resourceVersion":"288047"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:26:39.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-537" for this suite.
Dec  4 21:26:45.825: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:26:45.909: INFO: namespace daemonsets-537 deletion completed in 6.094219168s

• [SLOW TEST:18.424 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:26:45.912: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-9933
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-9933
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec  4 21:26:46.047: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec  4 21:27:10.136: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.1.118 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9933 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  4 21:27:10.136: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
Dec  4 21:27:11.268: INFO: Found all expected endpoints: [netserver-0]
Dec  4 21:27:11.271: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.2.111 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9933 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  4 21:27:11.271: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
Dec  4 21:27:12.432: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:27:12.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9933" for this suite.
Dec  4 21:27:34.448: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:27:34.521: INFO: namespace pod-network-test-9933 deletion completed in 22.082015847s

• [SLOW TEST:48.609 seconds]
[sig-network] Networking
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:27:34.522: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4166
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1649
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  4 21:27:34.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-4166'
Dec  4 21:27:34.764: INFO: stderr: ""
Dec  4 21:27:34.764: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1654
Dec  4 21:27:34.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 delete pods e2e-test-nginx-pod --namespace=kubectl-4166'
Dec  4 21:27:49.706: INFO: stderr: ""
Dec  4 21:27:49.706: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:27:49.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4166" for this suite.
Dec  4 21:27:55.719: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:27:55.796: INFO: namespace kubectl-4166 deletion completed in 6.085403922s

• [SLOW TEST:21.274 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:27:55.797: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6013
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name projected-secret-test-efc1c2f0-16dc-11ea-8695-527d496f91de
STEP: Creating a pod to test consume secrets
Dec  4 21:27:55.944: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-efc26c2d-16dc-11ea-8695-527d496f91de" in namespace "projected-6013" to be "success or failure"
Dec  4 21:27:55.949: INFO: Pod "pod-projected-secrets-efc26c2d-16dc-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 4.413589ms
Dec  4 21:27:57.951: INFO: Pod "pod-projected-secrets-efc26c2d-16dc-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007017969s
STEP: Saw pod success
Dec  4 21:27:57.951: INFO: Pod "pod-projected-secrets-efc26c2d-16dc-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 21:27:57.953: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker9f6c17cd60 pod pod-projected-secrets-efc26c2d-16dc-11ea-8695-527d496f91de container secret-volume-test: <nil>
STEP: delete the pod
Dec  4 21:27:57.981: INFO: Waiting for pod pod-projected-secrets-efc26c2d-16dc-11ea-8695-527d496f91de to disappear
Dec  4 21:27:57.982: INFO: Pod pod-projected-secrets-efc26c2d-16dc-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:27:57.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6013" for this suite.
Dec  4 21:28:03.995: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:28:04.070: INFO: namespace projected-6013 deletion completed in 6.08450851s

• [SLOW TEST:8.273 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:28:04.070: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7504
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-7504
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating stateful set ss in namespace statefulset-7504
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7504
Dec  4 21:28:04.226: INFO: Found 0 stateful pods, waiting for 1
Dec  4 21:28:14.229: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Dec  4 21:28:14.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  4 21:28:14.441: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  4 21:28:14.441: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  4 21:28:14.441: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  4 21:28:14.445: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec  4 21:28:24.448: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  4 21:28:24.448: INFO: Waiting for statefulset status.replicas updated to 0
Dec  4 21:28:24.462: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
Dec  4 21:28:24.462: INFO: ss-0  alex-slot1-v2-vsp1-worker6c7da5d4d5  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:04 +0000 UTC  }]
Dec  4 21:28:24.462: INFO: 
Dec  4 21:28:24.462: INFO: StatefulSet ss has not reached scale 3, at 1
Dec  4 21:28:25.470: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995068226s
Dec  4 21:28:26.474: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.986442342s
Dec  4 21:28:27.477: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.983029508s
Dec  4 21:28:28.481: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.979929463s
Dec  4 21:28:29.484: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.976086682s
Dec  4 21:28:30.487: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.97279185s
Dec  4 21:28:31.492: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.969374578s
Dec  4 21:28:32.496: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.965124785s
Dec  4 21:28:33.499: INFO: Verifying statefulset ss doesn't scale past 3 for another 960.636922ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7504
Dec  4 21:28:34.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  4 21:28:34.721: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec  4 21:28:34.721: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  4 21:28:34.721: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  4 21:28:34.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  4 21:28:34.936: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec  4 21:28:34.936: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  4 21:28:34.936: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  4 21:28:34.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  4 21:28:35.137: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec  4 21:28:35.137: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  4 21:28:35.137: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  4 21:28:35.141: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Dec  4 21:28:45.145: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  4 21:28:45.145: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  4 21:28:45.145: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Dec  4 21:28:45.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  4 21:28:45.322: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  4 21:28:45.322: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  4 21:28:45.322: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  4 21:28:45.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  4 21:28:45.607: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  4 21:28:45.607: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  4 21:28:45.607: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  4 21:28:45.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  4 21:28:45.851: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  4 21:28:45.851: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  4 21:28:45.851: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  4 21:28:45.851: INFO: Waiting for statefulset status.replicas updated to 0
Dec  4 21:28:45.854: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Dec  4 21:28:55.859: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  4 21:28:55.859: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec  4 21:28:55.859: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec  4 21:28:55.872: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
Dec  4 21:28:55.872: INFO: ss-0  alex-slot1-v2-vsp1-worker6c7da5d4d5  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:04 +0000 UTC  }]
Dec  4 21:28:55.872: INFO: ss-1  alex-slot1-v2-vsp1-worker9f6c17cd60  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:24 +0000 UTC  }]
Dec  4 21:28:55.873: INFO: ss-2  alex-slot1-v2-vsp1-worker6c7da5d4d5  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:24 +0000 UTC  }]
Dec  4 21:28:55.873: INFO: 
Dec  4 21:28:55.873: INFO: StatefulSet ss has not reached scale 0, at 3
Dec  4 21:28:56.878: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
Dec  4 21:28:56.878: INFO: ss-0  alex-slot1-v2-vsp1-worker6c7da5d4d5  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:04 +0000 UTC  }]
Dec  4 21:28:56.878: INFO: ss-1  alex-slot1-v2-vsp1-worker9f6c17cd60  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:24 +0000 UTC  }]
Dec  4 21:28:56.878: INFO: ss-2  alex-slot1-v2-vsp1-worker6c7da5d4d5  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:24 +0000 UTC  }]
Dec  4 21:28:56.878: INFO: 
Dec  4 21:28:56.878: INFO: StatefulSet ss has not reached scale 0, at 3
Dec  4 21:28:57.881: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
Dec  4 21:28:57.881: INFO: ss-0  alex-slot1-v2-vsp1-worker6c7da5d4d5  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:04 +0000 UTC  }]
Dec  4 21:28:57.881: INFO: ss-1  alex-slot1-v2-vsp1-worker9f6c17cd60  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:24 +0000 UTC  }]
Dec  4 21:28:57.881: INFO: ss-2  alex-slot1-v2-vsp1-worker6c7da5d4d5  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:24 +0000 UTC  }]
Dec  4 21:28:57.881: INFO: 
Dec  4 21:28:57.881: INFO: StatefulSet ss has not reached scale 0, at 3
Dec  4 21:28:58.885: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
Dec  4 21:28:58.885: INFO: ss-1  alex-slot1-v2-vsp1-worker9f6c17cd60  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:24 +0000 UTC  }]
Dec  4 21:28:58.885: INFO: ss-2  alex-slot1-v2-vsp1-worker6c7da5d4d5  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:24 +0000 UTC  }]
Dec  4 21:28:58.885: INFO: 
Dec  4 21:28:58.885: INFO: StatefulSet ss has not reached scale 0, at 2
Dec  4 21:28:59.888: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
Dec  4 21:28:59.888: INFO: ss-1  alex-slot1-v2-vsp1-worker9f6c17cd60  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:24 +0000 UTC  }]
Dec  4 21:28:59.888: INFO: ss-2  alex-slot1-v2-vsp1-worker6c7da5d4d5  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:24 +0000 UTC  }]
Dec  4 21:28:59.888: INFO: 
Dec  4 21:28:59.888: INFO: StatefulSet ss has not reached scale 0, at 2
Dec  4 21:29:00.892: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
Dec  4 21:29:00.892: INFO: ss-1  alex-slot1-v2-vsp1-worker9f6c17cd60  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:24 +0000 UTC  }]
Dec  4 21:29:00.892: INFO: ss-2  alex-slot1-v2-vsp1-worker6c7da5d4d5  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:24 +0000 UTC  }]
Dec  4 21:29:00.892: INFO: 
Dec  4 21:29:00.892: INFO: StatefulSet ss has not reached scale 0, at 2
Dec  4 21:29:01.895: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
Dec  4 21:29:01.895: INFO: ss-1  alex-slot1-v2-vsp1-worker9f6c17cd60  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:24 +0000 UTC  }]
Dec  4 21:29:01.895: INFO: ss-2  alex-slot1-v2-vsp1-worker6c7da5d4d5  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:24 +0000 UTC  }]
Dec  4 21:29:01.895: INFO: 
Dec  4 21:29:01.895: INFO: StatefulSet ss has not reached scale 0, at 2
Dec  4 21:29:02.898: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
Dec  4 21:29:02.898: INFO: ss-1  alex-slot1-v2-vsp1-worker9f6c17cd60  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:24 +0000 UTC  }]
Dec  4 21:29:02.899: INFO: ss-2  alex-slot1-v2-vsp1-worker6c7da5d4d5  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:24 +0000 UTC  }]
Dec  4 21:29:02.899: INFO: 
Dec  4 21:29:02.899: INFO: StatefulSet ss has not reached scale 0, at 2
Dec  4 21:29:03.902: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
Dec  4 21:29:03.902: INFO: ss-1  alex-slot1-v2-vsp1-worker9f6c17cd60  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:24 +0000 UTC  }]
Dec  4 21:29:03.902: INFO: ss-2  alex-slot1-v2-vsp1-worker6c7da5d4d5  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:24 +0000 UTC  }]
Dec  4 21:29:03.903: INFO: 
Dec  4 21:29:03.903: INFO: StatefulSet ss has not reached scale 0, at 2
Dec  4 21:29:04.905: INFO: POD   NODE                                 PHASE    GRACE  CONDITIONS
Dec  4 21:29:04.905: INFO: ss-2  alex-slot1-v2-vsp1-worker6c7da5d4d5  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:28:24 +0000 UTC  }]
Dec  4 21:29:04.906: INFO: 
Dec  4 21:29:04.906: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7504
Dec  4 21:29:05.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  4 21:29:06.024: INFO: rc: 1
Dec  4 21:29:06.024: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc002e2cf90 exit status 1 <nil> <nil> true [0xc000f6b370 0xc000f6b5a0 0xc000f6b998] [0xc000f6b370 0xc000f6b5a0 0xc000f6b998] [0xc000f6b4e0 0xc000f6b920] [0xb916c0 0xb916c0] 0xc002e298c0 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

Dec  4 21:29:16.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  4 21:29:16.098: INFO: rc: 1
Dec  4 21:29:16.098: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002fbb8c0 exit status 1 <nil> <nil> true [0xc002070620 0xc002070678 0xc0020706a8] [0xc002070620 0xc002070678 0xc0020706a8] [0xc002070660 0xc002070688] [0xb916c0 0xb916c0] 0xc00268b020 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Dec  4 21:29:26.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  4 21:29:26.186: INFO: rc: 1
Dec  4 21:29:26.186: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002fbbc20 exit status 1 <nil> <nil> true [0xc0020706c8 0xc002070720 0xc002070738] [0xc0020706c8 0xc002070720 0xc002070738] [0xc002070708 0xc002070730] [0xb916c0 0xb916c0] 0xc00268b380 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Dec  4 21:29:36.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  4 21:29:36.268: INFO: rc: 1
Dec  4 21:29:36.268: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002e2d2c0 exit status 1 <nil> <nil> true [0xc000f6ba90 0xc000f6bb98 0xc000f6bcc8] [0xc000f6ba90 0xc000f6bb98 0xc000f6bcc8] [0xc000f6bb80 0xc000f6bc50] [0xb916c0 0xb916c0] 0xc002e29ce0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Dec  4 21:29:46.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  4 21:29:46.341: INFO: rc: 1
Dec  4 21:29:46.341: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002e2d620 exit status 1 <nil> <nil> true [0xc000f6bd70 0xc000f6be38 0xc000f6be78] [0xc000f6bd70 0xc000f6be38 0xc000f6be78] [0xc000f6be08 0xc000f6be60] [0xb916c0 0xb916c0] 0xc002768240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Dec  4 21:29:56.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  4 21:29:56.424: INFO: rc: 1
Dec  4 21:29:56.424: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002fbbfb0 exit status 1 <nil> <nil> true [0xc002070748 0xc002070760 0xc0020707c0] [0xc002070748 0xc002070760 0xc0020707c0] [0xc002070758 0xc0020707a0] [0xb916c0 0xb916c0] 0xc00268b920 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Dec  4 21:30:06.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  4 21:30:06.505: INFO: rc: 1
Dec  4 21:30:06.505: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002e2d950 exit status 1 <nil> <nil> true [0xc000f6be98 0xc000f6bf20 0xc001a14040] [0xc000f6be98 0xc000f6bf20 0xc001a14040] [0xc000f6beb8 0xc000f6bff0] [0xb916c0 0xb916c0] 0xc002768fc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Dec  4 21:30:16.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  4 21:30:16.589: INFO: rc: 1
Dec  4 21:30:16.589: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002e2dcb0 exit status 1 <nil> <nil> true [0xc001a14120 0xc001a142d8 0xc001a14590] [0xc001a14120 0xc001a142d8 0xc001a14590] [0xc001a141f8 0xc001a144a8] [0xb916c0 0xb916c0] 0xc002769920 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Dec  4 21:30:26.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  4 21:30:26.675: INFO: rc: 1
Dec  4 21:30:26.675: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0025a6180 exit status 1 <nil> <nil> true [0xc001a14620 0xc001a148a8 0xc001a149b0] [0xc001a14620 0xc001a148a8 0xc001a149b0] [0xc001a147d8 0xc001a14990] [0xb916c0 0xb916c0] 0xc001822180 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Dec  4 21:30:36.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  4 21:30:36.748: INFO: rc: 1
Dec  4 21:30:36.748: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00175c3c0 exit status 1 <nil> <nil> true [0xc0020707d0 0xc0020707f8 0xc002070810] [0xc0020707d0 0xc0020707f8 0xc002070810] [0xc0020707f0 0xc002070808] [0xb916c0 0xb916c0] 0xc00268bec0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Dec  4 21:30:46.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  4 21:30:46.845: INFO: rc: 1
Dec  4 21:30:46.846: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0025a64e0 exit status 1 <nil> <nil> true [0xc001a14a40 0xc001a14bb0 0xc001a14c78] [0xc001a14a40 0xc001a14bb0 0xc001a14c78] [0xc001a14b88 0xc001a14c50] [0xb916c0 0xb916c0] 0xc001822480 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Dec  4 21:30:56.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  4 21:30:56.921: INFO: rc: 1
Dec  4 21:30:56.921: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002fba300 exit status 1 <nil> <nil> true [0xc000f6a718 0xc000f6aa70 0xc000f6b0b0] [0xc000f6a718 0xc000f6aa70 0xc000f6b0b0] [0xc000f6a9a0 0xc000f6ae20] [0xb916c0 0xb916c0] 0xc002768840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Dec  4 21:31:06.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  4 21:31:06.997: INFO: rc: 1
Dec  4 21:31:06.997: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002e2c360 exit status 1 <nil> <nil> true [0xc0000c41c8 0xc0000c4d58 0xc0000c5220] [0xc0000c41c8 0xc0000c4d58 0xc0000c5220] [0xc0000c4bf8 0xc0000c50d8] [0xb916c0 0xb916c0] 0xc002e28540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Dec  4 21:31:16.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  4 21:31:17.077: INFO: rc: 1
Dec  4 21:31:17.078: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002fba630 exit status 1 <nil> <nil> true [0xc000f6b240 0xc000f6b3d0 0xc000f6b7c0] [0xc000f6b240 0xc000f6b3d0 0xc000f6b7c0] [0xc000f6b370 0xc000f6b5a0] [0xb916c0 0xb916c0] 0xc002769500 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Dec  4 21:31:27.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  4 21:31:27.150: INFO: rc: 1
Dec  4 21:31:27.150: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002e2c6c0 exit status 1 <nil> <nil> true [0xc0000c5288 0xc0000c5788 0xc0000c5958] [0xc0000c5288 0xc0000c5788 0xc0000c5958] [0xc0000c5690 0xc0000c58e0] [0xb916c0 0xb916c0] 0xc002e28ba0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Dec  4 21:31:37.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  4 21:31:37.228: INFO: rc: 1
Dec  4 21:31:37.228: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002fba990 exit status 1 <nil> <nil> true [0xc000f6b920 0xc000f6baf0 0xc000f6bbb8] [0xc000f6b920 0xc000f6baf0 0xc000f6bbb8] [0xc000f6ba90 0xc000f6bb98] [0xb916c0 0xb916c0] 0xc002769e60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Dec  4 21:31:47.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  4 21:31:47.304: INFO: rc: 1
Dec  4 21:31:47.304: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002fbacf0 exit status 1 <nil> <nil> true [0xc000f6bc50 0xc000f6bdd8 0xc000f6be48] [0xc000f6bc50 0xc000f6bdd8 0xc000f6be48] [0xc000f6bd70 0xc000f6be38] [0xb916c0 0xb916c0] 0xc00268a420 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Dec  4 21:31:57.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  4 21:31:57.375: INFO: rc: 1
Dec  4 21:31:57.375: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002fbb050 exit status 1 <nil> <nil> true [0xc000f6be60 0xc000f6bea8 0xc000f6bf68] [0xc000f6be60 0xc000f6bea8 0xc000f6bf68] [0xc000f6be98 0xc000f6bf20] [0xb916c0 0xb916c0] 0xc00268a780 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Dec  4 21:32:07.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  4 21:32:07.461: INFO: rc: 1
Dec  4 21:32:07.461: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002e2ca20 exit status 1 <nil> <nil> true [0xc0000c59d0 0xc0000c5b68 0xc0000c5d40] [0xc0000c59d0 0xc0000c5b68 0xc0000c5d40] [0xc0000c5ad0 0xc0000c5c98] [0xb916c0 0xb916c0] 0xc002e29260 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Dec  4 21:32:17.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  4 21:32:17.540: INFO: rc: 1
Dec  4 21:32:17.540: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002fbb3b0 exit status 1 <nil> <nil> true [0xc000f6bff0 0xc0007bd500 0xc0007bdb00] [0xc000f6bff0 0xc0007bd500 0xc0007bdb00] [0xc000010308 0xc0007bda38] [0xb916c0 0xb916c0] 0xc00268aae0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Dec  4 21:32:27.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  4 21:32:27.617: INFO: rc: 1
Dec  4 21:32:27.617: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002fbb710 exit status 1 <nil> <nil> true [0xc0007bdda0 0xc0007bdf08 0xc002070078] [0xc0007bdda0 0xc0007bdf08 0xc002070078] [0xc0007bded0 0xc002070028] [0xb916c0 0xb916c0] 0xc00268ae40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Dec  4 21:32:37.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  4 21:32:37.688: INFO: rc: 1
Dec  4 21:32:37.688: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002e2cf60 exit status 1 <nil> <nil> true [0xc0000c5dd0 0xc0000c5f48 0xc001a141a0] [0xc0000c5dd0 0xc0000c5f48 0xc001a141a0] [0xc0000c5ec0 0xc001a14120] [0xb916c0 0xb916c0] 0xc002e29920 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Dec  4 21:32:47.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  4 21:32:47.775: INFO: rc: 1
Dec  4 21:32:47.775: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002fbbad0 exit status 1 <nil> <nil> true [0xc0020700d8 0xc002070150 0xc0020701e8] [0xc0020700d8 0xc002070150 0xc0020701e8] [0xc002070110 0xc0020701c0] [0xb916c0 0xb916c0] 0xc00268b1a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Dec  4 21:32:57.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  4 21:32:57.851: INFO: rc: 1
Dec  4 21:32:57.851: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002e2c330 exit status 1 <nil> <nil> true [0xc0007bd500 0xc0007bdb00 0xc0007bded0] [0xc0007bd500 0xc0007bdb00 0xc0007bded0] [0xc0007bda38 0xc0007bde78] [0xb916c0 0xb916c0] 0xc002768840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Dec  4 21:33:07.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  4 21:33:07.922: INFO: rc: 1
Dec  4 21:33:07.922: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002fba360 exit status 1 <nil> <nil> true [0xc000010010 0xc0000c45b8 0xc0000c4f60] [0xc000010010 0xc0000c45b8 0xc0000c4f60] [0xc0000c41c8 0xc0000c4d58] [0xb916c0 0xb916c0] 0xc002e28540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Dec  4 21:33:17.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  4 21:33:17.990: INFO: rc: 1
Dec  4 21:33:17.990: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002fba6f0 exit status 1 <nil> <nil> true [0xc0000c50d8 0xc0000c5588 0xc0000c5830] [0xc0000c50d8 0xc0000c5588 0xc0000c5830] [0xc0000c5288 0xc0000c5788] [0xb916c0 0xb916c0] 0xc002e28ba0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Dec  4 21:33:27.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  4 21:33:28.060: INFO: rc: 1
Dec  4 21:33:28.060: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002e2c6f0 exit status 1 <nil> <nil> true [0xc0007bdf08 0xc000f6a8f8 0xc000f6ac48] [0xc0007bdf08 0xc000f6a8f8 0xc000f6ac48] [0xc000f6a718 0xc000f6aa70] [0xb916c0 0xb916c0] 0xc002769500 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Dec  4 21:33:38.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  4 21:33:38.141: INFO: rc: 1
Dec  4 21:33:38.141: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002fbaa50 exit status 1 <nil> <nil> true [0xc0000c58e0 0xc0000c5ab8 0xc0000c5c28] [0xc0000c58e0 0xc0000c5ab8 0xc0000c5c28] [0xc0000c59d0 0xc0000c5b68] [0xb916c0 0xb916c0] 0xc002e29260 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Dec  4 21:33:48.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  4 21:33:48.229: INFO: rc: 1
Dec  4 21:33:48.229: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002fbade0 exit status 1 <nil> <nil> true [0xc0000c5c98 0xc0000c5ea8 0xc001a14040] [0xc0000c5c98 0xc0000c5ea8 0xc001a14040] [0xc0000c5dd0 0xc0000c5f48] [0xb916c0 0xb916c0] 0xc002e29920 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Dec  4 21:33:58.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  4 21:33:58.304: INFO: rc: 1
Dec  4 21:33:58.305: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002fbb140 exit status 1 <nil> <nil> true [0xc001a14120 0xc001a142d8 0xc001a14590] [0xc001a14120 0xc001a142d8 0xc001a14590] [0xc001a141f8 0xc001a144a8] [0xb916c0 0xb916c0] 0xc002e29ce0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Dec  4 21:34:08.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7504 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  4 21:34:08.376: INFO: rc: 1
Dec  4 21:34:08.376: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: 
Dec  4 21:34:08.376: INFO: Scaling statefulset ss to 0
Dec  4 21:34:08.381: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Dec  4 21:34:08.383: INFO: Deleting all statefulset in ns statefulset-7504
Dec  4 21:34:08.384: INFO: Scaling statefulset ss to 0
Dec  4 21:34:08.388: INFO: Waiting for statefulset status.replicas updated to 0
Dec  4 21:34:08.390: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:34:08.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7504" for this suite.
Dec  4 21:34:14.412: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:34:14.479: INFO: namespace statefulset-7504 deletion completed in 6.078287608s

• [SLOW TEST:370.409 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:34:14.480: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-6002
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  4 21:34:14.612: INFO: Creating ReplicaSet my-hostname-basic-d1783ed3-16dd-11ea-8695-527d496f91de
Dec  4 21:34:14.629: INFO: Pod name my-hostname-basic-d1783ed3-16dd-11ea-8695-527d496f91de: Found 0 pods out of 1
Dec  4 21:34:19.633: INFO: Pod name my-hostname-basic-d1783ed3-16dd-11ea-8695-527d496f91de: Found 1 pods out of 1
Dec  4 21:34:19.633: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-d1783ed3-16dd-11ea-8695-527d496f91de" is running
Dec  4 21:34:19.635: INFO: Pod "my-hostname-basic-d1783ed3-16dd-11ea-8695-527d496f91de-ppjvh" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-04 21:34:13 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-04 21:34:15 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-04 21:34:15 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-04 21:34:14 +0000 UTC Reason: Message:}])
Dec  4 21:34:19.635: INFO: Trying to dial the pod
Dec  4 21:34:24.645: INFO: Controller my-hostname-basic-d1783ed3-16dd-11ea-8695-527d496f91de: Got expected result from replica 1 [my-hostname-basic-d1783ed3-16dd-11ea-8695-527d496f91de-ppjvh]: "my-hostname-basic-d1783ed3-16dd-11ea-8695-527d496f91de-ppjvh", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:34:24.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6002" for this suite.
Dec  4 21:34:30.658: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:34:30.748: INFO: namespace replicaset-6002 deletion completed in 6.099094111s

• [SLOW TEST:16.268 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:34:30.748: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-410
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1174
STEP: creating the pod
Dec  4 21:34:30.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 create -f - --namespace=kubectl-410'
Dec  4 21:34:31.325: INFO: stderr: ""
Dec  4 21:34:31.325: INFO: stdout: "pod/pause created\n"
Dec  4 21:34:31.325: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Dec  4 21:34:31.325: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-410" to be "running and ready"
Dec  4 21:34:31.331: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 5.909488ms
Dec  4 21:34:33.334: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.008576787s
Dec  4 21:34:33.334: INFO: Pod "pause" satisfied condition "running and ready"
Dec  4 21:34:33.334: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: adding the label testing-label with value testing-label-value to a pod
Dec  4 21:34:33.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 label pods pause testing-label=testing-label-value --namespace=kubectl-410'
Dec  4 21:34:33.426: INFO: stderr: ""
Dec  4 21:34:33.426: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Dec  4 21:34:33.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get pod pause -L testing-label --namespace=kubectl-410'
Dec  4 21:34:33.503: INFO: stderr: ""
Dec  4 21:34:33.503: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Dec  4 21:34:33.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 label pods pause testing-label- --namespace=kubectl-410'
Dec  4 21:34:33.584: INFO: stderr: ""
Dec  4 21:34:33.584: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Dec  4 21:34:33.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get pod pause -L testing-label --namespace=kubectl-410'
Dec  4 21:34:33.668: INFO: stderr: ""
Dec  4 21:34:33.668: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1181
STEP: using delete to clean up resources
Dec  4 21:34:33.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 delete --grace-period=0 --force -f - --namespace=kubectl-410'
Dec  4 21:34:33.768: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  4 21:34:33.768: INFO: stdout: "pod \"pause\" force deleted\n"
Dec  4 21:34:33.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get rc,svc -l name=pause --no-headers --namespace=kubectl-410'
Dec  4 21:34:33.880: INFO: stderr: "No resources found.\n"
Dec  4 21:34:33.880: INFO: stdout: ""
Dec  4 21:34:33.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get pods -l name=pause --namespace=kubectl-410 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec  4 21:34:33.961: INFO: stderr: ""
Dec  4 21:34:33.961: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:34:33.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-410" for this suite.
Dec  4 21:34:39.974: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:34:40.051: INFO: namespace kubectl-410 deletion completed in 6.08571787s

• [SLOW TEST:9.303 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:34:40.054: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-336
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-e0b68adb-16dd-11ea-8695-527d496f91de
STEP: Creating a pod to test consume secrets
Dec  4 21:34:40.202: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e0b6f1bd-16dd-11ea-8695-527d496f91de" in namespace "projected-336" to be "success or failure"
Dec  4 21:34:40.206: INFO: Pod "pod-projected-secrets-e0b6f1bd-16dd-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 4.167199ms
Dec  4 21:34:42.209: INFO: Pod "pod-projected-secrets-e0b6f1bd-16dd-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006945757s
Dec  4 21:34:44.212: INFO: Pod "pod-projected-secrets-e0b6f1bd-16dd-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009864942s
STEP: Saw pod success
Dec  4 21:34:44.212: INFO: Pod "pod-projected-secrets-e0b6f1bd-16dd-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 21:34:44.214: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker9f6c17cd60 pod pod-projected-secrets-e0b6f1bd-16dd-11ea-8695-527d496f91de container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  4 21:34:44.242: INFO: Waiting for pod pod-projected-secrets-e0b6f1bd-16dd-11ea-8695-527d496f91de to disappear
Dec  4 21:34:44.243: INFO: Pod pod-projected-secrets-e0b6f1bd-16dd-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:34:44.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-336" for this suite.
Dec  4 21:34:50.255: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:34:50.325: INFO: namespace projected-336 deletion completed in 6.078105513s

• [SLOW TEST:10.271 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:34:50.325: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-142
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-142
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec  4 21:34:50.460: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec  4 21:35:14.539: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.2.117:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-142 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  4 21:35:14.539: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
Dec  4 21:35:14.679: INFO: Found all expected endpoints: [netserver-0]
Dec  4 21:35:14.681: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.1.123:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-142 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  4 21:35:14.681: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
Dec  4 21:35:14.829: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:35:14.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-142" for this suite.
Dec  4 21:35:36.843: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:35:36.929: INFO: namespace pod-network-test-142 deletion completed in 22.096856657s

• [SLOW TEST:46.604 seconds]
[sig-network] Networking
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:35:36.930: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-4296
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test hostPath mode
Dec  4 21:35:37.074: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-4296" to be "success or failure"
Dec  4 21:35:37.082: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 8.024744ms
Dec  4 21:35:39.084: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010494679s
Dec  4 21:35:41.088: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013793509s
STEP: Saw pod success
Dec  4 21:35:41.088: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Dec  4 21:35:41.090: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker6c7da5d4d5 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Dec  4 21:35:41.200: INFO: Waiting for pod pod-host-path-test to disappear
Dec  4 21:35:41.220: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:35:41.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-4296" for this suite.
Dec  4 21:35:47.273: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:35:47.338: INFO: namespace hostpath-4296 deletion completed in 6.095213758s

• [SLOW TEST:10.409 seconds]
[sig-storage] HostPath
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:35:47.339: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4424
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec  4 21:35:47.481: INFO: Waiting up to 5m0s for pod "pod-08d1a5a1-16de-11ea-8695-527d496f91de" in namespace "emptydir-4424" to be "success or failure"
Dec  4 21:35:47.486: INFO: Pod "pod-08d1a5a1-16de-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 4.893998ms
Dec  4 21:35:49.488: INFO: Pod "pod-08d1a5a1-16de-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007225101s
Dec  4 21:35:51.493: INFO: Pod "pod-08d1a5a1-16de-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012544901s
STEP: Saw pod success
Dec  4 21:35:51.493: INFO: Pod "pod-08d1a5a1-16de-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 21:35:51.497: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker9f6c17cd60 pod pod-08d1a5a1-16de-11ea-8695-527d496f91de container test-container: <nil>
STEP: delete the pod
Dec  4 21:35:51.516: INFO: Waiting for pod pod-08d1a5a1-16de-11ea-8695-527d496f91de to disappear
Dec  4 21:35:51.518: INFO: Pod pod-08d1a5a1-16de-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:35:51.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4424" for this suite.
Dec  4 21:35:57.544: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:35:57.628: INFO: namespace emptydir-4424 deletion completed in 6.096467173s

• [SLOW TEST:10.289 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:35:57.630: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-577
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1384
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  4 21:35:57.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-577'
Dec  4 21:35:57.882: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec  4 21:35:57.882: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1390
Dec  4 21:35:57.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 delete deployment e2e-test-nginx-deployment --namespace=kubectl-577'
Dec  4 21:35:58.021: INFO: stderr: ""
Dec  4 21:35:58.021: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:35:58.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-577" for this suite.
Dec  4 21:36:04.041: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:36:04.113: INFO: namespace kubectl-577 deletion completed in 6.087670747s

• [SLOW TEST:6.482 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:36:04.114: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-2624
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Dec  4 21:36:04.245: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:36:07.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2624" for this suite.
Dec  4 21:36:13.314: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:36:13.400: INFO: namespace init-container-2624 deletion completed in 6.098595498s

• [SLOW TEST:9.286 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:36:13.400: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3541
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:37:13.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3541" for this suite.
Dec  4 21:37:35.567: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:37:35.643: INFO: namespace container-probe-3541 deletion completed in 22.085892083s

• [SLOW TEST:82.243 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:37:35.644: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8779
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  4 21:37:35.785: INFO: Waiting up to 5m0s for pod "downwardapi-volume-495f9d13-16de-11ea-8695-527d496f91de" in namespace "projected-8779" to be "success or failure"
Dec  4 21:37:35.791: INFO: Pod "downwardapi-volume-495f9d13-16de-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021451ms
Dec  4 21:37:37.797: INFO: Pod "downwardapi-volume-495f9d13-16de-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011568864s
Dec  4 21:37:39.800: INFO: Pod "downwardapi-volume-495f9d13-16de-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014754366s
STEP: Saw pod success
Dec  4 21:37:39.800: INFO: Pod "downwardapi-volume-495f9d13-16de-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 21:37:39.803: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker9f6c17cd60 pod downwardapi-volume-495f9d13-16de-11ea-8695-527d496f91de container client-container: <nil>
STEP: delete the pod
Dec  4 21:37:39.828: INFO: Waiting for pod downwardapi-volume-495f9d13-16de-11ea-8695-527d496f91de to disappear
Dec  4 21:37:39.830: INFO: Pod downwardapi-volume-495f9d13-16de-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:37:39.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8779" for this suite.
Dec  4 21:37:45.849: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:37:45.920: INFO: namespace projected-8779 deletion completed in 6.0870679s

• [SLOW TEST:10.275 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:37:45.920: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7324
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  4 21:37:46.064: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Dec  4 21:37:51.068: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec  4 21:37:51.068: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Dec  4 21:37:55.104: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-7324,SelfLink:/apis/apps/v1/namespaces/deployment-7324/deployments/test-cleanup-deployment,UID:527db1db-16de-11ea-825c-005056950656,ResourceVersion:289914,Generation:1,CreationTimestamp:2019-12-04 21:37:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 1,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-12-04 21:37:51 +0000 UTC 2019-12-04 21:37:51 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-12-04 21:37:53 +0000 UTC 2019-12-04 21:37:51 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-cleanup-deployment-6865c98b76" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Dec  4 21:37:55.106: INFO: New ReplicaSet "test-cleanup-deployment-6865c98b76" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-6865c98b76,GenerateName:,Namespace:deployment-7324,SelfLink:/apis/apps/v1/namespaces/deployment-7324/replicasets/test-cleanup-deployment-6865c98b76,UID:527fd63f-16de-11ea-825c-005056950656,ResourceVersion:289904,Generation:1,CreationTimestamp:2019-12-04 21:37:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 6865c98b76,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 527db1db-16de-11ea-825c-005056950656 0xc0022283a7 0xc0022283a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 6865c98b76,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 6865c98b76,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Dec  4 21:37:55.108: INFO: Pod "test-cleanup-deployment-6865c98b76-zj9t6" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-6865c98b76-zj9t6,GenerateName:test-cleanup-deployment-6865c98b76-,Namespace:deployment-7324,SelfLink:/api/v1/namespaces/deployment-7324/pods/test-cleanup-deployment-6865c98b76-zj9t6,UID:52807ac8-16de-11ea-825c-005056950656,ResourceVersion:289903,Generation:0,CreationTimestamp:2019-12-04 21:37:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 6865c98b76,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.1.127/32,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-6865c98b76 527fd63f-16de-11ea-825c-005056950656 0xc002229bc7 0xc002229bc8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-k8qtq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-k8qtq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-k8qtq true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v2-vsp1-worker9f6c17cd60,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002229d70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002229d90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:37:50 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:37:52 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:37:52 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:37:51 +0000 UTC  }],Message:,Reason:,HostIP:10.10.128.22,PodIP:192.168.1.127,StartTime:2019-12-04 21:37:50 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-12-04 21:37:51 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://33b13ebf2c9c0ba00b11228ac85465bd3ca69808f5cb2bbf375436a46e51088c}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:37:55.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7324" for this suite.
Dec  4 21:38:01.120: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:38:01.190: INFO: namespace deployment-7324 deletion completed in 6.078374976s

• [SLOW TEST:15.270 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:38:01.190: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4038
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-4038
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-4038
STEP: Creating statefulset with conflicting port in namespace statefulset-4038
STEP: Waiting until pod test-pod will start running in namespace statefulset-4038
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-4038
Dec  4 21:38:05.384: INFO: Observed stateful pod in namespace: statefulset-4038, name: ss-0, uid: 58ad747d-16de-11ea-825c-005056950656, status phase: Pending. Waiting for statefulset controller to delete.
Dec  4 21:38:09.674: INFO: Observed stateful pod in namespace: statefulset-4038, name: ss-0, uid: 58ad747d-16de-11ea-825c-005056950656, status phase: Failed. Waiting for statefulset controller to delete.
Dec  4 21:38:09.686: INFO: Observed stateful pod in namespace: statefulset-4038, name: ss-0, uid: 58ad747d-16de-11ea-825c-005056950656, status phase: Failed. Waiting for statefulset controller to delete.
Dec  4 21:38:09.692: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-4038
STEP: Removing pod with conflicting port in namespace statefulset-4038
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-4038 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Dec  4 21:38:13.737: INFO: Deleting all statefulset in ns statefulset-4038
Dec  4 21:38:13.740: INFO: Scaling statefulset ss to 0
Dec  4 21:38:23.755: INFO: Waiting for statefulset status.replicas updated to 0
Dec  4 21:38:23.757: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:38:23.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4038" for this suite.
Dec  4 21:38:29.788: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:38:29.896: INFO: namespace statefulset-4038 deletion completed in 6.119976902s

• [SLOW TEST:28.706 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:38:29.903: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-124
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Dec  4 21:38:30.051: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec  4 21:38:30.057: INFO: Waiting for terminating namespaces to be deleted...
Dec  4 21:38:30.059: INFO: 
Logging pods the kubelet thinks is on node alex-slot1-v2-vsp1-worker6c7da5d4d5 before test
Dec  4 21:38:30.067: INFO: metallb-controller-577b6fb44-cxzgk from ccp started at 2019-12-02 22:34:24 +0000 UTC (1 container statuses recorded)
Dec  4 21:38:30.067: INFO: 	Container controller ready: true, restart count 0
Dec  4 21:38:30.067: INFO: ccp-monitor-prometheus-alertmanager-69f5f8bfbb-wrqkt from ccp started at 2019-12-02 22:34:32 +0000 UTC (2 container statuses recorded)
Dec  4 21:38:30.067: INFO: 	Container prometheus-alertmanager ready: true, restart count 0
Dec  4 21:38:30.067: INFO: 	Container prometheus-alertmanager-configmap-reload ready: true, restart count 0
Dec  4 21:38:30.067: INFO: ccp-monitor-prometheus-kube-state-metrics-dc8476fb4-h4mhf from ccp started at 2019-12-02 22:34:32 +0000 UTC (1 container statuses recorded)
Dec  4 21:38:30.067: INFO: 	Container prometheus-kube-state-metrics ready: true, restart count 0
Dec  4 21:38:30.067: INFO: ccp-monitor-grafana-cf94d4698-582qk from ccp started at 2019-12-02 22:34:33 +0000 UTC (1 container statuses recorded)
Dec  4 21:38:30.067: INFO: 	Container grafana ready: true, restart count 0
Dec  4 21:38:30.067: INFO: fluentd-es-v2.0.2-99h8x from ccp started at 2019-12-02 22:34:33 +0000 UTC (1 container statuses recorded)
Dec  4 21:38:30.067: INFO: 	Container fluentd-es ready: true, restart count 0
Dec  4 21:38:30.067: INFO: coredns-69cb7f6694-qph4l from kube-system started at 2019-12-02 22:29:13 +0000 UTC (1 container statuses recorded)
Dec  4 21:38:30.067: INFO: 	Container coredns ready: true, restart count 0
Dec  4 21:38:30.067: INFO: nginx-ingress-controller-nsxf4 from ccp started at 2019-12-02 22:34:24 +0000 UTC (1 container statuses recorded)
Dec  4 21:38:30.067: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec  4 21:38:30.067: INFO: nvidia-device-plugin-daemonset-vpx26 from kube-system started at 2019-12-02 22:29:04 +0000 UTC (1 container statuses recorded)
Dec  4 21:38:30.067: INFO: 	Container nvidia-device-plugin-ctr ready: true, restart count 0
Dec  4 21:38:30.067: INFO: ccp-harbor-harbor-adminserver-5c6f5d5b8c-w2p9p from default started at 2019-12-02 22:34:37 +0000 UTC (1 container statuses recorded)
Dec  4 21:38:30.067: INFO: 	Container adminserver ready: true, restart count 1
Dec  4 21:38:30.067: INFO: ccp-harbor-harbor-disable-self-reg-job-98zz9 from default started at 2019-12-02 22:34:37 +0000 UTC (1 container statuses recorded)
Dec  4 21:38:30.067: INFO: 	Container self-registration ready: false, restart count 0
Dec  4 21:38:30.067: INFO: ccp-efk-elasticsearch-curator-1575334800-f92rq from ccp started at 2019-12-03 01:00:04 +0000 UTC (1 container statuses recorded)
Dec  4 21:38:30.067: INFO: 	Container elasticsearch-curator ready: false, restart count 0
Dec  4 21:38:30.067: INFO: sonobuoy-systemd-logs-daemon-set-f8654487368d4bd0-qjhgr from sonobuoy started at 2019-12-04 20:32:52 +0000 UTC (2 container statuses recorded)
Dec  4 21:38:30.067: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec  4 21:38:30.067: INFO: 	Container systemd-logs ready: true, restart count 0
Dec  4 21:38:30.067: INFO: cert-manager-799695b5bc-8gkw4 from ccp started at 2019-12-02 22:29:09 +0000 UTC (1 container statuses recorded)
Dec  4 21:38:30.067: INFO: 	Container cert-manager ready: true, restart count 0
Dec  4 21:38:30.067: INFO: metallb-speaker-np9rf from ccp started at 2019-12-02 22:34:24 +0000 UTC (1 container statuses recorded)
Dec  4 21:38:30.067: INFO: 	Container speaker ready: true, restart count 0
Dec  4 21:38:30.067: INFO: ccp-monitor-prometheus-port-update-1a9rs-qj4gx from ccp started at 2019-12-02 22:34:32 +0000 UTC (1 container statuses recorded)
Dec  4 21:38:30.067: INFO: 	Container ccp-monitor-prometheus-port-update ready: false, restart count 0
Dec  4 21:38:30.067: INFO: ccp-harbor-harbor-core-6c5d44857d-cnzn8 from default started at 2019-12-02 22:34:37 +0000 UTC (1 container statuses recorded)
Dec  4 21:38:30.067: INFO: 	Container core ready: true, restart count 0
Dec  4 21:38:30.067: INFO: kube-proxy-2krtb from kube-system started at 2019-12-02 22:28:54 +0000 UTC (1 container statuses recorded)
Dec  4 21:38:30.067: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  4 21:38:30.067: INFO: calico-node-27hrx from kube-system started at 2019-12-02 22:28:54 +0000 UTC (1 container statuses recorded)
Dec  4 21:38:30.067: INFO: 	Container calico-node ready: true, restart count 0
Dec  4 21:38:30.067: INFO: ccp-monitor-prometheus-node-exporter-d6xhl from ccp started at 2019-12-02 22:34:32 +0000 UTC (1 container statuses recorded)
Dec  4 21:38:30.067: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Dec  4 21:38:30.067: INFO: ccp-efk-kibana-56866fb888-sx6dh from ccp started at 2019-12-02 22:34:33 +0000 UTC (1 container statuses recorded)
Dec  4 21:38:30.067: INFO: 	Container kibana ready: true, restart count 0
Dec  4 21:38:30.067: INFO: ccp-harbor-harbor-database-0 from default started at 2019-12-02 22:34:38 +0000 UTC (1 container statuses recorded)
Dec  4 21:38:30.067: INFO: 	Container database ready: true, restart count 0
Dec  4 21:38:30.067: INFO: sonobuoy from sonobuoy started at 2019-12-04 20:32:21 +0000 UTC (1 container statuses recorded)
Dec  4 21:38:30.067: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec  4 21:38:30.067: INFO: 
Logging pods the kubelet thinks is on node alex-slot1-v2-vsp1-worker9f6c17cd60 before test
Dec  4 21:38:30.077: INFO: nginx-ingress-controller-dbfcd from ccp started at 2019-12-02 22:34:23 +0000 UTC (1 container statuses recorded)
Dec  4 21:38:30.077: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec  4 21:38:30.077: INFO: ccp-monitor-prometheus-node-exporter-mzgmm from ccp started at 2019-12-02 22:34:31 +0000 UTC (1 container statuses recorded)
Dec  4 21:38:30.077: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Dec  4 21:38:30.077: INFO: ccp-harbor-harbor-portal-7445674c74-x2xqp from default started at 2019-12-02 22:34:36 +0000 UTC (1 container statuses recorded)
Dec  4 21:38:30.077: INFO: 	Container portal ready: true, restart count 0
Dec  4 21:38:30.077: INFO: ccp-efk-elasticsearch-curator-1575421200-wwgv5 from ccp started at 2019-12-04 01:00:03 +0000 UTC (1 container statuses recorded)
Dec  4 21:38:30.077: INFO: 	Container elasticsearch-curator ready: false, restart count 0
Dec  4 21:38:30.077: INFO: calico-node-p5tcj from kube-system started at 2019-12-02 22:28:53 +0000 UTC (1 container statuses recorded)
Dec  4 21:38:30.077: INFO: 	Container calico-node ready: true, restart count 0
Dec  4 21:38:30.077: INFO: fluentd-es-v2.0.2-f4j7n from ccp started at 2019-12-02 22:34:33 +0000 UTC (1 container statuses recorded)
Dec  4 21:38:30.077: INFO: 	Container fluentd-es ready: true, restart count 0
Dec  4 21:38:30.077: INFO: elasticsearch-logging-0 from ccp started at 2019-12-02 22:34:56 +0000 UTC (1 container statuses recorded)
Dec  4 21:38:30.077: INFO: 	Container elasticsearch-logging ready: true, restart count 0
Dec  4 21:38:30.077: INFO: ccp-monitor-prometheus-server-7745ccd984-jw4hf from ccp started at 2019-12-02 22:34:33 +0000 UTC (3 container statuses recorded)
Dec  4 21:38:30.077: INFO: 	Container nginx-proxy ready: true, restart count 0
Dec  4 21:38:30.077: INFO: 	Container prometheus-server ready: true, restart count 0
Dec  4 21:38:30.077: INFO: 	Container prometheus-server-configmap-reload ready: true, restart count 0
Dec  4 21:38:30.077: INFO: ccp-harbor-harbor-jobservice-755d997849-vvz4g from default started at 2019-12-02 22:34:37 +0000 UTC (1 container statuses recorded)
Dec  4 21:38:30.077: INFO: 	Container jobservice ready: true, restart count 0
Dec  4 21:38:30.077: INFO: ccp-harbor-harbor-registry-77f88c7594-xd2nm from default started at 2019-12-02 22:34:38 +0000 UTC (2 container statuses recorded)
Dec  4 21:38:30.077: INFO: 	Container registry ready: true, restart count 0
Dec  4 21:38:30.077: INFO: 	Container registryctl ready: true, restart count 0
Dec  4 21:38:30.077: INFO: sonobuoy-systemd-logs-daemon-set-f8654487368d4bd0-j78pc from sonobuoy started at 2019-12-04 20:32:46 +0000 UTC (2 container statuses recorded)
Dec  4 21:38:30.077: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec  4 21:38:30.077: INFO: 	Container systemd-logs ready: true, restart count 0
Dec  4 21:38:30.077: INFO: nvidia-device-plugin-daemonset-km64c from kube-system started at 2019-12-02 22:29:03 +0000 UTC (1 container statuses recorded)
Dec  4 21:38:30.077: INFO: 	Container nvidia-device-plugin-ctr ready: true, restart count 0
Dec  4 21:38:30.077: INFO: nginx-ingress-default-backend-dc5596844-xss9h from ccp started at 2019-12-02 22:34:23 +0000 UTC (1 container statuses recorded)
Dec  4 21:38:30.077: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Dec  4 21:38:30.077: INFO: ccp-monitor-prometheus-pushgateway-7dcdb9d494-pzvxm from ccp started at 2019-12-02 22:34:31 +0000 UTC (1 container statuses recorded)
Dec  4 21:38:30.077: INFO: 	Container prometheus-pushgateway ready: true, restart count 0
Dec  4 21:38:30.077: INFO: metallb-speaker-xm9d9 from ccp started at 2019-12-02 22:34:24 +0000 UTC (1 container statuses recorded)
Dec  4 21:38:30.077: INFO: 	Container speaker ready: true, restart count 0
Dec  4 21:38:30.077: INFO: kubernetes-dashboard-67d74b6485-wv99t from ccp started at 2019-12-02 22:34:25 +0000 UTC (1 container statuses recorded)
Dec  4 21:38:30.077: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Dec  4 21:38:30.077: INFO: ccp-monitor-grafana-set-datasource-zkb8j from ccp started at 2019-12-02 22:34:31 +0000 UTC (1 container statuses recorded)
Dec  4 21:38:30.077: INFO: 	Container ccp-monitor-grafana-set-datasource ready: false, restart count 0
Dec  4 21:38:30.077: INFO: coredns-69cb7f6694-m5vmc from kube-system started at 2019-12-02 22:29:13 +0000 UTC (1 container statuses recorded)
Dec  4 21:38:30.077: INFO: 	Container coredns ready: true, restart count 0
Dec  4 21:38:30.077: INFO: kube-proxy-xxz5s from kube-system started at 2019-12-02 22:28:53 +0000 UTC (1 container statuses recorded)
Dec  4 21:38:30.077: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  4 21:38:30.077: INFO: ccp-monitor-prometheus-pass-job-no1bx-7bmdq from ccp started at 2019-12-02 22:34:31 +0000 UTC (1 container statuses recorded)
Dec  4 21:38:30.077: INFO: 	Container ccp-monitor-prometheus-pass-container ready: false, restart count 0
Dec  4 21:38:30.077: INFO: ccp-harbor-harbor-redis-0 from default started at 2019-12-02 22:34:38 +0000 UTC (1 container statuses recorded)
Dec  4 21:38:30.077: INFO: 	Container redis ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: verifying the node has the label node alex-slot1-v2-vsp1-worker6c7da5d4d5
STEP: verifying the node has the label node alex-slot1-v2-vsp1-worker9f6c17cd60
Dec  4 21:38:30.136: INFO: Pod ccp-efk-kibana-56866fb888-sx6dh requesting resource cpu=0m on Node alex-slot1-v2-vsp1-worker6c7da5d4d5
Dec  4 21:38:30.136: INFO: Pod ccp-monitor-grafana-cf94d4698-582qk requesting resource cpu=0m on Node alex-slot1-v2-vsp1-worker6c7da5d4d5
Dec  4 21:38:30.136: INFO: Pod ccp-monitor-prometheus-alertmanager-69f5f8bfbb-wrqkt requesting resource cpu=0m on Node alex-slot1-v2-vsp1-worker6c7da5d4d5
Dec  4 21:38:30.136: INFO: Pod ccp-monitor-prometheus-kube-state-metrics-dc8476fb4-h4mhf requesting resource cpu=0m on Node alex-slot1-v2-vsp1-worker6c7da5d4d5
Dec  4 21:38:30.136: INFO: Pod ccp-monitor-prometheus-node-exporter-d6xhl requesting resource cpu=0m on Node alex-slot1-v2-vsp1-worker6c7da5d4d5
Dec  4 21:38:30.136: INFO: Pod ccp-monitor-prometheus-node-exporter-mzgmm requesting resource cpu=0m on Node alex-slot1-v2-vsp1-worker9f6c17cd60
Dec  4 21:38:30.136: INFO: Pod ccp-monitor-prometheus-pushgateway-7dcdb9d494-pzvxm requesting resource cpu=0m on Node alex-slot1-v2-vsp1-worker9f6c17cd60
Dec  4 21:38:30.136: INFO: Pod ccp-monitor-prometheus-server-7745ccd984-jw4hf requesting resource cpu=0m on Node alex-slot1-v2-vsp1-worker9f6c17cd60
Dec  4 21:38:30.136: INFO: Pod cert-manager-799695b5bc-8gkw4 requesting resource cpu=0m on Node alex-slot1-v2-vsp1-worker6c7da5d4d5
Dec  4 21:38:30.136: INFO: Pod elasticsearch-logging-0 requesting resource cpu=100m on Node alex-slot1-v2-vsp1-worker9f6c17cd60
Dec  4 21:38:30.136: INFO: Pod fluentd-es-v2.0.2-99h8x requesting resource cpu=100m on Node alex-slot1-v2-vsp1-worker6c7da5d4d5
Dec  4 21:38:30.136: INFO: Pod fluentd-es-v2.0.2-f4j7n requesting resource cpu=100m on Node alex-slot1-v2-vsp1-worker9f6c17cd60
Dec  4 21:38:30.136: INFO: Pod kubernetes-dashboard-67d74b6485-wv99t requesting resource cpu=100m on Node alex-slot1-v2-vsp1-worker9f6c17cd60
Dec  4 21:38:30.136: INFO: Pod metallb-controller-577b6fb44-cxzgk requesting resource cpu=0m on Node alex-slot1-v2-vsp1-worker6c7da5d4d5
Dec  4 21:38:30.136: INFO: Pod metallb-speaker-np9rf requesting resource cpu=0m on Node alex-slot1-v2-vsp1-worker6c7da5d4d5
Dec  4 21:38:30.136: INFO: Pod metallb-speaker-xm9d9 requesting resource cpu=0m on Node alex-slot1-v2-vsp1-worker9f6c17cd60
Dec  4 21:38:30.136: INFO: Pod nginx-ingress-controller-dbfcd requesting resource cpu=0m on Node alex-slot1-v2-vsp1-worker9f6c17cd60
Dec  4 21:38:30.136: INFO: Pod nginx-ingress-controller-nsxf4 requesting resource cpu=0m on Node alex-slot1-v2-vsp1-worker6c7da5d4d5
Dec  4 21:38:30.136: INFO: Pod nginx-ingress-default-backend-dc5596844-xss9h requesting resource cpu=0m on Node alex-slot1-v2-vsp1-worker9f6c17cd60
Dec  4 21:38:30.136: INFO: Pod ccp-harbor-harbor-adminserver-5c6f5d5b8c-w2p9p requesting resource cpu=0m on Node alex-slot1-v2-vsp1-worker6c7da5d4d5
Dec  4 21:38:30.136: INFO: Pod ccp-harbor-harbor-core-6c5d44857d-cnzn8 requesting resource cpu=0m on Node alex-slot1-v2-vsp1-worker6c7da5d4d5
Dec  4 21:38:30.136: INFO: Pod ccp-harbor-harbor-database-0 requesting resource cpu=0m on Node alex-slot1-v2-vsp1-worker6c7da5d4d5
Dec  4 21:38:30.136: INFO: Pod ccp-harbor-harbor-jobservice-755d997849-vvz4g requesting resource cpu=0m on Node alex-slot1-v2-vsp1-worker9f6c17cd60
Dec  4 21:38:30.136: INFO: Pod ccp-harbor-harbor-portal-7445674c74-x2xqp requesting resource cpu=0m on Node alex-slot1-v2-vsp1-worker9f6c17cd60
Dec  4 21:38:30.136: INFO: Pod ccp-harbor-harbor-redis-0 requesting resource cpu=0m on Node alex-slot1-v2-vsp1-worker9f6c17cd60
Dec  4 21:38:30.136: INFO: Pod ccp-harbor-harbor-registry-77f88c7594-xd2nm requesting resource cpu=0m on Node alex-slot1-v2-vsp1-worker9f6c17cd60
Dec  4 21:38:30.136: INFO: Pod calico-node-27hrx requesting resource cpu=250m on Node alex-slot1-v2-vsp1-worker6c7da5d4d5
Dec  4 21:38:30.136: INFO: Pod calico-node-p5tcj requesting resource cpu=250m on Node alex-slot1-v2-vsp1-worker9f6c17cd60
Dec  4 21:38:30.136: INFO: Pod coredns-69cb7f6694-m5vmc requesting resource cpu=100m on Node alex-slot1-v2-vsp1-worker9f6c17cd60
Dec  4 21:38:30.136: INFO: Pod coredns-69cb7f6694-qph4l requesting resource cpu=100m on Node alex-slot1-v2-vsp1-worker6c7da5d4d5
Dec  4 21:38:30.136: INFO: Pod kube-proxy-2krtb requesting resource cpu=0m on Node alex-slot1-v2-vsp1-worker6c7da5d4d5
Dec  4 21:38:30.136: INFO: Pod kube-proxy-xxz5s requesting resource cpu=0m on Node alex-slot1-v2-vsp1-worker9f6c17cd60
Dec  4 21:38:30.136: INFO: Pod nvidia-device-plugin-daemonset-km64c requesting resource cpu=0m on Node alex-slot1-v2-vsp1-worker9f6c17cd60
Dec  4 21:38:30.136: INFO: Pod nvidia-device-plugin-daemonset-vpx26 requesting resource cpu=0m on Node alex-slot1-v2-vsp1-worker6c7da5d4d5
Dec  4 21:38:30.136: INFO: Pod sonobuoy requesting resource cpu=0m on Node alex-slot1-v2-vsp1-worker6c7da5d4d5
Dec  4 21:38:30.136: INFO: Pod sonobuoy-systemd-logs-daemon-set-f8654487368d4bd0-j78pc requesting resource cpu=0m on Node alex-slot1-v2-vsp1-worker9f6c17cd60
Dec  4 21:38:30.136: INFO: Pod sonobuoy-systemd-logs-daemon-set-f8654487368d4bd0-qjhgr requesting resource cpu=0m on Node alex-slot1-v2-vsp1-worker6c7da5d4d5
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-69c626d4-16de-11ea-8695-527d496f91de.15dd48d1b384d329], Reason = [Scheduled], Message = [Successfully assigned sched-pred-124/filler-pod-69c626d4-16de-11ea-8695-527d496f91de to alex-slot1-v2-vsp1-worker6c7da5d4d5]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-69c626d4-16de-11ea-8695-527d496f91de.15dd48d2f082634a], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-69c626d4-16de-11ea-8695-527d496f91de.15dd48d2f4de7e23], Reason = [Created], Message = [Created container filler-pod-69c626d4-16de-11ea-8695-527d496f91de]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-69c626d4-16de-11ea-8695-527d496f91de.15dd48d2fce7daf6], Reason = [Started], Message = [Started container filler-pod-69c626d4-16de-11ea-8695-527d496f91de]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-69c74e5a-16de-11ea-8695-527d496f91de.15dd48d1b4829359], Reason = [Scheduled], Message = [Successfully assigned sched-pred-124/filler-pod-69c74e5a-16de-11ea-8695-527d496f91de to alex-slot1-v2-vsp1-worker9f6c17cd60]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-69c74e5a-16de-11ea-8695-527d496f91de.15dd48d1b565f0b0], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-69c74e5a-16de-11ea-8695-527d496f91de.15dd48d1b9b7f830], Reason = [Created], Message = [Created container filler-pod-69c74e5a-16de-11ea-8695-527d496f91de]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-69c74e5a-16de-11ea-8695-527d496f91de.15dd48d1c5af8b60], Reason = [Started], Message = [Started container filler-pod-69c74e5a-16de-11ea-8695-527d496f91de]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15dd48d22e1bbc3e], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had taints that the pod didn't tolerate, 2 Insufficient cpu.]
STEP: removing the label node off the node alex-slot1-v2-vsp1-worker6c7da5d4d5
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node alex-slot1-v2-vsp1-worker9f6c17cd60
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:38:33.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-124" for this suite.
Dec  4 21:38:39.283: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:38:39.348: INFO: namespace sched-pred-124 deletion completed in 6.075030592s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:9.445 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:38:39.349: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-2600
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating replication controller my-hostname-basic-6f5788cf-16de-11ea-8695-527d496f91de
Dec  4 21:38:39.487: INFO: Pod name my-hostname-basic-6f5788cf-16de-11ea-8695-527d496f91de: Found 0 pods out of 1
Dec  4 21:38:44.490: INFO: Pod name my-hostname-basic-6f5788cf-16de-11ea-8695-527d496f91de: Found 1 pods out of 1
Dec  4 21:38:44.490: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-6f5788cf-16de-11ea-8695-527d496f91de" are running
Dec  4 21:38:44.493: INFO: Pod "my-hostname-basic-6f5788cf-16de-11ea-8695-527d496f91de-62w7d" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-04 21:38:43 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-04 21:38:46 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-04 21:38:46 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-04 21:38:39 +0000 UTC Reason: Message:}])
Dec  4 21:38:44.493: INFO: Trying to dial the pod
Dec  4 21:38:49.507: INFO: Controller my-hostname-basic-6f5788cf-16de-11ea-8695-527d496f91de: Got expected result from replica 1 [my-hostname-basic-6f5788cf-16de-11ea-8695-527d496f91de-62w7d]: "my-hostname-basic-6f5788cf-16de-11ea-8695-527d496f91de-62w7d", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:38:49.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2600" for this suite.
Dec  4 21:38:55.526: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:38:55.606: INFO: namespace replication-controller-2600 deletion completed in 6.090185329s

• [SLOW TEST:16.258 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:38:55.607: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1493
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  4 21:38:55.751: INFO: Waiting up to 5m0s for pod "downwardapi-volume-79091b41-16de-11ea-8695-527d496f91de" in namespace "projected-1493" to be "success or failure"
Dec  4 21:38:55.757: INFO: Pod "downwardapi-volume-79091b41-16de-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 5.569537ms
Dec  4 21:38:57.760: INFO: Pod "downwardapi-volume-79091b41-16de-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00881193s
STEP: Saw pod success
Dec  4 21:38:57.760: INFO: Pod "downwardapi-volume-79091b41-16de-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 21:38:57.763: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker9f6c17cd60 pod downwardapi-volume-79091b41-16de-11ea-8695-527d496f91de container client-container: <nil>
STEP: delete the pod
Dec  4 21:38:57.783: INFO: Waiting for pod downwardapi-volume-79091b41-16de-11ea-8695-527d496f91de to disappear
Dec  4 21:38:57.785: INFO: Pod downwardapi-volume-79091b41-16de-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:38:57.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1493" for this suite.
Dec  4 21:39:03.797: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:39:04.006: INFO: namespace projected-1493 deletion completed in 6.21781237s

• [SLOW TEST:8.399 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:39:04.006: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9951
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  4 21:39:04.202: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7e125755-16de-11ea-8695-527d496f91de" in namespace "projected-9951" to be "success or failure"
Dec  4 21:39:04.207: INFO: Pod "downwardapi-volume-7e125755-16de-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 3.435397ms
Dec  4 21:39:06.210: INFO: Pod "downwardapi-volume-7e125755-16de-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006018973s
Dec  4 21:39:08.213: INFO: Pod "downwardapi-volume-7e125755-16de-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009159279s
STEP: Saw pod success
Dec  4 21:39:08.213: INFO: Pod "downwardapi-volume-7e125755-16de-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 21:39:08.217: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker9f6c17cd60 pod downwardapi-volume-7e125755-16de-11ea-8695-527d496f91de container client-container: <nil>
STEP: delete the pod
Dec  4 21:39:08.237: INFO: Waiting for pod downwardapi-volume-7e125755-16de-11ea-8695-527d496f91de to disappear
Dec  4 21:39:08.241: INFO: Pod downwardapi-volume-7e125755-16de-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:39:08.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9951" for this suite.
Dec  4 21:39:14.255: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:39:14.321: INFO: namespace projected-9951 deletion completed in 6.075613864s

• [SLOW TEST:10.315 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:39:14.328: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9105
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  4 21:39:14.474: INFO: Waiting up to 5m0s for pod "downwardapi-volume-84327a20-16de-11ea-8695-527d496f91de" in namespace "downward-api-9105" to be "success or failure"
Dec  4 21:39:14.483: INFO: Pod "downwardapi-volume-84327a20-16de-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 5.905692ms
Dec  4 21:39:16.487: INFO: Pod "downwardapi-volume-84327a20-16de-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010265384s
Dec  4 21:39:18.491: INFO: Pod "downwardapi-volume-84327a20-16de-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01376416s
STEP: Saw pod success
Dec  4 21:39:18.491: INFO: Pod "downwardapi-volume-84327a20-16de-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 21:39:18.493: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker9f6c17cd60 pod downwardapi-volume-84327a20-16de-11ea-8695-527d496f91de container client-container: <nil>
STEP: delete the pod
Dec  4 21:39:18.510: INFO: Waiting for pod downwardapi-volume-84327a20-16de-11ea-8695-527d496f91de to disappear
Dec  4 21:39:18.512: INFO: Pod downwardapi-volume-84327a20-16de-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:39:18.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9105" for this suite.
Dec  4 21:39:24.525: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:39:24.600: INFO: namespace downward-api-9105 deletion completed in 6.083591757s

• [SLOW TEST:10.272 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:39:24.600: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7340
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  4 21:39:24.788: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"8a566833-16de-11ea-825c-005056950656", Controller:(*bool)(0xc002faa182), BlockOwnerDeletion:(*bool)(0xc002faa183)}}
Dec  4 21:39:24.802: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"8a533c5f-16de-11ea-825c-005056950656", Controller:(*bool)(0xc002e0c3ea), BlockOwnerDeletion:(*bool)(0xc002e0c3eb)}}
Dec  4 21:39:24.811: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"8a5432fd-16de-11ea-825c-005056950656", Controller:(*bool)(0xc002faa38a), BlockOwnerDeletion:(*bool)(0xc002faa38b)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:39:29.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7340" for this suite.
Dec  4 21:39:35.834: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:39:35.994: INFO: namespace gc-7340 deletion completed in 6.169508338s

• [SLOW TEST:11.394 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:39:35.997: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-3419
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:40:02.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3419" for this suite.
Dec  4 21:40:08.410: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:40:08.483: INFO: namespace container-runtime-3419 deletion completed in 6.081065706s

• [SLOW TEST:32.486 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  blackbox test
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:40:08.483: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8367
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-a4791e3a-16de-11ea-8695-527d496f91de
STEP: Creating a pod to test consume configMaps
Dec  4 21:40:08.627: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a47992c6-16de-11ea-8695-527d496f91de" in namespace "projected-8367" to be "success or failure"
Dec  4 21:40:08.636: INFO: Pod "pod-projected-configmaps-a47992c6-16de-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 9.137003ms
Dec  4 21:40:10.639: INFO: Pod "pod-projected-configmaps-a47992c6-16de-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012295356s
Dec  4 21:40:12.642: INFO: Pod "pod-projected-configmaps-a47992c6-16de-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01524422s
STEP: Saw pod success
Dec  4 21:40:12.642: INFO: Pod "pod-projected-configmaps-a47992c6-16de-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 21:40:12.644: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker6c7da5d4d5 pod pod-projected-configmaps-a47992c6-16de-11ea-8695-527d496f91de container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec  4 21:40:12.668: INFO: Waiting for pod pod-projected-configmaps-a47992c6-16de-11ea-8695-527d496f91de to disappear
Dec  4 21:40:12.672: INFO: Pod pod-projected-configmaps-a47992c6-16de-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:40:12.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8367" for this suite.
Dec  4 21:40:18.703: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:40:18.835: INFO: namespace projected-8367 deletion completed in 6.15933778s

• [SLOW TEST:10.352 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:40:18.837: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9861
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  4 21:40:18.973: INFO: Creating deployment "test-recreate-deployment"
Dec  4 21:40:18.994: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Dec  4 21:40:19.022: INFO: Waiting deployment "test-recreate-deployment" to complete
Dec  4 21:40:19.027: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711092418, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711092418, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711092419, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711092418, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6566d46b4b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  4 21:40:21.030: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Dec  4 21:40:21.039: INFO: Updating deployment test-recreate-deployment
Dec  4 21:40:21.039: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Dec  4 21:40:21.142: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-9861,SelfLink:/apis/apps/v1/namespaces/deployment-9861/deployments/test-recreate-deployment,UID:aaa5953c-16de-11ea-825c-005056950656,ResourceVersion:290651,Generation:2,CreationTimestamp:2019-12-04 21:40:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-12-04 21:40:21 +0000 UTC 2019-12-04 21:40:21 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-12-04 21:40:21 +0000 UTC 2019-12-04 21:40:18 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-745fb9c84c" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Dec  4 21:40:21.145: INFO: New ReplicaSet "test-recreate-deployment-745fb9c84c" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-745fb9c84c,GenerateName:,Namespace:deployment-9861,SelfLink:/apis/apps/v1/namespaces/deployment-9861/replicasets/test-recreate-deployment-745fb9c84c,UID:abe7021a-16de-11ea-825c-005056950656,ResourceVersion:290647,Generation:1,CreationTimestamp:2019-12-04 21:40:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 745fb9c84c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment aaa5953c-16de-11ea-825c-005056950656 0xc002c5af57 0xc002c5af58}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 745fb9c84c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 745fb9c84c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  4 21:40:21.145: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Dec  4 21:40:21.145: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-6566d46b4b,GenerateName:,Namespace:deployment-9861,SelfLink:/apis/apps/v1/namespaces/deployment-9861/replicasets/test-recreate-deployment-6566d46b4b,UID:aaa68117-16de-11ea-825c-005056950656,ResourceVersion:290637,Generation:2,CreationTimestamp:2019-12-04 21:40:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6566d46b4b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment aaa5953c-16de-11ea-825c-005056950656 0xc002c5ae87 0xc002c5ae88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6566d46b4b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6566d46b4b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  4 21:40:21.148: INFO: Pod "test-recreate-deployment-745fb9c84c-9hl2b" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-745fb9c84c-9hl2b,GenerateName:test-recreate-deployment-745fb9c84c-,Namespace:deployment-9861,SelfLink:/api/v1/namespaces/deployment-9861/pods/test-recreate-deployment-745fb9c84c-9hl2b,UID:abe7b89d-16de-11ea-825c-005056950656,ResourceVersion:290649,Generation:0,CreationTimestamp:2019-12-04 21:40:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 745fb9c84c,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-745fb9c84c abe7021a-16de-11ea-825c-005056950656 0xc002c5b807 0xc002c5b808}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-fx7lj {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fx7lj,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fx7lj true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v2-vsp1-worker6c7da5d4d5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c5b870} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c5b890}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:40:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:40:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:40:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 21:40:21 +0000 UTC  }],Message:,Reason:,HostIP:10.10.128.23,PodIP:,StartTime:2019-12-04 21:40:25 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:40:21.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9861" for this suite.
Dec  4 21:40:27.157: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:40:27.242: INFO: namespace deployment-9861 deletion completed in 6.091006446s

• [SLOW TEST:8.405 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:40:27.242: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-3442
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test env composition
Dec  4 21:40:27.400: INFO: Waiting up to 5m0s for pod "var-expansion-afa89c94-16de-11ea-8695-527d496f91de" in namespace "var-expansion-3442" to be "success or failure"
Dec  4 21:40:27.406: INFO: Pod "var-expansion-afa89c94-16de-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 5.222247ms
Dec  4 21:40:29.409: INFO: Pod "var-expansion-afa89c94-16de-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007907962s
STEP: Saw pod success
Dec  4 21:40:29.409: INFO: Pod "var-expansion-afa89c94-16de-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 21:40:29.413: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker9f6c17cd60 pod var-expansion-afa89c94-16de-11ea-8695-527d496f91de container dapi-container: <nil>
STEP: delete the pod
Dec  4 21:40:29.438: INFO: Waiting for pod var-expansion-afa89c94-16de-11ea-8695-527d496f91de to disappear
Dec  4 21:40:29.440: INFO: Pod var-expansion-afa89c94-16de-11ea-8695-527d496f91de no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:40:29.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3442" for this suite.
Dec  4 21:40:35.452: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:40:35.535: INFO: namespace var-expansion-3442 deletion completed in 6.09213431s

• [SLOW TEST:8.293 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:40:35.535: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2750
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-2750/configmap-test-b499dfdb-16de-11ea-8695-527d496f91de
STEP: Creating a pod to test consume configMaps
Dec  4 21:40:35.685: INFO: Waiting up to 5m0s for pod "pod-configmaps-b49a5d55-16de-11ea-8695-527d496f91de" in namespace "configmap-2750" to be "success or failure"
Dec  4 21:40:35.696: INFO: Pod "pod-configmaps-b49a5d55-16de-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 11.090313ms
Dec  4 21:40:37.699: INFO: Pod "pod-configmaps-b49a5d55-16de-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013756892s
STEP: Saw pod success
Dec  4 21:40:37.699: INFO: Pod "pod-configmaps-b49a5d55-16de-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 21:40:37.701: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker6c7da5d4d5 pod pod-configmaps-b49a5d55-16de-11ea-8695-527d496f91de container env-test: <nil>
STEP: delete the pod
Dec  4 21:40:37.718: INFO: Waiting for pod pod-configmaps-b49a5d55-16de-11ea-8695-527d496f91de to disappear
Dec  4 21:40:37.720: INFO: Pod pod-configmaps-b49a5d55-16de-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:40:37.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2750" for this suite.
Dec  4 21:40:43.734: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:40:43.817: INFO: namespace configmap-2750 deletion completed in 6.091710928s

• [SLOW TEST:8.281 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:40:43.817: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-7208
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-r7fn
STEP: Creating a pod to test atomic-volume-subpath
Dec  4 21:40:43.979: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-r7fn" in namespace "subpath-7208" to be "success or failure"
Dec  4 21:40:43.988: INFO: Pod "pod-subpath-test-configmap-r7fn": Phase="Pending", Reason="", readiness=false. Elapsed: 8.842016ms
Dec  4 21:40:45.991: INFO: Pod "pod-subpath-test-configmap-r7fn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01242259s
Dec  4 21:40:47.994: INFO: Pod "pod-subpath-test-configmap-r7fn": Phase="Running", Reason="", readiness=true. Elapsed: 4.015275932s
Dec  4 21:40:49.998: INFO: Pod "pod-subpath-test-configmap-r7fn": Phase="Running", Reason="", readiness=true. Elapsed: 6.019640164s
Dec  4 21:40:52.002: INFO: Pod "pod-subpath-test-configmap-r7fn": Phase="Running", Reason="", readiness=true. Elapsed: 8.022880024s
Dec  4 21:40:54.005: INFO: Pod "pod-subpath-test-configmap-r7fn": Phase="Running", Reason="", readiness=true. Elapsed: 10.025947502s
Dec  4 21:40:56.008: INFO: Pod "pod-subpath-test-configmap-r7fn": Phase="Running", Reason="", readiness=true. Elapsed: 12.02911454s
Dec  4 21:40:58.012: INFO: Pod "pod-subpath-test-configmap-r7fn": Phase="Running", Reason="", readiness=true. Elapsed: 14.03377713s
Dec  4 21:41:00.015: INFO: Pod "pod-subpath-test-configmap-r7fn": Phase="Running", Reason="", readiness=true. Elapsed: 16.036743968s
Dec  4 21:41:02.018: INFO: Pod "pod-subpath-test-configmap-r7fn": Phase="Running", Reason="", readiness=true. Elapsed: 18.039659024s
Dec  4 21:41:04.021: INFO: Pod "pod-subpath-test-configmap-r7fn": Phase="Running", Reason="", readiness=true. Elapsed: 20.042406984s
Dec  4 21:41:06.024: INFO: Pod "pod-subpath-test-configmap-r7fn": Phase="Running", Reason="", readiness=true. Elapsed: 22.04562355s
Dec  4 21:41:08.030: INFO: Pod "pod-subpath-test-configmap-r7fn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.050839892s
STEP: Saw pod success
Dec  4 21:41:08.030: INFO: Pod "pod-subpath-test-configmap-r7fn" satisfied condition "success or failure"
Dec  4 21:41:08.032: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker9f6c17cd60 pod pod-subpath-test-configmap-r7fn container test-container-subpath-configmap-r7fn: <nil>
STEP: delete the pod
Dec  4 21:41:08.054: INFO: Waiting for pod pod-subpath-test-configmap-r7fn to disappear
Dec  4 21:41:08.064: INFO: Pod pod-subpath-test-configmap-r7fn no longer exists
STEP: Deleting pod pod-subpath-test-configmap-r7fn
Dec  4 21:41:08.065: INFO: Deleting pod "pod-subpath-test-configmap-r7fn" in namespace "subpath-7208"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:41:08.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7208" for this suite.
Dec  4 21:41:14.079: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:41:14.155: INFO: namespace subpath-7208 deletion completed in 6.083838231s

• [SLOW TEST:30.338 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:41:14.157: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-1483
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:41:17.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1483" for this suite.
Dec  4 21:41:39.350: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:41:39.426: INFO: namespace replication-controller-1483 deletion completed in 22.091161536s

• [SLOW TEST:25.269 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:41:39.426: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5594
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-daafc501-16de-11ea-8695-527d496f91de
STEP: Creating a pod to test consume configMaps
Dec  4 21:41:39.587: INFO: Waiting up to 5m0s for pod "pod-configmaps-dab079f0-16de-11ea-8695-527d496f91de" in namespace "configmap-5594" to be "success or failure"
Dec  4 21:41:39.592: INFO: Pod "pod-configmaps-dab079f0-16de-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 5.733315ms
Dec  4 21:41:41.595: INFO: Pod "pod-configmaps-dab079f0-16de-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008541712s
STEP: Saw pod success
Dec  4 21:41:41.595: INFO: Pod "pod-configmaps-dab079f0-16de-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 21:41:41.597: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker9f6c17cd60 pod pod-configmaps-dab079f0-16de-11ea-8695-527d496f91de container configmap-volume-test: <nil>
STEP: delete the pod
Dec  4 21:41:41.618: INFO: Waiting for pod pod-configmaps-dab079f0-16de-11ea-8695-527d496f91de to disappear
Dec  4 21:41:41.620: INFO: Pod pod-configmaps-dab079f0-16de-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:41:41.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5594" for this suite.
Dec  4 21:41:47.637: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:41:47.714: INFO: namespace configmap-5594 deletion completed in 6.086212637s

• [SLOW TEST:8.288 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:41:47.715: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8729
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Dec  4 21:41:47.867: INFO: Waiting up to 5m0s for pod "downward-api-df9fbac1-16de-11ea-8695-527d496f91de" in namespace "downward-api-8729" to be "success or failure"
Dec  4 21:41:47.872: INFO: Pod "downward-api-df9fbac1-16de-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 4.940807ms
Dec  4 21:41:49.876: INFO: Pod "downward-api-df9fbac1-16de-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008000721s
STEP: Saw pod success
Dec  4 21:41:49.876: INFO: Pod "downward-api-df9fbac1-16de-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 21:41:49.878: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker6c7da5d4d5 pod downward-api-df9fbac1-16de-11ea-8695-527d496f91de container dapi-container: <nil>
STEP: delete the pod
Dec  4 21:41:49.909: INFO: Waiting for pod downward-api-df9fbac1-16de-11ea-8695-527d496f91de to disappear
Dec  4 21:41:49.918: INFO: Pod downward-api-df9fbac1-16de-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:41:49.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8729" for this suite.
Dec  4 21:41:55.933: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:41:56.003: INFO: namespace downward-api-8729 deletion completed in 6.079779252s

• [SLOW TEST:8.288 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:41:56.004: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1502
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  4 21:41:56.145: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e48f6187-16de-11ea-8695-527d496f91de" in namespace "downward-api-1502" to be "success or failure"
Dec  4 21:41:56.152: INFO: Pod "downwardapi-volume-e48f6187-16de-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 6.428805ms
Dec  4 21:41:58.155: INFO: Pod "downwardapi-volume-e48f6187-16de-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009479372s
STEP: Saw pod success
Dec  4 21:41:58.155: INFO: Pod "downwardapi-volume-e48f6187-16de-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 21:41:58.157: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker6c7da5d4d5 pod downwardapi-volume-e48f6187-16de-11ea-8695-527d496f91de container client-container: <nil>
STEP: delete the pod
Dec  4 21:41:58.183: INFO: Waiting for pod downwardapi-volume-e48f6187-16de-11ea-8695-527d496f91de to disappear
Dec  4 21:41:58.186: INFO: Pod downwardapi-volume-e48f6187-16de-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:41:58.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1502" for this suite.
Dec  4 21:42:04.200: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:42:04.443: INFO: namespace downward-api-1502 deletion completed in 6.251637652s

• [SLOW TEST:8.439 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:42:04.444: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4162
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  4 21:42:04.587: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e997829a-16de-11ea-8695-527d496f91de" in namespace "projected-4162" to be "success or failure"
Dec  4 21:42:04.591: INFO: Pod "downwardapi-volume-e997829a-16de-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 3.934205ms
Dec  4 21:42:06.595: INFO: Pod "downwardapi-volume-e997829a-16de-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007137924s
STEP: Saw pod success
Dec  4 21:42:06.595: INFO: Pod "downwardapi-volume-e997829a-16de-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 21:42:06.597: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker6c7da5d4d5 pod downwardapi-volume-e997829a-16de-11ea-8695-527d496f91de container client-container: <nil>
STEP: delete the pod
Dec  4 21:42:06.617: INFO: Waiting for pod downwardapi-volume-e997829a-16de-11ea-8695-527d496f91de to disappear
Dec  4 21:42:06.619: INFO: Pod downwardapi-volume-e997829a-16de-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:42:06.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4162" for this suite.
Dec  4 21:42:12.632: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:42:12.718: INFO: namespace projected-4162 deletion completed in 6.094714774s

• [SLOW TEST:8.274 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:42:12.719: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8867
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  4 21:42:12.860: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ee854115-16de-11ea-8695-527d496f91de" in namespace "downward-api-8867" to be "success or failure"
Dec  4 21:42:12.866: INFO: Pod "downwardapi-volume-ee854115-16de-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 5.369706ms
Dec  4 21:42:14.868: INFO: Pod "downwardapi-volume-ee854115-16de-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008230576s
STEP: Saw pod success
Dec  4 21:42:14.868: INFO: Pod "downwardapi-volume-ee854115-16de-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 21:42:14.871: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker9f6c17cd60 pod downwardapi-volume-ee854115-16de-11ea-8695-527d496f91de container client-container: <nil>
STEP: delete the pod
Dec  4 21:42:14.889: INFO: Waiting for pod downwardapi-volume-ee854115-16de-11ea-8695-527d496f91de to disappear
Dec  4 21:42:14.890: INFO: Pod downwardapi-volume-ee854115-16de-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:42:14.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8867" for this suite.
Dec  4 21:42:20.900: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:42:20.967: INFO: namespace downward-api-8867 deletion completed in 6.07398132s

• [SLOW TEST:8.248 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:42:20.970: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-2999
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Dec  4 21:42:21.110: INFO: Pod name pod-release: Found 0 pods out of 1
Dec  4 21:42:26.114: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:42:26.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2999" for this suite.
Dec  4 21:42:32.181: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:42:32.315: INFO: namespace replication-controller-2999 deletion completed in 6.159089784s

• [SLOW TEST:11.345 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:42:32.315: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8516
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  4 21:42:56.472: INFO: Container started at 2019-12-04 21:42:32 +0000 UTC, pod became ready at 2019-12-04 21:42:54 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:42:56.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8516" for this suite.
Dec  4 21:43:18.484: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:43:18.551: INFO: namespace container-probe-8516 deletion completed in 22.075238544s

• [SLOW TEST:46.236 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:43:18.553: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3053
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1480
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  4 21:43:18.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-3053'
Dec  4 21:43:18.844: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec  4 21:43:18.844: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Dec  4 21:43:18.849: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Dec  4 21:43:18.859: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Dec  4 21:43:18.871: INFO: scanned /root for discovery docs: <nil>
Dec  4 21:43:18.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-3053'
Dec  4 21:43:33.863: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Dec  4 21:43:33.863: INFO: stdout: "Created e2e-test-nginx-rc-f28bd5b395917f800819ce297955e3f2\nScaling up e2e-test-nginx-rc-f28bd5b395917f800819ce297955e3f2 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-f28bd5b395917f800819ce297955e3f2 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-f28bd5b395917f800819ce297955e3f2 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Dec  4 21:43:33.863: INFO: stdout: "Created e2e-test-nginx-rc-f28bd5b395917f800819ce297955e3f2\nScaling up e2e-test-nginx-rc-f28bd5b395917f800819ce297955e3f2 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-f28bd5b395917f800819ce297955e3f2 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-f28bd5b395917f800819ce297955e3f2 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Dec  4 21:43:33.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-3053'
Dec  4 21:43:33.948: INFO: stderr: ""
Dec  4 21:43:33.948: INFO: stdout: "e2e-test-nginx-rc-f28bd5b395917f800819ce297955e3f2-ns9xl "
Dec  4 21:43:33.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get pods e2e-test-nginx-rc-f28bd5b395917f800819ce297955e3f2-ns9xl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3053'
Dec  4 21:43:34.027: INFO: stderr: ""
Dec  4 21:43:34.027: INFO: stdout: "true"
Dec  4 21:43:34.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 get pods e2e-test-nginx-rc-f28bd5b395917f800819ce297955e3f2-ns9xl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3053'
Dec  4 21:43:34.108: INFO: stderr: ""
Dec  4 21:43:34.108: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Dec  4 21:43:34.108: INFO: e2e-test-nginx-rc-f28bd5b395917f800819ce297955e3f2-ns9xl is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1486
Dec  4 21:43:34.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 delete rc e2e-test-nginx-rc --namespace=kubectl-3053'
Dec  4 21:43:34.210: INFO: stderr: ""
Dec  4 21:43:34.210: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:43:34.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3053" for this suite.
Dec  4 21:43:54.225: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:43:54.293: INFO: namespace kubectl-3053 deletion completed in 20.078548983s

• [SLOW TEST:35.740 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:43:54.293: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9438
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating all guestbook components
Dec  4 21:43:54.424: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Dec  4 21:43:54.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 create -f - --namespace=kubectl-9438'
Dec  4 21:43:54.636: INFO: stderr: ""
Dec  4 21:43:54.636: INFO: stdout: "service/redis-slave created\n"
Dec  4 21:43:54.636: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Dec  4 21:43:54.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 create -f - --namespace=kubectl-9438'
Dec  4 21:43:54.882: INFO: stderr: ""
Dec  4 21:43:54.882: INFO: stdout: "service/redis-master created\n"
Dec  4 21:43:54.882: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Dec  4 21:43:54.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 create -f - --namespace=kubectl-9438'
Dec  4 21:43:55.129: INFO: stderr: ""
Dec  4 21:43:55.129: INFO: stdout: "service/frontend created\n"
Dec  4 21:43:55.130: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Dec  4 21:43:55.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 create -f - --namespace=kubectl-9438'
Dec  4 21:43:55.357: INFO: stderr: ""
Dec  4 21:43:55.357: INFO: stdout: "deployment.apps/frontend created\n"
Dec  4 21:43:55.358: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec  4 21:43:55.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 create -f - --namespace=kubectl-9438'
Dec  4 21:43:55.610: INFO: stderr: ""
Dec  4 21:43:55.610: INFO: stdout: "deployment.apps/redis-master created\n"
Dec  4 21:43:55.610: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Dec  4 21:43:55.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 create -f - --namespace=kubectl-9438'
Dec  4 21:43:55.851: INFO: stderr: ""
Dec  4 21:43:55.851: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Dec  4 21:43:55.851: INFO: Waiting for all frontend pods to be Running.
Dec  4 21:44:15.903: INFO: Waiting for frontend to serve content.
Dec  4 21:44:15.913: INFO: Trying to add a new entry to the guestbook.
Dec  4 21:44:15.924: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Dec  4 21:44:15.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 delete --grace-period=0 --force -f - --namespace=kubectl-9438'
Dec  4 21:44:16.056: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  4 21:44:16.057: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Dec  4 21:44:16.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 delete --grace-period=0 --force -f - --namespace=kubectl-9438'
Dec  4 21:44:16.194: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  4 21:44:16.194: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Dec  4 21:44:16.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 delete --grace-period=0 --force -f - --namespace=kubectl-9438'
Dec  4 21:44:16.333: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  4 21:44:16.333: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec  4 21:44:16.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 delete --grace-period=0 --force -f - --namespace=kubectl-9438'
Dec  4 21:44:16.462: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  4 21:44:16.462: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec  4 21:44:16.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 delete --grace-period=0 --force -f - --namespace=kubectl-9438'
Dec  4 21:44:16.554: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  4 21:44:16.554: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Dec  4 21:44:16.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 delete --grace-period=0 --force -f - --namespace=kubectl-9438'
Dec  4 21:44:16.637: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec  4 21:44:16.637: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:44:16.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9438" for this suite.
Dec  4 21:44:54.651: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:44:54.741: INFO: namespace kubectl-9438 deletion completed in 38.099603337s

• [SLOW TEST:60.448 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:44:54.742: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-8155
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  4 21:44:54.881: INFO: (0) /api/v1/nodes/alex-slot1-v2-vsp1-worker6c7da5d4d5/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 3.800678ms)
Dec  4 21:44:54.884: INFO: (1) /api/v1/nodes/alex-slot1-v2-vsp1-worker6c7da5d4d5/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.879099ms)
Dec  4 21:44:54.887: INFO: (2) /api/v1/nodes/alex-slot1-v2-vsp1-worker6c7da5d4d5/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.988513ms)
Dec  4 21:44:54.890: INFO: (3) /api/v1/nodes/alex-slot1-v2-vsp1-worker6c7da5d4d5/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.716162ms)
Dec  4 21:44:54.893: INFO: (4) /api/v1/nodes/alex-slot1-v2-vsp1-worker6c7da5d4d5/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.303441ms)
Dec  4 21:44:54.895: INFO: (5) /api/v1/nodes/alex-slot1-v2-vsp1-worker6c7da5d4d5/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.307601ms)
Dec  4 21:44:54.898: INFO: (6) /api/v1/nodes/alex-slot1-v2-vsp1-worker6c7da5d4d5/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.079917ms)
Dec  4 21:44:54.900: INFO: (7) /api/v1/nodes/alex-slot1-v2-vsp1-worker6c7da5d4d5/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.225432ms)
Dec  4 21:44:54.903: INFO: (8) /api/v1/nodes/alex-slot1-v2-vsp1-worker6c7da5d4d5/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.482051ms)
Dec  4 21:44:54.906: INFO: (9) /api/v1/nodes/alex-slot1-v2-vsp1-worker6c7da5d4d5/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.783082ms)
Dec  4 21:44:54.910: INFO: (10) /api/v1/nodes/alex-slot1-v2-vsp1-worker6c7da5d4d5/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 3.841177ms)
Dec  4 21:44:54.912: INFO: (11) /api/v1/nodes/alex-slot1-v2-vsp1-worker6c7da5d4d5/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 1.912631ms)
Dec  4 21:44:54.914: INFO: (12) /api/v1/nodes/alex-slot1-v2-vsp1-worker6c7da5d4d5/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.183146ms)
Dec  4 21:44:54.917: INFO: (13) /api/v1/nodes/alex-slot1-v2-vsp1-worker6c7da5d4d5/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.766645ms)
Dec  4 21:44:54.920: INFO: (14) /api/v1/nodes/alex-slot1-v2-vsp1-worker6c7da5d4d5/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.228935ms)
Dec  4 21:44:54.922: INFO: (15) /api/v1/nodes/alex-slot1-v2-vsp1-worker6c7da5d4d5/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.529738ms)
Dec  4 21:44:54.925: INFO: (16) /api/v1/nodes/alex-slot1-v2-vsp1-worker6c7da5d4d5/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.466383ms)
Dec  4 21:44:54.927: INFO: (17) /api/v1/nodes/alex-slot1-v2-vsp1-worker6c7da5d4d5/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.35198ms)
Dec  4 21:44:54.930: INFO: (18) /api/v1/nodes/alex-slot1-v2-vsp1-worker6c7da5d4d5/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.354954ms)
Dec  4 21:44:54.932: INFO: (19) /api/v1/nodes/alex-slot1-v2-vsp1-worker6c7da5d4d5/proxy/logs/: <pre>
<a href="apt/">apt/</a>
<a href="audit/">audit/</a>
<a href="auth.log">auth.log</a>
<a href... (200; 2.368545ms)
[AfterEach] version v1
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:44:54.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8155" for this suite.
Dec  4 21:45:00.945: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:45:01.019: INFO: namespace proxy-8155 deletion completed in 6.082056605s

• [SLOW TEST:6.277 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:45:01.025: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4300
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-52d810b2-16df-11ea-8695-527d496f91de
STEP: Creating secret with name s-test-opt-upd-52d810ed-16df-11ea-8695-527d496f91de
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-52d810b2-16df-11ea-8695-527d496f91de
STEP: Updating secret s-test-opt-upd-52d810ed-16df-11ea-8695-527d496f91de
STEP: Creating secret with name s-test-opt-create-52d8110c-16df-11ea-8695-527d496f91de
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:46:15.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4300" for this suite.
Dec  4 21:46:37.530: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:46:37.608: INFO: namespace secrets-4300 deletion completed in 22.087814108s

• [SLOW TEST:96.583 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:46:37.609: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-8343
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's command
Dec  4 21:46:37.755: INFO: Waiting up to 5m0s for pod "var-expansion-8c697702-16df-11ea-8695-527d496f91de" in namespace "var-expansion-8343" to be "success or failure"
Dec  4 21:46:37.765: INFO: Pod "var-expansion-8c697702-16df-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 9.33614ms
Dec  4 21:46:39.768: INFO: Pod "var-expansion-8c697702-16df-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013037557s
Dec  4 21:46:41.771: INFO: Pod "var-expansion-8c697702-16df-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016084556s
STEP: Saw pod success
Dec  4 21:46:41.772: INFO: Pod "var-expansion-8c697702-16df-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 21:46:41.775: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker9f6c17cd60 pod var-expansion-8c697702-16df-11ea-8695-527d496f91de container dapi-container: <nil>
STEP: delete the pod
Dec  4 21:46:41.794: INFO: Waiting for pod var-expansion-8c697702-16df-11ea-8695-527d496f91de to disappear
Dec  4 21:46:41.796: INFO: Pod var-expansion-8c697702-16df-11ea-8695-527d496f91de no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:46:41.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8343" for this suite.
Dec  4 21:46:47.812: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:46:47.883: INFO: namespace var-expansion-8343 deletion completed in 6.082928742s

• [SLOW TEST:10.273 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:46:47.883: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2392
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-9289f0fe-16df-11ea-8695-527d496f91de
STEP: Creating a pod to test consume secrets
Dec  4 21:46:48.035: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-928a83dc-16df-11ea-8695-527d496f91de" in namespace "projected-2392" to be "success or failure"
Dec  4 21:46:48.044: INFO: Pod "pod-projected-secrets-928a83dc-16df-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 8.33597ms
Dec  4 21:46:50.047: INFO: Pod "pod-projected-secrets-928a83dc-16df-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011469073s
Dec  4 21:46:52.050: INFO: Pod "pod-projected-secrets-928a83dc-16df-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014757943s
STEP: Saw pod success
Dec  4 21:46:52.050: INFO: Pod "pod-projected-secrets-928a83dc-16df-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 21:46:52.052: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker9f6c17cd60 pod pod-projected-secrets-928a83dc-16df-11ea-8695-527d496f91de container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  4 21:46:52.095: INFO: Waiting for pod pod-projected-secrets-928a83dc-16df-11ea-8695-527d496f91de to disappear
Dec  4 21:46:52.100: INFO: Pod pod-projected-secrets-928a83dc-16df-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:46:52.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2392" for this suite.
Dec  4 21:46:58.121: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:46:58.196: INFO: namespace projected-2392 deletion completed in 6.08875969s

• [SLOW TEST:10.313 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:46:58.197: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9276
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Starting the proxy
Dec  4 21:46:58.334: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-246239386 proxy --unix-socket=/tmp/kubectl-proxy-unix646896433/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:46:58.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9276" for this suite.
Dec  4 21:47:04.415: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:47:04.490: INFO: namespace kubectl-9276 deletion completed in 6.084191038s

• [SLOW TEST:6.294 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:47:04.494: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3344
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1521
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec  4 21:47:04.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=kubectl-3344'
Dec  4 21:47:05.041: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec  4 21:47:05.041: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1526
Dec  4 21:47:07.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 delete deployment e2e-test-nginx-deployment --namespace=kubectl-3344'
Dec  4 21:47:07.156: INFO: stderr: ""
Dec  4 21:47:07.156: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:47:07.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3344" for this suite.
Dec  4 21:47:29.168: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:47:29.234: INFO: namespace kubectl-3344 deletion completed in 22.07448859s

• [SLOW TEST:24.740 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:47:29.238: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-2397
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Dec  4 21:47:34.409: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:47:34.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2397" for this suite.
Dec  4 21:47:56.494: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:47:56.573: INFO: namespace replicaset-2397 deletion completed in 22.106381337s

• [SLOW TEST:27.336 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:47:56.576: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4042
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on tmpfs
Dec  4 21:47:56.720: INFO: Waiting up to 5m0s for pod "pod-bb7a4bfa-16df-11ea-8695-527d496f91de" in namespace "emptydir-4042" to be "success or failure"
Dec  4 21:47:56.728: INFO: Pod "pod-bb7a4bfa-16df-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 7.610006ms
Dec  4 21:47:58.733: INFO: Pod "pod-bb7a4bfa-16df-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012314997s
Dec  4 21:48:00.735: INFO: Pod "pod-bb7a4bfa-16df-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014999992s
STEP: Saw pod success
Dec  4 21:48:00.735: INFO: Pod "pod-bb7a4bfa-16df-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 21:48:00.737: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker9f6c17cd60 pod pod-bb7a4bfa-16df-11ea-8695-527d496f91de container test-container: <nil>
STEP: delete the pod
Dec  4 21:48:00.756: INFO: Waiting for pod pod-bb7a4bfa-16df-11ea-8695-527d496f91de to disappear
Dec  4 21:48:00.759: INFO: Pod pod-bb7a4bfa-16df-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:48:00.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4042" for this suite.
Dec  4 21:48:06.771: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:48:06.849: INFO: namespace emptydir-4042 deletion completed in 6.086349788s

• [SLOW TEST:10.274 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:48:06.850: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6630
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec  4 21:48:07.039: INFO: Waiting up to 5m0s for pod "pod-c1a15987-16df-11ea-8695-527d496f91de" in namespace "emptydir-6630" to be "success or failure"
Dec  4 21:48:07.042: INFO: Pod "pod-c1a15987-16df-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.741206ms
Dec  4 21:48:09.045: INFO: Pod "pod-c1a15987-16df-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005533138s
STEP: Saw pod success
Dec  4 21:48:09.045: INFO: Pod "pod-c1a15987-16df-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 21:48:09.047: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker6c7da5d4d5 pod pod-c1a15987-16df-11ea-8695-527d496f91de container test-container: <nil>
STEP: delete the pod
Dec  4 21:48:09.082: INFO: Waiting for pod pod-c1a15987-16df-11ea-8695-527d496f91de to disappear
Dec  4 21:48:09.084: INFO: Pod pod-c1a15987-16df-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:48:09.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6630" for this suite.
Dec  4 21:48:15.100: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:48:15.168: INFO: namespace emptydir-6630 deletion completed in 6.07790989s

• [SLOW TEST:8.318 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:48:15.171: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-4079
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
STEP: reading a file in the container
Dec  4 21:48:17.824: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4079 pod-service-account-c6dd694e-16df-11ea-8695-527d496f91de -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Dec  4 21:48:18.026: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4079 pod-service-account-c6dd694e-16df-11ea-8695-527d496f91de -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Dec  4 21:48:18.233: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4079 pod-service-account-c6dd694e-16df-11ea-8695-527d496f91de -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:48:18.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4079" for this suite.
Dec  4 21:48:24.513: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:48:24.591: INFO: namespace svcaccounts-4079 deletion completed in 6.086713347s

• [SLOW TEST:9.420 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:48:24.592: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-4820
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override command
Dec  4 21:48:24.742: INFO: Waiting up to 5m0s for pod "client-containers-cc2e9b66-16df-11ea-8695-527d496f91de" in namespace "containers-4820" to be "success or failure"
Dec  4 21:48:24.763: INFO: Pod "client-containers-cc2e9b66-16df-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 20.638396ms
Dec  4 21:48:26.765: INFO: Pod "client-containers-cc2e9b66-16df-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023311668s
STEP: Saw pod success
Dec  4 21:48:26.765: INFO: Pod "client-containers-cc2e9b66-16df-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 21:48:26.767: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker6c7da5d4d5 pod client-containers-cc2e9b66-16df-11ea-8695-527d496f91de container test-container: <nil>
STEP: delete the pod
Dec  4 21:48:26.790: INFO: Waiting for pod client-containers-cc2e9b66-16df-11ea-8695-527d496f91de to disappear
Dec  4 21:48:26.793: INFO: Pod client-containers-cc2e9b66-16df-11ea-8695-527d496f91de no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:48:26.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4820" for this suite.
Dec  4 21:48:32.807: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:48:32.880: INFO: namespace containers-4820 deletion completed in 6.082470652s

• [SLOW TEST:8.288 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:48:32.880: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-914
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-d11da5d8-16df-11ea-8695-527d496f91de
STEP: Creating a pod to test consume secrets
Dec  4 21:48:33.023: INFO: Waiting up to 5m0s for pod "pod-secrets-d11e3ae8-16df-11ea-8695-527d496f91de" in namespace "secrets-914" to be "success or failure"
Dec  4 21:48:33.028: INFO: Pod "pod-secrets-d11e3ae8-16df-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 4.996239ms
Dec  4 21:48:35.032: INFO: Pod "pod-secrets-d11e3ae8-16df-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008296337s
Dec  4 21:48:37.034: INFO: Pod "pod-secrets-d11e3ae8-16df-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01092385s
STEP: Saw pod success
Dec  4 21:48:37.034: INFO: Pod "pod-secrets-d11e3ae8-16df-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 21:48:37.036: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker9f6c17cd60 pod pod-secrets-d11e3ae8-16df-11ea-8695-527d496f91de container secret-volume-test: <nil>
STEP: delete the pod
Dec  4 21:48:37.057: INFO: Waiting for pod pod-secrets-d11e3ae8-16df-11ea-8695-527d496f91de to disappear
Dec  4 21:48:37.060: INFO: Pod pod-secrets-d11e3ae8-16df-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:48:37.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-914" for this suite.
Dec  4 21:48:43.074: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:48:43.145: INFO: namespace secrets-914 deletion completed in 6.08021217s

• [SLOW TEST:10.264 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:48:43.146: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-4429
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-4429
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec  4 21:48:43.284: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec  4 21:49:05.373: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.2.145:8080/dial?request=hostName&protocol=http&host=192.168.1.152&port=8080&tries=1'] Namespace:pod-network-test-4429 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  4 21:49:05.373: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
Dec  4 21:49:05.519: INFO: Waiting for endpoints: map[]
Dec  4 21:49:05.521: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.2.145:8080/dial?request=hostName&protocol=http&host=192.168.2.144&port=8080&tries=1'] Namespace:pod-network-test-4429 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec  4 21:49:05.522: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
Dec  4 21:49:05.659: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:49:05.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4429" for this suite.
Dec  4 21:49:27.672: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:49:27.744: INFO: namespace pod-network-test-4429 deletion completed in 22.080637602s

• [SLOW TEST:44.597 seconds]
[sig-network] Networking
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:49:27.744: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2242
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec  4 21:49:27.884: INFO: Waiting up to 5m0s for pod "pod-f1d16bc7-16df-11ea-8695-527d496f91de" in namespace "emptydir-2242" to be "success or failure"
Dec  4 21:49:27.891: INFO: Pod "pod-f1d16bc7-16df-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 6.382794ms
Dec  4 21:49:29.894: INFO: Pod "pod-f1d16bc7-16df-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010058316s
STEP: Saw pod success
Dec  4 21:49:29.894: INFO: Pod "pod-f1d16bc7-16df-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 21:49:29.896: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker6c7da5d4d5 pod pod-f1d16bc7-16df-11ea-8695-527d496f91de container test-container: <nil>
STEP: delete the pod
Dec  4 21:49:29.916: INFO: Waiting for pod pod-f1d16bc7-16df-11ea-8695-527d496f91de to disappear
Dec  4 21:49:29.918: INFO: Pod pod-f1d16bc7-16df-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:49:29.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2242" for this suite.
Dec  4 21:49:35.935: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:49:36.034: INFO: namespace emptydir-2242 deletion completed in 6.113004327s

• [SLOW TEST:8.290 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:49:36.035: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2900
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Dec  4 21:49:40.725: INFO: Successfully updated pod "annotationupdatef6c3d461-16df-11ea-8695-527d496f91de"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:49:42.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2900" for this suite.
Dec  4 21:50:04.757: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:50:04.837: INFO: namespace downward-api-2900 deletion completed in 22.092981429s

• [SLOW TEST:28.802 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:50:04.837: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2551
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: executing a command with run --rm and attach with stdin
Dec  4 21:50:04.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 --namespace=kubectl-2551 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Dec  4 21:50:07.331: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Dec  4 21:50:07.331: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:50:09.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2551" for this suite.
Dec  4 21:50:15.349: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:50:15.417: INFO: namespace kubectl-2551 deletion completed in 6.07911509s

• [SLOW TEST:10.581 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:50:15.419: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9867
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1204 21:50:25.647452      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec  4 21:50:25.647: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:50:25.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9867" for this suite.
Dec  4 21:50:31.661: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:50:31.741: INFO: namespace gc-9867 deletion completed in 6.090034022s

• [SLOW TEST:16.322 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:50:31.743: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-5417
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Dec  4 21:50:31.886: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:50:35.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5417" for this suite.
Dec  4 21:50:42.020: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:50:42.095: INFO: namespace init-container-5417 deletion completed in 6.084452373s

• [SLOW TEST:10.353 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:50:42.097: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7497
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-projected-all-test-volume-1e22e40b-16e0-11ea-8695-527d496f91de
STEP: Creating secret with name secret-projected-all-test-volume-1e22e3f9-16e0-11ea-8695-527d496f91de
STEP: Creating a pod to test Check all projections for projected volume plugin
Dec  4 21:50:42.244: INFO: Waiting up to 5m0s for pod "projected-volume-1e22e3ca-16e0-11ea-8695-527d496f91de" in namespace "projected-7497" to be "success or failure"
Dec  4 21:50:42.258: INFO: Pod "projected-volume-1e22e3ca-16e0-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 13.897301ms
Dec  4 21:50:44.260: INFO: Pod "projected-volume-1e22e3ca-16e0-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016248131s
STEP: Saw pod success
Dec  4 21:50:44.260: INFO: Pod "projected-volume-1e22e3ca-16e0-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 21:50:44.262: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker9f6c17cd60 pod projected-volume-1e22e3ca-16e0-11ea-8695-527d496f91de container projected-all-volume-test: <nil>
STEP: delete the pod
Dec  4 21:50:44.285: INFO: Waiting for pod projected-volume-1e22e3ca-16e0-11ea-8695-527d496f91de to disappear
Dec  4 21:50:44.287: INFO: Pod projected-volume-1e22e3ca-16e0-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:50:44.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7497" for this suite.
Dec  4 21:50:50.302: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:50:50.384: INFO: namespace projected-7497 deletion completed in 6.091963609s

• [SLOW TEST:8.287 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:50:50.385: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3483
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Dec  4 21:50:50.584: INFO: Waiting up to 5m0s for pod "downward-api-231c071f-16e0-11ea-8695-527d496f91de" in namespace "downward-api-3483" to be "success or failure"
Dec  4 21:50:50.589: INFO: Pod "downward-api-231c071f-16e0-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 4.352844ms
Dec  4 21:50:52.592: INFO: Pod "downward-api-231c071f-16e0-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00714325s
Dec  4 21:50:54.595: INFO: Pod "downward-api-231c071f-16e0-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010130117s
STEP: Saw pod success
Dec  4 21:50:54.595: INFO: Pod "downward-api-231c071f-16e0-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 21:50:54.597: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker9f6c17cd60 pod downward-api-231c071f-16e0-11ea-8695-527d496f91de container dapi-container: <nil>
STEP: delete the pod
Dec  4 21:50:54.617: INFO: Waiting for pod downward-api-231c071f-16e0-11ea-8695-527d496f91de to disappear
Dec  4 21:50:54.620: INFO: Pod downward-api-231c071f-16e0-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:50:54.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3483" for this suite.
Dec  4 21:51:00.633: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:51:00.718: INFO: namespace downward-api-3483 deletion completed in 6.09416703s

• [SLOW TEST:10.334 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:51:00.719: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7147
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:163
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  4 21:51:00.849: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:51:02.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7147" for this suite.
Dec  4 21:51:40.989: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:51:41.061: INFO: namespace pods-7147 deletion completed in 38.08312313s

• [SLOW TEST:40.342 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:51:41.064: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7071
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-7071
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Dec  4 21:51:41.221: INFO: Found 0 stateful pods, waiting for 3
Dec  4 21:51:51.224: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  4 21:51:51.224: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  4 21:51:51.224: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Dec  4 21:51:51.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7071 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  4 21:51:51.420: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  4 21:51:51.420: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  4 21:51:51.420: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Dec  4 21:52:01.448: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Dec  4 21:52:11.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7071 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  4 21:52:11.676: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec  4 21:52:11.676: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  4 21:52:11.676: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  4 21:52:41.691: INFO: Waiting for StatefulSet statefulset-7071/ss2 to complete update
Dec  4 21:52:41.691: INFO: Waiting for Pod statefulset-7071/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Rolling back to a previous revision
Dec  4 21:52:51.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7071 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  4 21:52:51.916: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  4 21:52:51.916: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  4 21:52:51.916: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  4 21:53:01.944: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Dec  4 21:53:11.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-7071 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  4 21:53:12.230: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec  4 21:53:12.230: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  4 21:53:12.230: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  4 21:53:22.253: INFO: Waiting for StatefulSet statefulset-7071/ss2 to complete update
Dec  4 21:53:22.253: INFO: Waiting for Pod statefulset-7071/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Dec  4 21:53:22.253: INFO: Waiting for Pod statefulset-7071/ss2-1 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Dec  4 21:53:32.258: INFO: Waiting for StatefulSet statefulset-7071/ss2 to complete update
Dec  4 21:53:32.258: INFO: Waiting for Pod statefulset-7071/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Dec  4 21:53:32.258: INFO: Waiting for Pod statefulset-7071/ss2-1 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Dec  4 21:53:42.258: INFO: Deleting all statefulset in ns statefulset-7071
Dec  4 21:53:42.260: INFO: Scaling statefulset ss2 to 0
Dec  4 21:53:52.284: INFO: Waiting for statefulset status.replicas updated to 0
Dec  4 21:53:52.285: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:53:52.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7071" for this suite.
Dec  4 21:53:58.321: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:53:58.398: INFO: namespace statefulset-7071 deletion completed in 6.088670225s

• [SLOW TEST:137.334 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:53:58.402: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4698
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-93255094-16e0-11ea-8695-527d496f91de
STEP: Creating a pod to test consume secrets
Dec  4 21:53:58.549: INFO: Waiting up to 5m0s for pod "pod-secrets-9325c185-16e0-11ea-8695-527d496f91de" in namespace "secrets-4698" to be "success or failure"
Dec  4 21:53:58.555: INFO: Pod "pod-secrets-9325c185-16e0-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 4.62658ms
Dec  4 21:54:00.559: INFO: Pod "pod-secrets-9325c185-16e0-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008663281s
Dec  4 21:54:02.561: INFO: Pod "pod-secrets-9325c185-16e0-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011558977s
STEP: Saw pod success
Dec  4 21:54:02.562: INFO: Pod "pod-secrets-9325c185-16e0-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 21:54:02.563: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker9f6c17cd60 pod pod-secrets-9325c185-16e0-11ea-8695-527d496f91de container secret-volume-test: <nil>
STEP: delete the pod
Dec  4 21:54:02.582: INFO: Waiting for pod pod-secrets-9325c185-16e0-11ea-8695-527d496f91de to disappear
Dec  4 21:54:02.584: INFO: Pod pod-secrets-9325c185-16e0-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:54:02.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4698" for this suite.
Dec  4 21:54:08.594: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:54:08.705: INFO: namespace secrets-4698 deletion completed in 6.11849823s

• [SLOW TEST:10.304 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:54:08.706: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4644
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-994f6c38-16e0-11ea-8695-527d496f91de
STEP: Creating a pod to test consume secrets
Dec  4 21:54:08.893: INFO: Waiting up to 5m0s for pod "pod-secrets-994ff088-16e0-11ea-8695-527d496f91de" in namespace "secrets-4644" to be "success or failure"
Dec  4 21:54:08.901: INFO: Pod "pod-secrets-994ff088-16e0-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 8.503901ms
Dec  4 21:54:10.904: INFO: Pod "pod-secrets-994ff088-16e0-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011333729s
STEP: Saw pod success
Dec  4 21:54:10.904: INFO: Pod "pod-secrets-994ff088-16e0-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 21:54:10.907: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker6c7da5d4d5 pod pod-secrets-994ff088-16e0-11ea-8695-527d496f91de container secret-volume-test: <nil>
STEP: delete the pod
Dec  4 21:54:10.926: INFO: Waiting for pod pod-secrets-994ff088-16e0-11ea-8695-527d496f91de to disappear
Dec  4 21:54:10.927: INFO: Pod pod-secrets-994ff088-16e0-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:54:10.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4644" for this suite.
Dec  4 21:54:16.938: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:54:17.017: INFO: namespace secrets-4644 deletion completed in 6.086559782s

• [SLOW TEST:8.311 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:54:17.017: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-6350
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:54:19.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6350" for this suite.
Dec  4 21:54:57.185: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:54:57.255: INFO: namespace kubelet-test-6350 deletion completed in 38.079386647s

• [SLOW TEST:40.238 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a read only busybox container
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:54:57.255: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-1473
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Dec  4 21:54:57.627: INFO: Pod name wrapped-volume-race-b6500f27-16e0-11ea-8695-527d496f91de: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-b6500f27-16e0-11ea-8695-527d496f91de in namespace emptydir-wrapper-1473, will wait for the garbage collector to delete the pods
Dec  4 21:55:13.764: INFO: Deleting ReplicationController wrapped-volume-race-b6500f27-16e0-11ea-8695-527d496f91de took: 13.296221ms
Dec  4 21:55:14.265: INFO: Terminating ReplicationController wrapped-volume-race-b6500f27-16e0-11ea-8695-527d496f91de pods took: 500.896662ms
STEP: Creating RC which spawns configmap-volume pods
Dec  4 21:55:51.993: INFO: Pod name wrapped-volume-race-d6c09bd1-16e0-11ea-8695-527d496f91de: Found 1 pods out of 5
Dec  4 21:55:56.998: INFO: Pod name wrapped-volume-race-d6c09bd1-16e0-11ea-8695-527d496f91de: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-d6c09bd1-16e0-11ea-8695-527d496f91de in namespace emptydir-wrapper-1473, will wait for the garbage collector to delete the pods
Dec  4 21:56:09.089: INFO: Deleting ReplicationController wrapped-volume-race-d6c09bd1-16e0-11ea-8695-527d496f91de took: 9.464043ms
Dec  4 21:56:09.491: INFO: Terminating ReplicationController wrapped-volume-race-d6c09bd1-16e0-11ea-8695-527d496f91de pods took: 402.517778ms
STEP: Creating RC which spawns configmap-volume pods
Dec  4 21:56:49.806: INFO: Pod name wrapped-volume-race-f938388b-16e0-11ea-8695-527d496f91de: Found 0 pods out of 5
Dec  4 21:56:54.812: INFO: Pod name wrapped-volume-race-f938388b-16e0-11ea-8695-527d496f91de: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-f938388b-16e0-11ea-8695-527d496f91de in namespace emptydir-wrapper-1473, will wait for the garbage collector to delete the pods
Dec  4 21:57:04.919: INFO: Deleting ReplicationController wrapped-volume-race-f938388b-16e0-11ea-8695-527d496f91de took: 15.751353ms
Dec  4 21:57:05.323: INFO: Terminating ReplicationController wrapped-volume-race-f938388b-16e0-11ea-8695-527d496f91de pods took: 403.897773ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:57:50.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1473" for this suite.
Dec  4 21:57:56.944: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:57:57.015: INFO: namespace emptydir-wrapper-1473 deletion completed in 6.08000238s

• [SLOW TEST:179.760 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:57:57.015: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9285
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  4 21:57:57.168: INFO: Waiting up to 5m0s for pod "downwardapi-volume-215dd459-16e1-11ea-8695-527d496f91de" in namespace "projected-9285" to be "success or failure"
Dec  4 21:57:57.171: INFO: Pod "downwardapi-volume-215dd459-16e1-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 3.247872ms
Dec  4 21:57:59.175: INFO: Pod "downwardapi-volume-215dd459-16e1-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006812409s
Dec  4 21:58:01.178: INFO: Pod "downwardapi-volume-215dd459-16e1-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009671281s
STEP: Saw pod success
Dec  4 21:58:01.178: INFO: Pod "downwardapi-volume-215dd459-16e1-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 21:58:01.180: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker6c7da5d4d5 pod downwardapi-volume-215dd459-16e1-11ea-8695-527d496f91de container client-container: <nil>
STEP: delete the pod
Dec  4 21:58:01.195: INFO: Waiting for pod downwardapi-volume-215dd459-16e1-11ea-8695-527d496f91de to disappear
Dec  4 21:58:01.198: INFO: Pod downwardapi-volume-215dd459-16e1-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:58:01.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9285" for this suite.
Dec  4 21:58:07.212: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:58:07.290: INFO: namespace projected-9285 deletion completed in 6.086383228s

• [SLOW TEST:10.275 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:58:07.291: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1697
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec  4 21:58:07.437: INFO: Waiting up to 5m0s for pod "pod-277e01fa-16e1-11ea-8695-527d496f91de" in namespace "emptydir-1697" to be "success or failure"
Dec  4 21:58:07.440: INFO: Pod "pod-277e01fa-16e1-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 3.04227ms
Dec  4 21:58:09.443: INFO: Pod "pod-277e01fa-16e1-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005909732s
Dec  4 21:58:11.445: INFO: Pod "pod-277e01fa-16e1-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008522495s
STEP: Saw pod success
Dec  4 21:58:11.445: INFO: Pod "pod-277e01fa-16e1-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 21:58:11.447: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker6c7da5d4d5 pod pod-277e01fa-16e1-11ea-8695-527d496f91de container test-container: <nil>
STEP: delete the pod
Dec  4 21:58:11.464: INFO: Waiting for pod pod-277e01fa-16e1-11ea-8695-527d496f91de to disappear
Dec  4 21:58:11.466: INFO: Pod pod-277e01fa-16e1-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:58:11.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1697" for this suite.
Dec  4 21:58:17.622: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:58:17.692: INFO: namespace emptydir-1697 deletion completed in 6.219227897s

• [SLOW TEST:10.401 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:58:17.693: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-3316
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Dec  4 21:58:17.836: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-3316,SelfLink:/api/v1/namespaces/watch-3316/configmaps/e2e-watch-test-watch-closed,UID:2db1d806-16e1-11ea-825c-005056950656,ResourceVersion:295315,Generation:0,CreationTimestamp:2019-12-04 21:58:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec  4 21:58:17.839: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-3316,SelfLink:/api/v1/namespaces/watch-3316/configmaps/e2e-watch-test-watch-closed,UID:2db1d806-16e1-11ea-825c-005056950656,ResourceVersion:295316,Generation:0,CreationTimestamp:2019-12-04 21:58:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Dec  4 21:58:17.848: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-3316,SelfLink:/api/v1/namespaces/watch-3316/configmaps/e2e-watch-test-watch-closed,UID:2db1d806-16e1-11ea-825c-005056950656,ResourceVersion:295317,Generation:0,CreationTimestamp:2019-12-04 21:58:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec  4 21:58:17.848: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-3316,SelfLink:/api/v1/namespaces/watch-3316/configmaps/e2e-watch-test-watch-closed,UID:2db1d806-16e1-11ea-825c-005056950656,ResourceVersion:295318,Generation:0,CreationTimestamp:2019-12-04 21:58:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:58:17.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3316" for this suite.
Dec  4 21:58:23.861: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:58:23.941: INFO: namespace watch-3316 deletion completed in 6.089419901s

• [SLOW TEST:6.249 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:58:23.946: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1614
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-31731788-16e1-11ea-8695-527d496f91de
STEP: Creating a pod to test consume secrets
Dec  4 21:58:24.140: INFO: Waiting up to 5m0s for pod "pod-secrets-3173803a-16e1-11ea-8695-527d496f91de" in namespace "secrets-1614" to be "success or failure"
Dec  4 21:58:24.145: INFO: Pod "pod-secrets-3173803a-16e1-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 5.573161ms
Dec  4 21:58:26.149: INFO: Pod "pod-secrets-3173803a-16e1-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009150075s
Dec  4 21:58:28.152: INFO: Pod "pod-secrets-3173803a-16e1-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01224289s
STEP: Saw pod success
Dec  4 21:58:28.152: INFO: Pod "pod-secrets-3173803a-16e1-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 21:58:28.154: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker9f6c17cd60 pod pod-secrets-3173803a-16e1-11ea-8695-527d496f91de container secret-volume-test: <nil>
STEP: delete the pod
Dec  4 21:58:28.176: INFO: Waiting for pod pod-secrets-3173803a-16e1-11ea-8695-527d496f91de to disappear
Dec  4 21:58:28.178: INFO: Pod pod-secrets-3173803a-16e1-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:58:28.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1614" for this suite.
Dec  4 21:58:34.192: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:58:34.260: INFO: namespace secrets-1614 deletion completed in 6.07730865s

• [SLOW TEST:10.314 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:58:34.261: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8198
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8198.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8198.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec  4 21:58:36.484: INFO: DNS probes using dns-8198/dns-test-3799287c-16e1-11ea-8695-527d496f91de succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:58:36.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8198" for this suite.
Dec  4 21:58:42.513: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:58:42.602: INFO: namespace dns-8198 deletion completed in 6.098605081s

• [SLOW TEST:8.341 seconds]
[sig-network] DNS
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:58:42.604: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6999
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Dec  4 21:58:42.735: INFO: namespace kubectl-6999
Dec  4 21:58:42.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 create -f - --namespace=kubectl-6999'
Dec  4 21:58:43.193: INFO: stderr: ""
Dec  4 21:58:43.193: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec  4 21:58:44.196: INFO: Selector matched 1 pods for map[app:redis]
Dec  4 21:58:44.196: INFO: Found 0 / 1
Dec  4 21:58:45.196: INFO: Selector matched 1 pods for map[app:redis]
Dec  4 21:58:45.196: INFO: Found 1 / 1
Dec  4 21:58:45.196: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec  4 21:58:45.199: INFO: Selector matched 1 pods for map[app:redis]
Dec  4 21:58:45.199: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec  4 21:58:45.199: INFO: wait on redis-master startup in kubectl-6999 
Dec  4 21:58:45.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 logs redis-master-pdxv4 redis-master --namespace=kubectl-6999'
Dec  4 21:58:45.307: INFO: stderr: ""
Dec  4 21:58:45.308: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 04 Dec 21:58:43.465 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 04 Dec 21:58:43.465 # Server started, Redis version 3.2.12\n1:M 04 Dec 21:58:43.465 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 04 Dec 21:58:43.465 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Dec  4 21:58:45.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-6999'
Dec  4 21:58:45.419: INFO: stderr: ""
Dec  4 21:58:45.419: INFO: stdout: "service/rm2 exposed\n"
Dec  4 21:58:45.423: INFO: Service rm2 in namespace kubectl-6999 found.
STEP: exposing service
Dec  4 21:58:47.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-6999'
Dec  4 21:58:47.558: INFO: stderr: ""
Dec  4 21:58:47.558: INFO: stdout: "service/rm3 exposed\n"
Dec  4 21:58:47.560: INFO: Service rm3 in namespace kubectl-6999 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:58:49.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6999" for this suite.
Dec  4 21:59:11.576: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:59:11.654: INFO: namespace kubectl-6999 deletion completed in 22.085471715s

• [SLOW TEST:29.050 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create services for rc  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:59:11.657: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-8234
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  4 21:59:11.788: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:59:12.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8234" for this suite.
Dec  4 21:59:18.347: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:59:18.432: INFO: namespace custom-resource-definition-8234 deletion completed in 6.093806837s

• [SLOW TEST:6.775 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:32
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:59:18.433: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8946
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Dec  4 21:59:18.575: INFO: Waiting up to 5m0s for pod "downwardapi-volume-51e5cf54-16e1-11ea-8695-527d496f91de" in namespace "projected-8946" to be "success or failure"
Dec  4 21:59:18.580: INFO: Pod "downwardapi-volume-51e5cf54-16e1-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 5.023228ms
Dec  4 21:59:20.583: INFO: Pod "downwardapi-volume-51e5cf54-16e1-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00797535s
STEP: Saw pod success
Dec  4 21:59:20.583: INFO: Pod "downwardapi-volume-51e5cf54-16e1-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 21:59:20.585: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker6c7da5d4d5 pod downwardapi-volume-51e5cf54-16e1-11ea-8695-527d496f91de container client-container: <nil>
STEP: delete the pod
Dec  4 21:59:20.610: INFO: Waiting for pod downwardapi-volume-51e5cf54-16e1-11ea-8695-527d496f91de to disappear
Dec  4 21:59:20.611: INFO: Pod downwardapi-volume-51e5cf54-16e1-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 21:59:20.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8946" for this suite.
Dec  4 21:59:26.623: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 21:59:26.697: INFO: namespace projected-8946 deletion completed in 6.083014424s

• [SLOW TEST:8.265 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 21:59:26.698: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5494
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-5494
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-5494
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5494
Dec  4 21:59:26.874: INFO: Found 0 stateful pods, waiting for 1
Dec  4 21:59:36.877: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Dec  4 21:59:36.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-5494 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  4 21:59:37.108: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  4 21:59:37.108: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  4 21:59:37.108: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  4 21:59:37.114: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec  4 21:59:47.117: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  4 21:59:47.117: INFO: Waiting for statefulset status.replicas updated to 0
Dec  4 21:59:47.129: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999767s
Dec  4 21:59:48.132: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997324317s
Dec  4 21:59:49.134: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.994308852s
Dec  4 21:59:50.137: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.991416771s
Dec  4 21:59:51.141: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.988553935s
Dec  4 21:59:52.145: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.985273394s
Dec  4 21:59:53.148: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.980421295s
Dec  4 21:59:54.151: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.978070305s
Dec  4 21:59:55.154: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.974741223s
Dec  4 21:59:56.157: INFO: Verifying statefulset ss doesn't scale past 1 for another 971.513946ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5494
Dec  4 21:59:57.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-5494 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  4 21:59:57.405: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec  4 21:59:57.405: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  4 21:59:57.405: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  4 21:59:57.408: INFO: Found 1 stateful pods, waiting for 3
Dec  4 22:00:07.411: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec  4 22:00:07.411: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec  4 22:00:07.411: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Dec  4 22:00:07.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-5494 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  4 22:00:07.712: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  4 22:00:07.712: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  4 22:00:07.712: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  4 22:00:07.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-5494 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  4 22:00:07.904: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  4 22:00:07.904: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  4 22:00:07.904: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  4 22:00:07.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-5494 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec  4 22:00:08.145: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec  4 22:00:08.145: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec  4 22:00:08.145: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec  4 22:00:08.145: INFO: Waiting for statefulset status.replicas updated to 0
Dec  4 22:00:08.148: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Dec  4 22:00:18.154: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec  4 22:00:18.154: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec  4 22:00:18.154: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec  4 22:00:18.167: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999623s
Dec  4 22:00:19.171: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99576587s
Dec  4 22:00:20.175: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992055828s
Dec  4 22:00:21.179: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.988273319s
Dec  4 22:00:22.182: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.984368896s
Dec  4 22:00:23.185: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.981209787s
Dec  4 22:00:24.189: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.977785953s
Dec  4 22:00:25.192: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.974197811s
Dec  4 22:00:26.196: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.970610567s
Dec  4 22:00:27.199: INFO: Verifying statefulset ss doesn't scale past 3 for another 967.098519ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5494
Dec  4 22:00:28.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-5494 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  4 22:00:28.426: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec  4 22:00:28.426: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  4 22:00:28.426: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  4 22:00:28.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-5494 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  4 22:00:28.621: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec  4 22:00:28.621: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  4 22:00:28.621: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  4 22:00:28.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 exec --namespace=statefulset-5494 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec  4 22:00:28.922: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec  4 22:00:28.922: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec  4 22:00:28.928: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec  4 22:00:28.928: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Dec  4 22:00:48.949: INFO: Deleting all statefulset in ns statefulset-5494
Dec  4 22:00:48.955: INFO: Scaling statefulset ss to 0
Dec  4 22:00:48.962: INFO: Waiting for statefulset status.replicas updated to 0
Dec  4 22:00:48.964: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 22:00:48.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5494" for this suite.
Dec  4 22:00:55.001: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 22:00:55.081: INFO: namespace statefulset-5494 deletion completed in 6.095849316s

• [SLOW TEST:88.383 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 22:00:55.085: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6649
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  4 22:00:55.287: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Dec  4 22:00:55.298: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 22:00:55.306: INFO: Number of nodes with available pods: 0
Dec  4 22:00:55.306: INFO: Node alex-slot1-v2-vsp1-worker6c7da5d4d5 is running more than one daemon pod
Dec  4 22:00:56.312: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 22:00:56.317: INFO: Number of nodes with available pods: 0
Dec  4 22:00:56.317: INFO: Node alex-slot1-v2-vsp1-worker6c7da5d4d5 is running more than one daemon pod
Dec  4 22:00:57.310: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 22:00:57.313: INFO: Number of nodes with available pods: 0
Dec  4 22:00:57.313: INFO: Node alex-slot1-v2-vsp1-worker6c7da5d4d5 is running more than one daemon pod
Dec  4 22:00:58.310: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 22:00:58.313: INFO: Number of nodes with available pods: 2
Dec  4 22:00:58.313: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Dec  4 22:00:58.347: INFO: Wrong image for pod: daemon-set-gr8l6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  4 22:00:58.347: INFO: Wrong image for pod: daemon-set-vhm8c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  4 22:00:58.351: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 22:00:59.354: INFO: Wrong image for pod: daemon-set-gr8l6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  4 22:00:59.354: INFO: Wrong image for pod: daemon-set-vhm8c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  4 22:00:59.358: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 22:01:00.355: INFO: Wrong image for pod: daemon-set-gr8l6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  4 22:01:00.355: INFO: Wrong image for pod: daemon-set-vhm8c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  4 22:01:00.358: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 22:01:01.355: INFO: Wrong image for pod: daemon-set-gr8l6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  4 22:01:01.355: INFO: Wrong image for pod: daemon-set-vhm8c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  4 22:01:01.355: INFO: Pod daemon-set-vhm8c is not available
Dec  4 22:01:01.358: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 22:01:02.354: INFO: Wrong image for pod: daemon-set-gr8l6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  4 22:01:02.354: INFO: Wrong image for pod: daemon-set-vhm8c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  4 22:01:02.354: INFO: Pod daemon-set-vhm8c is not available
Dec  4 22:01:02.357: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 22:01:03.354: INFO: Wrong image for pod: daemon-set-gr8l6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  4 22:01:03.354: INFO: Wrong image for pod: daemon-set-vhm8c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  4 22:01:03.354: INFO: Pod daemon-set-vhm8c is not available
Dec  4 22:01:03.358: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 22:01:04.355: INFO: Wrong image for pod: daemon-set-gr8l6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  4 22:01:04.355: INFO: Wrong image for pod: daemon-set-vhm8c. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  4 22:01:04.355: INFO: Pod daemon-set-vhm8c is not available
Dec  4 22:01:04.359: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 22:01:05.355: INFO: Wrong image for pod: daemon-set-gr8l6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  4 22:01:05.355: INFO: Pod daemon-set-jxgnq is not available
Dec  4 22:01:05.360: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 22:01:06.355: INFO: Wrong image for pod: daemon-set-gr8l6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  4 22:01:06.355: INFO: Pod daemon-set-jxgnq is not available
Dec  4 22:01:06.358: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 22:01:07.355: INFO: Wrong image for pod: daemon-set-gr8l6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  4 22:01:07.365: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 22:01:08.355: INFO: Wrong image for pod: daemon-set-gr8l6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  4 22:01:08.355: INFO: Pod daemon-set-gr8l6 is not available
Dec  4 22:01:08.359: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 22:01:09.354: INFO: Wrong image for pod: daemon-set-gr8l6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  4 22:01:09.354: INFO: Pod daemon-set-gr8l6 is not available
Dec  4 22:01:09.358: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 22:01:10.354: INFO: Wrong image for pod: daemon-set-gr8l6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  4 22:01:10.354: INFO: Pod daemon-set-gr8l6 is not available
Dec  4 22:01:10.358: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 22:01:11.354: INFO: Wrong image for pod: daemon-set-gr8l6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  4 22:01:11.354: INFO: Pod daemon-set-gr8l6 is not available
Dec  4 22:01:11.360: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 22:01:12.354: INFO: Wrong image for pod: daemon-set-gr8l6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  4 22:01:12.354: INFO: Pod daemon-set-gr8l6 is not available
Dec  4 22:01:12.358: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 22:01:13.355: INFO: Wrong image for pod: daemon-set-gr8l6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  4 22:01:13.355: INFO: Pod daemon-set-gr8l6 is not available
Dec  4 22:01:13.359: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 22:01:14.355: INFO: Wrong image for pod: daemon-set-gr8l6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  4 22:01:14.355: INFO: Pod daemon-set-gr8l6 is not available
Dec  4 22:01:14.359: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 22:01:15.354: INFO: Wrong image for pod: daemon-set-gr8l6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  4 22:01:15.354: INFO: Pod daemon-set-gr8l6 is not available
Dec  4 22:01:15.357: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 22:01:16.355: INFO: Wrong image for pod: daemon-set-gr8l6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  4 22:01:16.355: INFO: Pod daemon-set-gr8l6 is not available
Dec  4 22:01:16.359: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 22:01:17.354: INFO: Wrong image for pod: daemon-set-gr8l6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  4 22:01:17.354: INFO: Pod daemon-set-gr8l6 is not available
Dec  4 22:01:17.359: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 22:01:18.354: INFO: Wrong image for pod: daemon-set-gr8l6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  4 22:01:18.354: INFO: Pod daemon-set-gr8l6 is not available
Dec  4 22:01:18.358: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 22:01:19.354: INFO: Wrong image for pod: daemon-set-gr8l6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec  4 22:01:19.354: INFO: Pod daemon-set-gr8l6 is not available
Dec  4 22:01:19.359: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 22:01:20.355: INFO: Pod daemon-set-8pk5d is not available
Dec  4 22:01:20.360: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Dec  4 22:01:20.363: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 22:01:20.366: INFO: Number of nodes with available pods: 1
Dec  4 22:01:20.366: INFO: Node alex-slot1-v2-vsp1-worker6c7da5d4d5 is running more than one daemon pod
Dec  4 22:01:21.372: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 22:01:21.377: INFO: Number of nodes with available pods: 1
Dec  4 22:01:21.377: INFO: Node alex-slot1-v2-vsp1-worker6c7da5d4d5 is running more than one daemon pod
Dec  4 22:01:22.370: INFO: DaemonSet pods can't tolerate node alex-slot1-v2-vsp1-masterb17a9fb4bd with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Dec  4 22:01:22.372: INFO: Number of nodes with available pods: 2
Dec  4 22:01:22.372: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6649, will wait for the garbage collector to delete the pods
Dec  4 22:01:22.442: INFO: Deleting DaemonSet.extensions daemon-set took: 8.231767ms
Dec  4 22:01:22.846: INFO: Terminating DaemonSet.extensions daemon-set pods took: 404.3035ms
Dec  4 22:01:25.849: INFO: Number of nodes with available pods: 0
Dec  4 22:01:25.849: INFO: Number of running nodes: 0, number of available pods: 0
Dec  4 22:01:25.853: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6649/daemonsets","resourceVersion":"296098"},"items":null}

Dec  4 22:01:25.855: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6649/pods","resourceVersion":"296098"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 22:01:25.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6649" for this suite.
Dec  4 22:01:31.878: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 22:01:31.956: INFO: namespace daemonsets-6649 deletion completed in 6.087382878s

• [SLOW TEST:36.871 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 22:01:31.956: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2108
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  4 22:01:32.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 version'
Dec  4 22:01:32.185: INFO: stderr: ""
Dec  4 22:01:32.185: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.8\", GitCommit:\"211047e9a1922595eaa3a1127ed365e9299a6c23\", GitTreeState:\"clean\", BuildDate:\"2019-10-15T12:11:03Z\", GoVersion:\"go1.12.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.8\", GitCommit:\"211047e9a1922595eaa3a1127ed365e9299a6c23\", GitTreeState:\"clean\", BuildDate:\"2019-10-15T12:02:12Z\", GoVersion:\"go1.12.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 22:01:32.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2108" for this suite.
Dec  4 22:01:38.204: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 22:01:38.275: INFO: namespace kubectl-2108 deletion completed in 6.085709497s

• [SLOW TEST:6.319 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 22:01:38.278: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2993
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W1204 22:01:48.487911      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec  4 22:01:48.488: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 22:01:48.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2993" for this suite.
Dec  4 22:01:54.509: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 22:01:54.582: INFO: namespace gc-2993 deletion completed in 6.091279273s

• [SLOW TEST:16.304 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 22:01:54.583: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-5489
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  4 22:01:54.734: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Dec  4 22:01:54.745: INFO: Number of nodes with available pods: 0
Dec  4 22:01:54.745: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Dec  4 22:01:54.767: INFO: Number of nodes with available pods: 0
Dec  4 22:01:54.767: INFO: Node alex-slot1-v2-vsp1-worker6c7da5d4d5 is running more than one daemon pod
Dec  4 22:01:55.770: INFO: Number of nodes with available pods: 0
Dec  4 22:01:55.770: INFO: Node alex-slot1-v2-vsp1-worker6c7da5d4d5 is running more than one daemon pod
Dec  4 22:01:56.771: INFO: Number of nodes with available pods: 0
Dec  4 22:01:56.771: INFO: Node alex-slot1-v2-vsp1-worker6c7da5d4d5 is running more than one daemon pod
Dec  4 22:01:57.770: INFO: Number of nodes with available pods: 1
Dec  4 22:01:57.770: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Dec  4 22:01:57.790: INFO: Number of nodes with available pods: 1
Dec  4 22:01:57.790: INFO: Number of running nodes: 0, number of available pods: 1
Dec  4 22:01:58.793: INFO: Number of nodes with available pods: 0
Dec  4 22:01:58.793: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Dec  4 22:01:58.802: INFO: Number of nodes with available pods: 0
Dec  4 22:01:58.802: INFO: Node alex-slot1-v2-vsp1-worker6c7da5d4d5 is running more than one daemon pod
Dec  4 22:01:59.806: INFO: Number of nodes with available pods: 0
Dec  4 22:01:59.806: INFO: Node alex-slot1-v2-vsp1-worker6c7da5d4d5 is running more than one daemon pod
Dec  4 22:02:00.805: INFO: Number of nodes with available pods: 0
Dec  4 22:02:00.805: INFO: Node alex-slot1-v2-vsp1-worker6c7da5d4d5 is running more than one daemon pod
Dec  4 22:02:01.806: INFO: Number of nodes with available pods: 0
Dec  4 22:02:01.806: INFO: Node alex-slot1-v2-vsp1-worker6c7da5d4d5 is running more than one daemon pod
Dec  4 22:02:02.805: INFO: Number of nodes with available pods: 0
Dec  4 22:02:02.806: INFO: Node alex-slot1-v2-vsp1-worker6c7da5d4d5 is running more than one daemon pod
Dec  4 22:02:03.805: INFO: Number of nodes with available pods: 1
Dec  4 22:02:03.805: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5489, will wait for the garbage collector to delete the pods
Dec  4 22:02:03.868: INFO: Deleting DaemonSet.extensions daemon-set took: 7.057291ms
Dec  4 22:02:04.268: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.435274ms
Dec  4 22:02:07.871: INFO: Number of nodes with available pods: 0
Dec  4 22:02:07.871: INFO: Number of running nodes: 0, number of available pods: 0
Dec  4 22:02:07.876: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5489/daemonsets","resourceVersion":"296323"},"items":null}

Dec  4 22:02:07.877: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5489/pods","resourceVersion":"296323"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 22:02:07.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5489" for this suite.
Dec  4 22:02:13.912: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 22:02:14.014: INFO: namespace daemonsets-5489 deletion completed in 6.109755745s

• [SLOW TEST:19.431 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 22:02:14.014: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5577
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-ba8ddaa7-16e1-11ea-8695-527d496f91de
STEP: Creating a pod to test consume configMaps
Dec  4 22:02:14.161: INFO: Waiting up to 5m0s for pod "pod-configmaps-ba8e49fb-16e1-11ea-8695-527d496f91de" in namespace "configmap-5577" to be "success or failure"
Dec  4 22:02:14.166: INFO: Pod "pod-configmaps-ba8e49fb-16e1-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 4.939533ms
Dec  4 22:02:16.169: INFO: Pod "pod-configmaps-ba8e49fb-16e1-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007602481s
STEP: Saw pod success
Dec  4 22:02:16.169: INFO: Pod "pod-configmaps-ba8e49fb-16e1-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 22:02:16.171: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker6c7da5d4d5 pod pod-configmaps-ba8e49fb-16e1-11ea-8695-527d496f91de container configmap-volume-test: <nil>
STEP: delete the pod
Dec  4 22:02:16.190: INFO: Waiting for pod pod-configmaps-ba8e49fb-16e1-11ea-8695-527d496f91de to disappear
Dec  4 22:02:16.192: INFO: Pod pod-configmaps-ba8e49fb-16e1-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 22:02:16.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5577" for this suite.
Dec  4 22:02:22.206: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 22:02:22.285: INFO: namespace configmap-5577 deletion completed in 6.090484095s

• [SLOW TEST:8.272 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 22:02:22.286: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9282
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Dec  4 22:02:26.963: INFO: Successfully updated pod "labelsupdatebf7b1e28-16e1-11ea-8695-527d496f91de"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 22:02:28.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9282" for this suite.
Dec  4 22:02:50.997: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 22:02:51.088: INFO: namespace downward-api-9282 deletion completed in 22.101061275s

• [SLOW TEST:28.802 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 22:02:51.093: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7591
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-d0a748eb-16e1-11ea-8695-527d496f91de
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-d0a748eb-16e1-11ea-8695-527d496f91de
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 22:02:55.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7591" for this suite.
Dec  4 22:03:17.286: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 22:03:17.373: INFO: namespace configmap-7591 deletion completed in 22.096887582s

• [SLOW TEST:26.281 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 22:03:17.376: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-7138
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Dec  4 22:03:17.524: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec  4 22:03:17.534: INFO: Waiting for terminating namespaces to be deleted...
Dec  4 22:03:17.535: INFO: 
Logging pods the kubelet thinks is on node alex-slot1-v2-vsp1-worker6c7da5d4d5 before test
Dec  4 22:03:17.543: INFO: metallb-controller-577b6fb44-cxzgk from ccp started at 2019-12-02 22:34:24 +0000 UTC (1 container statuses recorded)
Dec  4 22:03:17.543: INFO: 	Container controller ready: true, restart count 0
Dec  4 22:03:17.543: INFO: ccp-monitor-prometheus-alertmanager-69f5f8bfbb-wrqkt from ccp started at 2019-12-02 22:34:32 +0000 UTC (2 container statuses recorded)
Dec  4 22:03:17.543: INFO: 	Container prometheus-alertmanager ready: true, restart count 0
Dec  4 22:03:17.543: INFO: 	Container prometheus-alertmanager-configmap-reload ready: true, restart count 0
Dec  4 22:03:17.544: INFO: ccp-monitor-prometheus-kube-state-metrics-dc8476fb4-h4mhf from ccp started at 2019-12-02 22:34:32 +0000 UTC (1 container statuses recorded)
Dec  4 22:03:17.544: INFO: 	Container prometheus-kube-state-metrics ready: true, restart count 0
Dec  4 22:03:17.544: INFO: ccp-monitor-grafana-cf94d4698-582qk from ccp started at 2019-12-02 22:34:33 +0000 UTC (1 container statuses recorded)
Dec  4 22:03:17.544: INFO: 	Container grafana ready: true, restart count 0
Dec  4 22:03:17.544: INFO: fluentd-es-v2.0.2-99h8x from ccp started at 2019-12-02 22:34:33 +0000 UTC (1 container statuses recorded)
Dec  4 22:03:17.544: INFO: 	Container fluentd-es ready: true, restart count 0
Dec  4 22:03:17.544: INFO: coredns-69cb7f6694-qph4l from kube-system started at 2019-12-02 22:29:13 +0000 UTC (1 container statuses recorded)
Dec  4 22:03:17.544: INFO: 	Container coredns ready: true, restart count 0
Dec  4 22:03:17.544: INFO: nginx-ingress-controller-nsxf4 from ccp started at 2019-12-02 22:34:24 +0000 UTC (1 container statuses recorded)
Dec  4 22:03:17.544: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec  4 22:03:17.544: INFO: nvidia-device-plugin-daemonset-vpx26 from kube-system started at 2019-12-02 22:29:04 +0000 UTC (1 container statuses recorded)
Dec  4 22:03:17.544: INFO: 	Container nvidia-device-plugin-ctr ready: true, restart count 0
Dec  4 22:03:17.545: INFO: ccp-harbor-harbor-adminserver-5c6f5d5b8c-w2p9p from default started at 2019-12-02 22:34:37 +0000 UTC (1 container statuses recorded)
Dec  4 22:03:17.545: INFO: 	Container adminserver ready: true, restart count 1
Dec  4 22:03:17.545: INFO: ccp-harbor-harbor-disable-self-reg-job-98zz9 from default started at 2019-12-02 22:34:37 +0000 UTC (1 container statuses recorded)
Dec  4 22:03:17.545: INFO: 	Container self-registration ready: false, restart count 0
Dec  4 22:03:17.545: INFO: ccp-efk-elasticsearch-curator-1575334800-f92rq from ccp started at 2019-12-03 01:00:04 +0000 UTC (1 container statuses recorded)
Dec  4 22:03:17.545: INFO: 	Container elasticsearch-curator ready: false, restart count 0
Dec  4 22:03:17.545: INFO: sonobuoy-systemd-logs-daemon-set-f8654487368d4bd0-qjhgr from sonobuoy started at 2019-12-04 20:32:52 +0000 UTC (2 container statuses recorded)
Dec  4 22:03:17.545: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec  4 22:03:17.545: INFO: 	Container systemd-logs ready: true, restart count 0
Dec  4 22:03:17.545: INFO: cert-manager-799695b5bc-8gkw4 from ccp started at 2019-12-02 22:29:09 +0000 UTC (1 container statuses recorded)
Dec  4 22:03:17.545: INFO: 	Container cert-manager ready: true, restart count 0
Dec  4 22:03:17.545: INFO: metallb-speaker-np9rf from ccp started at 2019-12-02 22:34:24 +0000 UTC (1 container statuses recorded)
Dec  4 22:03:17.546: INFO: 	Container speaker ready: true, restart count 0
Dec  4 22:03:17.546: INFO: ccp-monitor-prometheus-port-update-1a9rs-qj4gx from ccp started at 2019-12-02 22:34:32 +0000 UTC (1 container statuses recorded)
Dec  4 22:03:17.546: INFO: 	Container ccp-monitor-prometheus-port-update ready: false, restart count 0
Dec  4 22:03:17.546: INFO: ccp-harbor-harbor-core-6c5d44857d-cnzn8 from default started at 2019-12-02 22:34:37 +0000 UTC (1 container statuses recorded)
Dec  4 22:03:17.546: INFO: 	Container core ready: true, restart count 0
Dec  4 22:03:17.546: INFO: kube-proxy-2krtb from kube-system started at 2019-12-02 22:28:54 +0000 UTC (1 container statuses recorded)
Dec  4 22:03:17.546: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  4 22:03:17.546: INFO: calico-node-27hrx from kube-system started at 2019-12-02 22:28:54 +0000 UTC (1 container statuses recorded)
Dec  4 22:03:17.546: INFO: 	Container calico-node ready: true, restart count 0
Dec  4 22:03:17.546: INFO: ccp-monitor-prometheus-node-exporter-d6xhl from ccp started at 2019-12-02 22:34:32 +0000 UTC (1 container statuses recorded)
Dec  4 22:03:17.546: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Dec  4 22:03:17.546: INFO: ccp-efk-kibana-56866fb888-sx6dh from ccp started at 2019-12-02 22:34:33 +0000 UTC (1 container statuses recorded)
Dec  4 22:03:17.546: INFO: 	Container kibana ready: true, restart count 0
Dec  4 22:03:17.547: INFO: ccp-harbor-harbor-database-0 from default started at 2019-12-02 22:34:38 +0000 UTC (1 container statuses recorded)
Dec  4 22:03:17.547: INFO: 	Container database ready: true, restart count 0
Dec  4 22:03:17.547: INFO: sonobuoy from sonobuoy started at 2019-12-04 20:32:21 +0000 UTC (1 container statuses recorded)
Dec  4 22:03:17.547: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec  4 22:03:17.547: INFO: 
Logging pods the kubelet thinks is on node alex-slot1-v2-vsp1-worker9f6c17cd60 before test
Dec  4 22:03:17.556: INFO: nvidia-device-plugin-daemonset-km64c from kube-system started at 2019-12-02 22:29:03 +0000 UTC (1 container statuses recorded)
Dec  4 22:03:17.556: INFO: 	Container nvidia-device-plugin-ctr ready: true, restart count 0
Dec  4 22:03:17.556: INFO: nginx-ingress-default-backend-dc5596844-xss9h from ccp started at 2019-12-02 22:34:23 +0000 UTC (1 container statuses recorded)
Dec  4 22:03:17.556: INFO: 	Container nginx-ingress-default-backend ready: true, restart count 0
Dec  4 22:03:17.556: INFO: ccp-monitor-prometheus-pushgateway-7dcdb9d494-pzvxm from ccp started at 2019-12-02 22:34:31 +0000 UTC (1 container statuses recorded)
Dec  4 22:03:17.556: INFO: 	Container prometheus-pushgateway ready: true, restart count 0
Dec  4 22:03:17.556: INFO: metallb-speaker-xm9d9 from ccp started at 2019-12-02 22:34:24 +0000 UTC (1 container statuses recorded)
Dec  4 22:03:17.556: INFO: 	Container speaker ready: true, restart count 0
Dec  4 22:03:17.556: INFO: kubernetes-dashboard-67d74b6485-wv99t from ccp started at 2019-12-02 22:34:25 +0000 UTC (1 container statuses recorded)
Dec  4 22:03:17.556: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Dec  4 22:03:17.556: INFO: ccp-monitor-grafana-set-datasource-zkb8j from ccp started at 2019-12-02 22:34:31 +0000 UTC (1 container statuses recorded)
Dec  4 22:03:17.556: INFO: 	Container ccp-monitor-grafana-set-datasource ready: false, restart count 0
Dec  4 22:03:17.556: INFO: coredns-69cb7f6694-m5vmc from kube-system started at 2019-12-02 22:29:13 +0000 UTC (1 container statuses recorded)
Dec  4 22:03:17.556: INFO: 	Container coredns ready: true, restart count 0
Dec  4 22:03:17.556: INFO: kube-proxy-xxz5s from kube-system started at 2019-12-02 22:28:53 +0000 UTC (1 container statuses recorded)
Dec  4 22:03:17.556: INFO: 	Container kube-proxy ready: true, restart count 0
Dec  4 22:03:17.556: INFO: ccp-monitor-prometheus-pass-job-no1bx-7bmdq from ccp started at 2019-12-02 22:34:31 +0000 UTC (1 container statuses recorded)
Dec  4 22:03:17.556: INFO: 	Container ccp-monitor-prometheus-pass-container ready: false, restart count 0
Dec  4 22:03:17.556: INFO: ccp-harbor-harbor-redis-0 from default started at 2019-12-02 22:34:38 +0000 UTC (1 container statuses recorded)
Dec  4 22:03:17.556: INFO: 	Container redis ready: true, restart count 0
Dec  4 22:03:17.556: INFO: nginx-ingress-controller-dbfcd from ccp started at 2019-12-02 22:34:23 +0000 UTC (1 container statuses recorded)
Dec  4 22:03:17.556: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec  4 22:03:17.556: INFO: ccp-monitor-prometheus-node-exporter-mzgmm from ccp started at 2019-12-02 22:34:31 +0000 UTC (1 container statuses recorded)
Dec  4 22:03:17.556: INFO: 	Container prometheus-node-exporter ready: true, restart count 0
Dec  4 22:03:17.556: INFO: ccp-harbor-harbor-portal-7445674c74-x2xqp from default started at 2019-12-02 22:34:36 +0000 UTC (1 container statuses recorded)
Dec  4 22:03:17.556: INFO: 	Container portal ready: true, restart count 0
Dec  4 22:03:17.556: INFO: ccp-efk-elasticsearch-curator-1575421200-wwgv5 from ccp started at 2019-12-04 01:00:03 +0000 UTC (1 container statuses recorded)
Dec  4 22:03:17.556: INFO: 	Container elasticsearch-curator ready: false, restart count 0
Dec  4 22:03:17.556: INFO: calico-node-p5tcj from kube-system started at 2019-12-02 22:28:53 +0000 UTC (1 container statuses recorded)
Dec  4 22:03:17.556: INFO: 	Container calico-node ready: true, restart count 0
Dec  4 22:03:17.556: INFO: fluentd-es-v2.0.2-f4j7n from ccp started at 2019-12-02 22:34:33 +0000 UTC (1 container statuses recorded)
Dec  4 22:03:17.556: INFO: 	Container fluentd-es ready: true, restart count 0
Dec  4 22:03:17.556: INFO: elasticsearch-logging-0 from ccp started at 2019-12-02 22:34:56 +0000 UTC (1 container statuses recorded)
Dec  4 22:03:17.556: INFO: 	Container elasticsearch-logging ready: true, restart count 0
Dec  4 22:03:17.556: INFO: ccp-monitor-prometheus-server-7745ccd984-jw4hf from ccp started at 2019-12-02 22:34:33 +0000 UTC (3 container statuses recorded)
Dec  4 22:03:17.556: INFO: 	Container nginx-proxy ready: true, restart count 0
Dec  4 22:03:17.556: INFO: 	Container prometheus-server ready: true, restart count 0
Dec  4 22:03:17.556: INFO: 	Container prometheus-server-configmap-reload ready: true, restart count 0
Dec  4 22:03:17.556: INFO: ccp-harbor-harbor-jobservice-755d997849-vvz4g from default started at 2019-12-02 22:34:37 +0000 UTC (1 container statuses recorded)
Dec  4 22:03:17.556: INFO: 	Container jobservice ready: true, restart count 0
Dec  4 22:03:17.556: INFO: ccp-harbor-harbor-registry-77f88c7594-xd2nm from default started at 2019-12-02 22:34:38 +0000 UTC (2 container statuses recorded)
Dec  4 22:03:17.556: INFO: 	Container registry ready: true, restart count 0
Dec  4 22:03:17.556: INFO: 	Container registryctl ready: true, restart count 0
Dec  4 22:03:17.556: INFO: sonobuoy-systemd-logs-daemon-set-f8654487368d4bd0-j78pc from sonobuoy started at 2019-12-04 20:32:46 +0000 UTC (2 container statuses recorded)
Dec  4 22:03:17.557: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec  4 22:03:17.557: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-e2bf7432-16e1-11ea-8695-527d496f91de 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-e2bf7432-16e1-11ea-8695-527d496f91de off the node alex-slot1-v2-vsp1-worker9f6c17cd60
STEP: verifying the node doesn't have the label kubernetes.io/e2e-e2bf7432-16e1-11ea-8695-527d496f91de
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 22:03:25.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7138" for this suite.
Dec  4 22:03:33.667: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 22:03:33.763: INFO: namespace sched-pred-7138 deletion completed in 8.102946113s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:16.387 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 22:03:33.764: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-7689
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec  4 22:03:41.962: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  4 22:03:41.975: INFO: Pod pod-with-prestop-http-hook still exists
Dec  4 22:03:43.975: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  4 22:03:43.978: INFO: Pod pod-with-prestop-http-hook still exists
Dec  4 22:03:45.975: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  4 22:03:45.978: INFO: Pod pod-with-prestop-http-hook still exists
Dec  4 22:03:47.975: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  4 22:03:47.979: INFO: Pod pod-with-prestop-http-hook still exists
Dec  4 22:03:49.975: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  4 22:03:49.977: INFO: Pod pod-with-prestop-http-hook still exists
Dec  4 22:03:51.975: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  4 22:03:51.978: INFO: Pod pod-with-prestop-http-hook still exists
Dec  4 22:03:53.975: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  4 22:03:53.980: INFO: Pod pod-with-prestop-http-hook still exists
Dec  4 22:03:55.975: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec  4 22:03:55.977: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 22:03:55.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7689" for this suite.
Dec  4 22:04:17.994: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 22:04:18.063: INFO: namespace container-lifecycle-hook-7689 deletion completed in 22.076198962s

• [SLOW TEST:44.299 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 22:04:18.063: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7874
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-048675f7-16e2-11ea-8695-527d496f91de
STEP: Creating a pod to test consume secrets
Dec  4 22:04:18.266: INFO: Waiting up to 5m0s for pod "pod-secrets-04870bf0-16e2-11ea-8695-527d496f91de" in namespace "secrets-7874" to be "success or failure"
Dec  4 22:04:18.275: INFO: Pod "pod-secrets-04870bf0-16e2-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 8.689527ms
Dec  4 22:04:20.279: INFO: Pod "pod-secrets-04870bf0-16e2-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01264746s
STEP: Saw pod success
Dec  4 22:04:20.279: INFO: Pod "pod-secrets-04870bf0-16e2-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 22:04:20.282: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker6c7da5d4d5 pod pod-secrets-04870bf0-16e2-11ea-8695-527d496f91de container secret-volume-test: <nil>
STEP: delete the pod
Dec  4 22:04:20.304: INFO: Waiting for pod pod-secrets-04870bf0-16e2-11ea-8695-527d496f91de to disappear
Dec  4 22:04:20.307: INFO: Pod pod-secrets-04870bf0-16e2-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 22:04:20.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7874" for this suite.
Dec  4 22:04:26.320: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 22:04:26.411: INFO: namespace secrets-7874 deletion completed in 6.099720608s

• [SLOW TEST:8.348 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 22:04:26.413: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9513
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-09775a65-16e2-11ea-8695-527d496f91de
STEP: Creating a pod to test consume secrets
Dec  4 22:04:26.561: INFO: Waiting up to 5m0s for pod "pod-secrets-09780383-16e2-11ea-8695-527d496f91de" in namespace "secrets-9513" to be "success or failure"
Dec  4 22:04:26.564: INFO: Pod "pod-secrets-09780383-16e2-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 3.708813ms
Dec  4 22:04:28.567: INFO: Pod "pod-secrets-09780383-16e2-11ea-8695-527d496f91de": Phase="Running", Reason="", readiness=true. Elapsed: 2.005917651s
Dec  4 22:04:30.572: INFO: Pod "pod-secrets-09780383-16e2-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011236559s
STEP: Saw pod success
Dec  4 22:04:30.572: INFO: Pod "pod-secrets-09780383-16e2-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 22:04:30.574: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker9f6c17cd60 pod pod-secrets-09780383-16e2-11ea-8695-527d496f91de container secret-env-test: <nil>
STEP: delete the pod
Dec  4 22:04:30.594: INFO: Waiting for pod pod-secrets-09780383-16e2-11ea-8695-527d496f91de to disappear
Dec  4 22:04:30.595: INFO: Pod pod-secrets-09780383-16e2-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 22:04:30.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9513" for this suite.
Dec  4 22:04:36.607: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 22:04:36.695: INFO: namespace secrets-9513 deletion completed in 6.096247465s

• [SLOW TEST:10.282 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 22:04:36.696: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4196
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec  4 22:04:36.889: INFO: Waiting up to 5m0s for pod "pod-0fa0800b-16e2-11ea-8695-527d496f91de" in namespace "emptydir-4196" to be "success or failure"
Dec  4 22:04:36.896: INFO: Pod "pod-0fa0800b-16e2-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 6.977509ms
Dec  4 22:04:38.899: INFO: Pod "pod-0fa0800b-16e2-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010596816s
Dec  4 22:04:40.902: INFO: Pod "pod-0fa0800b-16e2-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013340182s
STEP: Saw pod success
Dec  4 22:04:40.903: INFO: Pod "pod-0fa0800b-16e2-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 22:04:40.905: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker6c7da5d4d5 pod pod-0fa0800b-16e2-11ea-8695-527d496f91de container test-container: <nil>
STEP: delete the pod
Dec  4 22:04:40.927: INFO: Waiting for pod pod-0fa0800b-16e2-11ea-8695-527d496f91de to disappear
Dec  4 22:04:40.929: INFO: Pod pod-0fa0800b-16e2-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 22:04:40.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4196" for this suite.
Dec  4 22:04:46.940: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 22:04:47.013: INFO: namespace emptydir-4196 deletion completed in 6.08024705s

• [SLOW TEST:10.317 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 22:04:47.021: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6232
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service endpoint-test2 in namespace services-6232
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6232 to expose endpoints map[]
Dec  4 22:04:47.170: INFO: successfully validated that service endpoint-test2 in namespace services-6232 exposes endpoints map[] (3.377478ms elapsed)
STEP: Creating pod pod1 in namespace services-6232
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6232 to expose endpoints map[pod1:[80]]
Dec  4 22:04:50.209: INFO: successfully validated that service endpoint-test2 in namespace services-6232 exposes endpoints map[pod1:[80]] (3.020651038s elapsed)
STEP: Creating pod pod2 in namespace services-6232
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6232 to expose endpoints map[pod1:[80] pod2:[80]]
Dec  4 22:04:52.246: INFO: successfully validated that service endpoint-test2 in namespace services-6232 exposes endpoints map[pod1:[80] pod2:[80]] (2.030255949s elapsed)
STEP: Deleting pod pod1 in namespace services-6232
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6232 to expose endpoints map[pod2:[80]]
Dec  4 22:04:52.271: INFO: successfully validated that service endpoint-test2 in namespace services-6232 exposes endpoints map[pod2:[80]] (12.967877ms elapsed)
STEP: Deleting pod pod2 in namespace services-6232
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6232 to expose endpoints map[]
Dec  4 22:04:52.288: INFO: successfully validated that service endpoint-test2 in namespace services-6232 exposes endpoints map[] (7.970669ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 22:04:52.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6232" for this suite.
Dec  4 22:04:58.344: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 22:04:58.412: INFO: namespace services-6232 deletion completed in 6.081554729s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:11.392 seconds]
[sig-network] Services
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 22:04:58.414: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5863
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec  4 22:04:58.555: INFO: Waiting up to 5m0s for pod "pod-1c8a93e8-16e2-11ea-8695-527d496f91de" in namespace "emptydir-5863" to be "success or failure"
Dec  4 22:04:58.560: INFO: Pod "pod-1c8a93e8-16e2-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 4.712811ms
Dec  4 22:05:00.564: INFO: Pod "pod-1c8a93e8-16e2-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009640756s
STEP: Saw pod success
Dec  4 22:05:00.565: INFO: Pod "pod-1c8a93e8-16e2-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 22:05:00.568: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker6c7da5d4d5 pod pod-1c8a93e8-16e2-11ea-8695-527d496f91de container test-container: <nil>
STEP: delete the pod
Dec  4 22:05:00.587: INFO: Waiting for pod pod-1c8a93e8-16e2-11ea-8695-527d496f91de to disappear
Dec  4 22:05:00.589: INFO: Pod pod-1c8a93e8-16e2-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 22:05:00.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5863" for this suite.
Dec  4 22:05:06.603: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 22:05:06.689: INFO: namespace emptydir-5863 deletion completed in 6.093766483s

• [SLOW TEST:8.275 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 22:05:06.694: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7332
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-217a8eab-16e2-11ea-8695-527d496f91de
STEP: Creating configMap with name cm-test-opt-upd-217a8ef7-16e2-11ea-8695-527d496f91de
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-217a8eab-16e2-11ea-8695-527d496f91de
STEP: Updating configmap cm-test-opt-upd-217a8ef7-16e2-11ea-8695-527d496f91de
STEP: Creating configMap with name cm-test-opt-create-217a8f0d-16e2-11ea-8695-527d496f91de
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 22:05:10.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7332" for this suite.
Dec  4 22:05:32.925: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 22:05:33.007: INFO: namespace projected-7332 deletion completed in 22.092324213s

• [SLOW TEST:26.314 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 22:05:33.009: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-8539
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:69
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Registering the sample API server.
Dec  4 22:05:33.938: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Dec  4 22:05:36.047: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093933, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093933, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093933, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093933, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  4 22:05:38.050: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093933, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093933, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093933, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093933, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  4 22:05:40.050: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093933, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093933, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093933, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093933, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  4 22:05:42.050: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093933, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093933, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093933, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093933, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  4 22:05:44.989: INFO: Waited 918.676928ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:60
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 22:05:45.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-8539" for this suite.
Dec  4 22:05:51.669: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 22:05:51.767: INFO: namespace aggregator-8539 deletion completed in 6.198459299s

• [SLOW TEST:18.758 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 22:05:51.768: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4637
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-3c5793e1-16e2-11ea-8695-527d496f91de
STEP: Creating a pod to test consume secrets
Dec  4 22:05:51.910: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3c5817b0-16e2-11ea-8695-527d496f91de" in namespace "projected-4637" to be "success or failure"
Dec  4 22:05:51.921: INFO: Pod "pod-projected-secrets-3c5817b0-16e2-11ea-8695-527d496f91de": Phase="Pending", Reason="", readiness=false. Elapsed: 10.620362ms
Dec  4 22:05:53.924: INFO: Pod "pod-projected-secrets-3c5817b0-16e2-11ea-8695-527d496f91de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013377597s
STEP: Saw pod success
Dec  4 22:05:53.924: INFO: Pod "pod-projected-secrets-3c5817b0-16e2-11ea-8695-527d496f91de" satisfied condition "success or failure"
Dec  4 22:05:53.926: INFO: Trying to get logs from node alex-slot1-v2-vsp1-worker6c7da5d4d5 pod pod-projected-secrets-3c5817b0-16e2-11ea-8695-527d496f91de container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec  4 22:05:53.942: INFO: Waiting for pod pod-projected-secrets-3c5817b0-16e2-11ea-8695-527d496f91de to disappear
Dec  4 22:05:53.944: INFO: Pod pod-projected-secrets-3c5817b0-16e2-11ea-8695-527d496f91de no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 22:05:53.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4637" for this suite.
Dec  4 22:05:59.955: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 22:06:00.032: INFO: namespace projected-4637 deletion completed in 6.084202212s

• [SLOW TEST:8.264 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 22:06:00.032: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-651
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:214
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Dec  4 22:06:00.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 create -f - --namespace=kubectl-651'
Dec  4 22:06:00.427: INFO: stderr: ""
Dec  4 22:06:00.427: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec  4 22:06:01.430: INFO: Selector matched 1 pods for map[app:redis]
Dec  4 22:06:01.430: INFO: Found 0 / 1
Dec  4 22:06:02.430: INFO: Selector matched 1 pods for map[app:redis]
Dec  4 22:06:02.430: INFO: Found 0 / 1
Dec  4 22:06:03.430: INFO: Selector matched 1 pods for map[app:redis]
Dec  4 22:06:03.430: INFO: Found 1 / 1
Dec  4 22:06:03.430: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Dec  4 22:06:03.433: INFO: Selector matched 1 pods for map[app:redis]
Dec  4 22:06:03.433: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec  4 22:06:03.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-246239386 patch pod redis-master-lcxjd --namespace=kubectl-651 -p {"metadata":{"annotations":{"x":"y"}}}'
Dec  4 22:06:03.528: INFO: stderr: ""
Dec  4 22:06:03.528: INFO: stdout: "pod/redis-master-lcxjd patched\n"
STEP: checking annotations
Dec  4 22:06:03.531: INFO: Selector matched 1 pods for map[app:redis]
Dec  4 22:06:03.531: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 22:06:03.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-651" for this suite.
Dec  4 22:06:25.544: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 22:06:25.628: INFO: namespace kubectl-651 deletion completed in 22.092764161s

• [SLOW TEST:25.596 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Dec  4 22:06:25.630: INFO: >>> kubeConfig: /tmp/kubeconfig-246239386
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5766
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Dec  4 22:06:25.788: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec  4 22:06:27.797: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Dec  4 22:06:29.799: INFO: Creating deployment "test-rollover-deployment"
Dec  4 22:06:29.809: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Dec  4 22:06:31.822: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Dec  4 22:06:31.827: INFO: Ensure that both replica sets have 1 created replica
Dec  4 22:06:31.830: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Dec  4 22:06:31.838: INFO: Updating deployment test-rollover-deployment
Dec  4 22:06:31.838: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Dec  4 22:06:33.846: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Dec  4 22:06:33.849: INFO: Make sure deployment "test-rollover-deployment" is complete
Dec  4 22:06:33.853: INFO: all replica sets need to contain the pod-template-hash label
Dec  4 22:06:33.853: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093989, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093989, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093993, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093989, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  4 22:06:35.859: INFO: all replica sets need to contain the pod-template-hash label
Dec  4 22:06:35.859: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093989, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093989, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093993, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093989, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  4 22:06:37.860: INFO: all replica sets need to contain the pod-template-hash label
Dec  4 22:06:37.860: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093989, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093989, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093993, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093989, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  4 22:06:39.859: INFO: all replica sets need to contain the pod-template-hash label
Dec  4 22:06:39.859: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093989, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093989, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093993, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093989, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  4 22:06:41.859: INFO: all replica sets need to contain the pod-template-hash label
Dec  4 22:06:41.859: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093989, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093989, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093993, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093989, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  4 22:06:43.859: INFO: all replica sets need to contain the pod-template-hash label
Dec  4 22:06:43.859: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093989, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093989, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093993, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093989, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  4 22:06:45.858: INFO: all replica sets need to contain the pod-template-hash label
Dec  4 22:06:45.858: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093989, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093989, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093993, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093989, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  4 22:06:47.859: INFO: all replica sets need to contain the pod-template-hash label
Dec  4 22:06:47.859: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093989, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093989, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093993, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093989, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  4 22:06:49.867: INFO: all replica sets need to contain the pod-template-hash label
Dec  4 22:06:49.867: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093989, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093989, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093993, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093989, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  4 22:06:51.858: INFO: all replica sets need to contain the pod-template-hash label
Dec  4 22:06:51.858: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093989, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093989, loc:(*time.Location)(0x882f100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093993, loc:(*time.Location)(0x882f100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63711093989, loc:(*time.Location)(0x882f100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-659c699649\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec  4 22:06:53.862: INFO: 
Dec  4 22:06:53.862: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Dec  4 22:06:53.879: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-5766,SelfLink:/apis/apps/v1/namespaces/deployment-5766/deployments/test-rollover-deployment,UID:52ee97c5-16e2-11ea-825c-005056950656,ResourceVersion:297447,Generation:2,CreationTimestamp:2019-12-04 22:06:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-12-04 22:06:29 +0000 UTC 2019-12-04 22:06:29 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-12-04 22:06:53 +0000 UTC 2019-12-04 22:06:29 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-659c699649" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Dec  4 22:06:53.881: INFO: New ReplicaSet "test-rollover-deployment-659c699649" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-659c699649,GenerateName:,Namespace:deployment-5766,SelfLink:/apis/apps/v1/namespaces/deployment-5766/replicasets/test-rollover-deployment-659c699649,UID:54256a7c-16e2-11ea-825c-005056950656,ResourceVersion:297438,Generation:2,CreationTimestamp:2019-12-04 22:06:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 659c699649,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 52ee97c5-16e2-11ea-825c-005056950656 0xc002999ec7 0xc002999ec8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 659c699649,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 659c699649,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Dec  4 22:06:53.881: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Dec  4 22:06:53.881: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-5766,SelfLink:/apis/apps/v1/namespaces/deployment-5766/replicasets/test-rollover-controller,UID:5086a913-16e2-11ea-825c-005056950656,ResourceVersion:297446,Generation:2,CreationTimestamp:2019-12-04 22:06:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 52ee97c5-16e2-11ea-825c-005056950656 0xc002999df7 0xc002999df8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  4 22:06:53.881: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-7b45b6464,GenerateName:,Namespace:deployment-5766,SelfLink:/apis/apps/v1/namespaces/deployment-5766/replicasets/test-rollover-deployment-7b45b6464,UID:52f054b7-16e2-11ea-825c-005056950656,ResourceVersion:297388,Generation:2,CreationTimestamp:2019-12-04 22:06:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 7b45b6464,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 52ee97c5-16e2-11ea-825c-005056950656 0xc002999f90 0xc002999f91}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 7b45b6464,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 7b45b6464,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec  4 22:06:53.884: INFO: Pod "test-rollover-deployment-659c699649-dvw5r" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-659c699649-dvw5r,GenerateName:test-rollover-deployment-659c699649-,Namespace:deployment-5766,SelfLink:/api/v1/namespaces/deployment-5766/pods/test-rollover-deployment-659c699649-dvw5r,UID:5429b777-16e2-11ea-825c-005056950656,ResourceVersion:297401,Generation:0,CreationTimestamp:2019-12-04 22:06:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 659c699649,},Annotations:map[string]string{cni.projectcalico.org/podIP: 192.168.2.195/32,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-659c699649 54256a7c-16e2-11ea-825c-005056950656 0xc002296027 0xc002296028}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-j24mx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-j24mx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-j24mx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:alex-slot1-v2-vsp1-worker6c7da5d4d5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002296090} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0022960b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 22:06:36 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 22:06:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 22:06:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-04 22:06:31 +0000 UTC  }],Message:,Reason:,HostIP:10.10.128.23,PodIP:192.168.2.195,StartTime:2019-12-04 22:06:36 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-12-04 22:06:37 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://fe454435c9602f0e9ee45f9502a1ead745838a16a2209040d46508e6468dbb15}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Dec  4 22:06:53.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5766" for this suite.
Dec  4 22:06:59.905: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec  4 22:06:59.999: INFO: namespace deployment-5766 deletion completed in 6.110675502s

• [SLOW TEST:34.369 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.8-beta.0.27+211047e9a19225/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSDec  4 22:07:00.001: INFO: Running AfterSuite actions on all nodes
Dec  4 22:07:00.001: INFO: Running AfterSuite actions on node 1
Dec  4 22:07:00.001: INFO: Skipping dumping logs from cluster

Ran 204 of 3586 Specs in 5605.517 seconds
SUCCESS! -- 204 Passed | 0 Failed | 0 Pending | 3382 Skipped PASS

Ginkgo ran 1 suite in 1h33m27.079213525s
Test Suite Passed
