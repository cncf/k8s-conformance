Client Version: version.Info{Major:"1", Minor:"14", GitVersion:"v1.14.0", GitCommit:"641856db18352033a0d96dbc99153fa3b27298e5", GitTreeState:"clean", BuildDate:"2019-03-26T00:21:23Z", GoVersion:"go1.12.1", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"14", GitVersion:"v1.14.0", GitCommit:"641856db18352033a0d96dbc99153fa3b27298e5", GitTreeState:"clean", BuildDate:"2019-03-25T23:47:43Z", GoVersion:"go1.12.1", Compiler:"gc", Platform:"linux/amd64"}
Conformance test: not doing test setup.
I0326 18:51:46.165266  168179 e2e.go:240] Starting e2e run "e0596475-5032-11e9-9719-a08cfdecc127" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: [1m1553651505[0m - Will randomize all specs
Will run [1m204[0m of [1m3584[0m specs

Mar 26 18:51:46.226: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
Mar 26 18:51:46.229: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Mar 26 18:51:46.241: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Mar 26 18:51:46.265: INFO: The status of Pod coredns-fb8b8dccf-5bwzj is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar 26 18:51:46.265: INFO: The status of Pod coredns-fb8b8dccf-wvb9q is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar 26 18:51:46.265: INFO: 6 / 8 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Mar 26 18:51:46.265: INFO: expected 2 pod replicas in namespace 'kube-system', 0 are Running and Ready.
Mar 26 18:51:46.266: INFO: POD                      NODE                       PHASE    GRACE  CONDITIONS
Mar 26 18:51:46.266: INFO: coredns-fb8b8dccf-5bwzj  conformance-control-plane  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 18:51:44 -0700 PDT  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 18:51:44 -0700 PDT ContainersNotReady containers with unready status: [coredns]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 18:51:44 -0700 PDT ContainersNotReady containers with unready status: [coredns]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 18:51:44 -0700 PDT  }]
Mar 26 18:51:46.266: INFO: coredns-fb8b8dccf-wvb9q  conformance-control-plane  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 18:51:44 -0700 PDT  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 18:51:44 -0700 PDT ContainersNotReady containers with unready status: [coredns]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 18:51:44 -0700 PDT ContainersNotReady containers with unready status: [coredns]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 18:51:44 -0700 PDT  }]
Mar 26 18:51:46.266: INFO: 
Mar 26 18:51:48.279: INFO: The status of Pod coredns-fb8b8dccf-5bwzj is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar 26 18:51:48.279: INFO: The status of Pod coredns-fb8b8dccf-wvb9q is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar 26 18:51:48.279: INFO: 6 / 8 pods in namespace 'kube-system' are running and ready (2 seconds elapsed)
Mar 26 18:51:48.279: INFO: expected 2 pod replicas in namespace 'kube-system', 0 are Running and Ready.
Mar 26 18:51:48.279: INFO: POD                      NODE                       PHASE    GRACE  CONDITIONS
Mar 26 18:51:48.279: INFO: coredns-fb8b8dccf-5bwzj  conformance-control-plane  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 18:51:44 -0700 PDT  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 18:51:44 -0700 PDT ContainersNotReady containers with unready status: [coredns]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 18:51:44 -0700 PDT ContainersNotReady containers with unready status: [coredns]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 18:51:44 -0700 PDT  }]
Mar 26 18:51:48.279: INFO: coredns-fb8b8dccf-wvb9q  conformance-control-plane  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 18:51:44 -0700 PDT  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 18:51:44 -0700 PDT ContainersNotReady containers with unready status: [coredns]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 18:51:44 -0700 PDT ContainersNotReady containers with unready status: [coredns]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 18:51:44 -0700 PDT  }]
Mar 26 18:51:48.279: INFO: 
Mar 26 18:51:50.279: INFO: The status of Pod coredns-fb8b8dccf-wvb9q is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar 26 18:51:50.279: INFO: 7 / 8 pods in namespace 'kube-system' are running and ready (4 seconds elapsed)
Mar 26 18:51:50.279: INFO: expected 2 pod replicas in namespace 'kube-system', 1 are Running and Ready.
Mar 26 18:51:50.279: INFO: POD                      NODE                       PHASE    GRACE  CONDITIONS
Mar 26 18:51:50.279: INFO: coredns-fb8b8dccf-wvb9q  conformance-control-plane  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 18:51:44 -0700 PDT  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 18:51:44 -0700 PDT ContainersNotReady containers with unready status: [coredns]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 18:51:44 -0700 PDT ContainersNotReady containers with unready status: [coredns]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 18:51:44 -0700 PDT  }]
Mar 26 18:51:50.279: INFO: 
Mar 26 18:51:52.279: INFO: The status of Pod coredns-fb8b8dccf-wvb9q is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar 26 18:51:52.279: INFO: 7 / 8 pods in namespace 'kube-system' are running and ready (6 seconds elapsed)
Mar 26 18:51:52.279: INFO: expected 2 pod replicas in namespace 'kube-system', 1 are Running and Ready.
Mar 26 18:51:52.279: INFO: POD                      NODE                       PHASE    GRACE  CONDITIONS
Mar 26 18:51:52.279: INFO: coredns-fb8b8dccf-wvb9q  conformance-control-plane  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 18:51:44 -0700 PDT  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 18:51:44 -0700 PDT ContainersNotReady containers with unready status: [coredns]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 18:51:44 -0700 PDT ContainersNotReady containers with unready status: [coredns]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 18:51:44 -0700 PDT  }]
Mar 26 18:51:52.279: INFO: 
Mar 26 18:51:54.279: INFO: The status of Pod coredns-fb8b8dccf-wvb9q is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar 26 18:51:54.279: INFO: 7 / 8 pods in namespace 'kube-system' are running and ready (8 seconds elapsed)
Mar 26 18:51:54.279: INFO: expected 2 pod replicas in namespace 'kube-system', 1 are Running and Ready.
Mar 26 18:51:54.279: INFO: POD                      NODE                       PHASE    GRACE  CONDITIONS
Mar 26 18:51:54.279: INFO: coredns-fb8b8dccf-wvb9q  conformance-control-plane  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 18:51:44 -0700 PDT  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 18:51:44 -0700 PDT ContainersNotReady containers with unready status: [coredns]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 18:51:44 -0700 PDT ContainersNotReady containers with unready status: [coredns]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 18:51:44 -0700 PDT  }]
Mar 26 18:51:54.279: INFO: 
Mar 26 18:51:56.279: INFO: The status of Pod coredns-fb8b8dccf-wvb9q is Running (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar 26 18:51:56.279: INFO: 7 / 8 pods in namespace 'kube-system' are running and ready (10 seconds elapsed)
Mar 26 18:51:56.279: INFO: expected 2 pod replicas in namespace 'kube-system', 1 are Running and Ready.
Mar 26 18:51:56.279: INFO: POD                      NODE                       PHASE    GRACE  CONDITIONS
Mar 26 18:51:56.279: INFO: coredns-fb8b8dccf-wvb9q  conformance-control-plane  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 18:51:44 -0700 PDT  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 18:51:44 -0700 PDT ContainersNotReady containers with unready status: [coredns]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 18:51:44 -0700 PDT ContainersNotReady containers with unready status: [coredns]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 18:51:44 -0700 PDT  }]
Mar 26 18:51:56.279: INFO: 
Mar 26 18:51:58.280: INFO: 8 / 8 pods in namespace 'kube-system' are running and ready (12 seconds elapsed)
Mar 26 18:51:58.280: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Mar 26 18:51:58.280: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Mar 26 18:51:58.290: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Mar 26 18:51:58.290: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'weave-net' (0 seconds elapsed)
Mar 26 18:51:58.290: INFO: e2e test version: v1.14.0
Mar 26 18:51:58.291: INFO: kube-apiserver version: v1.14.0
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] Daemon set [Serial][0m 
  [1mshould update pod when spec was updated and update strategy is RollingUpdate [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-apps] Daemon set [Serial]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 18:51:58.292: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename daemonsets
Mar 26 18:51:58.323: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Mar 26 18:51:58.333: INFO: Creating simple daemon set daemon-set
[1mSTEP[0m: Check that daemon pods launch on every node of the cluster.
Mar 26 18:51:58.338: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:51:58.347: INFO: Number of nodes with available pods: 0
Mar 26 18:51:58.347: INFO: Node conformance-worker is running more than one daemon pod
Mar 26 18:51:59.351: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:51:59.354: INFO: Number of nodes with available pods: 0
Mar 26 18:51:59.354: INFO: Node conformance-worker is running more than one daemon pod
Mar 26 18:52:00.352: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:52:00.355: INFO: Number of nodes with available pods: 0
Mar 26 18:52:00.355: INFO: Node conformance-worker is running more than one daemon pod
Mar 26 18:52:01.351: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:52:01.353: INFO: Number of nodes with available pods: 0
Mar 26 18:52:01.353: INFO: Node conformance-worker is running more than one daemon pod
Mar 26 18:52:02.351: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:52:02.353: INFO: Number of nodes with available pods: 0
Mar 26 18:52:02.353: INFO: Node conformance-worker is running more than one daemon pod
Mar 26 18:52:03.351: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:52:03.355: INFO: Number of nodes with available pods: 2
Mar 26 18:52:03.355: INFO: Number of running nodes: 2, number of available pods: 2
[1mSTEP[0m: Update daemon pods image.
[1mSTEP[0m: Check that daemon pods images are updated.
Mar 26 18:52:03.380: INFO: Wrong image for pod: daemon-set-88tv8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 26 18:52:03.380: INFO: Wrong image for pod: daemon-set-sm49x. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 26 18:52:03.382: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:52:04.386: INFO: Wrong image for pod: daemon-set-88tv8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 26 18:52:04.386: INFO: Wrong image for pod: daemon-set-sm49x. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 26 18:52:04.390: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:52:05.386: INFO: Wrong image for pod: daemon-set-88tv8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 26 18:52:05.386: INFO: Wrong image for pod: daemon-set-sm49x. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 26 18:52:05.388: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:52:06.385: INFO: Wrong image for pod: daemon-set-88tv8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 26 18:52:06.385: INFO: Wrong image for pod: daemon-set-sm49x. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 26 18:52:06.388: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:52:07.387: INFO: Wrong image for pod: daemon-set-88tv8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 26 18:52:07.387: INFO: Wrong image for pod: daemon-set-sm49x. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 26 18:52:07.390: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:52:08.387: INFO: Wrong image for pod: daemon-set-88tv8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 26 18:52:08.387: INFO: Wrong image for pod: daemon-set-sm49x. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 26 18:52:08.387: INFO: Pod daemon-set-sm49x is not available
Mar 26 18:52:08.390: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:52:09.387: INFO: Wrong image for pod: daemon-set-88tv8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 26 18:52:09.387: INFO: Wrong image for pod: daemon-set-sm49x. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 26 18:52:09.387: INFO: Pod daemon-set-sm49x is not available
Mar 26 18:52:09.390: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:52:10.387: INFO: Wrong image for pod: daemon-set-88tv8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 26 18:52:10.387: INFO: Wrong image for pod: daemon-set-sm49x. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 26 18:52:10.387: INFO: Pod daemon-set-sm49x is not available
Mar 26 18:52:10.390: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:52:11.387: INFO: Wrong image for pod: daemon-set-88tv8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 26 18:52:11.387: INFO: Wrong image for pod: daemon-set-sm49x. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 26 18:52:11.387: INFO: Pod daemon-set-sm49x is not available
Mar 26 18:52:11.390: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:52:12.386: INFO: Wrong image for pod: daemon-set-88tv8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 26 18:52:12.386: INFO: Wrong image for pod: daemon-set-sm49x. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 26 18:52:12.386: INFO: Pod daemon-set-sm49x is not available
Mar 26 18:52:12.388: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:52:13.386: INFO: Wrong image for pod: daemon-set-88tv8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 26 18:52:13.386: INFO: Wrong image for pod: daemon-set-sm49x. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 26 18:52:13.386: INFO: Pod daemon-set-sm49x is not available
Mar 26 18:52:13.389: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:52:14.387: INFO: Wrong image for pod: daemon-set-88tv8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 26 18:52:14.387: INFO: Wrong image for pod: daemon-set-sm49x. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 26 18:52:14.387: INFO: Pod daemon-set-sm49x is not available
Mar 26 18:52:14.390: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:52:15.386: INFO: Wrong image for pod: daemon-set-88tv8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 26 18:52:15.386: INFO: Wrong image for pod: daemon-set-sm49x. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 26 18:52:15.386: INFO: Pod daemon-set-sm49x is not available
Mar 26 18:52:15.389: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:52:16.386: INFO: Wrong image for pod: daemon-set-88tv8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 26 18:52:16.386: INFO: Pod daemon-set-qzq7c is not available
Mar 26 18:52:16.389: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:52:17.386: INFO: Wrong image for pod: daemon-set-88tv8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 26 18:52:17.386: INFO: Pod daemon-set-qzq7c is not available
Mar 26 18:52:17.389: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:52:18.387: INFO: Wrong image for pod: daemon-set-88tv8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 26 18:52:18.387: INFO: Pod daemon-set-qzq7c is not available
Mar 26 18:52:18.390: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:52:19.387: INFO: Wrong image for pod: daemon-set-88tv8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 26 18:52:19.390: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:52:20.387: INFO: Wrong image for pod: daemon-set-88tv8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 26 18:52:20.390: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:52:21.387: INFO: Wrong image for pod: daemon-set-88tv8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 26 18:52:21.387: INFO: Pod daemon-set-88tv8 is not available
Mar 26 18:52:21.390: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:52:22.386: INFO: Wrong image for pod: daemon-set-88tv8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 26 18:52:22.386: INFO: Pod daemon-set-88tv8 is not available
Mar 26 18:52:22.390: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:52:23.386: INFO: Wrong image for pod: daemon-set-88tv8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 26 18:52:23.386: INFO: Pod daemon-set-88tv8 is not available
Mar 26 18:52:23.389: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:52:24.387: INFO: Wrong image for pod: daemon-set-88tv8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 26 18:52:24.387: INFO: Pod daemon-set-88tv8 is not available
Mar 26 18:52:24.390: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:52:25.386: INFO: Wrong image for pod: daemon-set-88tv8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Mar 26 18:52:25.386: INFO: Pod daemon-set-88tv8 is not available
Mar 26 18:52:25.388: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:52:26.387: INFO: Pod daemon-set-rddc7 is not available
Mar 26 18:52:26.390: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[1mSTEP[0m: Check that daemon pods are still running on every node of the cluster.
Mar 26 18:52:26.394: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:52:26.397: INFO: Number of nodes with available pods: 1
Mar 26 18:52:26.397: INFO: Node conformance-worker is running more than one daemon pod
Mar 26 18:52:27.402: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:52:27.405: INFO: Number of nodes with available pods: 1
Mar 26 18:52:27.405: INFO: Node conformance-worker is running more than one daemon pod
Mar 26 18:52:28.401: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:52:28.404: INFO: Number of nodes with available pods: 1
Mar 26 18:52:28.404: INFO: Node conformance-worker is running more than one daemon pod
Mar 26 18:52:29.401: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:52:29.403: INFO: Number of nodes with available pods: 1
Mar 26 18:52:29.403: INFO: Node conformance-worker is running more than one daemon pod
Mar 26 18:52:30.402: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 18:52:30.405: INFO: Number of nodes with available pods: 2
Mar 26 18:52:30.405: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
[1mSTEP[0m: Deleting DaemonSet "daemon-set"
[1mSTEP[0m: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4750, will wait for the garbage collector to delete the pods
Mar 26 18:52:30.476: INFO: Deleting DaemonSet.extensions daemon-set took: 5.982148ms
Mar 26 18:52:30.776: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.190065ms
Mar 26 18:52:35.579: INFO: Number of nodes with available pods: 0
Mar 26 18:52:35.579: INFO: Number of running nodes: 0, number of available pods: 0
Mar 26 18:52:35.581: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4750/daemonsets","resourceVersion":"810"},"items":null}

Mar 26 18:52:35.583: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4750/pods","resourceVersion":"810"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 18:52:35.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "daemonsets-4750" for this suite.
Mar 26 18:52:41.603: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 18:52:41.659: INFO: namespace daemonsets-4750 deletion completed in 6.064371053s
[32mâ€¢[0m
[90m------------------------------[0m
[0m[sig-storage] Secrets[0m 
  [1mshould be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Secrets
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 18:52:41.659: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename secrets
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating secret with name secret-test-01e49667-5033-11e9-9719-a08cfdecc127
[1mSTEP[0m: Creating a pod to test consume secrets
Mar 26 18:52:41.713: INFO: Waiting up to 5m0s for pod "pod-secrets-01e8dd08-5033-11e9-9719-a08cfdecc127" in namespace "secrets-4523" to be "success or failure"
Mar 26 18:52:41.715: INFO: Pod "pod-secrets-01e8dd08-5033-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 1.929053ms
Mar 26 18:52:43.718: INFO: Pod "pod-secrets-01e8dd08-5033-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005138866s
Mar 26 18:52:45.721: INFO: Pod "pod-secrets-01e8dd08-5033-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007807957s
[1mSTEP[0m: Saw pod success
Mar 26 18:52:45.721: INFO: Pod "pod-secrets-01e8dd08-5033-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 18:52:45.723: INFO: Trying to get logs from node conformance-worker2 pod pod-secrets-01e8dd08-5033-11e9-9719-a08cfdecc127 container secret-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar 26 18:52:45.744: INFO: Waiting for pod pod-secrets-01e8dd08-5033-11e9-9719-a08cfdecc127 to disappear
Mar 26 18:52:45.751: INFO: Pod pod-secrets-01e8dd08-5033-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] Secrets
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 18:52:45.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "secrets-4523" for this suite.
Mar 26 18:52:51.759: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 18:52:51.815: INFO: namespace secrets-4523 deletion completed in 6.062254937s
[1mSTEP[0m: Destroying namespace "secret-namespace-6135" for this suite.
Mar 26 18:52:57.824: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 18:52:57.888: INFO: namespace secret-namespace-6135 deletion completed in 6.073547997s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-node] ConfigMap[0m 
  [1mshould be consumable via environment variable [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-node] ConfigMap
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 18:52:57.888: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename configmap
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating configMap configmap-6799/configmap-test-0b92cd8e-5033-11e9-9719-a08cfdecc127
[1mSTEP[0m: Creating a pod to test consume configMaps
Mar 26 18:52:57.928: INFO: Waiting up to 5m0s for pod "pod-configmaps-0b931491-5033-11e9-9719-a08cfdecc127" in namespace "configmap-6799" to be "success or failure"
Mar 26 18:52:57.930: INFO: Pod "pod-configmaps-0b931491-5033-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 1.671046ms
Mar 26 18:52:59.933: INFO: Pod "pod-configmaps-0b931491-5033-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004278155s
Mar 26 18:53:01.936: INFO: Pod "pod-configmaps-0b931491-5033-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007937986s
[1mSTEP[0m: Saw pod success
Mar 26 18:53:01.936: INFO: Pod "pod-configmaps-0b931491-5033-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 18:53:01.940: INFO: Trying to get logs from node conformance-worker pod pod-configmaps-0b931491-5033-11e9-9719-a08cfdecc127 container env-test: <nil>
[1mSTEP[0m: delete the pod
Mar 26 18:53:01.968: INFO: Waiting for pod pod-configmaps-0b931491-5033-11e9-9719-a08cfdecc127 to disappear
Mar 26 18:53:01.971: INFO: Pod pod-configmaps-0b931491-5033-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-node] ConfigMap
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 18:53:01.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "configmap-6799" for this suite.
Mar 26 18:53:07.983: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 18:53:08.051: INFO: namespace configmap-6799 deletion completed in 6.078102081s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Secrets[0m 
  [1mshould be consumable via the environment [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-api-machinery] Secrets
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 18:53:08.052: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename secrets
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: creating secret secrets-843/secret-test-11a039e5-5033-11e9-9719-a08cfdecc127
[1mSTEP[0m: Creating a pod to test consume secrets
Mar 26 18:53:08.084: INFO: Waiting up to 5m0s for pod "pod-configmaps-11a08dce-5033-11e9-9719-a08cfdecc127" in namespace "secrets-843" to be "success or failure"
Mar 26 18:53:08.086: INFO: Pod "pod-configmaps-11a08dce-5033-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 1.887107ms
Mar 26 18:53:10.089: INFO: Pod "pod-configmaps-11a08dce-5033-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00510499s
Mar 26 18:53:12.093: INFO: Pod "pod-configmaps-11a08dce-5033-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008731412s
[1mSTEP[0m: Saw pod success
Mar 26 18:53:12.093: INFO: Pod "pod-configmaps-11a08dce-5033-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 18:53:12.096: INFO: Trying to get logs from node conformance-worker2 pod pod-configmaps-11a08dce-5033-11e9-9719-a08cfdecc127 container env-test: <nil>
[1mSTEP[0m: delete the pod
Mar 26 18:53:12.114: INFO: Waiting for pod pod-configmaps-11a08dce-5033-11e9-9719-a08cfdecc127 to disappear
Mar 26 18:53:12.116: INFO: Pod pod-configmaps-11a08dce-5033-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 18:53:12.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "secrets-843" for this suite.
Mar 26 18:53:18.126: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 18:53:18.195: INFO: namespace secrets-843 deletion completed in 6.076471736s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] ConfigMap[0m 
  [1mshould be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] ConfigMap
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 18:53:18.195: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename configmap
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating configMap with name configmap-test-volume-17ac1428-5033-11e9-9719-a08cfdecc127
[1mSTEP[0m: Creating a pod to test consume configMaps
Mar 26 18:53:18.227: INFO: Waiting up to 5m0s for pod "pod-configmaps-17ac635b-5033-11e9-9719-a08cfdecc127" in namespace "configmap-7092" to be "success or failure"
Mar 26 18:53:18.234: INFO: Pod "pod-configmaps-17ac635b-5033-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 6.662824ms
Mar 26 18:53:20.238: INFO: Pod "pod-configmaps-17ac635b-5033-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010878953s
Mar 26 18:53:22.242: INFO: Pod "pod-configmaps-17ac635b-5033-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014578227s
[1mSTEP[0m: Saw pod success
Mar 26 18:53:22.242: INFO: Pod "pod-configmaps-17ac635b-5033-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 18:53:22.245: INFO: Trying to get logs from node conformance-worker pod pod-configmaps-17ac635b-5033-11e9-9719-a08cfdecc127 container configmap-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar 26 18:53:22.259: INFO: Waiting for pod pod-configmaps-17ac635b-5033-11e9-9719-a08cfdecc127 to disappear
Mar 26 18:53:22.261: INFO: Pod pod-configmaps-17ac635b-5033-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 18:53:22.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "configmap-7092" for this suite.
Mar 26 18:53:28.273: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 18:53:28.340: INFO: namespace configmap-7092 deletion completed in 6.076675309s
[32mâ€¢[0m
[90m------------------------------[0m
[0m[sig-apps] ReplicaSet[0m 
  [1mshould serve a basic image on each replica with a public image  [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-apps] ReplicaSet
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 18:53:28.340: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename replicaset
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Mar 26 18:53:28.376: INFO: Creating ReplicaSet my-hostname-basic-1db99a0a-5033-11e9-9719-a08cfdecc127
Mar 26 18:53:28.380: INFO: Pod name my-hostname-basic-1db99a0a-5033-11e9-9719-a08cfdecc127: Found 0 pods out of 1
Mar 26 18:53:33.385: INFO: Pod name my-hostname-basic-1db99a0a-5033-11e9-9719-a08cfdecc127: Found 1 pods out of 1
Mar 26 18:53:33.385: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-1db99a0a-5033-11e9-9719-a08cfdecc127" is running
Mar 26 18:53:33.388: INFO: Pod "my-hostname-basic-1db99a0a-5033-11e9-9719-a08cfdecc127-tc45k" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-26 18:53:28 -0700 PDT Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-26 18:53:31 -0700 PDT Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-26 18:53:31 -0700 PDT Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-26 18:53:28 -0700 PDT Reason: Message:}])
Mar 26 18:53:33.388: INFO: Trying to dial the pod
Mar 26 18:53:38.400: INFO: Controller my-hostname-basic-1db99a0a-5033-11e9-9719-a08cfdecc127: Got expected result from replica 1 [my-hostname-basic-1db99a0a-5033-11e9-9719-a08cfdecc127-tc45k]: "my-hostname-basic-1db99a0a-5033-11e9-9719-a08cfdecc127-tc45k", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 18:53:38.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "replicaset-3553" for this suite.
Mar 26 18:53:44.413: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 18:53:44.481: INFO: namespace replicaset-3553 deletion completed in 6.077594152s
[32mâ€¢[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] StatefulSet[0m [90m[k8s.io] Basic StatefulSet functionality [StatefulSetBasic][0m 
  [1mshould perform rolling updates and roll backs of template modifications [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-apps] StatefulSet
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 18:53:44.481: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename statefulset
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
[1mSTEP[0m: Creating service test in namespace statefulset-4314
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating a new StatefulSet
Mar 26 18:53:44.515: INFO: Found 0 stateful pods, waiting for 3
Mar 26 18:53:54.519: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 26 18:53:54.519: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 26 18:53:54.519: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Mar 26 18:53:54.528: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-4314 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar 26 18:53:54.784: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Mar 26 18:53:54.784: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar 26 18:53:54.784: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

[1mSTEP[0m: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Mar 26 18:54:04.813: INFO: Updating stateful set ss2
[1mSTEP[0m: Creating a new revision
[1mSTEP[0m: Updating Pods in reverse ordinal order
Mar 26 18:54:14.840: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-4314 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 26 18:54:15.102: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Mar 26 18:54:15.102: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar 26 18:54:15.102: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

[1mSTEP[0m: Rolling back to a previous revision
Mar 26 18:54:45.122: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-4314 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar 26 18:54:45.396: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Mar 26 18:54:45.396: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar 26 18:54:45.396: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar 26 18:54:55.423: INFO: Updating stateful set ss2
[1mSTEP[0m: Rolling back update in reverse ordinal order
Mar 26 18:55:05.441: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-4314 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 26 18:55:05.696: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Mar 26 18:55:05.696: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar 26 18:55:05.696: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar 26 18:55:15.712: INFO: Waiting for StatefulSet statefulset-4314/ss2 to complete update
Mar 26 18:55:15.712: INFO: Waiting for Pod statefulset-4314/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
Mar 26 18:55:15.712: INFO: Waiting for Pod statefulset-4314/ss2-1 to have revision ss2-787997d666 update revision ss2-c79899b9
Mar 26 18:55:25.717: INFO: Waiting for StatefulSet statefulset-4314/ss2 to complete update
Mar 26 18:55:25.717: INFO: Waiting for Pod statefulset-4314/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
Mar 26 18:55:35.719: INFO: Waiting for StatefulSet statefulset-4314/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Mar 26 18:55:45.722: INFO: Deleting all statefulset in ns statefulset-4314
Mar 26 18:55:45.724: INFO: Scaling statefulset ss2 to 0
Mar 26 18:55:55.735: INFO: Waiting for statefulset status.replicas updated to 0
Mar 26 18:55:55.738: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 18:55:55.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "statefulset-4314" for this suite.
Mar 26 18:56:01.758: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 18:56:01.814: INFO: namespace statefulset-4314 deletion completed in 6.064879703s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90m[k8s.io] Kubectl run deployment[0m 
  [1mshould create a deployment from an image  [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 18:56:01.815: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run deployment
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1455
[It] should create a deployment from an image  [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: running the image docker.io/library/nginx:1.14-alpine
Mar 26 18:56:01.841: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=kubectl-4604'
Mar 26 18:56:02.232: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar 26 18:56:02.232: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
[1mSTEP[0m: verifying the deployment e2e-test-nginx-deployment was created
[1mSTEP[0m: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1460
Mar 26 18:56:04.244: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance delete deployment e2e-test-nginx-deployment --namespace=kubectl-4604'
Mar 26 18:56:04.329: INFO: stderr: ""
Mar 26 18:56:04.329: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 18:56:04.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-4604" for this suite.
Mar 26 18:56:10.341: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 18:56:10.408: INFO: namespace kubectl-4604 deletion completed in 6.076189799s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Service endpoints latency[0m 
  [1mshould not be very high  [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-network] Service endpoints latency
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 18:56:10.409: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename svc-latency
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: creating replication controller svc-latency-rc in namespace svc-latency-1805
I0326 18:56:10.446343  168179 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1805, replica count: 1
I0326 18:56:11.496710  168179 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0326 18:56:12.496878  168179 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0326 18:56:13.497092  168179 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 26 18:56:13.612: INFO: Created: latency-svc-2tjjc
Mar 26 18:56:13.615: INFO: Got endpoints: latency-svc-2tjjc [17.837135ms]
Mar 26 18:56:13.626: INFO: Created: latency-svc-kh87t
Mar 26 18:56:13.630: INFO: Got endpoints: latency-svc-kh87t [15.061587ms]
Mar 26 18:56:13.635: INFO: Created: latency-svc-26r4k
Mar 26 18:56:13.639: INFO: Got endpoints: latency-svc-26r4k [24.090324ms]
Mar 26 18:56:13.648: INFO: Created: latency-svc-kb6z2
Mar 26 18:56:13.653: INFO: Got endpoints: latency-svc-kb6z2 [37.846852ms]
Mar 26 18:56:13.659: INFO: Created: latency-svc-nhjw9
Mar 26 18:56:13.662: INFO: Got endpoints: latency-svc-nhjw9 [47.520857ms]
Mar 26 18:56:13.682: INFO: Created: latency-svc-758tg
Mar 26 18:56:13.682: INFO: Got endpoints: latency-svc-758tg [67.314836ms]
Mar 26 18:56:13.702: INFO: Created: latency-svc-jzcvc
Mar 26 18:56:13.705: INFO: Got endpoints: latency-svc-jzcvc [89.997826ms]
Mar 26 18:56:13.714: INFO: Created: latency-svc-98zf2
Mar 26 18:56:13.716: INFO: Got endpoints: latency-svc-98zf2 [101.504068ms]
Mar 26 18:56:13.728: INFO: Created: latency-svc-gzqqd
Mar 26 18:56:13.735: INFO: Got endpoints: latency-svc-gzqqd [120.214323ms]
Mar 26 18:56:13.740: INFO: Created: latency-svc-zkjxb
Mar 26 18:56:13.751: INFO: Got endpoints: latency-svc-zkjxb [136.141766ms]
Mar 26 18:56:13.765: INFO: Created: latency-svc-dzfwq
Mar 26 18:56:13.771: INFO: Got endpoints: latency-svc-dzfwq [156.21551ms]
Mar 26 18:56:13.771: INFO: Created: latency-svc-2j728
Mar 26 18:56:13.779: INFO: Got endpoints: latency-svc-2j728 [163.547463ms]
Mar 26 18:56:13.790: INFO: Created: latency-svc-tvnwq
Mar 26 18:56:13.790: INFO: Got endpoints: latency-svc-tvnwq [174.741724ms]
Mar 26 18:56:13.800: INFO: Created: latency-svc-qxpmd
Mar 26 18:56:13.817: INFO: Got endpoints: latency-svc-qxpmd [202.418565ms]
Mar 26 18:56:13.823: INFO: Created: latency-svc-zxhzn
Mar 26 18:56:13.824: INFO: Got endpoints: latency-svc-zxhzn [209.017495ms]
Mar 26 18:56:13.851: INFO: Created: latency-svc-q4kcq
Mar 26 18:56:13.853: INFO: Got endpoints: latency-svc-q4kcq [238.286927ms]
Mar 26 18:56:13.860: INFO: Created: latency-svc-vjrfl
Mar 26 18:56:13.868: INFO: Got endpoints: latency-svc-vjrfl [238.431504ms]
Mar 26 18:56:13.870: INFO: Created: latency-svc-2lsnt
Mar 26 18:56:13.875: INFO: Got endpoints: latency-svc-2lsnt [236.243277ms]
Mar 26 18:56:13.882: INFO: Created: latency-svc-z4srg
Mar 26 18:56:13.885: INFO: Got endpoints: latency-svc-z4srg [231.803784ms]
Mar 26 18:56:13.891: INFO: Created: latency-svc-bnwcd
Mar 26 18:56:13.893: INFO: Got endpoints: latency-svc-bnwcd [230.370647ms]
Mar 26 18:56:13.899: INFO: Created: latency-svc-sqgph
Mar 26 18:56:13.901: INFO: Got endpoints: latency-svc-sqgph [218.901099ms]
Mar 26 18:56:13.913: INFO: Created: latency-svc-jcgmb
Mar 26 18:56:13.943: INFO: Got endpoints: latency-svc-jcgmb [238.063807ms]
Mar 26 18:56:13.945: INFO: Created: latency-svc-trxxc
Mar 26 18:56:13.948: INFO: Got endpoints: latency-svc-trxxc [231.206706ms]
Mar 26 18:56:13.959: INFO: Created: latency-svc-4jm8l
Mar 26 18:56:13.960: INFO: Got endpoints: latency-svc-4jm8l [224.808491ms]
Mar 26 18:56:13.974: INFO: Created: latency-svc-bk2kx
Mar 26 18:56:13.975: INFO: Got endpoints: latency-svc-bk2kx [223.610979ms]
Mar 26 18:56:13.981: INFO: Created: latency-svc-5j7ks
Mar 26 18:56:13.984: INFO: Got endpoints: latency-svc-5j7ks [213.041466ms]
Mar 26 18:56:13.993: INFO: Created: latency-svc-gnzzh
Mar 26 18:56:13.999: INFO: Got endpoints: latency-svc-gnzzh [220.88031ms]
Mar 26 18:56:14.007: INFO: Created: latency-svc-9jnrr
Mar 26 18:56:14.010: INFO: Got endpoints: latency-svc-9jnrr [219.783292ms]
Mar 26 18:56:14.017: INFO: Created: latency-svc-xh49s
Mar 26 18:56:14.027: INFO: Got endpoints: latency-svc-xh49s [209.88926ms]
Mar 26 18:56:14.028: INFO: Created: latency-svc-hc6lk
Mar 26 18:56:14.034: INFO: Got endpoints: latency-svc-hc6lk [24.263489ms]
Mar 26 18:56:14.042: INFO: Created: latency-svc-r22nm
Mar 26 18:56:14.054: INFO: Got endpoints: latency-svc-r22nm [229.736258ms]
Mar 26 18:56:14.056: INFO: Created: latency-svc-vr2rw
Mar 26 18:56:14.060: INFO: Got endpoints: latency-svc-vr2rw [206.260509ms]
Mar 26 18:56:14.067: INFO: Created: latency-svc-bb2k4
Mar 26 18:56:14.069: INFO: Got endpoints: latency-svc-bb2k4 [201.023312ms]
Mar 26 18:56:14.078: INFO: Created: latency-svc-gqwqp
Mar 26 18:56:14.083: INFO: Got endpoints: latency-svc-gqwqp [207.724454ms]
Mar 26 18:56:14.093: INFO: Created: latency-svc-7qglw
Mar 26 18:56:14.095: INFO: Got endpoints: latency-svc-7qglw [210.723528ms]
Mar 26 18:56:14.105: INFO: Created: latency-svc-9c9kg
Mar 26 18:56:14.107: INFO: Got endpoints: latency-svc-9c9kg [214.002378ms]
Mar 26 18:56:14.120: INFO: Created: latency-svc-rk7dp
Mar 26 18:56:14.120: INFO: Got endpoints: latency-svc-rk7dp [219.147785ms]
Mar 26 18:56:14.131: INFO: Created: latency-svc-tffv2
Mar 26 18:56:14.133: INFO: Got endpoints: latency-svc-tffv2 [189.705685ms]
Mar 26 18:56:14.141: INFO: Created: latency-svc-mgxm4
Mar 26 18:56:14.146: INFO: Got endpoints: latency-svc-mgxm4 [197.887837ms]
Mar 26 18:56:14.166: INFO: Created: latency-svc-5k76v
Mar 26 18:56:14.168: INFO: Got endpoints: latency-svc-5k76v [208.426351ms]
Mar 26 18:56:14.176: INFO: Created: latency-svc-ng6wn
Mar 26 18:56:14.178: INFO: Got endpoints: latency-svc-ng6wn [203.564365ms]
Mar 26 18:56:14.187: INFO: Created: latency-svc-56nvc
Mar 26 18:56:14.195: INFO: Created: latency-svc-b5f5d
Mar 26 18:56:14.204: INFO: Created: latency-svc-gqx5h
Mar 26 18:56:14.216: INFO: Got endpoints: latency-svc-56nvc [232.167919ms]
Mar 26 18:56:14.219: INFO: Created: latency-svc-rxr9g
Mar 26 18:56:14.235: INFO: Created: latency-svc-4wswr
Mar 26 18:56:14.243: INFO: Created: latency-svc-n55k2
Mar 26 18:56:14.254: INFO: Created: latency-svc-wqbxc
Mar 26 18:56:14.305: INFO: Got endpoints: latency-svc-b5f5d [305.602873ms]
Mar 26 18:56:14.330: INFO: Created: latency-svc-ngnfx
Mar 26 18:56:14.330: INFO: Got endpoints: latency-svc-gqx5h [302.11805ms]
Mar 26 18:56:14.330: INFO: Created: latency-svc-8b2n8
Mar 26 18:56:14.344: INFO: Created: latency-svc-4gsmq
Mar 26 18:56:14.352: INFO: Created: latency-svc-6fkjg
Mar 26 18:56:14.361: INFO: Created: latency-svc-pgr5b
Mar 26 18:56:14.369: INFO: Got endpoints: latency-svc-rxr9g [335.504211ms]
Mar 26 18:56:14.376: INFO: Created: latency-svc-h4m26
Mar 26 18:56:14.391: INFO: Created: latency-svc-svqbv
Mar 26 18:56:14.414: INFO: Created: latency-svc-cqwhs
Mar 26 18:56:14.416: INFO: Got endpoints: latency-svc-4wswr [361.697193ms]
Mar 26 18:56:14.446: INFO: Created: latency-svc-4g4fp
Mar 26 18:56:14.470: INFO: Got endpoints: latency-svc-n55k2 [410.15205ms]
Mar 26 18:56:14.477: INFO: Created: latency-svc-br8rw
Mar 26 18:56:14.492: INFO: Created: latency-svc-lk9ct
Mar 26 18:56:14.506: INFO: Created: latency-svc-8pmfs
Mar 26 18:56:14.534: INFO: Got endpoints: latency-svc-wqbxc [464.329231ms]
Mar 26 18:56:14.535: INFO: Created: latency-svc-lmgzk
Mar 26 18:56:14.545: INFO: Created: latency-svc-htktk
Mar 26 18:56:14.556: INFO: Created: latency-svc-9w9th
Mar 26 18:56:14.564: INFO: Got endpoints: latency-svc-8b2n8 [481.4963ms]
Mar 26 18:56:14.576: INFO: Created: latency-svc-l96jj
Mar 26 18:56:14.615: INFO: Got endpoints: latency-svc-ngnfx [519.731539ms]
Mar 26 18:56:14.658: INFO: Created: latency-svc-2k9n4
Mar 26 18:56:14.665: INFO: Got endpoints: latency-svc-4gsmq [557.714721ms]
Mar 26 18:56:14.675: INFO: Created: latency-svc-pg5vg
Mar 26 18:56:14.715: INFO: Got endpoints: latency-svc-6fkjg [594.262507ms]
Mar 26 18:56:14.727: INFO: Created: latency-svc-plb26
Mar 26 18:56:14.854: INFO: Got endpoints: latency-svc-h4m26 [708.421299ms]
Mar 26 18:56:14.854: INFO: Got endpoints: latency-svc-pgr5b [721.303607ms]
Mar 26 18:56:14.869: INFO: Got endpoints: latency-svc-svqbv [700.747366ms]
Mar 26 18:56:14.870: INFO: Created: latency-svc-jxmtj
Mar 26 18:56:14.883: INFO: Created: latency-svc-sjw64
Mar 26 18:56:14.891: INFO: Created: latency-svc-h48sx
Mar 26 18:56:14.917: INFO: Got endpoints: latency-svc-cqwhs [738.728061ms]
Mar 26 18:56:14.928: INFO: Created: latency-svc-8zh25
Mar 26 18:56:14.966: INFO: Got endpoints: latency-svc-4g4fp [749.512431ms]
Mar 26 18:56:14.977: INFO: Created: latency-svc-vzfdc
Mar 26 18:56:15.016: INFO: Got endpoints: latency-svc-br8rw [710.987894ms]
Mar 26 18:56:15.027: INFO: Created: latency-svc-qbn6p
Mar 26 18:56:15.065: INFO: Got endpoints: latency-svc-lk9ct [735.063598ms]
Mar 26 18:56:15.085: INFO: Created: latency-svc-6p5px
Mar 26 18:56:15.117: INFO: Got endpoints: latency-svc-8pmfs [747.679016ms]
Mar 26 18:56:15.129: INFO: Created: latency-svc-dv4dl
Mar 26 18:56:15.164: INFO: Got endpoints: latency-svc-lmgzk [748.927827ms]
Mar 26 18:56:15.193: INFO: Created: latency-svc-rhx2w
Mar 26 18:56:15.216: INFO: Got endpoints: latency-svc-htktk [745.759685ms]
Mar 26 18:56:15.230: INFO: Created: latency-svc-q9hrb
Mar 26 18:56:15.265: INFO: Got endpoints: latency-svc-9w9th [731.032919ms]
Mar 26 18:56:15.279: INFO: Created: latency-svc-6fljb
Mar 26 18:56:15.316: INFO: Got endpoints: latency-svc-l96jj [751.185655ms]
Mar 26 18:56:15.325: INFO: Created: latency-svc-fsbj4
Mar 26 18:56:15.365: INFO: Got endpoints: latency-svc-2k9n4 [749.971521ms]
Mar 26 18:56:15.376: INFO: Created: latency-svc-xpt4f
Mar 26 18:56:15.415: INFO: Got endpoints: latency-svc-pg5vg [750.298518ms]
Mar 26 18:56:15.431: INFO: Created: latency-svc-ltndt
Mar 26 18:56:15.465: INFO: Got endpoints: latency-svc-plb26 [750.0751ms]
Mar 26 18:56:15.476: INFO: Created: latency-svc-2qfrd
Mar 26 18:56:15.524: INFO: Got endpoints: latency-svc-jxmtj [670.196852ms]
Mar 26 18:56:15.540: INFO: Created: latency-svc-2dcvt
Mar 26 18:56:15.564: INFO: Got endpoints: latency-svc-sjw64 [710.359797ms]
Mar 26 18:56:15.575: INFO: Created: latency-svc-6glwf
Mar 26 18:56:15.615: INFO: Got endpoints: latency-svc-h48sx [745.495138ms]
Mar 26 18:56:15.636: INFO: Created: latency-svc-jhjwd
Mar 26 18:56:15.665: INFO: Got endpoints: latency-svc-8zh25 [747.414058ms]
Mar 26 18:56:15.675: INFO: Created: latency-svc-2cmns
Mar 26 18:56:15.715: INFO: Got endpoints: latency-svc-vzfdc [749.220172ms]
Mar 26 18:56:15.733: INFO: Created: latency-svc-hv7xt
Mar 26 18:56:15.765: INFO: Got endpoints: latency-svc-qbn6p [748.711749ms]
Mar 26 18:56:15.785: INFO: Created: latency-svc-gmmwv
Mar 26 18:56:15.815: INFO: Got endpoints: latency-svc-6p5px [749.952731ms]
Mar 26 18:56:15.826: INFO: Created: latency-svc-twhlm
Mar 26 18:56:15.864: INFO: Got endpoints: latency-svc-dv4dl [747.38417ms]
Mar 26 18:56:15.881: INFO: Created: latency-svc-hzhnx
Mar 26 18:56:15.915: INFO: Got endpoints: latency-svc-rhx2w [750.948347ms]
Mar 26 18:56:15.930: INFO: Created: latency-svc-lq9db
Mar 26 18:56:15.965: INFO: Got endpoints: latency-svc-q9hrb [749.820767ms]
Mar 26 18:56:15.977: INFO: Created: latency-svc-vpqbl
Mar 26 18:56:16.015: INFO: Got endpoints: latency-svc-6fljb [750.31096ms]
Mar 26 18:56:16.038: INFO: Created: latency-svc-28fx7
Mar 26 18:56:16.073: INFO: Got endpoints: latency-svc-fsbj4 [757.521252ms]
Mar 26 18:56:16.083: INFO: Created: latency-svc-b4fhs
Mar 26 18:56:16.115: INFO: Got endpoints: latency-svc-xpt4f [750.054859ms]
Mar 26 18:56:16.129: INFO: Created: latency-svc-g7bt4
Mar 26 18:56:16.165: INFO: Got endpoints: latency-svc-ltndt [749.921135ms]
Mar 26 18:56:16.189: INFO: Created: latency-svc-lqc4x
Mar 26 18:56:16.215: INFO: Got endpoints: latency-svc-2qfrd [749.920537ms]
Mar 26 18:56:16.231: INFO: Created: latency-svc-88mzt
Mar 26 18:56:16.264: INFO: Got endpoints: latency-svc-2dcvt [740.03513ms]
Mar 26 18:56:16.278: INFO: Created: latency-svc-r9cm2
Mar 26 18:56:16.315: INFO: Got endpoints: latency-svc-6glwf [750.273674ms]
Mar 26 18:56:16.325: INFO: Created: latency-svc-2jsd8
Mar 26 18:56:16.365: INFO: Got endpoints: latency-svc-jhjwd [750.013753ms]
Mar 26 18:56:16.380: INFO: Created: latency-svc-4kqmv
Mar 26 18:56:16.415: INFO: Got endpoints: latency-svc-2cmns [750.253087ms]
Mar 26 18:56:16.425: INFO: Created: latency-svc-f9tcn
Mar 26 18:56:16.464: INFO: Got endpoints: latency-svc-hv7xt [749.111426ms]
Mar 26 18:56:16.481: INFO: Created: latency-svc-wsvg4
Mar 26 18:56:16.515: INFO: Got endpoints: latency-svc-gmmwv [749.863842ms]
Mar 26 18:56:16.530: INFO: Created: latency-svc-9qklr
Mar 26 18:56:16.565: INFO: Got endpoints: latency-svc-twhlm [750.251783ms]
Mar 26 18:56:16.577: INFO: Created: latency-svc-2rdln
Mar 26 18:56:16.623: INFO: Got endpoints: latency-svc-hzhnx [758.875386ms]
Mar 26 18:56:16.638: INFO: Created: latency-svc-jtnbc
Mar 26 18:56:16.665: INFO: Got endpoints: latency-svc-lq9db [749.154259ms]
Mar 26 18:56:16.679: INFO: Created: latency-svc-fq7q7
Mar 26 18:56:16.716: INFO: Got endpoints: latency-svc-vpqbl [750.810578ms]
Mar 26 18:56:16.739: INFO: Created: latency-svc-msvhk
Mar 26 18:56:16.764: INFO: Got endpoints: latency-svc-28fx7 [749.339992ms]
Mar 26 18:56:16.779: INFO: Created: latency-svc-pb5z6
Mar 26 18:56:16.815: INFO: Got endpoints: latency-svc-b4fhs [741.327851ms]
Mar 26 18:56:16.834: INFO: Created: latency-svc-z6ggk
Mar 26 18:56:16.867: INFO: Got endpoints: latency-svc-g7bt4 [751.861355ms]
Mar 26 18:56:16.881: INFO: Created: latency-svc-xhzrp
Mar 26 18:56:16.915: INFO: Got endpoints: latency-svc-lqc4x [750.435851ms]
Mar 26 18:56:16.929: INFO: Created: latency-svc-h8g6w
Mar 26 18:56:16.973: INFO: Got endpoints: latency-svc-88mzt [758.569297ms]
Mar 26 18:56:16.991: INFO: Created: latency-svc-qltm2
Mar 26 18:56:17.015: INFO: Got endpoints: latency-svc-r9cm2 [750.317179ms]
Mar 26 18:56:17.026: INFO: Created: latency-svc-j55f6
Mar 26 18:56:17.064: INFO: Got endpoints: latency-svc-2jsd8 [749.549449ms]
Mar 26 18:56:17.086: INFO: Created: latency-svc-9bvst
Mar 26 18:56:17.115: INFO: Got endpoints: latency-svc-4kqmv [750.021249ms]
Mar 26 18:56:17.127: INFO: Created: latency-svc-lk2xh
Mar 26 18:56:17.165: INFO: Got endpoints: latency-svc-f9tcn [749.798159ms]
Mar 26 18:56:17.176: INFO: Created: latency-svc-mlz5t
Mar 26 18:56:17.217: INFO: Got endpoints: latency-svc-wsvg4 [752.487447ms]
Mar 26 18:56:17.232: INFO: Created: latency-svc-vw8tn
Mar 26 18:56:17.264: INFO: Got endpoints: latency-svc-9qklr [749.636708ms]
Mar 26 18:56:17.274: INFO: Created: latency-svc-jnsxd
Mar 26 18:56:17.315: INFO: Got endpoints: latency-svc-2rdln [750.127468ms]
Mar 26 18:56:17.328: INFO: Created: latency-svc-6wjp7
Mar 26 18:56:17.365: INFO: Got endpoints: latency-svc-jtnbc [741.327054ms]
Mar 26 18:56:17.376: INFO: Created: latency-svc-kpjbs
Mar 26 18:56:17.415: INFO: Got endpoints: latency-svc-fq7q7 [750.486694ms]
Mar 26 18:56:17.429: INFO: Created: latency-svc-5kncz
Mar 26 18:56:17.465: INFO: Got endpoints: latency-svc-msvhk [748.450141ms]
Mar 26 18:56:17.481: INFO: Created: latency-svc-j6pdr
Mar 26 18:56:17.523: INFO: Got endpoints: latency-svc-pb5z6 [758.760385ms]
Mar 26 18:56:17.533: INFO: Created: latency-svc-g5nmf
Mar 26 18:56:17.565: INFO: Got endpoints: latency-svc-z6ggk [750.031546ms]
Mar 26 18:56:17.581: INFO: Created: latency-svc-4rb5s
Mar 26 18:56:17.615: INFO: Got endpoints: latency-svc-xhzrp [747.6609ms]
Mar 26 18:56:17.635: INFO: Created: latency-svc-9zz6x
Mar 26 18:56:17.665: INFO: Got endpoints: latency-svc-h8g6w [749.210252ms]
Mar 26 18:56:17.674: INFO: Created: latency-svc-6bl79
Mar 26 18:56:17.715: INFO: Got endpoints: latency-svc-qltm2 [741.764609ms]
Mar 26 18:56:17.724: INFO: Created: latency-svc-kzd5j
Mar 26 18:56:17.764: INFO: Got endpoints: latency-svc-j55f6 [749.688432ms]
Mar 26 18:56:17.778: INFO: Created: latency-svc-vrhml
Mar 26 18:56:17.815: INFO: Got endpoints: latency-svc-9bvst [750.158526ms]
Mar 26 18:56:17.827: INFO: Created: latency-svc-zbw5p
Mar 26 18:56:17.865: INFO: Got endpoints: latency-svc-lk2xh [749.760928ms]
Mar 26 18:56:17.878: INFO: Created: latency-svc-vz4bq
Mar 26 18:56:17.915: INFO: Got endpoints: latency-svc-mlz5t [750.474185ms]
Mar 26 18:56:17.927: INFO: Created: latency-svc-gcxt9
Mar 26 18:56:17.964: INFO: Got endpoints: latency-svc-vw8tn [747.531429ms]
Mar 26 18:56:17.982: INFO: Created: latency-svc-pf2f2
Mar 26 18:56:18.015: INFO: Got endpoints: latency-svc-jnsxd [750.233389ms]
Mar 26 18:56:18.026: INFO: Created: latency-svc-49pzf
Mar 26 18:56:18.066: INFO: Got endpoints: latency-svc-6wjp7 [750.819581ms]
Mar 26 18:56:18.089: INFO: Created: latency-svc-rm2nk
Mar 26 18:56:18.115: INFO: Got endpoints: latency-svc-kpjbs [749.842007ms]
Mar 26 18:56:18.128: INFO: Created: latency-svc-gdplx
Mar 26 18:56:18.165: INFO: Got endpoints: latency-svc-5kncz [749.462866ms]
Mar 26 18:56:18.211: INFO: Created: latency-svc-wsr76
Mar 26 18:56:18.217: INFO: Got endpoints: latency-svc-j6pdr [752.492248ms]
Mar 26 18:56:18.228: INFO: Created: latency-svc-77qwr
Mar 26 18:56:18.265: INFO: Got endpoints: latency-svc-g5nmf [741.557605ms]
Mar 26 18:56:18.277: INFO: Created: latency-svc-psdz4
Mar 26 18:56:18.320: INFO: Got endpoints: latency-svc-4rb5s [755.586462ms]
Mar 26 18:56:18.332: INFO: Created: latency-svc-rwbwq
Mar 26 18:56:18.365: INFO: Got endpoints: latency-svc-9zz6x [749.943425ms]
Mar 26 18:56:18.376: INFO: Created: latency-svc-rgjwx
Mar 26 18:56:18.415: INFO: Got endpoints: latency-svc-6bl79 [750.49384ms]
Mar 26 18:56:18.432: INFO: Created: latency-svc-92pcb
Mar 26 18:56:18.465: INFO: Got endpoints: latency-svc-kzd5j [750.054343ms]
Mar 26 18:56:18.482: INFO: Created: latency-svc-c4qdk
Mar 26 18:56:18.515: INFO: Got endpoints: latency-svc-vrhml [750.717326ms]
Mar 26 18:56:18.541: INFO: Created: latency-svc-7rqr7
Mar 26 18:56:18.565: INFO: Got endpoints: latency-svc-zbw5p [750.626314ms]
Mar 26 18:56:18.578: INFO: Created: latency-svc-9flzk
Mar 26 18:56:18.615: INFO: Got endpoints: latency-svc-vz4bq [749.973785ms]
Mar 26 18:56:18.629: INFO: Created: latency-svc-thvn4
Mar 26 18:56:18.665: INFO: Got endpoints: latency-svc-gcxt9 [749.277101ms]
Mar 26 18:56:18.676: INFO: Created: latency-svc-4727z
Mar 26 18:56:18.715: INFO: Got endpoints: latency-svc-pf2f2 [750.19901ms]
Mar 26 18:56:18.727: INFO: Created: latency-svc-vpnwf
Mar 26 18:56:18.765: INFO: Got endpoints: latency-svc-49pzf [749.960577ms]
Mar 26 18:56:18.780: INFO: Created: latency-svc-vzwh2
Mar 26 18:56:18.815: INFO: Got endpoints: latency-svc-rm2nk [749.104401ms]
Mar 26 18:56:18.826: INFO: Created: latency-svc-5k2qx
Mar 26 18:56:18.864: INFO: Got endpoints: latency-svc-gdplx [749.568744ms]
Mar 26 18:56:18.875: INFO: Created: latency-svc-tjw66
Mar 26 18:56:18.915: INFO: Got endpoints: latency-svc-wsr76 [750.380805ms]
Mar 26 18:56:18.927: INFO: Created: latency-svc-kxnmk
Mar 26 18:56:18.969: INFO: Got endpoints: latency-svc-77qwr [751.784398ms]
Mar 26 18:56:18.979: INFO: Created: latency-svc-lfqrr
Mar 26 18:56:19.015: INFO: Got endpoints: latency-svc-psdz4 [749.862478ms]
Mar 26 18:56:19.025: INFO: Created: latency-svc-7crhb
Mar 26 18:56:19.065: INFO: Got endpoints: latency-svc-rwbwq [744.265696ms]
Mar 26 18:56:19.085: INFO: Created: latency-svc-8j7kq
Mar 26 18:56:19.119: INFO: Got endpoints: latency-svc-rgjwx [754.043205ms]
Mar 26 18:56:19.128: INFO: Created: latency-svc-5nzhz
Mar 26 18:56:19.165: INFO: Got endpoints: latency-svc-92pcb [749.505737ms]
Mar 26 18:56:19.177: INFO: Created: latency-svc-cb5gc
Mar 26 18:56:19.216: INFO: Got endpoints: latency-svc-c4qdk [751.003666ms]
Mar 26 18:56:19.226: INFO: Created: latency-svc-q657f
Mar 26 18:56:19.264: INFO: Got endpoints: latency-svc-7rqr7 [749.279792ms]
Mar 26 18:56:19.275: INFO: Created: latency-svc-rm8gf
Mar 26 18:56:19.315: INFO: Got endpoints: latency-svc-9flzk [750.009627ms]
Mar 26 18:56:19.327: INFO: Created: latency-svc-7tm5b
Mar 26 18:56:19.364: INFO: Got endpoints: latency-svc-thvn4 [749.660996ms]
Mar 26 18:56:19.377: INFO: Created: latency-svc-ccjvl
Mar 26 18:56:19.415: INFO: Got endpoints: latency-svc-4727z [750.644485ms]
Mar 26 18:56:19.428: INFO: Created: latency-svc-6k4bg
Mar 26 18:56:19.465: INFO: Got endpoints: latency-svc-vpnwf [750.082606ms]
Mar 26 18:56:19.479: INFO: Created: latency-svc-5lrbd
Mar 26 18:56:19.522: INFO: Got endpoints: latency-svc-vzwh2 [756.846073ms]
Mar 26 18:56:19.533: INFO: Created: latency-svc-rzktp
Mar 26 18:56:19.565: INFO: Got endpoints: latency-svc-5k2qx [749.458145ms]
Mar 26 18:56:19.575: INFO: Created: latency-svc-b282m
Mar 26 18:56:19.615: INFO: Got endpoints: latency-svc-tjw66 [750.920107ms]
Mar 26 18:56:19.642: INFO: Created: latency-svc-wv24v
Mar 26 18:56:19.665: INFO: Got endpoints: latency-svc-kxnmk [749.467406ms]
Mar 26 18:56:19.675: INFO: Created: latency-svc-2p7lw
Mar 26 18:56:19.722: INFO: Got endpoints: latency-svc-lfqrr [752.635942ms]
Mar 26 18:56:19.742: INFO: Created: latency-svc-7mkdj
Mar 26 18:56:19.766: INFO: Got endpoints: latency-svc-7crhb [751.132992ms]
Mar 26 18:56:19.777: INFO: Created: latency-svc-57wjg
Mar 26 18:56:19.815: INFO: Got endpoints: latency-svc-8j7kq [749.975999ms]
Mar 26 18:56:19.824: INFO: Created: latency-svc-j2pck
Mar 26 18:56:19.864: INFO: Got endpoints: latency-svc-5nzhz [745.608853ms]
Mar 26 18:56:19.876: INFO: Created: latency-svc-g45vh
Mar 26 18:56:19.915: INFO: Got endpoints: latency-svc-cb5gc [749.970039ms]
Mar 26 18:56:19.930: INFO: Created: latency-svc-4r72q
Mar 26 18:56:19.970: INFO: Got endpoints: latency-svc-q657f [753.73152ms]
Mar 26 18:56:19.980: INFO: Created: latency-svc-d6mbv
Mar 26 18:56:20.015: INFO: Got endpoints: latency-svc-rm8gf [750.051328ms]
Mar 26 18:56:20.031: INFO: Created: latency-svc-hp5bm
Mar 26 18:56:20.065: INFO: Got endpoints: latency-svc-7tm5b [749.629219ms]
Mar 26 18:56:20.087: INFO: Created: latency-svc-752l2
Mar 26 18:56:20.115: INFO: Got endpoints: latency-svc-ccjvl [750.508378ms]
Mar 26 18:56:20.127: INFO: Created: latency-svc-ww4bp
Mar 26 18:56:20.165: INFO: Got endpoints: latency-svc-6k4bg [749.715431ms]
Mar 26 18:56:20.175: INFO: Created: latency-svc-wvbv4
Mar 26 18:56:20.215: INFO: Got endpoints: latency-svc-5lrbd [749.885014ms]
Mar 26 18:56:20.226: INFO: Created: latency-svc-bn25f
Mar 26 18:56:20.265: INFO: Got endpoints: latency-svc-rzktp [742.998165ms]
Mar 26 18:56:20.275: INFO: Created: latency-svc-tsgsg
Mar 26 18:56:20.318: INFO: Got endpoints: latency-svc-b282m [753.017911ms]
Mar 26 18:56:20.328: INFO: Created: latency-svc-5zt5s
Mar 26 18:56:20.365: INFO: Got endpoints: latency-svc-wv24v [749.745543ms]
Mar 26 18:56:20.375: INFO: Created: latency-svc-cbpzp
Mar 26 18:56:20.415: INFO: Got endpoints: latency-svc-2p7lw [750.138379ms]
Mar 26 18:56:20.443: INFO: Created: latency-svc-h9fkh
Mar 26 18:56:20.464: INFO: Got endpoints: latency-svc-7mkdj [742.688152ms]
Mar 26 18:56:20.475: INFO: Created: latency-svc-rxnls
Mar 26 18:56:20.515: INFO: Got endpoints: latency-svc-57wjg [748.676497ms]
Mar 26 18:56:20.529: INFO: Created: latency-svc-vtrgw
Mar 26 18:56:20.565: INFO: Got endpoints: latency-svc-j2pck [749.964273ms]
Mar 26 18:56:20.590: INFO: Created: latency-svc-gpvlh
Mar 26 18:56:20.616: INFO: Got endpoints: latency-svc-g45vh [751.412369ms]
Mar 26 18:56:20.630: INFO: Created: latency-svc-mmcnz
Mar 26 18:56:20.665: INFO: Got endpoints: latency-svc-4r72q [749.99922ms]
Mar 26 18:56:20.675: INFO: Created: latency-svc-pzsqk
Mar 26 18:56:20.715: INFO: Got endpoints: latency-svc-d6mbv [744.766875ms]
Mar 26 18:56:20.726: INFO: Created: latency-svc-7wfdz
Mar 26 18:56:20.765: INFO: Got endpoints: latency-svc-hp5bm [750.107001ms]
Mar 26 18:56:20.776: INFO: Created: latency-svc-ckqgf
Mar 26 18:56:20.815: INFO: Got endpoints: latency-svc-752l2 [750.11323ms]
Mar 26 18:56:20.825: INFO: Created: latency-svc-8x62w
Mar 26 18:56:20.870: INFO: Got endpoints: latency-svc-ww4bp [754.684151ms]
Mar 26 18:56:20.883: INFO: Created: latency-svc-bb5qs
Mar 26 18:56:20.915: INFO: Got endpoints: latency-svc-wvbv4 [749.739054ms]
Mar 26 18:56:20.938: INFO: Created: latency-svc-r6cgk
Mar 26 18:56:20.965: INFO: Got endpoints: latency-svc-bn25f [750.033567ms]
Mar 26 18:56:20.986: INFO: Created: latency-svc-9qgwz
Mar 26 18:56:21.016: INFO: Got endpoints: latency-svc-tsgsg [751.387455ms]
Mar 26 18:56:21.026: INFO: Created: latency-svc-p8q8r
Mar 26 18:56:21.065: INFO: Got endpoints: latency-svc-5zt5s [747.283571ms]
Mar 26 18:56:21.075: INFO: Created: latency-svc-nd2hk
Mar 26 18:56:21.114: INFO: Got endpoints: latency-svc-cbpzp [749.414673ms]
Mar 26 18:56:21.131: INFO: Created: latency-svc-tcjgk
Mar 26 18:56:21.165: INFO: Got endpoints: latency-svc-h9fkh [749.876781ms]
Mar 26 18:56:21.176: INFO: Created: latency-svc-vgn7w
Mar 26 18:56:21.215: INFO: Got endpoints: latency-svc-rxnls [750.110056ms]
Mar 26 18:56:21.224: INFO: Created: latency-svc-6qzn4
Mar 26 18:56:21.265: INFO: Got endpoints: latency-svc-vtrgw [750.062032ms]
Mar 26 18:56:21.276: INFO: Created: latency-svc-44cpm
Mar 26 18:56:21.315: INFO: Got endpoints: latency-svc-gpvlh [750.632871ms]
Mar 26 18:56:21.338: INFO: Created: latency-svc-kk8qv
Mar 26 18:56:21.365: INFO: Got endpoints: latency-svc-mmcnz [748.899698ms]
Mar 26 18:56:21.375: INFO: Created: latency-svc-tscsd
Mar 26 18:56:21.418: INFO: Got endpoints: latency-svc-pzsqk [753.428689ms]
Mar 26 18:56:21.427: INFO: Created: latency-svc-pjk76
Mar 26 18:56:21.465: INFO: Got endpoints: latency-svc-7wfdz [749.936109ms]
Mar 26 18:56:21.515: INFO: Got endpoints: latency-svc-ckqgf [750.518997ms]
Mar 26 18:56:21.564: INFO: Got endpoints: latency-svc-8x62w [749.372602ms]
Mar 26 18:56:21.615: INFO: Got endpoints: latency-svc-bb5qs [745.544613ms]
Mar 26 18:56:21.665: INFO: Got endpoints: latency-svc-r6cgk [749.931993ms]
Mar 26 18:56:21.715: INFO: Got endpoints: latency-svc-9qgwz [749.772998ms]
Mar 26 18:56:21.765: INFO: Got endpoints: latency-svc-p8q8r [748.641432ms]
Mar 26 18:56:21.815: INFO: Got endpoints: latency-svc-nd2hk [749.823081ms]
Mar 26 18:56:21.866: INFO: Got endpoints: latency-svc-tcjgk [751.51928ms]
Mar 26 18:56:21.915: INFO: Got endpoints: latency-svc-vgn7w [749.903724ms]
Mar 26 18:56:21.965: INFO: Got endpoints: latency-svc-6qzn4 [749.947279ms]
Mar 26 18:56:22.016: INFO: Got endpoints: latency-svc-44cpm [750.752201ms]
Mar 26 18:56:22.065: INFO: Got endpoints: latency-svc-kk8qv [749.50156ms]
Mar 26 18:56:22.115: INFO: Got endpoints: latency-svc-tscsd [750.217453ms]
Mar 26 18:56:22.165: INFO: Got endpoints: latency-svc-pjk76 [746.468363ms]
Mar 26 18:56:22.165: INFO: Latencies: [15.061587ms 24.090324ms 24.263489ms 37.846852ms 47.520857ms 67.314836ms 89.997826ms 101.504068ms 120.214323ms 136.141766ms 156.21551ms 163.547463ms 174.741724ms 189.705685ms 197.887837ms 201.023312ms 202.418565ms 203.564365ms 206.260509ms 207.724454ms 208.426351ms 209.017495ms 209.88926ms 210.723528ms 213.041466ms 214.002378ms 218.901099ms 219.147785ms 219.783292ms 220.88031ms 223.610979ms 224.808491ms 229.736258ms 230.370647ms 231.206706ms 231.803784ms 232.167919ms 236.243277ms 238.063807ms 238.286927ms 238.431504ms 302.11805ms 305.602873ms 335.504211ms 361.697193ms 410.15205ms 464.329231ms 481.4963ms 519.731539ms 557.714721ms 594.262507ms 670.196852ms 700.747366ms 708.421299ms 710.359797ms 710.987894ms 721.303607ms 731.032919ms 735.063598ms 738.728061ms 740.03513ms 741.327054ms 741.327851ms 741.557605ms 741.764609ms 742.688152ms 742.998165ms 744.265696ms 744.766875ms 745.495138ms 745.544613ms 745.608853ms 745.759685ms 746.468363ms 747.283571ms 747.38417ms 747.414058ms 747.531429ms 747.6609ms 747.679016ms 748.450141ms 748.641432ms 748.676497ms 748.711749ms 748.899698ms 748.927827ms 749.104401ms 749.111426ms 749.154259ms 749.210252ms 749.220172ms 749.277101ms 749.279792ms 749.339992ms 749.372602ms 749.414673ms 749.458145ms 749.462866ms 749.467406ms 749.50156ms 749.505737ms 749.512431ms 749.549449ms 749.568744ms 749.629219ms 749.636708ms 749.660996ms 749.688432ms 749.715431ms 749.739054ms 749.745543ms 749.760928ms 749.772998ms 749.798159ms 749.820767ms 749.823081ms 749.842007ms 749.862478ms 749.863842ms 749.876781ms 749.885014ms 749.903724ms 749.920537ms 749.921135ms 749.931993ms 749.936109ms 749.943425ms 749.947279ms 749.952731ms 749.960577ms 749.964273ms 749.970039ms 749.971521ms 749.973785ms 749.975999ms 749.99922ms 750.009627ms 750.013753ms 750.021249ms 750.031546ms 750.033567ms 750.051328ms 750.054343ms 750.054859ms 750.062032ms 750.0751ms 750.082606ms 750.107001ms 750.110056ms 750.11323ms 750.127468ms 750.138379ms 750.158526ms 750.19901ms 750.217453ms 750.233389ms 750.251783ms 750.253087ms 750.273674ms 750.298518ms 750.31096ms 750.317179ms 750.380805ms 750.435851ms 750.474185ms 750.486694ms 750.49384ms 750.508378ms 750.518997ms 750.626314ms 750.632871ms 750.644485ms 750.717326ms 750.752201ms 750.810578ms 750.819581ms 750.920107ms 750.948347ms 751.003666ms 751.132992ms 751.185655ms 751.387455ms 751.412369ms 751.51928ms 751.784398ms 751.861355ms 752.487447ms 752.492248ms 752.635942ms 753.017911ms 753.428689ms 753.73152ms 754.043205ms 754.684151ms 755.586462ms 756.846073ms 757.521252ms 758.569297ms 758.760385ms 758.875386ms]
Mar 26 18:56:22.165: INFO: 50 %ile: 749.505737ms
Mar 26 18:56:22.165: INFO: 90 %ile: 751.185655ms
Mar 26 18:56:22.165: INFO: 99 %ile: 758.760385ms
Mar 26 18:56:22.165: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 18:56:22.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "svc-latency-1805" for this suite.
Mar 26 18:56:38.177: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 18:56:38.243: INFO: namespace svc-latency-1805 deletion completed in 16.076147859s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Subpath[0m [90mAtomic writer volumes[0m 
  [1mshould support subpaths with downward pod [LinuxOnly] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Subpath
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 18:56:38.244: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename subpath
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
[1mSTEP[0m: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating pod pod-subpath-test-downwardapi-lbzs
[1mSTEP[0m: Creating a pod to test atomic-volume-subpath
Mar 26 18:56:38.280: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-lbzs" in namespace "subpath-1094" to be "success or failure"
Mar 26 18:56:38.284: INFO: Pod "pod-subpath-test-downwardapi-lbzs": Phase="Pending", Reason="", readiness=false. Elapsed: 4.131787ms
Mar 26 18:56:40.287: INFO: Pod "pod-subpath-test-downwardapi-lbzs": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007730133s
Mar 26 18:56:42.292: INFO: Pod "pod-subpath-test-downwardapi-lbzs": Phase="Running", Reason="", readiness=true. Elapsed: 4.011790242s
Mar 26 18:56:44.294: INFO: Pod "pod-subpath-test-downwardapi-lbzs": Phase="Running", Reason="", readiness=true. Elapsed: 6.014536537s
Mar 26 18:56:46.298: INFO: Pod "pod-subpath-test-downwardapi-lbzs": Phase="Running", Reason="", readiness=true. Elapsed: 8.018492225s
Mar 26 18:56:48.302: INFO: Pod "pod-subpath-test-downwardapi-lbzs": Phase="Running", Reason="", readiness=true. Elapsed: 10.022444745s
Mar 26 18:56:50.306: INFO: Pod "pod-subpath-test-downwardapi-lbzs": Phase="Running", Reason="", readiness=true. Elapsed: 12.026449076s
Mar 26 18:56:52.311: INFO: Pod "pod-subpath-test-downwardapi-lbzs": Phase="Running", Reason="", readiness=true. Elapsed: 14.030866464s
Mar 26 18:56:54.315: INFO: Pod "pod-subpath-test-downwardapi-lbzs": Phase="Running", Reason="", readiness=true. Elapsed: 16.034849746s
Mar 26 18:56:56.319: INFO: Pod "pod-subpath-test-downwardapi-lbzs": Phase="Running", Reason="", readiness=true. Elapsed: 18.038979241s
Mar 26 18:56:58.323: INFO: Pod "pod-subpath-test-downwardapi-lbzs": Phase="Running", Reason="", readiness=true. Elapsed: 20.043221769s
Mar 26 18:57:00.327: INFO: Pod "pod-subpath-test-downwardapi-lbzs": Phase="Running", Reason="", readiness=true. Elapsed: 22.047376245s
Mar 26 18:57:02.331: INFO: Pod "pod-subpath-test-downwardapi-lbzs": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.050949857s
[1mSTEP[0m: Saw pod success
Mar 26 18:57:02.331: INFO: Pod "pod-subpath-test-downwardapi-lbzs" satisfied condition "success or failure"
Mar 26 18:57:02.334: INFO: Trying to get logs from node conformance-worker pod pod-subpath-test-downwardapi-lbzs container test-container-subpath-downwardapi-lbzs: <nil>
[1mSTEP[0m: delete the pod
Mar 26 18:57:02.353: INFO: Waiting for pod pod-subpath-test-downwardapi-lbzs to disappear
Mar 26 18:57:02.356: INFO: Pod pod-subpath-test-downwardapi-lbzs no longer exists
[1mSTEP[0m: Deleting pod pod-subpath-test-downwardapi-lbzs
Mar 26 18:57:02.356: INFO: Deleting pod "pod-subpath-test-downwardapi-lbzs" in namespace "subpath-1094"
[AfterEach] [sig-storage] Subpath
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 18:57:02.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "subpath-1094" for this suite.
Mar 26 18:57:08.368: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 18:57:08.437: INFO: namespace subpath-1094 deletion completed in 6.07676265s
[32mâ€¢[0m
[90m------------------------------[0m
[0m[sig-storage] ConfigMap[0m 
  [1mshould be consumable from pods in volume [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] ConfigMap
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 18:57:08.437: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename configmap
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating configMap with name configmap-test-volume-a0e95602-5033-11e9-9719-a08cfdecc127
[1mSTEP[0m: Creating a pod to test consume configMaps
Mar 26 18:57:08.477: INFO: Waiting up to 5m0s for pod "pod-configmaps-a0e9a474-5033-11e9-9719-a08cfdecc127" in namespace "configmap-7629" to be "success or failure"
Mar 26 18:57:08.479: INFO: Pod "pod-configmaps-a0e9a474-5033-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 1.783644ms
Mar 26 18:57:10.482: INFO: Pod "pod-configmaps-a0e9a474-5033-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005336748s
[1mSTEP[0m: Saw pod success
Mar 26 18:57:10.482: INFO: Pod "pod-configmaps-a0e9a474-5033-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 18:57:10.485: INFO: Trying to get logs from node conformance-worker pod pod-configmaps-a0e9a474-5033-11e9-9719-a08cfdecc127 container configmap-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar 26 18:57:10.503: INFO: Waiting for pod pod-configmaps-a0e9a474-5033-11e9-9719-a08cfdecc127 to disappear
Mar 26 18:57:10.505: INFO: Pod pod-configmaps-a0e9a474-5033-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 18:57:10.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "configmap-7629" for this suite.
Mar 26 18:57:16.515: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 18:57:16.580: INFO: namespace configmap-7629 deletion completed in 6.07277272s
[32mâ€¢[0m
[90m------------------------------[0m
[0m[sig-scheduling] SchedulerPredicates [Serial][0m 
  [1mvalidates that NodeSelector is respected if not matching  [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 18:57:16.580: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename sched-pred
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Mar 26 18:57:16.606: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 26 18:57:16.609: INFO: Waiting for terminating namespaces to be deleted...
Mar 26 18:57:16.611: INFO: 
Logging pods the kubelet thinks is on node conformance-worker before test
Mar 26 18:57:16.614: INFO: kube-proxy-tvs75 from kube-system started at 2019-03-26 18:51:05 -0700 PDT (1 container statuses recorded)
Mar 26 18:57:16.614: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 26 18:57:16.614: INFO: weave-net-n8g4b from kube-system started at 2019-03-26 18:51:05 -0700 PDT (2 container statuses recorded)
Mar 26 18:57:16.614: INFO: 	Container weave ready: true, restart count 1
Mar 26 18:57:16.614: INFO: 	Container weave-npc ready: true, restart count 0
Mar 26 18:57:16.614: INFO: 
Logging pods the kubelet thinks is on node conformance-worker2 before test
Mar 26 18:57:16.617: INFO: kube-proxy-4cdcp from kube-system started at 2019-03-26 18:51:05 -0700 PDT (1 container statuses recorded)
Mar 26 18:57:16.617: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 26 18:57:16.617: INFO: weave-net-pdrl8 from kube-system started at 2019-03-26 18:51:05 -0700 PDT (2 container statuses recorded)
Mar 26 18:57:16.617: INFO: 	Container weave ready: true, restart count 1
Mar 26 18:57:16.617: INFO: 	Container weave-npc ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Trying to schedule Pod with nonempty NodeSelector.
[1mSTEP[0m: Considering event: 
Type = [Warning], Name = [restricted-pod.158fae1d232ec0f9], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 18:57:17.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "sched-pred-5617" for this suite.
Mar 26 18:57:23.652: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 18:57:23.721: INFO: namespace sched-pred-5617 deletion completed in 6.082465325s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70
[32mâ€¢[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90m[k8s.io] Update Demo[0m 
  [1mshould do a rolling update of a replication controller  [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 18:57:23.721: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should do a rolling update of a replication controller  [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: creating the initial replication controller
Mar 26 18:57:23.750: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance create -f - --namespace=kubectl-2946'
Mar 26 18:57:23.959: INFO: stderr: ""
Mar 26 18:57:23.960: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
[1mSTEP[0m: waiting for all containers in name=update-demo pods to come up.
Mar 26 18:57:23.960: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2946'
Mar 26 18:57:24.035: INFO: stderr: ""
Mar 26 18:57:24.035: INFO: stdout: "update-demo-nautilus-9z9bd update-demo-nautilus-kkxt2 "
Mar 26 18:57:24.035: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get pods update-demo-nautilus-9z9bd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2946'
Mar 26 18:57:24.106: INFO: stderr: ""
Mar 26 18:57:24.106: INFO: stdout: ""
Mar 26 18:57:24.106: INFO: update-demo-nautilus-9z9bd is created but not running
Mar 26 18:57:29.106: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2946'
Mar 26 18:57:29.170: INFO: stderr: ""
Mar 26 18:57:29.170: INFO: stdout: "update-demo-nautilus-9z9bd update-demo-nautilus-kkxt2 "
Mar 26 18:57:29.170: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get pods update-demo-nautilus-9z9bd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2946'
Mar 26 18:57:29.251: INFO: stderr: ""
Mar 26 18:57:29.251: INFO: stdout: "true"
Mar 26 18:57:29.251: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get pods update-demo-nautilus-9z9bd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2946'
Mar 26 18:57:29.330: INFO: stderr: ""
Mar 26 18:57:29.330: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 26 18:57:29.330: INFO: validating pod update-demo-nautilus-9z9bd
Mar 26 18:57:29.333: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 26 18:57:29.333: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 26 18:57:29.333: INFO: update-demo-nautilus-9z9bd is verified up and running
Mar 26 18:57:29.333: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get pods update-demo-nautilus-kkxt2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2946'
Mar 26 18:57:29.403: INFO: stderr: ""
Mar 26 18:57:29.403: INFO: stdout: "true"
Mar 26 18:57:29.403: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get pods update-demo-nautilus-kkxt2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2946'
Mar 26 18:57:29.477: INFO: stderr: ""
Mar 26 18:57:29.477: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 26 18:57:29.477: INFO: validating pod update-demo-nautilus-kkxt2
Mar 26 18:57:29.480: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 26 18:57:29.480: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 26 18:57:29.480: INFO: update-demo-nautilus-kkxt2 is verified up and running
[1mSTEP[0m: rolling-update to new replication controller
Mar 26 18:57:29.549: INFO: scanned /usr/local/google/home/bentheelder for discovery docs: <nil>
Mar 26 18:57:29.549: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-2946'
Mar 26 18:57:51.888: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Mar 26 18:57:51.888: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
[1mSTEP[0m: waiting for all containers in name=update-demo pods to come up.
Mar 26 18:57:51.888: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2946'
Mar 26 18:57:51.989: INFO: stderr: ""
Mar 26 18:57:51.990: INFO: stdout: "update-demo-kitten-2ltj8 update-demo-kitten-jcf8h "
Mar 26 18:57:51.990: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get pods update-demo-kitten-2ltj8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2946'
Mar 26 18:57:52.064: INFO: stderr: ""
Mar 26 18:57:52.064: INFO: stdout: "true"
Mar 26 18:57:52.064: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get pods update-demo-kitten-2ltj8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2946'
Mar 26 18:57:52.127: INFO: stderr: ""
Mar 26 18:57:52.127: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Mar 26 18:57:52.127: INFO: validating pod update-demo-kitten-2ltj8
Mar 26 18:57:52.130: INFO: got data: {
  "image": "kitten.jpg"
}

Mar 26 18:57:52.130: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Mar 26 18:57:52.130: INFO: update-demo-kitten-2ltj8 is verified up and running
Mar 26 18:57:52.130: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get pods update-demo-kitten-jcf8h -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2946'
Mar 26 18:57:52.190: INFO: stderr: ""
Mar 26 18:57:52.190: INFO: stdout: "true"
Mar 26 18:57:52.190: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get pods update-demo-kitten-jcf8h -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2946'
Mar 26 18:57:52.263: INFO: stderr: ""
Mar 26 18:57:52.263: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Mar 26 18:57:52.263: INFO: validating pod update-demo-kitten-jcf8h
Mar 26 18:57:52.267: INFO: got data: {
  "image": "kitten.jpg"
}

Mar 26 18:57:52.267: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Mar 26 18:57:52.267: INFO: update-demo-kitten-jcf8h is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 18:57:52.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-2946" for this suite.
Mar 26 18:58:14.279: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 18:58:14.343: INFO: namespace kubectl-2946 deletion completed in 22.073095131s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Pods[0m 
  [1mshould contain environment variables for services [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [k8s.io] Pods
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 18:58:14.343: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename pods
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Mar 26 18:58:18.404: INFO: Waiting up to 5m0s for pod "client-envvars-ca97dc47-5033-11e9-9719-a08cfdecc127" in namespace "pods-5911" to be "success or failure"
Mar 26 18:58:18.407: INFO: Pod "client-envvars-ca97dc47-5033-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.734994ms
Mar 26 18:58:20.411: INFO: Pod "client-envvars-ca97dc47-5033-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006604693s
Mar 26 18:58:22.415: INFO: Pod "client-envvars-ca97dc47-5033-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010613825s
[1mSTEP[0m: Saw pod success
Mar 26 18:58:22.415: INFO: Pod "client-envvars-ca97dc47-5033-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 18:58:22.418: INFO: Trying to get logs from node conformance-worker pod client-envvars-ca97dc47-5033-11e9-9719-a08cfdecc127 container env3cont: <nil>
[1mSTEP[0m: delete the pod
Mar 26 18:58:22.438: INFO: Waiting for pod client-envvars-ca97dc47-5033-11e9-9719-a08cfdecc127 to disappear
Mar 26 18:58:22.440: INFO: Pod client-envvars-ca97dc47-5033-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [k8s.io] Pods
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 18:58:22.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "pods-5911" for this suite.
Mar 26 18:59:00.449: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 18:59:00.520: INFO: namespace pods-5911 deletion completed in 38.078370381s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Secrets[0m 
  [1mshould be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Secrets
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 18:59:00.521: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename secrets
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating secret with name secret-test-map-e3b71671-5033-11e9-9719-a08cfdecc127
[1mSTEP[0m: Creating a pod to test consume secrets
Mar 26 18:59:00.555: INFO: Waiting up to 5m0s for pod "pod-secrets-e3b7661e-5033-11e9-9719-a08cfdecc127" in namespace "secrets-9845" to be "success or failure"
Mar 26 18:59:00.559: INFO: Pod "pod-secrets-e3b7661e-5033-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 4.236026ms
Mar 26 18:59:02.563: INFO: Pod "pod-secrets-e3b7661e-5033-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008395873s
[1mSTEP[0m: Saw pod success
Mar 26 18:59:02.564: INFO: Pod "pod-secrets-e3b7661e-5033-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 18:59:02.567: INFO: Trying to get logs from node conformance-worker2 pod pod-secrets-e3b7661e-5033-11e9-9719-a08cfdecc127 container secret-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar 26 18:59:02.582: INFO: Waiting for pod pod-secrets-e3b7661e-5033-11e9-9719-a08cfdecc127 to disappear
Mar 26 18:59:02.589: INFO: Pod pod-secrets-e3b7661e-5033-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] Secrets
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 18:59:02.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "secrets-9845" for this suite.
Mar 26 18:59:08.600: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 18:59:08.669: INFO: namespace secrets-9845 deletion completed in 6.076276796s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Namespaces [Serial][0m 
  [1mshould ensure that all pods are removed when a namespace is deleted [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 18:59:08.669: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename namespaces
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating a test namespace
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[1mSTEP[0m: Creating a pod in the namespace
[1mSTEP[0m: Waiting for the pod to have running status
[1mSTEP[0m: Deleting the namespace
[1mSTEP[0m: Waiting for the namespace to be removed.
[1mSTEP[0m: Recreating the namespace
[1mSTEP[0m: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 18:59:32.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "namespaces-1211" for this suite.
Mar 26 18:59:38.779: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 18:59:38.848: INFO: namespace namespaces-1211 deletion completed in 6.075915671s
[1mSTEP[0m: Destroying namespace "nsdeletetest-5585" for this suite.
Mar 26 18:59:38.851: INFO: Namespace nsdeletetest-5585 was already deleted
[1mSTEP[0m: Destroying namespace "nsdeletetest-9423" for this suite.
Mar 26 18:59:44.859: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 18:59:44.911: INFO: namespace nsdeletetest-9423 deletion completed in 6.059682341s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Garbage collector[0m 
  [1mshould not be blocked by dependency circle [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-api-machinery] Garbage collector
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 18:59:44.911: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename gc
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Mar 26 18:59:44.952: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"fe2d9667-5033-11e9-b1ea-02429e4bb871", Controller:(*bool)(0xc001538c7a), BlockOwnerDeletion:(*bool)(0xc001538c7b)}}
Mar 26 18:59:44.961: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"fe2c8f00-5033-11e9-b1ea-02429e4bb871", Controller:(*bool)(0xc001539066), BlockOwnerDeletion:(*bool)(0xc001539067)}}
Mar 26 18:59:44.966: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"fe2cea9a-5033-11e9-b1ea-02429e4bb871", Controller:(*bool)(0xc0025bc686), BlockOwnerDeletion:(*bool)(0xc0025bc687)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 18:59:49.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "gc-49" for this suite.
Mar 26 18:59:55.990: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 18:59:56.049: INFO: namespace gc-49 deletion completed in 6.067395516s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Garbage collector[0m 
  [1mshould delete RS created by deployment when not orphaning [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-api-machinery] Garbage collector
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 18:59:56.049: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename gc
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: create the deployment
[1mSTEP[0m: Wait for the Deployment to create new ReplicaSet
[1mSTEP[0m: delete the deployment
[1mSTEP[0m: wait for all rs to be garbage collected
[1mSTEP[0m: expected 0 rs, got 1 rs
[1mSTEP[0m: expected 0 pods, got 2 pods
[1mSTEP[0m: Gathering metrics
W0326 18:59:57.113239  168179 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar 26 18:59:57.113: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 18:59:57.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "gc-7878" for this suite.
Mar 26 19:00:03.124: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:00:03.189: INFO: namespace gc-7878 deletion completed in 6.073663226s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Kubelet[0m [90mwhen scheduling a busybox command that always fails in a pod[0m 
  [1mshould be possible to delete [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [k8s.io] Kubelet
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:00:03.189: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename kubelet-test
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:00:03.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubelet-test-7892" for this suite.
Mar 26 19:00:09.243: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:00:09.309: INFO: namespace kubelet-test-7892 deletion completed in 6.080835618s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-node] ConfigMap[0m 
  [1mshould fail to create ConfigMap with empty key [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-node] ConfigMap
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:00:09.310: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename configmap
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating configMap that has name configmap-test-emptyKey-0cb90962-5034-11e9-9719-a08cfdecc127
[AfterEach] [sig-node] ConfigMap
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:00:09.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "configmap-6920" for this suite.
Mar 26 19:00:15.365: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:00:15.431: INFO: namespace configmap-6920 deletion completed in 6.077807091s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] EmptyDir volumes[0m 
  [1mshould support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] EmptyDir volumes
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:00:15.432: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename emptydir
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating a pod to test emptydir 0644 on node default medium
Mar 26 19:00:15.459: INFO: Waiting up to 5m0s for pod "pod-105ce2d4-5034-11e9-9719-a08cfdecc127" in namespace "emptydir-5338" to be "success or failure"
Mar 26 19:00:15.465: INFO: Pod "pod-105ce2d4-5034-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 6.442915ms
Mar 26 19:00:17.469: INFO: Pod "pod-105ce2d4-5034-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010371453s
[1mSTEP[0m: Saw pod success
Mar 26 19:00:17.469: INFO: Pod "pod-105ce2d4-5034-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 19:00:17.473: INFO: Trying to get logs from node conformance-worker2 pod pod-105ce2d4-5034-11e9-9719-a08cfdecc127 container test-container: <nil>
[1mSTEP[0m: delete the pod
Mar 26 19:00:17.490: INFO: Waiting for pod pod-105ce2d4-5034-11e9-9719-a08cfdecc127 to disappear
Mar 26 19:00:17.491: INFO: Pod pod-105ce2d4-5034-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:00:17.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "emptydir-5338" for this suite.
Mar 26 19:00:23.501: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:00:23.567: INFO: namespace emptydir-5338 deletion completed in 6.073276763s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-node] Downward API[0m 
  [1mshould provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-node] Downward API
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:00:23.567: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename downward-api
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating a pod to test downward api env vars
Mar 26 19:00:23.593: INFO: Waiting up to 5m0s for pod "downward-api-153622ed-5034-11e9-9719-a08cfdecc127" in namespace "downward-api-3144" to be "success or failure"
Mar 26 19:00:23.596: INFO: Pod "downward-api-153622ed-5034-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.946244ms
Mar 26 19:00:25.600: INFO: Pod "downward-api-153622ed-5034-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006521084s
[1mSTEP[0m: Saw pod success
Mar 26 19:00:25.600: INFO: Pod "downward-api-153622ed-5034-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 19:00:25.603: INFO: Trying to get logs from node conformance-worker pod downward-api-153622ed-5034-11e9-9719-a08cfdecc127 container dapi-container: <nil>
[1mSTEP[0m: delete the pod
Mar 26 19:00:25.619: INFO: Waiting for pod downward-api-153622ed-5034-11e9-9719-a08cfdecc127 to disappear
Mar 26 19:00:25.621: INFO: Pod downward-api-153622ed-5034-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-node] Downward API
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:00:25.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "downward-api-3144" for this suite.
Mar 26 19:00:31.631: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:00:31.697: INFO: namespace downward-api-3144 deletion completed in 6.073411097s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Kubelet[0m [90mwhen scheduling a busybox Pod with hostAliases[0m 
  [1mshould write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [k8s.io] Kubelet
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:00:31.697: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename kubelet-test
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:00:33.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubelet-test-2653" for this suite.
Mar 26 19:01:19.764: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:01:19.829: INFO: namespace kubelet-test-2653 deletion completed in 46.077399207s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] Deployment[0m 
  [1mRecreateDeployment should delete old pods and create new ones [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-apps] Deployment
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:01:19.829: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename deployment
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Mar 26 19:01:19.860: INFO: Creating deployment "test-recreate-deployment"
Mar 26 19:01:19.863: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Mar 26 19:01:19.867: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Mar 26 19:01:21.872: INFO: Waiting deployment "test-recreate-deployment" to complete
Mar 26 19:01:21.874: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Mar 26 19:01:21.879: INFO: Updating deployment test-recreate-deployment
Mar 26 19:01:21.879: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Mar 26 19:01:21.930: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-5661,SelfLink:/apis/apps/v1/namespaces/deployment-5661/deployments/test-recreate-deployment,UID:36c073cb-5034-11e9-b1ea-02429e4bb871,ResourceVersion:4104,Generation:2,CreationTimestamp:2019-03-26 19:01:19 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-03-26 19:01:21 -0700 PDT 2019-03-26 19:01:21 -0700 PDT MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-03-26 19:01:21 -0700 PDT 2019-03-26 19:01:19 -0700 PDT ReplicaSetUpdated ReplicaSet "test-recreate-deployment-c9cbd8684" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Mar 26 19:01:21.935: INFO: New ReplicaSet "test-recreate-deployment-c9cbd8684" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684,GenerateName:,Namespace:deployment-5661,SelfLink:/apis/apps/v1/namespaces/deployment-5661/replicasets/test-recreate-deployment-c9cbd8684,UID:37f74eae-5034-11e9-b1ea-02429e4bb871,ResourceVersion:4102,Generation:1,CreationTimestamp:2019-03-26 19:01:21 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 36c073cb-5034-11e9-b1ea-02429e4bb871 0xc00283c670 0xc00283c671}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar 26 19:01:21.935: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Mar 26 19:01:21.935: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-7d57d5ff7c,GenerateName:,Namespace:deployment-5661,SelfLink:/apis/apps/v1/namespaces/deployment-5661/replicasets/test-recreate-deployment-7d57d5ff7c,UID:36c0dc1e-5034-11e9-b1ea-02429e4bb871,ResourceVersion:4091,Generation:2,CreationTimestamp:2019-03-26 19:01:19 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 36c073cb-5034-11e9-b1ea-02429e4bb871 0xc00283c5b7 0xc00283c5b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar 26 19:01:21.942: INFO: Pod "test-recreate-deployment-c9cbd8684-2zvw6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684-2zvw6,GenerateName:test-recreate-deployment-c9cbd8684-,Namespace:deployment-5661,SelfLink:/api/v1/namespaces/deployment-5661/pods/test-recreate-deployment-c9cbd8684-2zvw6,UID:37f7a692-5034-11e9-b1ea-02429e4bb871,ResourceVersion:4105,Generation:0,CreationTimestamp:2019-03-26 19:01:21 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-c9cbd8684 37f74eae-5034-11e9-b1ea-02429e4bb871 0xc002a29f70 0xc002a29f71}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-s7tbs {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-s7tbs,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-s7tbs true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a29fd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a29ff0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:21 -0700 PDT  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:21 -0700 PDT ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:21 -0700 PDT ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:21 -0700 PDT  }],Message:,Reason:,HostIP:192.168.9.2,PodIP:,StartTime:2019-03-26 19:01:21 -0700 PDT,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:01:21.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "deployment-5661" for this suite.
Mar 26 19:01:27.955: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:01:28.028: INFO: namespace deployment-5661 deletion completed in 6.082893029s
[32mâ€¢[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] ConfigMap[0m 
  [1mshould be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] ConfigMap
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:01:28.028: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename configmap
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating configMap with name configmap-test-volume-map-3ba27dcc-5034-11e9-9719-a08cfdecc127
[1mSTEP[0m: Creating a pod to test consume configMaps
Mar 26 19:01:28.059: INFO: Waiting up to 5m0s for pod "pod-configmaps-3ba2e203-5034-11e9-9719-a08cfdecc127" in namespace "configmap-7024" to be "success or failure"
Mar 26 19:01:28.063: INFO: Pod "pod-configmaps-3ba2e203-5034-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 3.608114ms
Mar 26 19:01:30.067: INFO: Pod "pod-configmaps-3ba2e203-5034-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007685266s
[1mSTEP[0m: Saw pod success
Mar 26 19:01:30.067: INFO: Pod "pod-configmaps-3ba2e203-5034-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 19:01:30.071: INFO: Trying to get logs from node conformance-worker2 pod pod-configmaps-3ba2e203-5034-11e9-9719-a08cfdecc127 container configmap-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar 26 19:01:30.088: INFO: Waiting for pod pod-configmaps-3ba2e203-5034-11e9-9719-a08cfdecc127 to disappear
Mar 26 19:01:30.090: INFO: Pod pod-configmaps-3ba2e203-5034-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:01:30.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "configmap-7024" for this suite.
Mar 26 19:01:36.103: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:01:36.156: INFO: namespace configmap-7024 deletion completed in 6.063567528s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected configMap[0m 
  [1mshould be consumable in multiple volumes in the same pod [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Projected configMap
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:01:36.156: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating configMap with name projected-configmap-test-volume-407af7e4-5034-11e9-9719-a08cfdecc127
[1mSTEP[0m: Creating a pod to test consume configMaps
Mar 26 19:01:36.189: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-407b806c-5034-11e9-9719-a08cfdecc127" in namespace "projected-4904" to be "success or failure"
Mar 26 19:01:36.195: INFO: Pod "pod-projected-configmaps-407b806c-5034-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 5.144849ms
Mar 26 19:01:38.198: INFO: Pod "pod-projected-configmaps-407b806c-5034-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008556884s
Mar 26 19:01:40.202: INFO: Pod "pod-projected-configmaps-407b806c-5034-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012700442s
[1mSTEP[0m: Saw pod success
Mar 26 19:01:40.202: INFO: Pod "pod-projected-configmaps-407b806c-5034-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 19:01:40.205: INFO: Trying to get logs from node conformance-worker2 pod pod-projected-configmaps-407b806c-5034-11e9-9719-a08cfdecc127 container projected-configmap-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar 26 19:01:40.224: INFO: Waiting for pod pod-projected-configmaps-407b806c-5034-11e9-9719-a08cfdecc127 to disappear
Mar 26 19:01:40.226: INFO: Pod pod-projected-configmaps-407b806c-5034-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:01:40.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-4904" for this suite.
Mar 26 19:01:46.237: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:01:46.305: INFO: namespace projected-4904 deletion completed in 6.076256578s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] Deployment[0m 
  [1mdeployment should support proportional scaling [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-apps] Deployment
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:01:46.305: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename deployment
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Mar 26 19:01:46.333: INFO: Creating deployment "nginx-deployment"
Mar 26 19:01:46.335: INFO: Waiting for observed generation 1
Mar 26 19:01:48.340: INFO: Waiting for all required pods to come up
Mar 26 19:01:48.342: INFO: Pod name nginx: Found 10 pods out of 10
[1mSTEP[0m: ensuring each pod is running
Mar 26 19:01:50.350: INFO: Waiting for deployment "nginx-deployment" to complete
Mar 26 19:01:50.355: INFO: Updating deployment "nginx-deployment" with a non-existent image
Mar 26 19:01:50.361: INFO: Updating deployment nginx-deployment
Mar 26 19:01:50.361: INFO: Waiting for observed generation 2
Mar 26 19:01:52.366: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Mar 26 19:01:52.368: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Mar 26 19:01:52.370: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Mar 26 19:01:52.377: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Mar 26 19:01:52.377: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Mar 26 19:01:52.379: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Mar 26 19:01:52.382: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Mar 26 19:01:52.382: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Mar 26 19:01:52.386: INFO: Updating deployment nginx-deployment
Mar 26 19:01:52.386: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Mar 26 19:01:52.396: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Mar 26 19:01:52.400: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Mar 26 19:01:52.419: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-2028,SelfLink:/apis/apps/v1/namespaces/deployment-2028/deployments/nginx-deployment,UID:4687e421-5034-11e9-b1ea-02429e4bb871,ResourceVersion:4477,Generation:3,CreationTimestamp:2019-03-26 19:01:46 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Progressing True 2019-03-26 19:01:50 -0700 PDT 2019-03-26 19:01:46 -0700 PDT ReplicaSetUpdated ReplicaSet "nginx-deployment-5f9595f595" is progressing.} {Available False 2019-03-26 19:01:52 -0700 PDT 2019-03-26 19:01:52 -0700 PDT MinimumReplicasUnavailable Deployment does not have minimum availability.}],ReadyReplicas:8,CollisionCount:nil,},}

Mar 26 19:01:52.436: INFO: New ReplicaSet "nginx-deployment-5f9595f595" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595,GenerateName:,Namespace:deployment-2028,SelfLink:/apis/apps/v1/namespaces/deployment-2028/replicasets/nginx-deployment-5f9595f595,UID:48ee8b13-5034-11e9-b1ea-02429e4bb871,ResourceVersion:4462,Generation:3,CreationTimestamp:2019-03-26 19:01:50 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 4687e421-5034-11e9-b1ea-02429e4bb871 0xc0025e9807 0xc0025e9808}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar 26 19:01:52.436: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Mar 26 19:01:52.437: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8,GenerateName:,Namespace:deployment-2028,SelfLink:/apis/apps/v1/namespaces/deployment-2028/replicasets/nginx-deployment-6f478d8d8,UID:46884322-5034-11e9-b1ea-02429e4bb871,ResourceVersion:4459,Generation:3,CreationTimestamp:2019-03-26 19:01:46 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 4687e421-5034-11e9-b1ea-02429e4bb871 0xc0025e98d7 0xc0025e98d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Mar 26 19:01:52.447: INFO: Pod "nginx-deployment-5f9595f595-2rmwj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-2rmwj,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-2028,SelfLink:/api/v1/namespaces/deployment-2028/pods/nginx-deployment-5f9595f595-2rmwj,UID:4a291db8-5034-11e9-b1ea-02429e4bb871,ResourceVersion:4511,Generation:0,CreationTimestamp:2019-03-26 19:01:52 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 48ee8b13-5034-11e9-b1ea-02429e4bb871 0xc002ca5a10 0xc002ca5a11}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cjb6l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cjb6l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-cjb6l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002ca5a80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002ca5aa0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:52 -0700 PDT  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 26 19:01:52.447: INFO: Pod "nginx-deployment-5f9595f595-4zdkd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-4zdkd,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-2028,SelfLink:/api/v1/namespaces/deployment-2028/pods/nginx-deployment-5f9595f595-4zdkd,UID:48f7412b-5034-11e9-b1ea-02429e4bb871,ResourceVersion:4435,Generation:0,CreationTimestamp:2019-03-26 19:01:50 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 48ee8b13-5034-11e9-b1ea-02429e4bb871 0xc002ca5b20 0xc002ca5b21}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cjb6l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cjb6l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-cjb6l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002ca5b90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002ca5bb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:50 -0700 PDT  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:50 -0700 PDT ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:50 -0700 PDT ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:50 -0700 PDT  }],Message:,Reason:,HostIP:192.168.9.2,PodIP:,StartTime:2019-03-26 19:01:50 -0700 PDT,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 26 19:01:52.447: INFO: Pod "nginx-deployment-5f9595f595-dvd69" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-dvd69,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-2028,SelfLink:/api/v1/namespaces/deployment-2028/pods/nginx-deployment-5f9595f595-dvd69,UID:48f7c71a-5034-11e9-b1ea-02429e4bb871,ResourceVersion:4436,Generation:0,CreationTimestamp:2019-03-26 19:01:50 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 48ee8b13-5034-11e9-b1ea-02429e4bb871 0xc002ca5c80 0xc002ca5c81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cjb6l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cjb6l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-cjb6l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002ca5cf0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002ca5d10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:50 -0700 PDT  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:50 -0700 PDT ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:50 -0700 PDT ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:50 -0700 PDT  }],Message:,Reason:,HostIP:192.168.9.3,PodIP:,StartTime:2019-03-26 19:01:50 -0700 PDT,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 26 19:01:52.448: INFO: Pod "nginx-deployment-5f9595f595-fmvjw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-fmvjw,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-2028,SelfLink:/api/v1/namespaces/deployment-2028/pods/nginx-deployment-5f9595f595-fmvjw,UID:48eedd6a-5034-11e9-b1ea-02429e4bb871,ResourceVersion:4409,Generation:0,CreationTimestamp:2019-03-26 19:01:50 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 48ee8b13-5034-11e9-b1ea-02429e4bb871 0xc002ca5de0 0xc002ca5de1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cjb6l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cjb6l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-cjb6l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002ca5e50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002ca5e70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:50 -0700 PDT  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:50 -0700 PDT ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:50 -0700 PDT ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:50 -0700 PDT  }],Message:,Reason:,HostIP:192.168.9.3,PodIP:,StartTime:2019-03-26 19:01:50 -0700 PDT,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 26 19:01:52.448: INFO: Pod "nginx-deployment-5f9595f595-fwhfn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-fwhfn,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-2028,SelfLink:/api/v1/namespaces/deployment-2028/pods/nginx-deployment-5f9595f595-fwhfn,UID:48ef8196-5034-11e9-b1ea-02429e4bb871,ResourceVersion:4414,Generation:0,CreationTimestamp:2019-03-26 19:01:50 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 48ee8b13-5034-11e9-b1ea-02429e4bb871 0xc002ca5f40 0xc002ca5f41}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cjb6l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cjb6l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-cjb6l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002ca5fb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002ca5fd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:50 -0700 PDT  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:50 -0700 PDT ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:50 -0700 PDT ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:50 -0700 PDT  }],Message:,Reason:,HostIP:192.168.9.2,PodIP:,StartTime:2019-03-26 19:01:50 -0700 PDT,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 26 19:01:52.448: INFO: Pod "nginx-deployment-5f9595f595-htchh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-htchh,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-2028,SelfLink:/api/v1/namespaces/deployment-2028/pods/nginx-deployment-5f9595f595-htchh,UID:48efa1e9-5034-11e9-b1ea-02429e4bb871,ResourceVersion:4419,Generation:0,CreationTimestamp:2019-03-26 19:01:50 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 48ee8b13-5034-11e9-b1ea-02429e4bb871 0xc002b860a0 0xc002b860a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cjb6l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cjb6l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-cjb6l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b86110} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b86130}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:50 -0700 PDT  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:50 -0700 PDT ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:50 -0700 PDT ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:50 -0700 PDT  }],Message:,Reason:,HostIP:192.168.9.3,PodIP:,StartTime:2019-03-26 19:01:50 -0700 PDT,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 26 19:01:52.448: INFO: Pod "nginx-deployment-5f9595f595-l29gd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-l29gd,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-2028,SelfLink:/api/v1/namespaces/deployment-2028/pods/nginx-deployment-5f9595f595-l29gd,UID:4a257e22-5034-11e9-b1ea-02429e4bb871,ResourceVersion:4474,Generation:0,CreationTimestamp:2019-03-26 19:01:52 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 48ee8b13-5034-11e9-b1ea-02429e4bb871 0xc002b86200 0xc002b86201}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cjb6l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cjb6l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-cjb6l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b86270} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b86290}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:52 -0700 PDT  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 26 19:01:52.448: INFO: Pod "nginx-deployment-5f9595f595-rhlcp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-rhlcp,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-2028,SelfLink:/api/v1/namespaces/deployment-2028/pods/nginx-deployment-5f9595f595-rhlcp,UID:4a26cf19-5034-11e9-b1ea-02429e4bb871,ResourceVersion:4493,Generation:0,CreationTimestamp:2019-03-26 19:01:52 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 48ee8b13-5034-11e9-b1ea-02429e4bb871 0xc002b86310 0xc002b86311}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cjb6l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cjb6l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-cjb6l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b86380} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b863a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:52 -0700 PDT  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 26 19:01:52.448: INFO: Pod "nginx-deployment-5f9595f595-tbkjr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-tbkjr,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-2028,SelfLink:/api/v1/namespaces/deployment-2028/pods/nginx-deployment-5f9595f595-tbkjr,UID:4a2908bd-5034-11e9-b1ea-02429e4bb871,ResourceVersion:4507,Generation:0,CreationTimestamp:2019-03-26 19:01:52 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 48ee8b13-5034-11e9-b1ea-02429e4bb871 0xc002b86420 0xc002b86421}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cjb6l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cjb6l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-cjb6l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b86490} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b864b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:52 -0700 PDT  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 26 19:01:52.448: INFO: Pod "nginx-deployment-5f9595f595-tvk7p" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-tvk7p,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-2028,SelfLink:/api/v1/namespaces/deployment-2028/pods/nginx-deployment-5f9595f595-tvk7p,UID:4a2a9978-5034-11e9-b1ea-02429e4bb871,ResourceVersion:4512,Generation:0,CreationTimestamp:2019-03-26 19:01:52 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 48ee8b13-5034-11e9-b1ea-02429e4bb871 0xc002b86530 0xc002b86531}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cjb6l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cjb6l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-cjb6l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b865a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b865c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 26 19:01:52.448: INFO: Pod "nginx-deployment-5f9595f595-vv5kd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-vv5kd,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-2028,SelfLink:/api/v1/namespaces/deployment-2028/pods/nginx-deployment-5f9595f595-vv5kd,UID:4a290f85-5034-11e9-b1ea-02429e4bb871,ResourceVersion:4510,Generation:0,CreationTimestamp:2019-03-26 19:01:52 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 48ee8b13-5034-11e9-b1ea-02429e4bb871 0xc002b86627 0xc002b86628}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cjb6l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cjb6l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-cjb6l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b86690} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b866b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:52 -0700 PDT  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 26 19:01:52.448: INFO: Pod "nginx-deployment-5f9595f595-wcmxk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-wcmxk,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-2028,SelfLink:/api/v1/namespaces/deployment-2028/pods/nginx-deployment-5f9595f595-wcmxk,UID:4a290d69-5034-11e9-b1ea-02429e4bb871,ResourceVersion:4509,Generation:0,CreationTimestamp:2019-03-26 19:01:52 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 48ee8b13-5034-11e9-b1ea-02429e4bb871 0xc002b86730 0xc002b86731}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cjb6l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cjb6l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-cjb6l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b867a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b867c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:52 -0700 PDT  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 26 19:01:52.448: INFO: Pod "nginx-deployment-5f9595f595-xklk2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-xklk2,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-2028,SelfLink:/api/v1/namespaces/deployment-2028/pods/nginx-deployment-5f9595f595-xklk2,UID:4a26d886-5034-11e9-b1ea-02429e4bb871,ResourceVersion:4487,Generation:0,CreationTimestamp:2019-03-26 19:01:52 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 48ee8b13-5034-11e9-b1ea-02429e4bb871 0xc002b86850 0xc002b86851}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cjb6l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cjb6l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-cjb6l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b868c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b868e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:52 -0700 PDT  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 26 19:01:52.448: INFO: Pod "nginx-deployment-6f478d8d8-2tf9p" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-2tf9p,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-2028,SelfLink:/api/v1/namespaces/deployment-2028/pods/nginx-deployment-6f478d8d8-2tf9p,UID:4a28edf7-5034-11e9-b1ea-02429e4bb871,ResourceVersion:4508,Generation:0,CreationTimestamp:2019-03-26 19:01:52 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 46884322-5034-11e9-b1ea-02429e4bb871 0xc002b86960 0xc002b86961}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cjb6l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cjb6l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cjb6l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b869d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b869f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:52 -0700 PDT  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 26 19:01:52.449: INFO: Pod "nginx-deployment-6f478d8d8-6z525" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-6z525,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-2028,SelfLink:/api/v1/namespaces/deployment-2028/pods/nginx-deployment-6f478d8d8-6z525,UID:468c22e1-5034-11e9-b1ea-02429e4bb871,ResourceVersion:4374,Generation:0,CreationTimestamp:2019-03-26 19:01:46 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 46884322-5034-11e9-b1ea-02429e4bb871 0xc002b86a80 0xc002b86a81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cjb6l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cjb6l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cjb6l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b86ae0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b86b00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:46 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:49 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:49 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:46 -0700 PDT  }],Message:,Reason:,HostIP:192.168.9.2,PodIP:10.46.0.4,StartTime:2019-03-26 19:01:46 -0700 PDT,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-26 19:01:48 -0700 PDT,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://d37101886d9ca273f4402e24cf51f6218a5b7692e9bbd100508df972904d7843}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 26 19:01:52.449: INFO: Pod "nginx-deployment-6f478d8d8-7286s" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-7286s,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-2028,SelfLink:/api/v1/namespaces/deployment-2028/pods/nginx-deployment-6f478d8d8-7286s,UID:4a28e33a-5034-11e9-b1ea-02429e4bb871,ResourceVersion:4505,Generation:0,CreationTimestamp:2019-03-26 19:01:52 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 46884322-5034-11e9-b1ea-02429e4bb871 0xc002b86bd0 0xc002b86bd1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cjb6l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cjb6l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cjb6l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b86c30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b86c50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:52 -0700 PDT  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 26 19:01:52.449: INFO: Pod "nginx-deployment-6f478d8d8-7rjvj" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-7rjvj,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-2028,SelfLink:/api/v1/namespaces/deployment-2028/pods/nginx-deployment-6f478d8d8-7rjvj,UID:4689162a-5034-11e9-b1ea-02429e4bb871,ResourceVersion:4369,Generation:0,CreationTimestamp:2019-03-26 19:01:46 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 46884322-5034-11e9-b1ea-02429e4bb871 0xc002b86cf0 0xc002b86cf1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cjb6l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cjb6l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cjb6l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b86d60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b86d80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:46 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:49 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:49 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:46 -0700 PDT  }],Message:,Reason:,HostIP:192.168.9.3,PodIP:10.32.0.2,StartTime:2019-03-26 19:01:46 -0700 PDT,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-26 19:01:48 -0700 PDT,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://62de7ab318bfa4e8e895549519fbb4a16fc3651c70126d81a7971408d92c9ae1}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 26 19:01:52.449: INFO: Pod "nginx-deployment-6f478d8d8-8scb7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-8scb7,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-2028,SelfLink:/api/v1/namespaces/deployment-2028/pods/nginx-deployment-6f478d8d8-8scb7,UID:4a26b181-5034-11e9-b1ea-02429e4bb871,ResourceVersion:4483,Generation:0,CreationTimestamp:2019-03-26 19:01:52 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 46884322-5034-11e9-b1ea-02429e4bb871 0xc002b86e60 0xc002b86e61}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cjb6l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cjb6l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cjb6l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b86ec0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b86ee0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:52 -0700 PDT  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 26 19:01:52.449: INFO: Pod "nginx-deployment-6f478d8d8-bl4ln" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-bl4ln,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-2028,SelfLink:/api/v1/namespaces/deployment-2028/pods/nginx-deployment-6f478d8d8-bl4ln,UID:4a26a9bd-5034-11e9-b1ea-02429e4bb871,ResourceVersion:4489,Generation:0,CreationTimestamp:2019-03-26 19:01:52 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 46884322-5034-11e9-b1ea-02429e4bb871 0xc002b86fa0 0xc002b86fa1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cjb6l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cjb6l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cjb6l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b87030} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b87050}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:52 -0700 PDT  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 26 19:01:52.449: INFO: Pod "nginx-deployment-6f478d8d8-blc4q" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-blc4q,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-2028,SelfLink:/api/v1/namespaces/deployment-2028/pods/nginx-deployment-6f478d8d8-blc4q,UID:4a28b4b1-5034-11e9-b1ea-02429e4bb871,ResourceVersion:4501,Generation:0,CreationTimestamp:2019-03-26 19:01:52 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 46884322-5034-11e9-b1ea-02429e4bb871 0xc002b870d0 0xc002b870d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cjb6l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cjb6l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cjb6l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b87150} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b87170}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:52 -0700 PDT  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 26 19:01:52.449: INFO: Pod "nginx-deployment-6f478d8d8-ck5wn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-ck5wn,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-2028,SelfLink:/api/v1/namespaces/deployment-2028/pods/nginx-deployment-6f478d8d8-ck5wn,UID:4a25745f-5034-11e9-b1ea-02429e4bb871,ResourceVersion:4471,Generation:0,CreationTimestamp:2019-03-26 19:01:52 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 46884322-5034-11e9-b1ea-02429e4bb871 0xc002b871f0 0xc002b871f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cjb6l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cjb6l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cjb6l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b87250} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b87270}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:52 -0700 PDT  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 26 19:01:52.449: INFO: Pod "nginx-deployment-6f478d8d8-dx7wk" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-dx7wk,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-2028,SelfLink:/api/v1/namespaces/deployment-2028/pods/nginx-deployment-6f478d8d8-dx7wk,UID:46896f2b-5034-11e9-b1ea-02429e4bb871,ResourceVersion:4347,Generation:0,CreationTimestamp:2019-03-26 19:01:46 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 46884322-5034-11e9-b1ea-02429e4bb871 0xc002b87310 0xc002b87311}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cjb6l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cjb6l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cjb6l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b87370} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b87390}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:46 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:48 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:48 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:46 -0700 PDT  }],Message:,Reason:,HostIP:192.168.9.3,PodIP:10.32.0.3,StartTime:2019-03-26 19:01:46 -0700 PDT,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-26 19:01:48 -0700 PDT,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://0e89838b8e946f8547bd93afbeec8132cfb4bd6f1ffc6144b5a95ab7055b189a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 26 19:01:52.449: INFO: Pod "nginx-deployment-6f478d8d8-lqt5s" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-lqt5s,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-2028,SelfLink:/api/v1/namespaces/deployment-2028/pods/nginx-deployment-6f478d8d8-lqt5s,UID:468c3d89-5034-11e9-b1ea-02429e4bb871,ResourceVersion:4366,Generation:0,CreationTimestamp:2019-03-26 19:01:46 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 46884322-5034-11e9-b1ea-02429e4bb871 0xc002b87460 0xc002b87461}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cjb6l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cjb6l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cjb6l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b874c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b874e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:46 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:49 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:49 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:46 -0700 PDT  }],Message:,Reason:,HostIP:192.168.9.2,PodIP:10.46.0.5,StartTime:2019-03-26 19:01:46 -0700 PDT,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-26 19:01:49 -0700 PDT,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://825c73c2742199a5acd12d64cd2f18fff4fc586464720b2bcef9de9aac10ee65}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 26 19:01:52.449: INFO: Pod "nginx-deployment-6f478d8d8-m48hc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-m48hc,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-2028,SelfLink:/api/v1/namespaces/deployment-2028/pods/nginx-deployment-6f478d8d8-m48hc,UID:4a257cb3-5034-11e9-b1ea-02429e4bb871,ResourceVersion:4513,Generation:0,CreationTimestamp:2019-03-26 19:01:52 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 46884322-5034-11e9-b1ea-02429e4bb871 0xc002b875b0 0xc002b875b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cjb6l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cjb6l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cjb6l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b87610} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b87630}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:52 -0700 PDT  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:52 -0700 PDT ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:52 -0700 PDT ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:52 -0700 PDT  }],Message:,Reason:,HostIP:192.168.9.2,PodIP:,StartTime:2019-03-26 19:01:52 -0700 PDT,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 26 19:01:52.450: INFO: Pod "nginx-deployment-6f478d8d8-m4mtj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-m4mtj,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-2028,SelfLink:/api/v1/namespaces/deployment-2028/pods/nginx-deployment-6f478d8d8-m4mtj,UID:4a26a7fa-5034-11e9-b1ea-02429e4bb871,ResourceVersion:4484,Generation:0,CreationTimestamp:2019-03-26 19:01:52 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 46884322-5034-11e9-b1ea-02429e4bb871 0xc002b876f0 0xc002b876f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cjb6l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cjb6l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cjb6l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b87750} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b87770}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:52 -0700 PDT  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 26 19:01:52.450: INFO: Pod "nginx-deployment-6f478d8d8-ppwxz" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-ppwxz,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-2028,SelfLink:/api/v1/namespaces/deployment-2028/pods/nginx-deployment-6f478d8d8-ppwxz,UID:468a36a0-5034-11e9-b1ea-02429e4bb871,ResourceVersion:4343,Generation:0,CreationTimestamp:2019-03-26 19:01:46 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 46884322-5034-11e9-b1ea-02429e4bb871 0xc002b877f0 0xc002b877f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cjb6l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cjb6l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cjb6l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b87850} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b87870}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:46 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:48 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:48 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:46 -0700 PDT  }],Message:,Reason:,HostIP:192.168.9.3,PodIP:10.32.0.4,StartTime:2019-03-26 19:01:46 -0700 PDT,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-26 19:01:48 -0700 PDT,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://cc70d2aaffa3eb0843658fca1816836cf32f2ca366a6d010949612789bec7a16}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 26 19:01:52.450: INFO: Pod "nginx-deployment-6f478d8d8-t85fd" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-t85fd,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-2028,SelfLink:/api/v1/namespaces/deployment-2028/pods/nginx-deployment-6f478d8d8-t85fd,UID:468a33ce-5034-11e9-b1ea-02429e4bb871,ResourceVersion:4341,Generation:0,CreationTimestamp:2019-03-26 19:01:46 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 46884322-5034-11e9-b1ea-02429e4bb871 0xc002b87940 0xc002b87941}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cjb6l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cjb6l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cjb6l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b879a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b879c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:46 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:48 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:48 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:46 -0700 PDT  }],Message:,Reason:,HostIP:192.168.9.2,PodIP:10.46.0.3,StartTime:2019-03-26 19:01:46 -0700 PDT,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-26 19:01:48 -0700 PDT,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://dffe2b6e46716732ea331951c079a532901081f3f5167ecaa9b55fcb184848ad}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 26 19:01:52.450: INFO: Pod "nginx-deployment-6f478d8d8-tgvvj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-tgvvj,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-2028,SelfLink:/api/v1/namespaces/deployment-2028/pods/nginx-deployment-6f478d8d8-tgvvj,UID:4a24c1ad-5034-11e9-b1ea-02429e4bb871,ResourceVersion:4502,Generation:0,CreationTimestamp:2019-03-26 19:01:52 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 46884322-5034-11e9-b1ea-02429e4bb871 0xc002b87a90 0xc002b87a91}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cjb6l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cjb6l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cjb6l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b87af0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b87b10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:52 -0700 PDT  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:52 -0700 PDT ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:52 -0700 PDT ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:52 -0700 PDT  }],Message:,Reason:,HostIP:192.168.9.3,PodIP:,StartTime:2019-03-26 19:01:52 -0700 PDT,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 26 19:01:52.450: INFO: Pod "nginx-deployment-6f478d8d8-w7wpx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-w7wpx,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-2028,SelfLink:/api/v1/namespaces/deployment-2028/pods/nginx-deployment-6f478d8d8-w7wpx,UID:4a28fc70-5034-11e9-b1ea-02429e4bb871,ResourceVersion:4506,Generation:0,CreationTimestamp:2019-03-26 19:01:52 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 46884322-5034-11e9-b1ea-02429e4bb871 0xc002b87bd0 0xc002b87bd1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cjb6l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cjb6l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cjb6l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b87c30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b87c50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:52 -0700 PDT  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 26 19:01:52.450: INFO: Pod "nginx-deployment-6f478d8d8-xb4kg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-xb4kg,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-2028,SelfLink:/api/v1/namespaces/deployment-2028/pods/nginx-deployment-6f478d8d8-xb4kg,UID:4a28d622-5034-11e9-b1ea-02429e4bb871,ResourceVersion:4503,Generation:0,CreationTimestamp:2019-03-26 19:01:52 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 46884322-5034-11e9-b1ea-02429e4bb871 0xc002b87cd0 0xc002b87cd1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cjb6l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cjb6l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cjb6l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b87d30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b87d50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:52 -0700 PDT  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 26 19:01:52.450: INFO: Pod "nginx-deployment-6f478d8d8-xptjg" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-xptjg,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-2028,SelfLink:/api/v1/namespaces/deployment-2028/pods/nginx-deployment-6f478d8d8-xptjg,UID:468a37d6-5034-11e9-b1ea-02429e4bb871,ResourceVersion:4348,Generation:0,CreationTimestamp:2019-03-26 19:01:46 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 46884322-5034-11e9-b1ea-02429e4bb871 0xc002b87dd0 0xc002b87dd1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cjb6l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cjb6l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cjb6l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b87e30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b87e50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:46 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:48 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:48 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:46 -0700 PDT  }],Message:,Reason:,HostIP:192.168.9.2,PodIP:10.46.0.1,StartTime:2019-03-26 19:01:46 -0700 PDT,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-26 19:01:48 -0700 PDT,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://e24c76f669f0d3acf347d697c33863470a5edb9eaf76ffd768a510495a6cd5e1}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 26 19:01:52.450: INFO: Pod "nginx-deployment-6f478d8d8-zqmbj" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-zqmbj,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-2028,SelfLink:/api/v1/namespaces/deployment-2028/pods/nginx-deployment-6f478d8d8-zqmbj,UID:46896de8-5034-11e9-b1ea-02429e4bb871,ResourceVersion:4363,Generation:0,CreationTimestamp:2019-03-26 19:01:46 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 46884322-5034-11e9-b1ea-02429e4bb871 0xc002b87f20 0xc002b87f21}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cjb6l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cjb6l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cjb6l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b87f80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b87fa0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:46 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:49 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:49 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:46 -0700 PDT  }],Message:,Reason:,HostIP:192.168.9.2,PodIP:10.46.0.2,StartTime:2019-03-26 19:01:46 -0700 PDT,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-26 19:01:49 -0700 PDT,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://ee30db5fa766fe549d0285d6b3eeaf496890114be70dba20ac4d52cd5ad262ab}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 26 19:01:52.450: INFO: Pod "nginx-deployment-6f478d8d8-zvm4d" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-zvm4d,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-2028,SelfLink:/api/v1/namespaces/deployment-2028/pods/nginx-deployment-6f478d8d8-zvm4d,UID:4a26919e-5034-11e9-b1ea-02429e4bb871,ResourceVersion:4485,Generation:0,CreationTimestamp:2019-03-26 19:01:52 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 46884322-5034-11e9-b1ea-02429e4bb871 0xc00279e070 0xc00279e071}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cjb6l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cjb6l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cjb6l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00279e0d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00279e0f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:01:52 -0700 PDT  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:01:52.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "deployment-2028" for this suite.
Mar 26 19:01:58.487: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:01:58.559: INFO: namespace deployment-2028 deletion completed in 6.090792793s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected downwardAPI[0m 
  [1mshould provide container's memory request [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Projected downwardAPI
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:01:58.559: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar 26 19:01:58.600: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4dd66424-5034-11e9-9719-a08cfdecc127" in namespace "projected-1155" to be "success or failure"
Mar 26 19:01:58.604: INFO: Pod "downwardapi-volume-4dd66424-5034-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 4.371259ms
Mar 26 19:02:00.607: INFO: Pod "downwardapi-volume-4dd66424-5034-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007018262s
Mar 26 19:02:02.609: INFO: Pod "downwardapi-volume-4dd66424-5034-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009518026s
Mar 26 19:02:04.613: INFO: Pod "downwardapi-volume-4dd66424-5034-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013319461s
[1mSTEP[0m: Saw pod success
Mar 26 19:02:04.613: INFO: Pod "downwardapi-volume-4dd66424-5034-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 19:02:04.615: INFO: Trying to get logs from node conformance-worker2 pod downwardapi-volume-4dd66424-5034-11e9-9719-a08cfdecc127 container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar 26 19:02:04.630: INFO: Waiting for pod downwardapi-volume-4dd66424-5034-11e9-9719-a08cfdecc127 to disappear
Mar 26 19:02:04.632: INFO: Pod downwardapi-volume-4dd66424-5034-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:02:04.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-1155" for this suite.
Mar 26 19:02:10.644: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:02:10.709: INFO: namespace projected-1155 deletion completed in 6.074375262s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-node] Downward API[0m 
  [1mshould provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-node] Downward API
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:02:10.709: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename downward-api
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating a pod to test downward api env vars
Mar 26 19:02:10.740: INFO: Waiting up to 5m0s for pod "downward-api-551356d2-5034-11e9-9719-a08cfdecc127" in namespace "downward-api-3671" to be "success or failure"
Mar 26 19:02:10.743: INFO: Pod "downward-api-551356d2-5034-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 3.09524ms
Mar 26 19:02:12.747: INFO: Pod "downward-api-551356d2-5034-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007122585s
Mar 26 19:02:14.751: INFO: Pod "downward-api-551356d2-5034-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011087463s
[1mSTEP[0m: Saw pod success
Mar 26 19:02:14.751: INFO: Pod "downward-api-551356d2-5034-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 19:02:14.754: INFO: Trying to get logs from node conformance-worker pod downward-api-551356d2-5034-11e9-9719-a08cfdecc127 container dapi-container: <nil>
[1mSTEP[0m: delete the pod
Mar 26 19:02:14.776: INFO: Waiting for pod downward-api-551356d2-5034-11e9-9719-a08cfdecc127 to disappear
Mar 26 19:02:14.778: INFO: Pod downward-api-551356d2-5034-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-node] Downward API
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:02:14.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "downward-api-3671" for this suite.
Mar 26 19:02:20.790: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:02:20.856: INFO: namespace downward-api-3671 deletion completed in 6.076199536s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] ReplicationController[0m 
  [1mshould release no longer matching pods [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-apps] ReplicationController
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:02:20.857: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename replication-controller
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Given a ReplicationController is created
[1mSTEP[0m: When the matched label of one of its pods change
Mar 26 19:02:20.886: INFO: Pod name pod-release: Found 0 pods out of 1
Mar 26 19:02:25.889: INFO: Pod name pod-release: Found 1 pods out of 1
[1mSTEP[0m: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:02:25.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "replication-controller-4920" for this suite.
Mar 26 19:02:31.922: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:02:31.989: INFO: namespace replication-controller-4920 deletion completed in 6.078585093s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] EmptyDir volumes[0m 
  [1mshould support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] EmptyDir volumes
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:02:31.990: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename emptydir
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating a pod to test emptydir 0777 on tmpfs
Mar 26 19:02:32.019: INFO: Waiting up to 5m0s for pod "pod-61c25a0f-5034-11e9-9719-a08cfdecc127" in namespace "emptydir-7750" to be "success or failure"
Mar 26 19:02:32.021: INFO: Pod "pod-61c25a0f-5034-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 1.810171ms
Mar 26 19:02:34.025: INFO: Pod "pod-61c25a0f-5034-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005739213s
Mar 26 19:02:36.029: INFO: Pod "pod-61c25a0f-5034-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009738437s
[1mSTEP[0m: Saw pod success
Mar 26 19:02:36.029: INFO: Pod "pod-61c25a0f-5034-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 19:02:36.032: INFO: Trying to get logs from node conformance-worker2 pod pod-61c25a0f-5034-11e9-9719-a08cfdecc127 container test-container: <nil>
[1mSTEP[0m: delete the pod
Mar 26 19:02:36.046: INFO: Waiting for pod pod-61c25a0f-5034-11e9-9719-a08cfdecc127 to disappear
Mar 26 19:02:36.048: INFO: Pod pod-61c25a0f-5034-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:02:36.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "emptydir-7750" for this suite.
Mar 26 19:02:42.059: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:02:42.113: INFO: namespace emptydir-7750 deletion completed in 6.063163848s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Subpath[0m [90mAtomic writer volumes[0m 
  [1mshould support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Subpath
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:02:42.113: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename subpath
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
[1mSTEP[0m: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating pod pod-subpath-test-configmap-qv8b
[1mSTEP[0m: Creating a pod to test atomic-volume-subpath
Mar 26 19:02:42.167: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-qv8b" in namespace "subpath-6707" to be "success or failure"
Mar 26 19:02:42.171: INFO: Pod "pod-subpath-test-configmap-qv8b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.735547ms
Mar 26 19:02:44.175: INFO: Pod "pod-subpath-test-configmap-qv8b": Phase="Running", Reason="", readiness=true. Elapsed: 2.0078579s
Mar 26 19:02:46.179: INFO: Pod "pod-subpath-test-configmap-qv8b": Phase="Running", Reason="", readiness=true. Elapsed: 4.011587632s
Mar 26 19:02:48.183: INFO: Pod "pod-subpath-test-configmap-qv8b": Phase="Running", Reason="", readiness=true. Elapsed: 6.015525603s
Mar 26 19:02:50.187: INFO: Pod "pod-subpath-test-configmap-qv8b": Phase="Running", Reason="", readiness=true. Elapsed: 8.019417365s
Mar 26 19:02:52.190: INFO: Pod "pod-subpath-test-configmap-qv8b": Phase="Running", Reason="", readiness=true. Elapsed: 10.023275863s
Mar 26 19:02:54.194: INFO: Pod "pod-subpath-test-configmap-qv8b": Phase="Running", Reason="", readiness=true. Elapsed: 12.026681765s
Mar 26 19:02:56.198: INFO: Pod "pod-subpath-test-configmap-qv8b": Phase="Running", Reason="", readiness=true. Elapsed: 14.030587648s
Mar 26 19:02:58.201: INFO: Pod "pod-subpath-test-configmap-qv8b": Phase="Running", Reason="", readiness=true. Elapsed: 16.033959311s
Mar 26 19:03:00.205: INFO: Pod "pod-subpath-test-configmap-qv8b": Phase="Running", Reason="", readiness=true. Elapsed: 18.037985277s
Mar 26 19:03:02.209: INFO: Pod "pod-subpath-test-configmap-qv8b": Phase="Running", Reason="", readiness=true. Elapsed: 20.041894119s
Mar 26 19:03:04.213: INFO: Pod "pod-subpath-test-configmap-qv8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.045883303s
[1mSTEP[0m: Saw pod success
Mar 26 19:03:04.213: INFO: Pod "pod-subpath-test-configmap-qv8b" satisfied condition "success or failure"
Mar 26 19:03:04.216: INFO: Trying to get logs from node conformance-worker pod pod-subpath-test-configmap-qv8b container test-container-subpath-configmap-qv8b: <nil>
[1mSTEP[0m: delete the pod
Mar 26 19:03:04.234: INFO: Waiting for pod pod-subpath-test-configmap-qv8b to disappear
Mar 26 19:03:04.236: INFO: Pod pod-subpath-test-configmap-qv8b no longer exists
[1mSTEP[0m: Deleting pod pod-subpath-test-configmap-qv8b
Mar 26 19:03:04.236: INFO: Deleting pod "pod-subpath-test-configmap-qv8b" in namespace "subpath-6707"
[AfterEach] [sig-storage] Subpath
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:03:04.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "subpath-6707" for this suite.
Mar 26 19:03:10.251: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:03:10.329: INFO: namespace subpath-6707 deletion completed in 6.086427749s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90m[k8s.io] Kubectl replace[0m 
  [1mshould update a single-container pod's image  [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:03:10.330: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl replace
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1619
[It] should update a single-container pod's image  [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: running the image docker.io/library/nginx:1.14-alpine
Mar 26 19:03:10.355: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-1289'
Mar 26 19:03:10.442: INFO: stderr: ""
Mar 26 19:03:10.442: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
[1mSTEP[0m: verifying the pod e2e-test-nginx-pod is running
[1mSTEP[0m: verifying the pod e2e-test-nginx-pod was created
Mar 26 19:03:15.492: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get pod e2e-test-nginx-pod --namespace=kubectl-1289 -o json'
Mar 26 19:03:15.567: INFO: stderr: ""
Mar 26 19:03:15.567: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2019-03-27T02:03:10Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-1289\",\n        \"resourceVersion\": \"5055\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-1289/pods/e2e-test-nginx-pod\",\n        \"uid\": \"78a5ce1c-5034-11e9-b1ea-02429e4bb871\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-8pmvs\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"conformance-worker\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-8pmvs\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-8pmvs\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-03-27T02:03:10Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-03-27T02:03:12Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-03-27T02:03:12Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-03-27T02:03:10Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://bdd11d85155d6f94ac701046c1bb2ca7f64b7b8ae5f10f326c6a7431912779ae\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-03-27T02:03:11Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.9.3\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.32.0.2\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-03-27T02:03:10Z\"\n    }\n}\n"
[1mSTEP[0m: replace the image in the pod
Mar 26 19:03:15.568: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance replace -f - --namespace=kubectl-1289'
Mar 26 19:03:15.721: INFO: stderr: ""
Mar 26 19:03:15.721: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
[1mSTEP[0m: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1624
Mar 26 19:03:15.723: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance delete pods e2e-test-nginx-pod --namespace=kubectl-1289'
Mar 26 19:03:17.938: INFO: stderr: ""
Mar 26 19:03:17.938: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:03:17.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-1289" for this suite.
Mar 26 19:03:23.948: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:03:24.014: INFO: namespace kubectl-1289 deletion completed in 6.074106099s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Downward API volume[0m 
  [1mshould provide container's cpu request [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Downward API volume
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:03:24.015: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename downward-api
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar 26 19:03:24.047: INFO: Waiting up to 5m0s for pod "downwardapi-volume-80c52868-5034-11e9-9719-a08cfdecc127" in namespace "downward-api-3536" to be "success or failure"
Mar 26 19:03:24.053: INFO: Pod "downwardapi-volume-80c52868-5034-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 5.750693ms
Mar 26 19:03:26.056: INFO: Pod "downwardapi-volume-80c52868-5034-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009112146s
[1mSTEP[0m: Saw pod success
Mar 26 19:03:26.056: INFO: Pod "downwardapi-volume-80c52868-5034-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 19:03:26.059: INFO: Trying to get logs from node conformance-worker pod downwardapi-volume-80c52868-5034-11e9-9719-a08cfdecc127 container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar 26 19:03:26.076: INFO: Waiting for pod downwardapi-volume-80c52868-5034-11e9-9719-a08cfdecc127 to disappear
Mar 26 19:03:26.078: INFO: Pod downwardapi-volume-80c52868-5034-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:03:26.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "downward-api-3536" for this suite.
Mar 26 19:03:32.090: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:03:32.159: INFO: namespace downward-api-3536 deletion completed in 6.078784052s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90m[k8s.io] Kubectl run --rm job[0m 
  [1mshould create a job from an image, then delete the job  [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:03:32.159: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create a job from an image, then delete the job  [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: executing a command with run --rm and attach with stdin
Mar 26 19:03:32.189: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance --namespace=kubectl-6566 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Mar 26 19:03:34.290: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Mar 26 19:03:34.290: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
[1mSTEP[0m: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:03:36.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-6566" for this suite.
Mar 26 19:03:42.311: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:03:42.378: INFO: namespace kubectl-6566 deletion completed in 6.076525969s
[32mâ€¢[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Docker Containers[0m 
  [1mshould use the image defaults if command and args are blank [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [k8s.io] Docker Containers
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:03:42.378: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename containers
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating a pod to test use defaults
Mar 26 19:03:42.413: INFO: Waiting up to 5m0s for pod "client-containers-8bb6b14d-5034-11e9-9719-a08cfdecc127" in namespace "containers-9377" to be "success or failure"
Mar 26 19:03:42.419: INFO: Pod "client-containers-8bb6b14d-5034-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 5.72541ms
Mar 26 19:03:44.423: INFO: Pod "client-containers-8bb6b14d-5034-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009855408s
Mar 26 19:03:46.427: INFO: Pod "client-containers-8bb6b14d-5034-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013834835s
[1mSTEP[0m: Saw pod success
Mar 26 19:03:46.427: INFO: Pod "client-containers-8bb6b14d-5034-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 19:03:46.430: INFO: Trying to get logs from node conformance-worker2 pod client-containers-8bb6b14d-5034-11e9-9719-a08cfdecc127 container test-container: <nil>
[1mSTEP[0m: delete the pod
Mar 26 19:03:46.448: INFO: Waiting for pod client-containers-8bb6b14d-5034-11e9-9719-a08cfdecc127 to disappear
Mar 26 19:03:46.450: INFO: Pod client-containers-8bb6b14d-5034-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:03:46.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "containers-9377" for this suite.
Mar 26 19:03:52.462: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:03:52.517: INFO: namespace containers-9377 deletion completed in 6.064865583s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-node] Downward API[0m 
  [1mshould provide pod UID as env vars [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-node] Downward API
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:03:52.517: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename downward-api
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating a pod to test downward api env vars
Mar 26 19:03:52.547: INFO: Waiting up to 5m0s for pod "downward-api-91c1eff9-5034-11e9-9719-a08cfdecc127" in namespace "downward-api-1629" to be "success or failure"
Mar 26 19:03:52.550: INFO: Pod "downward-api-91c1eff9-5034-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 3.550817ms
Mar 26 19:03:54.554: INFO: Pod "downward-api-91c1eff9-5034-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007531182s
[1mSTEP[0m: Saw pod success
Mar 26 19:03:54.554: INFO: Pod "downward-api-91c1eff9-5034-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 19:03:54.557: INFO: Trying to get logs from node conformance-worker pod downward-api-91c1eff9-5034-11e9-9719-a08cfdecc127 container dapi-container: <nil>
[1mSTEP[0m: delete the pod
Mar 26 19:03:54.579: INFO: Waiting for pod downward-api-91c1eff9-5034-11e9-9719-a08cfdecc127 to disappear
Mar 26 19:03:54.581: INFO: Pod downward-api-91c1eff9-5034-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-node] Downward API
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:03:54.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "downward-api-1629" for this suite.
Mar 26 19:04:00.592: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:04:00.647: INFO: namespace downward-api-1629 deletion completed in 6.063155303s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected configMap[0m 
  [1mshould be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Projected configMap
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:04:00.647: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating configMap with name projected-configmap-test-volume-969a8b27-5034-11e9-9719-a08cfdecc127
[1mSTEP[0m: Creating a pod to test consume configMaps
Mar 26 19:04:00.684: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-969b7e72-5034-11e9-9719-a08cfdecc127" in namespace "projected-2090" to be "success or failure"
Mar 26 19:04:00.686: INFO: Pod "pod-projected-configmaps-969b7e72-5034-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 1.930946ms
Mar 26 19:04:02.689: INFO: Pod "pod-projected-configmaps-969b7e72-5034-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005244946s
[1mSTEP[0m: Saw pod success
Mar 26 19:04:02.689: INFO: Pod "pod-projected-configmaps-969b7e72-5034-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 19:04:02.691: INFO: Trying to get logs from node conformance-worker pod pod-projected-configmaps-969b7e72-5034-11e9-9719-a08cfdecc127 container projected-configmap-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar 26 19:04:02.708: INFO: Waiting for pod pod-projected-configmaps-969b7e72-5034-11e9-9719-a08cfdecc127 to disappear
Mar 26 19:04:02.711: INFO: Pod pod-projected-configmaps-969b7e72-5034-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:04:02.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-2090" for this suite.
Mar 26 19:04:08.723: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:04:08.789: INFO: namespace projected-2090 deletion completed in 6.07493592s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] EmptyDir wrapper volumes[0m 
  [1mshould not cause race condition when used for configmaps [Serial] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:04:08.790: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename emptydir-wrapper
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating 50 configmaps
[1mSTEP[0m: Creating RC which spawns configmap-volume pods
Mar 26 19:04:09.063: INFO: Pod name wrapped-volume-race-9b85697c-5034-11e9-9719-a08cfdecc127: Found 5 pods out of 5
[1mSTEP[0m: Ensuring each pod is running
[1mSTEP[0m: deleting ReplicationController wrapped-volume-race-9b85697c-5034-11e9-9719-a08cfdecc127 in namespace emptydir-wrapper-8448, will wait for the garbage collector to delete the pods
Mar 26 19:04:25.182: INFO: Deleting ReplicationController wrapped-volume-race-9b85697c-5034-11e9-9719-a08cfdecc127 took: 7.071893ms
Mar 26 19:04:25.483: INFO: Terminating ReplicationController wrapped-volume-race-9b85697c-5034-11e9-9719-a08cfdecc127 pods took: 300.236896ms
[1mSTEP[0m: Creating RC which spawns configmap-volume pods
Mar 26 19:05:06.598: INFO: Pod name wrapped-volume-race-bde37238-5034-11e9-9719-a08cfdecc127: Found 0 pods out of 5
Mar 26 19:05:11.605: INFO: Pod name wrapped-volume-race-bde37238-5034-11e9-9719-a08cfdecc127: Found 5 pods out of 5
[1mSTEP[0m: Ensuring each pod is running
[1mSTEP[0m: deleting ReplicationController wrapped-volume-race-bde37238-5034-11e9-9719-a08cfdecc127 in namespace emptydir-wrapper-8448, will wait for the garbage collector to delete the pods
Mar 26 19:05:21.701: INFO: Deleting ReplicationController wrapped-volume-race-bde37238-5034-11e9-9719-a08cfdecc127 took: 6.626086ms
Mar 26 19:05:22.001: INFO: Terminating ReplicationController wrapped-volume-race-bde37238-5034-11e9-9719-a08cfdecc127 pods took: 300.270339ms
[1mSTEP[0m: Creating RC which spawns configmap-volume pods
Mar 26 19:06:05.610: INFO: Pod name wrapped-volume-race-e110fb88-5034-11e9-9719-a08cfdecc127: Found 0 pods out of 5
Mar 26 19:06:10.617: INFO: Pod name wrapped-volume-race-e110fb88-5034-11e9-9719-a08cfdecc127: Found 5 pods out of 5
[1mSTEP[0m: Ensuring each pod is running
[1mSTEP[0m: deleting ReplicationController wrapped-volume-race-e110fb88-5034-11e9-9719-a08cfdecc127 in namespace emptydir-wrapper-8448, will wait for the garbage collector to delete the pods
Mar 26 19:06:20.695: INFO: Deleting ReplicationController wrapped-volume-race-e110fb88-5034-11e9-9719-a08cfdecc127 took: 5.773791ms
Mar 26 19:06:20.995: INFO: Terminating ReplicationController wrapped-volume-race-e110fb88-5034-11e9-9719-a08cfdecc127 pods took: 300.217072ms
[1mSTEP[0m: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:07:05.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "emptydir-wrapper-8448" for this suite.
Mar 26 19:07:11.873: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:07:11.938: INFO: namespace emptydir-wrapper-8448 deletion completed in 6.071727339s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected downwardAPI[0m 
  [1mshould update annotations on modification [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Projected downwardAPI
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:07:11.938: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating the pod
Mar 26 19:07:14.498: INFO: Successfully updated pod "annotationupdate089edc22-5035-11e9-9719-a08cfdecc127"
[AfterEach] [sig-storage] Projected downwardAPI
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:07:16.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-1631" for this suite.
Mar 26 19:07:38.532: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:07:38.597: INFO: namespace projected-1631 deletion completed in 22.074741378s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90m[k8s.io] Kubectl api-versions[0m 
  [1mshould check if v1 is in available api versions  [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:07:38.597: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if v1 is in available api versions  [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: validating api versions
Mar 26 19:07:38.621: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance api-versions'
Mar 26 19:07:38.694: INFO: stderr: ""
Mar 26 19:07:38.694: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:07:38.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-2179" for this suite.
Mar 26 19:07:44.707: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:07:44.773: INFO: namespace kubectl-2179 deletion completed in 6.074955989s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] InitContainer [NodeConformance][0m 
  [1mshould not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:07:44.773: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename init-container
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: creating the pod
Mar 26 19:07:44.806: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:07:53.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "init-container-7230" for this suite.
Mar 26 19:07:59.836: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:07:59.898: INFO: namespace init-container-7230 deletion completed in 6.06974922s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] EmptyDir volumes[0m 
  [1mshould support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] EmptyDir volumes
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:07:59.898: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename emptydir
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating a pod to test emptydir 0777 on node default medium
Mar 26 19:07:59.928: INFO: Waiting up to 5m0s for pod "pod-25353584-5035-11e9-9719-a08cfdecc127" in namespace "emptydir-2948" to be "success or failure"
Mar 26 19:07:59.935: INFO: Pod "pod-25353584-5035-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 6.50591ms
Mar 26 19:08:01.939: INFO: Pod "pod-25353584-5035-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010407484s
Mar 26 19:08:03.943: INFO: Pod "pod-25353584-5035-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014035933s
[1mSTEP[0m: Saw pod success
Mar 26 19:08:03.943: INFO: Pod "pod-25353584-5035-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 19:08:03.946: INFO: Trying to get logs from node conformance-worker2 pod pod-25353584-5035-11e9-9719-a08cfdecc127 container test-container: <nil>
[1mSTEP[0m: delete the pod
Mar 26 19:08:03.966: INFO: Waiting for pod pod-25353584-5035-11e9-9719-a08cfdecc127 to disappear
Mar 26 19:08:03.968: INFO: Pod pod-25353584-5035-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:08:03.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "emptydir-2948" for this suite.
Mar 26 19:08:09.977: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:08:10.044: INFO: namespace emptydir-2948 deletion completed in 6.073869015s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Probing container[0m 
  [1mshould be restarted with a /healthz http liveness probe [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [k8s.io] Probing container
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:08:10.044: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename container-probe
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating pod liveness-http in namespace container-probe-1268
Mar 26 19:08:14.081: INFO: Started pod liveness-http in namespace container-probe-1268
[1mSTEP[0m: checking the pod's current state and verifying that restartCount is present
Mar 26 19:08:14.084: INFO: Initial restart count of pod liveness-http is 0
Mar 26 19:08:30.118: INFO: Restart count of pod container-probe-1268/liveness-http is now 1 (16.033961094s elapsed)
[1mSTEP[0m: deleting the pod
[AfterEach] [k8s.io] Probing container
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:08:30.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "container-probe-1268" for this suite.
Mar 26 19:08:36.142: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:08:36.210: INFO: namespace container-probe-1268 deletion completed in 6.078083876s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Variable Expansion[0m 
  [1mshould allow substituting values in a container's command [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [k8s.io] Variable Expansion
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:08:36.210: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename var-expansion
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating a pod to test substitution in container's command
Mar 26 19:08:36.243: INFO: Waiting up to 5m0s for pod "var-expansion-3ada6efa-5035-11e9-9719-a08cfdecc127" in namespace "var-expansion-2144" to be "success or failure"
Mar 26 19:08:36.244: INFO: Pod "var-expansion-3ada6efa-5035-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 1.548458ms
Mar 26 19:08:38.248: INFO: Pod "var-expansion-3ada6efa-5035-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005310538s
[1mSTEP[0m: Saw pod success
Mar 26 19:08:38.248: INFO: Pod "var-expansion-3ada6efa-5035-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 19:08:38.251: INFO: Trying to get logs from node conformance-worker2 pod var-expansion-3ada6efa-5035-11e9-9719-a08cfdecc127 container dapi-container: <nil>
[1mSTEP[0m: delete the pod
Mar 26 19:08:38.266: INFO: Waiting for pod var-expansion-3ada6efa-5035-11e9-9719-a08cfdecc127 to disappear
Mar 26 19:08:38.268: INFO: Pod var-expansion-3ada6efa-5035-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:08:38.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "var-expansion-2144" for this suite.
Mar 26 19:08:44.283: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:08:44.355: INFO: namespace var-expansion-2144 deletion completed in 6.08105831s
[32mâ€¢[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] StatefulSet[0m [90m[k8s.io] Basic StatefulSet functionality [StatefulSetBasic][0m 
  [1mScaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-apps] StatefulSet
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:08:44.355: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename statefulset
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
[1mSTEP[0m: Creating service test in namespace statefulset-3008
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Initializing watcher for selector baz=blah,foo=bar
[1mSTEP[0m: Creating stateful set ss in namespace statefulset-3008
[1mSTEP[0m: Waiting until all stateful set ss replicas will be running in namespace statefulset-3008
Mar 26 19:08:44.391: INFO: Found 0 stateful pods, waiting for 1
Mar 26 19:08:54.395: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
[1mSTEP[0m: Confirming that stateful set scale up will halt with unhealthy stateful pod
Mar 26 19:08:54.398: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-3008 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar 26 19:08:54.669: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Mar 26 19:08:54.669: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar 26 19:08:54.669: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar 26 19:08:54.672: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 26 19:09:04.676: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 26 19:09:04.676: INFO: Waiting for statefulset status.replicas updated to 0
Mar 26 19:09:04.687: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999591s
Mar 26 19:09:05.691: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997329219s
Mar 26 19:09:06.695: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.9930722s
Mar 26 19:09:07.700: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.989044484s
Mar 26 19:09:08.707: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.984846064s
Mar 26 19:09:09.710: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.977877505s
Mar 26 19:09:10.714: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.97444124s
Mar 26 19:09:11.718: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.970002837s
Mar 26 19:09:12.723: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.966018495s
Mar 26 19:09:13.727: INFO: Verifying statefulset ss doesn't scale past 1 for another 961.857555ms
[1mSTEP[0m: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3008
Mar 26 19:09:14.731: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-3008 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 26 19:09:15.004: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Mar 26 19:09:15.004: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar 26 19:09:15.004: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar 26 19:09:15.007: INFO: Found 1 stateful pods, waiting for 3
Mar 26 19:09:25.012: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 26 19:09:25.012: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 26 19:09:25.012: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
[1mSTEP[0m: Verifying that stateful set ss was scaled up in order
[1mSTEP[0m: Scale down will halt with unhealthy stateful pod
Mar 26 19:09:25.018: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-3008 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar 26 19:09:25.291: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Mar 26 19:09:25.291: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar 26 19:09:25.291: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar 26 19:09:25.292: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-3008 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar 26 19:09:25.543: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Mar 26 19:09:25.543: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar 26 19:09:25.543: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar 26 19:09:25.543: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-3008 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar 26 19:09:25.799: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Mar 26 19:09:25.799: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar 26 19:09:25.799: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar 26 19:09:25.799: INFO: Waiting for statefulset status.replicas updated to 0
Mar 26 19:09:25.802: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Mar 26 19:09:35.811: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 26 19:09:35.811: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 26 19:09:35.811: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 26 19:09:35.821: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999774s
Mar 26 19:09:36.824: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997478171s
Mar 26 19:09:37.828: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993971329s
Mar 26 19:09:38.833: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.989982392s
Mar 26 19:09:39.838: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.98536523s
Mar 26 19:09:40.842: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.980923664s
Mar 26 19:09:41.846: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.976468147s
Mar 26 19:09:42.852: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.97191712s
Mar 26 19:09:43.856: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.966475737s
Mar 26 19:09:44.861: INFO: Verifying statefulset ss doesn't scale past 3 for another 962.207376ms
[1mSTEP[0m: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3008
Mar 26 19:09:45.864: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-3008 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 26 19:09:46.135: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Mar 26 19:09:46.135: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar 26 19:09:46.135: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar 26 19:09:46.135: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-3008 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 26 19:09:46.386: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Mar 26 19:09:46.386: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar 26 19:09:46.386: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar 26 19:09:46.386: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-3008 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 26 19:09:46.641: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Mar 26 19:09:46.641: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar 26 19:09:46.641: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar 26 19:09:46.641: INFO: Scaling statefulset ss to 0
[1mSTEP[0m: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Mar 26 19:10:06.655: INFO: Deleting all statefulset in ns statefulset-3008
Mar 26 19:10:06.658: INFO: Scaling statefulset ss to 0
Mar 26 19:10:06.667: INFO: Waiting for statefulset status.replicas updated to 0
Mar 26 19:10:06.669: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:10:06.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "statefulset-3008" for this suite.
Mar 26 19:10:12.691: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:10:12.761: INFO: namespace statefulset-3008 deletion completed in 6.080981551s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Pods[0m 
  [1mshould support remote command execution over websockets [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [k8s.io] Pods
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:10:12.762: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename pods
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Mar 26 19:10:12.787: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: creating the pod
[1mSTEP[0m: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:10:17.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "pods-4289" for this suite.
Mar 26 19:11:07.016: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:11:07.090: INFO: namespace pods-4289 deletion completed in 50.081524023s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] ReplicaSet[0m 
  [1mshould adopt matching pods on creation and release no longer matching pods [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-apps] ReplicaSet
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:11:07.090: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename replicaset
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Given a Pod with a 'name' label pod-adoption-release is created
[1mSTEP[0m: When a replicaset with a matching selector is created
[1mSTEP[0m: Then the orphan pod is adopted
[1mSTEP[0m: When the matched label of one of its pods change
Mar 26 19:11:12.142: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
[1mSTEP[0m: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:11:13.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "replicaset-8113" for this suite.
Mar 26 19:11:35.173: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:11:35.243: INFO: namespace replicaset-8113 deletion completed in 22.080669579s
[32mâ€¢[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90m[k8s.io] Update Demo[0m 
  [1mshould create and stop a replication controller  [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:11:35.243: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should create and stop a replication controller  [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: creating a replication controller
Mar 26 19:11:35.271: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance create -f - --namespace=kubectl-1770'
Mar 26 19:11:35.739: INFO: stderr: ""
Mar 26 19:11:35.739: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
[1mSTEP[0m: waiting for all containers in name=update-demo pods to come up.
Mar 26 19:11:35.739: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1770'
Mar 26 19:11:35.811: INFO: stderr: ""
Mar 26 19:11:35.811: INFO: stdout: "update-demo-nautilus-4qs4t update-demo-nautilus-swpgb "
Mar 26 19:11:35.811: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get pods update-demo-nautilus-4qs4t -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1770'
Mar 26 19:11:35.879: INFO: stderr: ""
Mar 26 19:11:35.879: INFO: stdout: ""
Mar 26 19:11:35.879: INFO: update-demo-nautilus-4qs4t is created but not running
Mar 26 19:11:40.879: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1770'
Mar 26 19:11:40.956: INFO: stderr: ""
Mar 26 19:11:40.956: INFO: stdout: "update-demo-nautilus-4qs4t update-demo-nautilus-swpgb "
Mar 26 19:11:40.956: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get pods update-demo-nautilus-4qs4t -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1770'
Mar 26 19:11:41.030: INFO: stderr: ""
Mar 26 19:11:41.030: INFO: stdout: "true"
Mar 26 19:11:41.030: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get pods update-demo-nautilus-4qs4t -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1770'
Mar 26 19:11:41.100: INFO: stderr: ""
Mar 26 19:11:41.100: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 26 19:11:41.100: INFO: validating pod update-demo-nautilus-4qs4t
Mar 26 19:11:41.107: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 26 19:11:41.107: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 26 19:11:41.107: INFO: update-demo-nautilus-4qs4t is verified up and running
Mar 26 19:11:41.107: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get pods update-demo-nautilus-swpgb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1770'
Mar 26 19:11:41.178: INFO: stderr: ""
Mar 26 19:11:41.178: INFO: stdout: "true"
Mar 26 19:11:41.178: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get pods update-demo-nautilus-swpgb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1770'
Mar 26 19:11:41.249: INFO: stderr: ""
Mar 26 19:11:41.249: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 26 19:11:41.249: INFO: validating pod update-demo-nautilus-swpgb
Mar 26 19:11:41.253: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 26 19:11:41.253: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 26 19:11:41.253: INFO: update-demo-nautilus-swpgb is verified up and running
[1mSTEP[0m: using delete to clean up resources
Mar 26 19:11:41.253: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance delete --grace-period=0 --force -f - --namespace=kubectl-1770'
Mar 26 19:11:41.340: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 26 19:11:41.340: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 26 19:11:41.340: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1770'
Mar 26 19:11:41.415: INFO: stderr: "No resources found.\n"
Mar 26 19:11:41.415: INFO: stdout: ""
Mar 26 19:11:41.415: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get pods -l name=update-demo --namespace=kubectl-1770 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 26 19:11:41.488: INFO: stderr: ""
Mar 26 19:11:41.488: INFO: stdout: "update-demo-nautilus-4qs4t\nupdate-demo-nautilus-swpgb\n"
Mar 26 19:11:41.988: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1770'
Mar 26 19:11:42.066: INFO: stderr: "No resources found.\n"
Mar 26 19:11:42.066: INFO: stdout: ""
Mar 26 19:11:42.066: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get pods -l name=update-demo --namespace=kubectl-1770 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 26 19:11:42.130: INFO: stderr: ""
Mar 26 19:11:42.130: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:11:42.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-1770" for this suite.
Mar 26 19:11:48.139: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:11:48.208: INFO: namespace kubectl-1770 deletion completed in 6.075512184s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Garbage collector[0m 
  [1mshould keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-api-machinery] Garbage collector
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:11:48.208: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename gc
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: create the rc
[1mSTEP[0m: delete the rc
[1mSTEP[0m: wait for the rc to be deleted
[1mSTEP[0m: Gathering metrics
W0326 19:11:54.272929  168179 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar 26 19:11:54.272: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:11:54.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "gc-2971" for this suite.
Mar 26 19:12:00.284: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:12:00.352: INFO: namespace gc-2971 deletion completed in 6.07641176s
[32mâ€¢[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Probing container[0m 
  [1mshould *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [k8s.io] Probing container
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:12:00.352: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename container-probe
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating pod liveness-http in namespace container-probe-2953
Mar 26 19:12:02.388: INFO: Started pod liveness-http in namespace container-probe-2953
[1mSTEP[0m: checking the pod's current state and verifying that restartCount is present
Mar 26 19:12:02.391: INFO: Initial restart count of pod liveness-http is 0
[1mSTEP[0m: deleting the pod
[AfterEach] [k8s.io] Probing container
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:16:02.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "container-probe-2953" for this suite.
Mar 26 19:16:08.888: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:16:08.955: INFO: namespace container-probe-2953 deletion completed in 6.074882199s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Pods[0m 
  [1mshould be submitted and removed [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [k8s.io] Pods
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:16:08.955: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename pods
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be submitted and removed [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: creating the pod
[1mSTEP[0m: setting up watch
[1mSTEP[0m: submitting the pod to kubernetes
Mar 26 19:16:09.006: INFO: observed the pod list
[1mSTEP[0m: verifying the pod is in kubernetes
[1mSTEP[0m: verifying pod creation was observed
[1mSTEP[0m: deleting the pod gracefully
[1mSTEP[0m: verifying the kubelet observed the termination notice
[1mSTEP[0m: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:16:25.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "pods-4819" for this suite.
Mar 26 19:16:31.552: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:16:31.602: INFO: namespace pods-4819 deletion completed in 6.057342852s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] EmptyDir volumes[0m 
  [1mshould support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] EmptyDir volumes
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:16:31.602: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename emptydir
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating a pod to test emptydir 0644 on tmpfs
Mar 26 19:16:31.634: INFO: Waiting up to 5m0s for pod "pod-56356356-5036-11e9-9719-a08cfdecc127" in namespace "emptydir-8467" to be "success or failure"
Mar 26 19:16:31.635: INFO: Pod "pod-56356356-5036-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 1.305838ms
Mar 26 19:16:33.638: INFO: Pod "pod-56356356-5036-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004876736s
Mar 26 19:16:35.642: INFO: Pod "pod-56356356-5036-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008243922s
[1mSTEP[0m: Saw pod success
Mar 26 19:16:35.642: INFO: Pod "pod-56356356-5036-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 19:16:35.645: INFO: Trying to get logs from node conformance-worker pod pod-56356356-5036-11e9-9719-a08cfdecc127 container test-container: <nil>
[1mSTEP[0m: delete the pod
Mar 26 19:16:35.663: INFO: Waiting for pod pod-56356356-5036-11e9-9719-a08cfdecc127 to disappear
Mar 26 19:16:35.665: INFO: Pod pod-56356356-5036-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:16:35.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "emptydir-8467" for this suite.
Mar 26 19:16:41.678: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:16:41.737: INFO: namespace emptydir-8467 deletion completed in 6.070139116s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected downwardAPI[0m 
  [1mshould provide container's memory limit [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Projected downwardAPI
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:16:41.738: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar 26 19:16:41.774: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5c4034d2-5036-11e9-9719-a08cfdecc127" in namespace "projected-9659" to be "success or failure"
Mar 26 19:16:41.781: INFO: Pod "downwardapi-volume-5c4034d2-5036-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 6.885248ms
Mar 26 19:16:43.784: INFO: Pod "downwardapi-volume-5c4034d2-5036-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010334607s
[1mSTEP[0m: Saw pod success
Mar 26 19:16:43.785: INFO: Pod "downwardapi-volume-5c4034d2-5036-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 19:16:43.788: INFO: Trying to get logs from node conformance-worker2 pod downwardapi-volume-5c4034d2-5036-11e9-9719-a08cfdecc127 container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar 26 19:16:43.806: INFO: Waiting for pod downwardapi-volume-5c4034d2-5036-11e9-9719-a08cfdecc127 to disappear
Mar 26 19:16:43.811: INFO: Pod downwardapi-volume-5c4034d2-5036-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:16:43.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-9659" for this suite.
Mar 26 19:16:49.822: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:16:49.893: INFO: namespace projected-9659 deletion completed in 6.079265953s
[32mâ€¢[0m
[90m------------------------------[0m
[0m[sig-apps] Daemon set [Serial][0m 
  [1mshould run and stop complex daemon [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-apps] Daemon set [Serial]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:16:49.893: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename daemonsets
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Mar 26 19:16:49.931: INFO: Creating daemon "daemon-set" with a node selector
[1mSTEP[0m: Initially, daemon pods should not be running on any nodes.
Mar 26 19:16:49.935: INFO: Number of nodes with available pods: 0
Mar 26 19:16:49.935: INFO: Number of running nodes: 0, number of available pods: 0
[1mSTEP[0m: Change node label to blue, check that daemon pod is launched.
Mar 26 19:16:49.945: INFO: Number of nodes with available pods: 0
Mar 26 19:16:49.945: INFO: Node conformance-worker is running more than one daemon pod
Mar 26 19:16:50.949: INFO: Number of nodes with available pods: 0
Mar 26 19:16:50.949: INFO: Node conformance-worker is running more than one daemon pod
Mar 26 19:16:51.949: INFO: Number of nodes with available pods: 0
Mar 26 19:16:51.949: INFO: Node conformance-worker is running more than one daemon pod
Mar 26 19:16:52.949: INFO: Number of nodes with available pods: 1
Mar 26 19:16:52.949: INFO: Number of running nodes: 1, number of available pods: 1
[1mSTEP[0m: Update the node label to green, and wait for daemons to be unscheduled
Mar 26 19:16:52.969: INFO: Number of nodes with available pods: 1
Mar 26 19:16:52.969: INFO: Number of running nodes: 0, number of available pods: 1
Mar 26 19:16:53.973: INFO: Number of nodes with available pods: 0
Mar 26 19:16:53.973: INFO: Number of running nodes: 0, number of available pods: 0
[1mSTEP[0m: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Mar 26 19:16:53.990: INFO: Number of nodes with available pods: 0
Mar 26 19:16:53.990: INFO: Node conformance-worker is running more than one daemon pod
Mar 26 19:16:54.994: INFO: Number of nodes with available pods: 0
Mar 26 19:16:54.994: INFO: Node conformance-worker is running more than one daemon pod
Mar 26 19:16:55.994: INFO: Number of nodes with available pods: 0
Mar 26 19:16:55.994: INFO: Node conformance-worker is running more than one daemon pod
Mar 26 19:16:56.994: INFO: Number of nodes with available pods: 0
Mar 26 19:16:56.994: INFO: Node conformance-worker is running more than one daemon pod
Mar 26 19:16:57.994: INFO: Number of nodes with available pods: 0
Mar 26 19:16:57.994: INFO: Node conformance-worker is running more than one daemon pod
Mar 26 19:16:58.994: INFO: Number of nodes with available pods: 0
Mar 26 19:16:58.994: INFO: Node conformance-worker is running more than one daemon pod
Mar 26 19:16:59.994: INFO: Number of nodes with available pods: 0
Mar 26 19:16:59.994: INFO: Node conformance-worker is running more than one daemon pod
Mar 26 19:17:00.994: INFO: Number of nodes with available pods: 0
Mar 26 19:17:00.994: INFO: Node conformance-worker is running more than one daemon pod
Mar 26 19:17:01.994: INFO: Number of nodes with available pods: 0
Mar 26 19:17:01.994: INFO: Node conformance-worker is running more than one daemon pod
Mar 26 19:17:02.994: INFO: Number of nodes with available pods: 0
Mar 26 19:17:02.994: INFO: Node conformance-worker is running more than one daemon pod
Mar 26 19:17:03.994: INFO: Number of nodes with available pods: 0
Mar 26 19:17:03.994: INFO: Node conformance-worker is running more than one daemon pod
Mar 26 19:17:04.994: INFO: Number of nodes with available pods: 0
Mar 26 19:17:04.994: INFO: Node conformance-worker is running more than one daemon pod
Mar 26 19:17:05.994: INFO: Number of nodes with available pods: 0
Mar 26 19:17:05.994: INFO: Node conformance-worker is running more than one daemon pod
Mar 26 19:17:06.994: INFO: Number of nodes with available pods: 0
Mar 26 19:17:06.994: INFO: Node conformance-worker is running more than one daemon pod
Mar 26 19:17:07.994: INFO: Number of nodes with available pods: 1
Mar 26 19:17:07.994: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
[1mSTEP[0m: Deleting DaemonSet "daemon-set"
[1mSTEP[0m: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4008, will wait for the garbage collector to delete the pods
Mar 26 19:17:08.059: INFO: Deleting DaemonSet.extensions daemon-set took: 6.069453ms
Mar 26 19:17:08.359: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.252778ms
Mar 26 19:17:15.662: INFO: Number of nodes with available pods: 0
Mar 26 19:17:15.663: INFO: Number of running nodes: 0, number of available pods: 0
Mar 26 19:17:15.665: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4008/daemonsets","resourceVersion":"8274"},"items":null}

Mar 26 19:17:15.668: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4008/pods","resourceVersion":"8274"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:17:15.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "daemonsets-4008" for this suite.
Mar 26 19:17:21.691: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:17:21.754: INFO: namespace daemonsets-4008 deletion completed in 6.070266134s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Downward API volume[0m 
  [1mshould update labels on modification [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Downward API volume
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:17:21.755: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename downward-api
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating the pod
Mar 26 19:17:24.310: INFO: Successfully updated pod "labelsupdate741998ec-5036-11e9-9719-a08cfdecc127"
[AfterEach] [sig-storage] Downward API volume
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:17:26.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "downward-api-8103" for this suite.
Mar 26 19:17:48.348: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:17:48.414: INFO: namespace downward-api-8103 deletion completed in 22.080481214s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90m[k8s.io] Kubectl cluster-info[0m 
  [1mshould check if Kubernetes master services is included in cluster-info  [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:17:48.414: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: validating cluster-info
Mar 26 19:17:48.447: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance cluster-info'
Mar 26 19:17:48.530: INFO: stderr: ""
Mar 26 19:17:48.530: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://localhost:38707\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://localhost:38707/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:17:48.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-5308" for this suite.
Mar 26 19:17:54.540: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:17:54.604: INFO: namespace kubectl-5308 deletion completed in 6.071207932s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Pods[0m 
  [1mshould be updated [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [k8s.io] Pods
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:17:54.604: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename pods
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be updated [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: creating the pod
[1mSTEP[0m: submitting the pod to kubernetes
[1mSTEP[0m: verifying the pod is in kubernetes
[1mSTEP[0m: updating the pod
Mar 26 19:17:59.155: INFO: Successfully updated pod "pod-update-87ada3ab-5036-11e9-9719-a08cfdecc127"
[1mSTEP[0m: verifying the updated pod is in kubernetes
Mar 26 19:17:59.162: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:17:59.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "pods-6671" for this suite.
Mar 26 19:18:21.173: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:18:21.241: INFO: namespace pods-6671 deletion completed in 22.076731691s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] DNS[0m 
  [1mshould provide DNS for services  [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-network] DNS
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:18:21.241: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename dns
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating a test headless service
[1mSTEP[0m: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3740.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3740.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3740.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3740.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3740.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3740.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3740.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3740.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3740.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3740.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3740.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3740.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3740.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 48.170.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.170.48_udp@PTR;check="$$(dig +tcp +noall +answer +search 48.170.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.170.48_tcp@PTR;sleep 1; done

[1mSTEP[0m: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3740.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3740.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3740.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3740.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3740.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3740.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3740.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3740.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3740.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3740.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3740.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3740.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3740.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 48.170.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.170.48_udp@PTR;check="$$(dig +tcp +noall +answer +search 48.170.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.170.48_tcp@PTR;sleep 1; done

[1mSTEP[0m: creating a pod to probe DNS
[1mSTEP[0m: submitting the pod to kubernetes
[1mSTEP[0m: retrieving the pod
[1mSTEP[0m: looking for the results for each expected name from probers
Mar 26 19:18:39.309: INFO: Unable to read wheezy_udp@dns-test-service.dns-3740.svc.cluster.local from pod dns-3740/dns-test-97913b98-5036-11e9-9719-a08cfdecc127: the server could not find the requested resource (get pods dns-test-97913b98-5036-11e9-9719-a08cfdecc127)
Mar 26 19:18:39.312: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3740.svc.cluster.local from pod dns-3740/dns-test-97913b98-5036-11e9-9719-a08cfdecc127: the server could not find the requested resource (get pods dns-test-97913b98-5036-11e9-9719-a08cfdecc127)
Mar 26 19:18:39.316: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3740.svc.cluster.local from pod dns-3740/dns-test-97913b98-5036-11e9-9719-a08cfdecc127: the server could not find the requested resource (get pods dns-test-97913b98-5036-11e9-9719-a08cfdecc127)
Mar 26 19:18:39.319: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3740.svc.cluster.local from pod dns-3740/dns-test-97913b98-5036-11e9-9719-a08cfdecc127: the server could not find the requested resource (get pods dns-test-97913b98-5036-11e9-9719-a08cfdecc127)
Mar 26 19:18:39.321: INFO: Unable to read wheezy_udp@_http._tcp.test-service-2.dns-3740.svc.cluster.local from pod dns-3740/dns-test-97913b98-5036-11e9-9719-a08cfdecc127: the server could not find the requested resource (get pods dns-test-97913b98-5036-11e9-9719-a08cfdecc127)
Mar 26 19:18:39.324: INFO: Unable to read wheezy_tcp@_http._tcp.test-service-2.dns-3740.svc.cluster.local from pod dns-3740/dns-test-97913b98-5036-11e9-9719-a08cfdecc127: the server could not find the requested resource (get pods dns-test-97913b98-5036-11e9-9719-a08cfdecc127)
Mar 26 19:18:39.326: INFO: Unable to read wheezy_udp@PodARecord from pod dns-3740/dns-test-97913b98-5036-11e9-9719-a08cfdecc127: the server could not find the requested resource (get pods dns-test-97913b98-5036-11e9-9719-a08cfdecc127)
Mar 26 19:18:39.328: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-3740/dns-test-97913b98-5036-11e9-9719-a08cfdecc127: the server could not find the requested resource (get pods dns-test-97913b98-5036-11e9-9719-a08cfdecc127)
Mar 26 19:18:39.330: INFO: Unable to read 10.99.170.48_udp@PTR from pod dns-3740/dns-test-97913b98-5036-11e9-9719-a08cfdecc127: the server could not find the requested resource (get pods dns-test-97913b98-5036-11e9-9719-a08cfdecc127)
Mar 26 19:18:39.332: INFO: Unable to read 10.99.170.48_tcp@PTR from pod dns-3740/dns-test-97913b98-5036-11e9-9719-a08cfdecc127: the server could not find the requested resource (get pods dns-test-97913b98-5036-11e9-9719-a08cfdecc127)
Mar 26 19:18:39.335: INFO: Unable to read jessie_udp@dns-test-service.dns-3740.svc.cluster.local from pod dns-3740/dns-test-97913b98-5036-11e9-9719-a08cfdecc127: the server could not find the requested resource (get pods dns-test-97913b98-5036-11e9-9719-a08cfdecc127)
Mar 26 19:18:39.337: INFO: Unable to read jessie_tcp@dns-test-service.dns-3740.svc.cluster.local from pod dns-3740/dns-test-97913b98-5036-11e9-9719-a08cfdecc127: the server could not find the requested resource (get pods dns-test-97913b98-5036-11e9-9719-a08cfdecc127)
Mar 26 19:18:39.339: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3740.svc.cluster.local from pod dns-3740/dns-test-97913b98-5036-11e9-9719-a08cfdecc127: the server could not find the requested resource (get pods dns-test-97913b98-5036-11e9-9719-a08cfdecc127)
Mar 26 19:18:39.341: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3740.svc.cluster.local from pod dns-3740/dns-test-97913b98-5036-11e9-9719-a08cfdecc127: the server could not find the requested resource (get pods dns-test-97913b98-5036-11e9-9719-a08cfdecc127)
Mar 26 19:18:39.343: INFO: Unable to read jessie_udp@_http._tcp.test-service-2.dns-3740.svc.cluster.local from pod dns-3740/dns-test-97913b98-5036-11e9-9719-a08cfdecc127: the server could not find the requested resource (get pods dns-test-97913b98-5036-11e9-9719-a08cfdecc127)
Mar 26 19:18:39.345: INFO: Unable to read jessie_tcp@_http._tcp.test-service-2.dns-3740.svc.cluster.local from pod dns-3740/dns-test-97913b98-5036-11e9-9719-a08cfdecc127: the server could not find the requested resource (get pods dns-test-97913b98-5036-11e9-9719-a08cfdecc127)
Mar 26 19:18:39.349: INFO: Unable to read jessie_udp@PodARecord from pod dns-3740/dns-test-97913b98-5036-11e9-9719-a08cfdecc127: the server could not find the requested resource (get pods dns-test-97913b98-5036-11e9-9719-a08cfdecc127)
Mar 26 19:18:39.352: INFO: Unable to read jessie_tcp@PodARecord from pod dns-3740/dns-test-97913b98-5036-11e9-9719-a08cfdecc127: the server could not find the requested resource (get pods dns-test-97913b98-5036-11e9-9719-a08cfdecc127)
Mar 26 19:18:39.354: INFO: Unable to read 10.99.170.48_udp@PTR from pod dns-3740/dns-test-97913b98-5036-11e9-9719-a08cfdecc127: the server could not find the requested resource (get pods dns-test-97913b98-5036-11e9-9719-a08cfdecc127)
Mar 26 19:18:39.356: INFO: Unable to read 10.99.170.48_tcp@PTR from pod dns-3740/dns-test-97913b98-5036-11e9-9719-a08cfdecc127: the server could not find the requested resource (get pods dns-test-97913b98-5036-11e9-9719-a08cfdecc127)
Mar 26 19:18:39.356: INFO: Lookups using dns-3740/dns-test-97913b98-5036-11e9-9719-a08cfdecc127 failed for: [wheezy_udp@dns-test-service.dns-3740.svc.cluster.local wheezy_tcp@dns-test-service.dns-3740.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3740.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3740.svc.cluster.local wheezy_udp@_http._tcp.test-service-2.dns-3740.svc.cluster.local wheezy_tcp@_http._tcp.test-service-2.dns-3740.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord 10.99.170.48_udp@PTR 10.99.170.48_tcp@PTR jessie_udp@dns-test-service.dns-3740.svc.cluster.local jessie_tcp@dns-test-service.dns-3740.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3740.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3740.svc.cluster.local jessie_udp@_http._tcp.test-service-2.dns-3740.svc.cluster.local jessie_tcp@_http._tcp.test-service-2.dns-3740.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord 10.99.170.48_udp@PTR 10.99.170.48_tcp@PTR]

Mar 26 19:18:44.418: INFO: DNS probes using dns-3740/dns-test-97913b98-5036-11e9-9719-a08cfdecc127 succeeded

[1mSTEP[0m: deleting the pod
[1mSTEP[0m: deleting the test service
[1mSTEP[0m: deleting the test headless service
[AfterEach] [sig-network] DNS
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:18:44.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "dns-3740" for this suite.
Mar 26 19:18:50.481: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:18:50.550: INFO: namespace dns-3740 deletion completed in 6.07654829s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Watchers[0m 
  [1mshould observe an object deletion if it stops meeting the requirements of the selector [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-api-machinery] Watchers
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:18:50.550: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename watch
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: creating a watch on configmaps with a certain label
[1mSTEP[0m: creating a new configmap
[1mSTEP[0m: modifying the configmap once
[1mSTEP[0m: changing the label value of the configmap
[1mSTEP[0m: Expecting to observe a delete notification for the watched object
Mar 26 19:18:50.586: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3947,SelfLink:/api/v1/namespaces/watch-3947/configmaps/e2e-watch-test-label-changed,UID:a906c70e-5036-11e9-b1ea-02429e4bb871,ResourceVersion:8581,Generation:0,CreationTimestamp:2019-03-26 19:18:50 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar 26 19:18:50.586: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3947,SelfLink:/api/v1/namespaces/watch-3947/configmaps/e2e-watch-test-label-changed,UID:a906c70e-5036-11e9-b1ea-02429e4bb871,ResourceVersion:8582,Generation:0,CreationTimestamp:2019-03-26 19:18:50 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Mar 26 19:18:50.586: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3947,SelfLink:/api/v1/namespaces/watch-3947/configmaps/e2e-watch-test-label-changed,UID:a906c70e-5036-11e9-b1ea-02429e4bb871,ResourceVersion:8583,Generation:0,CreationTimestamp:2019-03-26 19:18:50 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
[1mSTEP[0m: modifying the configmap a second time
[1mSTEP[0m: Expecting not to observe a notification because the object no longer meets the selector's requirements
[1mSTEP[0m: changing the label value of the configmap back
[1mSTEP[0m: modifying the configmap a third time
[1mSTEP[0m: deleting the configmap
[1mSTEP[0m: Expecting to observe an add notification for the watched object when the label value was restored
Mar 26 19:19:00.608: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3947,SelfLink:/api/v1/namespaces/watch-3947/configmaps/e2e-watch-test-label-changed,UID:a906c70e-5036-11e9-b1ea-02429e4bb871,ResourceVersion:8599,Generation:0,CreationTimestamp:2019-03-26 19:18:50 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar 26 19:19:00.608: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3947,SelfLink:/api/v1/namespaces/watch-3947/configmaps/e2e-watch-test-label-changed,UID:a906c70e-5036-11e9-b1ea-02429e4bb871,ResourceVersion:8600,Generation:0,CreationTimestamp:2019-03-26 19:18:50 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Mar 26 19:19:00.608: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3947,SelfLink:/api/v1/namespaces/watch-3947/configmaps/e2e-watch-test-label-changed,UID:a906c70e-5036-11e9-b1ea-02429e4bb871,ResourceVersion:8601,Generation:0,CreationTimestamp:2019-03-26 19:18:50 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:19:00.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "watch-3947" for this suite.
Mar 26 19:19:06.621: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:19:06.691: INFO: namespace watch-3947 deletion completed in 6.078527474s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] ConfigMap[0m 
  [1moptional updates should be reflected in volume [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] ConfigMap
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:19:06.691: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename configmap
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating configMap with name cm-test-opt-del-b2a63670-5036-11e9-9719-a08cfdecc127
[1mSTEP[0m: Creating configMap with name cm-test-opt-upd-b2a636af-5036-11e9-9719-a08cfdecc127
[1mSTEP[0m: Creating the pod
[1mSTEP[0m: Deleting configmap cm-test-opt-del-b2a63670-5036-11e9-9719-a08cfdecc127
[1mSTEP[0m: Updating configmap cm-test-opt-upd-b2a636af-5036-11e9-9719-a08cfdecc127
[1mSTEP[0m: Creating configMap with name cm-test-opt-create-b2a636c4-5036-11e9-9719-a08cfdecc127
[1mSTEP[0m: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:19:14.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "configmap-4370" for this suite.
Mar 26 19:19:36.829: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:19:36.904: INFO: namespace configmap-4370 deletion completed in 22.083927145s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-scheduling] SchedulerPredicates [Serial][0m 
  [1mvalidates that NodeSelector is respected if matching  [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:19:36.904: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename sched-pred
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Mar 26 19:19:36.927: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 26 19:19:36.931: INFO: Waiting for terminating namespaces to be deleted...
Mar 26 19:19:36.933: INFO: 
Logging pods the kubelet thinks is on node conformance-worker before test
Mar 26 19:19:36.936: INFO: weave-net-n8g4b from kube-system started at 2019-03-26 18:51:05 -0700 PDT (2 container statuses recorded)
Mar 26 19:19:36.936: INFO: 	Container weave ready: true, restart count 1
Mar 26 19:19:36.936: INFO: 	Container weave-npc ready: true, restart count 0
Mar 26 19:19:36.936: INFO: kube-proxy-tvs75 from kube-system started at 2019-03-26 18:51:05 -0700 PDT (1 container statuses recorded)
Mar 26 19:19:36.936: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 26 19:19:36.936: INFO: 
Logging pods the kubelet thinks is on node conformance-worker2 before test
Mar 26 19:19:36.939: INFO: kube-proxy-4cdcp from kube-system started at 2019-03-26 18:51:05 -0700 PDT (1 container statuses recorded)
Mar 26 19:19:36.939: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 26 19:19:36.939: INFO: weave-net-pdrl8 from kube-system started at 2019-03-26 18:51:05 -0700 PDT (2 container statuses recorded)
Mar 26 19:19:36.939: INFO: 	Container weave ready: true, restart count 1
Mar 26 19:19:36.939: INFO: 	Container weave-npc ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Trying to launch a pod without a label to get a node which can launch it.
[1mSTEP[0m: Explicitly delete pod here to free the resource it takes.
[1mSTEP[0m: Trying to apply a random label on the found node.
[1mSTEP[0m: verifying the node has the label kubernetes.io/e2e-c5ddb8ef-5036-11e9-9719-a08cfdecc127 42
[1mSTEP[0m: Trying to relaunch the pod, now with labels.
[1mSTEP[0m: removing the label kubernetes.io/e2e-c5ddb8ef-5036-11e9-9719-a08cfdecc127 off the node conformance-worker
[1mSTEP[0m: verifying the node doesn't have the label kubernetes.io/e2e-c5ddb8ef-5036-11e9-9719-a08cfdecc127
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:19:42.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "sched-pred-2455" for this suite.
Mar 26 19:19:51.006: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:19:51.073: INFO: namespace sched-pred-2455 deletion completed in 8.075239388s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected downwardAPI[0m 
  [1mshould provide container's cpu limit [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Projected downwardAPI
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:19:51.073: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar 26 19:19:51.109: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cd1ad144-5036-11e9-9719-a08cfdecc127" in namespace "projected-7916" to be "success or failure"
Mar 26 19:19:51.121: INFO: Pod "downwardapi-volume-cd1ad144-5036-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 11.364223ms
Mar 26 19:19:53.125: INFO: Pod "downwardapi-volume-cd1ad144-5036-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015664679s
Mar 26 19:19:55.129: INFO: Pod "downwardapi-volume-cd1ad144-5036-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019855632s
[1mSTEP[0m: Saw pod success
Mar 26 19:19:55.129: INFO: Pod "downwardapi-volume-cd1ad144-5036-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 19:19:55.132: INFO: Trying to get logs from node conformance-worker pod downwardapi-volume-cd1ad144-5036-11e9-9719-a08cfdecc127 container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar 26 19:19:55.152: INFO: Waiting for pod downwardapi-volume-cd1ad144-5036-11e9-9719-a08cfdecc127 to disappear
Mar 26 19:19:55.153: INFO: Pod downwardapi-volume-cd1ad144-5036-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:19:55.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-7916" for this suite.
Mar 26 19:20:01.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:20:01.231: INFO: namespace projected-7916 deletion completed in 6.07538442s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected configMap[0m 
  [1mshould be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Projected configMap
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:20:01.231: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating configMap with name projected-configmap-test-volume-d3282957-5036-11e9-9719-a08cfdecc127
[1mSTEP[0m: Creating a pod to test consume configMaps
Mar 26 19:20:01.264: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d3287cec-5036-11e9-9719-a08cfdecc127" in namespace "projected-9850" to be "success or failure"
Mar 26 19:20:01.266: INFO: Pod "pod-projected-configmaps-d3287cec-5036-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 1.740055ms
Mar 26 19:20:03.270: INFO: Pod "pod-projected-configmaps-d3287cec-5036-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005662212s
Mar 26 19:20:05.274: INFO: Pod "pod-projected-configmaps-d3287cec-5036-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00972666s
[1mSTEP[0m: Saw pod success
Mar 26 19:20:05.274: INFO: Pod "pod-projected-configmaps-d3287cec-5036-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 19:20:05.277: INFO: Trying to get logs from node conformance-worker pod pod-projected-configmaps-d3287cec-5036-11e9-9719-a08cfdecc127 container projected-configmap-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar 26 19:20:05.297: INFO: Waiting for pod pod-projected-configmaps-d3287cec-5036-11e9-9719-a08cfdecc127 to disappear
Mar 26 19:20:05.299: INFO: Pod pod-projected-configmaps-d3287cec-5036-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:20:05.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-9850" for this suite.
Mar 26 19:20:11.309: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:20:11.363: INFO: namespace projected-9850 deletion completed in 6.06222806s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Downward API volume[0m 
  [1mshould provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Downward API volume
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:20:11.363: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename downward-api
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar 26 19:20:11.399: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d932f652-5036-11e9-9719-a08cfdecc127" in namespace "downward-api-6492" to be "success or failure"
Mar 26 19:20:11.403: INFO: Pod "downwardapi-volume-d932f652-5036-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 3.814782ms
Mar 26 19:20:13.406: INFO: Pod "downwardapi-volume-d932f652-5036-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007226022s
[1mSTEP[0m: Saw pod success
Mar 26 19:20:13.406: INFO: Pod "downwardapi-volume-d932f652-5036-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 19:20:13.409: INFO: Trying to get logs from node conformance-worker2 pod downwardapi-volume-d932f652-5036-11e9-9719-a08cfdecc127 container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar 26 19:20:13.426: INFO: Waiting for pod downwardapi-volume-d932f652-5036-11e9-9719-a08cfdecc127 to disappear
Mar 26 19:20:13.428: INFO: Pod downwardapi-volume-d932f652-5036-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:20:13.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "downward-api-6492" for this suite.
Mar 26 19:20:19.439: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:20:19.506: INFO: namespace downward-api-6492 deletion completed in 6.075026078s
[32mâ€¢[0m
[90m------------------------------[0m
[0m[sig-storage] EmptyDir volumes[0m 
  [1mvolume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] EmptyDir volumes
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:20:19.506: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename emptydir
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating a pod to test emptydir volume type on tmpfs
Mar 26 19:20:19.535: INFO: Waiting up to 5m0s for pod "pod-de0c591f-5036-11e9-9719-a08cfdecc127" in namespace "emptydir-8606" to be "success or failure"
Mar 26 19:20:19.538: INFO: Pod "pod-de0c591f-5036-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.388157ms
Mar 26 19:20:21.539: INFO: Pod "pod-de0c591f-5036-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004247043s
Mar 26 19:20:23.543: INFO: Pod "pod-de0c591f-5036-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008064818s
[1mSTEP[0m: Saw pod success
Mar 26 19:20:23.543: INFO: Pod "pod-de0c591f-5036-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 19:20:23.546: INFO: Trying to get logs from node conformance-worker pod pod-de0c591f-5036-11e9-9719-a08cfdecc127 container test-container: <nil>
[1mSTEP[0m: delete the pod
Mar 26 19:20:23.563: INFO: Waiting for pod pod-de0c591f-5036-11e9-9719-a08cfdecc127 to disappear
Mar 26 19:20:23.565: INFO: Pod pod-de0c591f-5036-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:20:23.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "emptydir-8606" for this suite.
Mar 26 19:20:29.575: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:20:29.642: INFO: namespace emptydir-8606 deletion completed in 6.074543942s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Downward API volume[0m 
  [1mshould provide container's memory request [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Downward API volume
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:20:29.642: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename downward-api
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar 26 19:20:29.676: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e417008a-5036-11e9-9719-a08cfdecc127" in namespace "downward-api-8205" to be "success or failure"
Mar 26 19:20:29.678: INFO: Pod "downwardapi-volume-e417008a-5036-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.407964ms
Mar 26 19:20:31.682: INFO: Pod "downwardapi-volume-e417008a-5036-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006520064s
[1mSTEP[0m: Saw pod success
Mar 26 19:20:31.682: INFO: Pod "downwardapi-volume-e417008a-5036-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 19:20:31.686: INFO: Trying to get logs from node conformance-worker2 pod downwardapi-volume-e417008a-5036-11e9-9719-a08cfdecc127 container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar 26 19:20:31.700: INFO: Waiting for pod downwardapi-volume-e417008a-5036-11e9-9719-a08cfdecc127 to disappear
Mar 26 19:20:31.703: INFO: Pod downwardapi-volume-e417008a-5036-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:20:31.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "downward-api-8205" for this suite.
Mar 26 19:20:37.714: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:20:37.780: INFO: namespace downward-api-8205 deletion completed in 6.074670285s
[32mâ€¢[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] ConfigMap[0m 
  [1mupdates should be reflected in volume [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] ConfigMap
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:20:37.780: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename configmap
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating configMap with name configmap-test-upd-e8f11d0c-5036-11e9-9719-a08cfdecc127
[1mSTEP[0m: Creating the pod
[1mSTEP[0m: Updating configmap configmap-test-upd-e8f11d0c-5036-11e9-9719-a08cfdecc127
[1mSTEP[0m: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:21:54.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "configmap-1365" for this suite.
Mar 26 19:22:16.248: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:22:16.314: INFO: namespace configmap-1365 deletion completed in 22.07993953s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Downward API volume[0m 
  [1mshould provide podname only [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Downward API volume
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:22:16.315: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename downward-api
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar 26 19:22:16.346: INFO: Waiting up to 5m0s for pod "downwardapi-volume-23ac18c2-5037-11e9-9719-a08cfdecc127" in namespace "downward-api-4177" to be "success or failure"
Mar 26 19:22:16.347: INFO: Pod "downwardapi-volume-23ac18c2-5037-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 1.460955ms
Mar 26 19:22:18.350: INFO: Pod "downwardapi-volume-23ac18c2-5037-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004387542s
[1mSTEP[0m: Saw pod success
Mar 26 19:22:18.350: INFO: Pod "downwardapi-volume-23ac18c2-5037-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 19:22:18.353: INFO: Trying to get logs from node conformance-worker2 pod downwardapi-volume-23ac18c2-5037-11e9-9719-a08cfdecc127 container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar 26 19:22:18.368: INFO: Waiting for pod downwardapi-volume-23ac18c2-5037-11e9-9719-a08cfdecc127 to disappear
Mar 26 19:22:18.371: INFO: Pod downwardapi-volume-23ac18c2-5037-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:22:18.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "downward-api-4177" for this suite.
Mar 26 19:22:24.383: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:22:24.452: INFO: namespace downward-api-4177 deletion completed in 6.078710068s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] Daemon set [Serial][0m 
  [1mshould run and stop simple daemon [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-apps] Daemon set [Serial]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:22:24.452: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename daemonsets
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating simple DaemonSet "daemon-set"
[1mSTEP[0m: Check that daemon pods launch on every node of the cluster.
Mar 26 19:22:24.499: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 19:22:24.500: INFO: Number of nodes with available pods: 0
Mar 26 19:22:24.500: INFO: Node conformance-worker is running more than one daemon pod
Mar 26 19:22:25.503: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 19:22:25.506: INFO: Number of nodes with available pods: 0
Mar 26 19:22:25.506: INFO: Node conformance-worker is running more than one daemon pod
Mar 26 19:22:26.505: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 19:22:26.509: INFO: Number of nodes with available pods: 2
Mar 26 19:22:26.509: INFO: Number of running nodes: 2, number of available pods: 2
[1mSTEP[0m: Stop a daemon pod, check that the daemon pod is revived.
Mar 26 19:22:26.523: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 19:22:26.525: INFO: Number of nodes with available pods: 1
Mar 26 19:22:26.525: INFO: Node conformance-worker is running more than one daemon pod
Mar 26 19:22:27.530: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 19:22:27.533: INFO: Number of nodes with available pods: 1
Mar 26 19:22:27.533: INFO: Node conformance-worker is running more than one daemon pod
Mar 26 19:22:28.529: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 19:22:28.533: INFO: Number of nodes with available pods: 1
Mar 26 19:22:28.533: INFO: Node conformance-worker is running more than one daemon pod
Mar 26 19:22:29.528: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 19:22:29.531: INFO: Number of nodes with available pods: 1
Mar 26 19:22:29.532: INFO: Node conformance-worker is running more than one daemon pod
Mar 26 19:22:30.530: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 19:22:30.533: INFO: Number of nodes with available pods: 1
Mar 26 19:22:30.533: INFO: Node conformance-worker is running more than one daemon pod
Mar 26 19:22:31.529: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 19:22:31.532: INFO: Number of nodes with available pods: 1
Mar 26 19:22:31.532: INFO: Node conformance-worker is running more than one daemon pod
Mar 26 19:22:32.529: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 19:22:32.533: INFO: Number of nodes with available pods: 2
Mar 26 19:22:32.533: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
[1mSTEP[0m: Deleting DaemonSet "daemon-set"
[1mSTEP[0m: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7114, will wait for the garbage collector to delete the pods
Mar 26 19:22:32.595: INFO: Deleting DaemonSet.extensions daemon-set took: 6.863667ms
Mar 26 19:22:32.896: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.306089ms
Mar 26 19:22:45.598: INFO: Number of nodes with available pods: 0
Mar 26 19:22:45.598: INFO: Number of running nodes: 0, number of available pods: 0
Mar 26 19:22:45.601: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7114/daemonsets","resourceVersion":"9318"},"items":null}

Mar 26 19:22:45.603: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7114/pods","resourceVersion":"9318"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:22:45.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "daemonsets-7114" for this suite.
Mar 26 19:22:51.625: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:22:51.689: INFO: namespace daemonsets-7114 deletion completed in 6.07409613s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] KubeletManagedEtcHosts[0m 
  [1mshould test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:22:51.689: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename e2e-kubelet-etc-hosts
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Setting up the test
[1mSTEP[0m: Creating hostNetwork=false pod
[1mSTEP[0m: Creating hostNetwork=true pod
[1mSTEP[0m: Running the test
[1mSTEP[0m: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Mar 26 19:23:01.752: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5809 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 19:23:01.752: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
Mar 26 19:23:01.915: INFO: Exec stderr: ""
Mar 26 19:23:01.915: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5809 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 19:23:01.915: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
Mar 26 19:23:02.062: INFO: Exec stderr: ""
Mar 26 19:23:02.062: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5809 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 19:23:02.062: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
Mar 26 19:23:02.219: INFO: Exec stderr: ""
Mar 26 19:23:02.219: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5809 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 19:23:02.219: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
Mar 26 19:23:02.379: INFO: Exec stderr: ""
[1mSTEP[0m: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Mar 26 19:23:02.379: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5809 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 19:23:02.379: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
Mar 26 19:23:02.555: INFO: Exec stderr: ""
Mar 26 19:23:02.555: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5809 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 19:23:02.555: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
Mar 26 19:23:02.709: INFO: Exec stderr: ""
[1mSTEP[0m: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Mar 26 19:23:02.709: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5809 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 19:23:02.709: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
Mar 26 19:23:02.873: INFO: Exec stderr: ""
Mar 26 19:23:02.873: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5809 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 19:23:02.873: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
Mar 26 19:23:03.022: INFO: Exec stderr: ""
Mar 26 19:23:03.022: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5809 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 19:23:03.022: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
Mar 26 19:23:03.192: INFO: Exec stderr: ""
Mar 26 19:23:03.192: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5809 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 19:23:03.192: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
Mar 26 19:23:03.348: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:23:03.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "e2e-kubelet-etc-hosts-5809" for this suite.
Mar 26 19:23:41.361: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:23:41.422: INFO: namespace e2e-kubelet-etc-hosts-5809 deletion completed in 38.071078842s
[32mâ€¢[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Aggregator[0m 
  [1mShould be able to support the 1.10 Sample API Server using the current Aggregator [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-api-machinery] Aggregator
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:23:41.422: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename aggregator
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:69
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Registering the sample API server.
Mar 26 19:23:41.906: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Mar 26 19:23:43.947: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63689250221, loc:(*time.Location)(0x89eb0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63689250221, loc:(*time.Location)(0x89eb0e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63689250221, loc:(*time.Location)(0x89eb0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63689250221, loc:(*time.Location)(0x89eb0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 19:23:45.951: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63689250221, loc:(*time.Location)(0x89eb0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63689250221, loc:(*time.Location)(0x89eb0e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63689250221, loc:(*time.Location)(0x89eb0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63689250221, loc:(*time.Location)(0x89eb0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 19:23:47.951: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63689250221, loc:(*time.Location)(0x89eb0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63689250221, loc:(*time.Location)(0x89eb0e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63689250221, loc:(*time.Location)(0x89eb0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63689250221, loc:(*time.Location)(0x89eb0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 19:23:49.950: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63689250221, loc:(*time.Location)(0x89eb0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63689250221, loc:(*time.Location)(0x89eb0e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63689250221, loc:(*time.Location)(0x89eb0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63689250221, loc:(*time.Location)(0x89eb0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 19:23:54.380: INFO: Waited 2.423992067s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:60
[AfterEach] [sig-api-machinery] Aggregator
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:23:54.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "aggregator-2248" for this suite.
Mar 26 19:24:00.915: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:24:00.965: INFO: namespace aggregator-2248 deletion completed in 6.151594965s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] [sig-node] PreStop[0m 
  [1mshould call prestop when killing a pod  [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [k8s.io] [sig-node] PreStop
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:24:00.965: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename prestop
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating server pod server in namespace prestop-8931
[1mSTEP[0m: Waiting for pods to come up.
[1mSTEP[0m: Creating tester pod tester in namespace prestop-8931
[1mSTEP[0m: Deleting pre-stop pod
Mar 26 19:24:12.026: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
[1mSTEP[0m: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:24:12.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "prestop-8931" for this suite.
Mar 26 19:24:50.048: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:24:50.116: INFO: namespace prestop-8931 deletion completed in 38.077956955s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected configMap[0m 
  [1mupdates should be reflected in volume [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Projected configMap
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:24:50.116: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating projection with configMap that has name projected-configmap-test-upd-7f58a0bf-5037-11e9-9719-a08cfdecc127
[1mSTEP[0m: Creating the pod
[1mSTEP[0m: Updating configmap projected-configmap-test-upd-7f58a0bf-5037-11e9-9719-a08cfdecc127
[1mSTEP[0m: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:26:12.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-1146" for this suite.
Mar 26 19:26:34.617: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:26:34.683: INFO: namespace projected-1146 deletion completed in 22.075230317s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] Daemon set [Serial][0m 
  [1mshould rollback without unnecessary restarts [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-apps] Daemon set [Serial]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:26:34.683: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename daemonsets
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Mar 26 19:26:34.737: INFO: Create a RollingUpdate DaemonSet
Mar 26 19:26:34.739: INFO: Check that daemon pods launch on every node of the cluster
Mar 26 19:26:34.740: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 19:26:34.742: INFO: Number of nodes with available pods: 0
Mar 26 19:26:34.742: INFO: Node conformance-worker is running more than one daemon pod
Mar 26 19:26:35.746: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 19:26:35.749: INFO: Number of nodes with available pods: 0
Mar 26 19:26:35.749: INFO: Node conformance-worker is running more than one daemon pod
Mar 26 19:26:36.746: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 19:26:36.750: INFO: Number of nodes with available pods: 1
Mar 26 19:26:36.750: INFO: Node conformance-worker2 is running more than one daemon pod
Mar 26 19:26:37.745: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 19:26:37.747: INFO: Number of nodes with available pods: 2
Mar 26 19:26:37.747: INFO: Number of running nodes: 2, number of available pods: 2
Mar 26 19:26:37.747: INFO: Update the DaemonSet to trigger a rollout
Mar 26 19:26:37.753: INFO: Updating DaemonSet daemon-set
Mar 26 19:26:45.762: INFO: Roll back the DaemonSet before rollout is complete
Mar 26 19:26:45.769: INFO: Updating DaemonSet daemon-set
Mar 26 19:26:45.769: INFO: Make sure DaemonSet rollback is complete
Mar 26 19:26:45.771: INFO: Wrong image for pod: daemon-set-j7tsq. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Mar 26 19:26:45.771: INFO: Pod daemon-set-j7tsq is not available
Mar 26 19:26:45.777: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 19:26:46.782: INFO: Wrong image for pod: daemon-set-j7tsq. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Mar 26 19:26:46.782: INFO: Pod daemon-set-j7tsq is not available
Mar 26 19:26:46.785: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 19:26:47.781: INFO: Wrong image for pod: daemon-set-j7tsq. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Mar 26 19:26:47.781: INFO: Pod daemon-set-j7tsq is not available
Mar 26 19:26:47.785: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 19:26:48.781: INFO: Wrong image for pod: daemon-set-j7tsq. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Mar 26 19:26:48.781: INFO: Pod daemon-set-j7tsq is not available
Mar 26 19:26:48.785: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 19:26:49.781: INFO: Wrong image for pod: daemon-set-j7tsq. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Mar 26 19:26:49.781: INFO: Pod daemon-set-j7tsq is not available
Mar 26 19:26:49.784: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 19:26:50.781: INFO: Wrong image for pod: daemon-set-j7tsq. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Mar 26 19:26:50.781: INFO: Pod daemon-set-j7tsq is not available
Mar 26 19:26:50.785: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 19:26:51.780: INFO: Wrong image for pod: daemon-set-j7tsq. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Mar 26 19:26:51.780: INFO: Pod daemon-set-j7tsq is not available
Mar 26 19:26:51.783: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 19:26:52.781: INFO: Wrong image for pod: daemon-set-j7tsq. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Mar 26 19:26:52.781: INFO: Pod daemon-set-j7tsq is not available
Mar 26 19:26:52.783: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 19:26:53.781: INFO: Wrong image for pod: daemon-set-j7tsq. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Mar 26 19:26:53.781: INFO: Pod daemon-set-j7tsq is not available
Mar 26 19:26:53.786: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 19:26:54.779: INFO: Wrong image for pod: daemon-set-j7tsq. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Mar 26 19:26:54.779: INFO: Pod daemon-set-j7tsq is not available
Mar 26 19:26:54.781: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 19:26:55.781: INFO: Pod daemon-set-dtbp6 is not available
Mar 26 19:26:55.785: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
[1mSTEP[0m: Deleting DaemonSet "daemon-set"
[1mSTEP[0m: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3099, will wait for the garbage collector to delete the pods
Mar 26 19:26:55.853: INFO: Deleting DaemonSet.extensions daemon-set took: 8.005874ms
Mar 26 19:26:56.153: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.186641ms
Mar 26 19:26:58.456: INFO: Number of nodes with available pods: 0
Mar 26 19:26:58.456: INFO: Number of running nodes: 0, number of available pods: 0
Mar 26 19:26:58.459: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3099/daemonsets","resourceVersion":"10038"},"items":null}

Mar 26 19:26:58.462: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3099/pods","resourceVersion":"10038"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:26:58.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "daemonsets-3099" for this suite.
Mar 26 19:27:04.484: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:27:04.551: INFO: namespace daemonsets-3099 deletion completed in 6.076488246s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Networking[0m [90mGranular Checks: Pods[0m 
  [1mshould function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-network] Networking
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:27:04.551: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename pod-network-test
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Performing setup for networking test in namespace pod-network-test-182
[1mSTEP[0m: creating a selector
[1mSTEP[0m: Creating the service pods in kubernetes
Mar 26 19:27:04.580: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[1mSTEP[0m: Creating test pods
Mar 26 19:27:30.638: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.32.0.3:8080/dial?request=hostName&protocol=http&host=10.46.0.2&port=8080&tries=1'] Namespace:pod-network-test-182 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 19:27:30.638: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
Mar 26 19:27:30.861: INFO: Waiting for endpoints: map[]
Mar 26 19:27:30.863: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.32.0.3:8080/dial?request=hostName&protocol=http&host=10.32.0.2&port=8080&tries=1'] Namespace:pod-network-test-182 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 19:27:30.863: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
Mar 26 19:27:31.026: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:27:31.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "pod-network-test-182" for this suite.
Mar 26 19:27:53.040: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:27:53.106: INFO: namespace pod-network-test-182 deletion completed in 22.076349089s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Kubelet[0m [90mwhen scheduling a busybox command that always fails in a pod[0m 
  [1mshould have an terminated reason [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [k8s.io] Kubelet
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:27:53.106: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename kubelet-test
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:27:57.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubelet-test-7155" for this suite.
Mar 26 19:28:03.181: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:28:03.243: INFO: namespace kubelet-test-7155 deletion completed in 6.072638476s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] StatefulSet[0m [90m[k8s.io] Basic StatefulSet functionality [StatefulSetBasic][0m 
  [1mBurst scaling should run to completion even with unhealthy pods [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-apps] StatefulSet
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:28:03.244: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename statefulset
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
[1mSTEP[0m: Creating service test in namespace statefulset-9205
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating stateful set ss in namespace statefulset-9205
[1mSTEP[0m: Waiting until all stateful set ss replicas will be running in namespace statefulset-9205
Mar 26 19:28:03.274: INFO: Found 0 stateful pods, waiting for 1
Mar 26 19:28:13.278: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
[1mSTEP[0m: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Mar 26 19:28:13.281: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar 26 19:28:13.564: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Mar 26 19:28:13.564: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar 26 19:28:13.564: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar 26 19:28:13.567: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 26 19:28:23.572: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 26 19:28:23.572: INFO: Waiting for statefulset status.replicas updated to 0
Mar 26 19:28:23.583: INFO: POD   NODE                 PHASE    GRACE  CONDITIONS
Mar 26 19:28:23.583: INFO: ss-0  conformance-worker2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:03 -0700 PDT  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:14 -0700 PDT ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:14 -0700 PDT ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:03 -0700 PDT  }]
Mar 26 19:28:23.583: INFO: 
Mar 26 19:28:23.583: INFO: StatefulSet ss has not reached scale 3, at 1
Mar 26 19:28:24.586: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997023943s
Mar 26 19:28:25.590: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993513872s
Mar 26 19:28:26.595: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.989565881s
Mar 26 19:28:27.599: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.985239954s
Mar 26 19:28:28.603: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.981156877s
Mar 26 19:28:29.607: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.977200751s
Mar 26 19:28:30.611: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.973046459s
Mar 26 19:28:31.616: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.968501199s
Mar 26 19:28:32.620: INFO: Verifying statefulset ss doesn't scale past 3 for another 963.852144ms
[1mSTEP[0m: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9205
Mar 26 19:28:33.625: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 26 19:28:33.895: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Mar 26 19:28:33.895: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar 26 19:28:33.895: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar 26 19:28:33.895: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 26 19:28:34.147: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar 26 19:28:34.147: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar 26 19:28:34.147: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar 26 19:28:34.147: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 26 19:28:34.381: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar 26 19:28:34.381: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Mar 26 19:28:34.381: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Mar 26 19:28:34.383: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 26 19:28:34.383: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 26 19:28:34.383: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
[1mSTEP[0m: Scale down will not halt with unhealthy stateful pod
Mar 26 19:28:34.385: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar 26 19:28:34.623: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Mar 26 19:28:34.623: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar 26 19:28:34.623: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar 26 19:28:34.623: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar 26 19:28:34.864: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Mar 26 19:28:34.864: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar 26 19:28:34.864: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar 26 19:28:34.864: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Mar 26 19:28:35.095: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Mar 26 19:28:35.095: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Mar 26 19:28:35.095: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Mar 26 19:28:35.095: INFO: Waiting for statefulset status.replicas updated to 0
Mar 26 19:28:35.098: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Mar 26 19:28:45.105: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 26 19:28:45.105: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 26 19:28:45.105: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 26 19:28:45.115: INFO: POD   NODE                 PHASE    GRACE  CONDITIONS
Mar 26 19:28:45.115: INFO: ss-0  conformance-worker2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:03 -0700 PDT  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:35 -0700 PDT ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:35 -0700 PDT ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:03 -0700 PDT  }]
Mar 26 19:28:45.115: INFO: ss-1  conformance-worker   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:23 -0700 PDT  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:35 -0700 PDT ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:35 -0700 PDT ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:23 -0700 PDT  }]
Mar 26 19:28:45.115: INFO: ss-2  conformance-worker2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:23 -0700 PDT  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:35 -0700 PDT ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:35 -0700 PDT ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:23 -0700 PDT  }]
Mar 26 19:28:45.115: INFO: 
Mar 26 19:28:45.115: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 26 19:28:46.120: INFO: POD   NODE                 PHASE    GRACE  CONDITIONS
Mar 26 19:28:46.120: INFO: ss-0  conformance-worker2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:03 -0700 PDT  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:35 -0700 PDT ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:35 -0700 PDT ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:03 -0700 PDT  }]
Mar 26 19:28:46.120: INFO: ss-1  conformance-worker   Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:23 -0700 PDT  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:35 -0700 PDT ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:35 -0700 PDT ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:23 -0700 PDT  }]
Mar 26 19:28:46.120: INFO: ss-2  conformance-worker2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:23 -0700 PDT  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:35 -0700 PDT ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:35 -0700 PDT ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:23 -0700 PDT  }]
Mar 26 19:28:46.120: INFO: 
Mar 26 19:28:46.120: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 26 19:28:47.123: INFO: POD   NODE                 PHASE    GRACE  CONDITIONS
Mar 26 19:28:47.123: INFO: ss-0  conformance-worker2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:03 -0700 PDT  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:35 -0700 PDT ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:35 -0700 PDT ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:03 -0700 PDT  }]
Mar 26 19:28:47.123: INFO: ss-1  conformance-worker   Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:23 -0700 PDT  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:35 -0700 PDT ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:35 -0700 PDT ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:23 -0700 PDT  }]
Mar 26 19:28:47.123: INFO: ss-2  conformance-worker2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:23 -0700 PDT  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:35 -0700 PDT ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:35 -0700 PDT ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:23 -0700 PDT  }]
Mar 26 19:28:47.123: INFO: 
Mar 26 19:28:47.123: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 26 19:28:48.127: INFO: POD   NODE                 PHASE    GRACE  CONDITIONS
Mar 26 19:28:48.127: INFO: ss-0  conformance-worker2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:03 -0700 PDT  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:35 -0700 PDT ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:35 -0700 PDT ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:03 -0700 PDT  }]
Mar 26 19:28:48.127: INFO: 
Mar 26 19:28:48.127: INFO: StatefulSet ss has not reached scale 0, at 1
Mar 26 19:28:49.131: INFO: POD   NODE                 PHASE    GRACE  CONDITIONS
Mar 26 19:28:49.131: INFO: ss-0  conformance-worker2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:03 -0700 PDT  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:35 -0700 PDT ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:35 -0700 PDT ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:03 -0700 PDT  }]
Mar 26 19:28:49.131: INFO: 
Mar 26 19:28:49.131: INFO: StatefulSet ss has not reached scale 0, at 1
Mar 26 19:28:50.135: INFO: POD   NODE                 PHASE    GRACE  CONDITIONS
Mar 26 19:28:50.135: INFO: ss-0  conformance-worker2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:03 -0700 PDT  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:35 -0700 PDT ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:35 -0700 PDT ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:03 -0700 PDT  }]
Mar 26 19:28:50.135: INFO: 
Mar 26 19:28:50.135: INFO: StatefulSet ss has not reached scale 0, at 1
Mar 26 19:28:51.139: INFO: POD   NODE                 PHASE    GRACE  CONDITIONS
Mar 26 19:28:51.140: INFO: ss-0  conformance-worker2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:03 -0700 PDT  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:35 -0700 PDT ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:35 -0700 PDT ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:03 -0700 PDT  }]
Mar 26 19:28:51.140: INFO: 
Mar 26 19:28:51.140: INFO: StatefulSet ss has not reached scale 0, at 1
Mar 26 19:28:52.144: INFO: POD   NODE                 PHASE    GRACE  CONDITIONS
Mar 26 19:28:52.144: INFO: ss-0  conformance-worker2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:03 -0700 PDT  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:35 -0700 PDT ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:35 -0700 PDT ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:03 -0700 PDT  }]
Mar 26 19:28:52.144: INFO: 
Mar 26 19:28:52.144: INFO: StatefulSet ss has not reached scale 0, at 1
Mar 26 19:28:53.148: INFO: POD   NODE                 PHASE    GRACE  CONDITIONS
Mar 26 19:28:53.148: INFO: ss-0  conformance-worker2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:03 -0700 PDT  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:35 -0700 PDT ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:35 -0700 PDT ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:03 -0700 PDT  }]
Mar 26 19:28:53.148: INFO: 
Mar 26 19:28:53.148: INFO: StatefulSet ss has not reached scale 0, at 1
Mar 26 19:28:54.152: INFO: POD   NODE                 PHASE    GRACE  CONDITIONS
Mar 26 19:28:54.152: INFO: ss-0  conformance-worker2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:03 -0700 PDT  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:35 -0700 PDT ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:35 -0700 PDT ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:28:03 -0700 PDT  }]
Mar 26 19:28:54.152: INFO: 
Mar 26 19:28:54.152: INFO: StatefulSet ss has not reached scale 0, at 1
[1mSTEP[0m: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9205
Mar 26 19:28:55.157: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 26 19:28:55.255: INFO: rc: 1
Mar 26 19:28:55.255: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc002399b60 exit status 1 <nil> <nil> true [0xc00232e990 0xc00232e9a8 0xc00232e9c0] [0xc00232e990 0xc00232e9a8 0xc00232e9c0] [0xc00232e9a0 0xc00232e9b8] [0x9bf9d0 0x9bf9d0] 0xc002689620 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

Mar 26 19:29:05.256: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 26 19:29:05.326: INFO: rc: 1
Mar 26 19:29:05.326: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0001e8d80 exit status 1 <nil> <nil> true [0xc001c3c768 0xc001c3c780 0xc001c3c798] [0xc001c3c768 0xc001c3c780 0xc001c3c798] [0xc001c3c778 0xc001c3c790] [0x9bf9d0 0x9bf9d0] 0xc003723200 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar 26 19:29:15.327: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 26 19:29:15.395: INFO: rc: 1
Mar 26 19:29:15.395: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0001e9500 exit status 1 <nil> <nil> true [0xc001c3c7a0 0xc001c3c7b8 0xc001c3c7d8] [0xc001c3c7a0 0xc001c3c7b8 0xc001c3c7d8] [0xc001c3c7b0 0xc001c3c7d0] [0x9bf9d0 0x9bf9d0] 0xc003723d40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar 26 19:29:25.395: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 26 19:29:25.471: INFO: rc: 1
Mar 26 19:29:25.471: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc003674a20 exit status 1 <nil> <nil> true [0xc001ff4568 0xc001ff4598 0xc001ff45b8] [0xc001ff4568 0xc001ff4598 0xc001ff45b8] [0xc001ff4578 0xc001ff45b0] [0x9bf9d0 0x9bf9d0] 0xc001dd6ba0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar 26 19:29:35.471: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 26 19:29:35.545: INFO: rc: 1
Mar 26 19:29:35.545: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0001e9770 exit status 1 <nil> <nil> true [0xc001c3c7e0 0xc001c3c7f8 0xc001c3c810] [0xc001c3c7e0 0xc001c3c7f8 0xc001c3c810] [0xc001c3c7f0 0xc001c3c808] [0x9bf9d0 0x9bf9d0] 0xc0023cc360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar 26 19:29:45.545: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 26 19:29:45.618: INFO: rc: 1
Mar 26 19:29:45.618: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0012fa240 exit status 1 <nil> <nil> true [0xc0003f2098 0xc00017c000 0xc00017c6b8] [0xc0003f2098 0xc00017c000 0xc00017c6b8] [0xc0003f2358 0xc00017c120] [0x9bf9d0 0x9bf9d0] 0xc0037222a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar 26 19:29:55.618: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 26 19:29:55.687: INFO: rc: 1
Mar 26 19:29:55.687: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0012fa5a0 exit status 1 <nil> <nil> true [0xc00017c748 0xc00017ca00 0xc00017cfd8] [0xc00017c748 0xc00017ca00 0xc00017cfd8] [0xc00017c998 0xc00017cd90] [0x9bf9d0 0x9bf9d0] 0xc0037226c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar 26 19:30:05.688: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 26 19:30:05.758: INFO: rc: 1
Mar 26 19:30:05.758: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0022ee240 exit status 1 <nil> <nil> true [0xc0000103f8 0xc000010558 0xc0000107d8] [0xc0000103f8 0xc000010558 0xc0000107d8] [0xc0000104f8 0xc000010768] [0x9bf9d0 0x9bf9d0] 0xc0020310e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar 26 19:30:15.758: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 26 19:30:15.833: INFO: rc: 1
Mar 26 19:30:15.833: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0012fa810 exit status 1 <nil> <nil> true [0xc00017cff8 0xc00017d2c0 0xc00017d470] [0xc00017cff8 0xc00017d2c0 0xc00017d470] [0xc00017d1b8 0xc00017d448] [0x9bf9d0 0x9bf9d0] 0xc003722ba0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar 26 19:30:25.833: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 26 19:30:25.903: INFO: rc: 1
Mar 26 19:30:25.903: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0012faa80 exit status 1 <nil> <nil> true [0xc00017d550 0xc00017d680 0xc00017d828] [0xc00017d550 0xc00017d680 0xc00017d828] [0xc00017d628 0xc00017d7a8] [0x9bf9d0 0x9bf9d0] 0xc003722ea0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar 26 19:30:35.904: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 26 19:30:35.960: INFO: rc: 1
Mar 26 19:30:35.960: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002914240 exit status 1 <nil> <nil> true [0xc00232e000 0xc00232e018 0xc00232e030] [0xc00232e000 0xc00232e018 0xc00232e030] [0xc00232e010 0xc00232e028] [0x9bf9d0 0x9bf9d0] 0xc002e96240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar 26 19:30:45.960: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 26 19:30:46.032: INFO: rc: 1
Mar 26 19:30:46.032: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0012fae40 exit status 1 <nil> <nil> true [0xc00017d878 0xc00017d938 0xc00017da90] [0xc00017d878 0xc00017d938 0xc00017da90] [0xc00017d918 0xc00017d9c8] [0x9bf9d0 0x9bf9d0] 0xc0037231a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar 26 19:30:56.033: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 26 19:30:56.116: INFO: rc: 1
Mar 26 19:30:56.116: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0022ee4e0 exit status 1 <nil> <nil> true [0xc000010808 0xc0000108d0 0xc000010970] [0xc000010808 0xc0000108d0 0xc000010970] [0xc000010888 0xc000010958] [0x9bf9d0 0x9bf9d0] 0xc002031d40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar 26 19:31:06.117: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 26 19:31:06.186: INFO: rc: 1
Mar 26 19:31:06.186: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0020c8270 exit status 1 <nil> <nil> true [0xc001c3c000 0xc001c3c018 0xc001c3c030] [0xc001c3c000 0xc001c3c018 0xc001c3c030] [0xc001c3c010 0xc001c3c028] [0x9bf9d0 0x9bf9d0] 0xc0020da240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar 26 19:31:16.186: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 26 19:31:16.253: INFO: rc: 1
Mar 26 19:31:16.253: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0020c84b0 exit status 1 <nil> <nil> true [0xc001c3c038 0xc001c3c050 0xc001c3c068] [0xc001c3c038 0xc001c3c050 0xc001c3c068] [0xc001c3c048 0xc001c3c060] [0x9bf9d0 0x9bf9d0] 0xc0020da780 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar 26 19:31:26.253: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 26 19:31:26.324: INFO: rc: 1
Mar 26 19:31:26.324: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0020c86f0 exit status 1 <nil> <nil> true [0xc001c3c070 0xc001c3c088 0xc001c3c0a0] [0xc001c3c070 0xc001c3c088 0xc001c3c0a0] [0xc001c3c080 0xc001c3c098] [0x9bf9d0 0x9bf9d0] 0xc0020db380 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar 26 19:31:36.324: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 26 19:31:36.392: INFO: rc: 1
Mar 26 19:31:36.392: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002914510 exit status 1 <nil> <nil> true [0xc00232e038 0xc00232e050 0xc00232e068] [0xc00232e038 0xc00232e050 0xc00232e068] [0xc00232e048 0xc00232e060] [0x9bf9d0 0x9bf9d0] 0xc002e965a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar 26 19:31:46.392: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 26 19:31:46.463: INFO: rc: 1
Mar 26 19:31:46.463: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0022ee210 exit status 1 <nil> <nil> true [0xc0003f2098 0xc0000103f8 0xc000010558] [0xc0003f2098 0xc0000103f8 0xc000010558] [0xc0003f2358 0xc0000104f8] [0x9bf9d0 0x9bf9d0] 0xc0020310e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar 26 19:31:56.464: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 26 19:31:56.532: INFO: rc: 1
Mar 26 19:31:56.532: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0022ee4b0 exit status 1 <nil> <nil> true [0xc000010668 0xc000010808 0xc0000108d0] [0xc000010668 0xc000010808 0xc0000108d0] [0xc0000107d8 0xc000010888] [0x9bf9d0 0x9bf9d0] 0xc002031d40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar 26 19:32:06.532: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 26 19:32:06.608: INFO: rc: 1
Mar 26 19:32:06.608: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002914210 exit status 1 <nil> <nil> true [0xc00232e000 0xc00232e018 0xc00232e030] [0xc00232e000 0xc00232e018 0xc00232e030] [0xc00232e010 0xc00232e028] [0x9bf9d0 0x9bf9d0] 0xc002e96240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar 26 19:32:16.608: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 26 19:32:16.679: INFO: rc: 1
Mar 26 19:32:16.679: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0022ee780 exit status 1 <nil> <nil> true [0xc000010948 0xc0000109d8 0xc000010a80] [0xc000010948 0xc0000109d8 0xc000010a80] [0xc000010970 0xc000010a70] [0x9bf9d0 0x9bf9d0] 0xc0020da240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar 26 19:32:26.679: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 26 19:32:26.743: INFO: rc: 1
Mar 26 19:32:26.743: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0022ee9f0 exit status 1 <nil> <nil> true [0xc000010ab8 0xc000010b30 0xc000010bb8] [0xc000010ab8 0xc000010b30 0xc000010bb8] [0xc000010b20 0xc000010b88] [0x9bf9d0 0x9bf9d0] 0xc0020da780 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar 26 19:32:36.743: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 26 19:32:36.809: INFO: rc: 1
Mar 26 19:32:36.809: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0012fa270 exit status 1 <nil> <nil> true [0xc001c3c000 0xc001c3c018 0xc001c3c030] [0xc001c3c000 0xc001c3c018 0xc001c3c030] [0xc001c3c010 0xc001c3c028] [0x9bf9d0 0x9bf9d0] 0xc0037222a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar 26 19:32:46.809: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 26 19:32:46.880: INFO: rc: 1
Mar 26 19:32:46.880: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0012fa600 exit status 1 <nil> <nil> true [0xc001c3c038 0xc001c3c050 0xc001c3c068] [0xc001c3c038 0xc001c3c050 0xc001c3c068] [0xc001c3c048 0xc001c3c060] [0x9bf9d0 0x9bf9d0] 0xc0037226c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar 26 19:32:56.880: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 26 19:32:56.939: INFO: rc: 1
Mar 26 19:32:56.939: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002914480 exit status 1 <nil> <nil> true [0xc00232e038 0xc00232e050 0xc00232e068] [0xc00232e038 0xc00232e050 0xc00232e068] [0xc00232e048 0xc00232e060] [0x9bf9d0 0x9bf9d0] 0xc002e965a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar 26 19:33:06.939: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 26 19:33:07.007: INFO: rc: 1
Mar 26 19:33:07.007: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002914720 exit status 1 <nil> <nil> true [0xc00232e070 0xc00232e088 0xc00232e0a0] [0xc00232e070 0xc00232e088 0xc00232e0a0] [0xc00232e080 0xc00232e098] [0x9bf9d0 0x9bf9d0] 0xc002e968a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar 26 19:33:17.008: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 26 19:33:17.078: INFO: rc: 1
Mar 26 19:33:17.078: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0020c82d0 exit status 1 <nil> <nil> true [0xc00017c000 0xc00017c6b8 0xc00017c998] [0xc00017c000 0xc00017c6b8 0xc00017c998] [0xc00017c120 0xc00017c990] [0x9bf9d0 0x9bf9d0] 0xc0026102a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar 26 19:33:27.078: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 26 19:33:27.144: INFO: rc: 1
Mar 26 19:33:27.144: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0012fa8a0 exit status 1 <nil> <nil> true [0xc001c3c070 0xc001c3c088 0xc001c3c0a0] [0xc001c3c070 0xc001c3c088 0xc001c3c0a0] [0xc001c3c080 0xc001c3c098] [0x9bf9d0 0x9bf9d0] 0xc003722ba0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar 26 19:33:37.144: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 26 19:33:37.218: INFO: rc: 1
Mar 26 19:33:37.218: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0020c8540 exit status 1 <nil> <nil> true [0xc00017ca00 0xc00017cfd8 0xc00017d1b8] [0xc00017ca00 0xc00017cfd8 0xc00017d1b8] [0xc00017cd90 0xc00017d180] [0x9bf9d0 0x9bf9d0] 0xc0026105a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar 26 19:33:47.219: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 26 19:33:47.293: INFO: rc: 1
Mar 26 19:33:47.293: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl [kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0012fab10 exit status 1 <nil> <nil> true [0xc001c3c0b0 0xc001c3c0c8 0xc001c3c0e0] [0xc001c3c0b0 0xc001c3c0c8 0xc001c3c0e0] [0xc001c3c0c0 0xc001c3c0d8] [0x9bf9d0 0x9bf9d0] 0xc003722ea0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Mar 26 19:33:57.293: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance exec --namespace=statefulset-9205 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Mar 26 19:33:57.365: INFO: rc: 1
Mar 26 19:33:57.365: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: 
Mar 26 19:33:57.365: INFO: Scaling statefulset ss to 0
Mar 26 19:33:57.371: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Mar 26 19:33:57.373: INFO: Deleting all statefulset in ns statefulset-9205
Mar 26 19:33:57.375: INFO: Scaling statefulset ss to 0
Mar 26 19:33:57.381: INFO: Waiting for statefulset status.replicas updated to 0
Mar 26 19:33:57.383: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:33:57.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "statefulset-9205" for this suite.
Mar 26 19:34:03.400: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:34:03.470: INFO: namespace statefulset-9205 deletion completed in 6.077281555s

[32mâ€¢ [SLOW TEST:360.227 seconds][0m
[sig-apps] StatefulSet
[90m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22[0m
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  [90m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687[0m
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    [90m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[90m------------------------------[0m
[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90m[k8s.io] Kubectl expose[0m 
  [1mshould create services for rc  [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:34:03.470: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create services for rc  [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: creating Redis RC
Mar 26 19:34:03.493: INFO: namespace kubectl-7126
Mar 26 19:34:03.493: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance create -f - --namespace=kubectl-7126'
Mar 26 19:34:04.022: INFO: stderr: ""
Mar 26 19:34:04.022: INFO: stdout: "replicationcontroller/redis-master created\n"
[1mSTEP[0m: Waiting for Redis master to start.
Mar 26 19:34:05.025: INFO: Selector matched 1 pods for map[app:redis]
Mar 26 19:34:05.025: INFO: Found 0 / 1
Mar 26 19:34:06.026: INFO: Selector matched 1 pods for map[app:redis]
Mar 26 19:34:06.026: INFO: Found 0 / 1
Mar 26 19:34:07.026: INFO: Selector matched 1 pods for map[app:redis]
Mar 26 19:34:07.026: INFO: Found 1 / 1
Mar 26 19:34:07.026: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 26 19:34:07.030: INFO: Selector matched 1 pods for map[app:redis]
Mar 26 19:34:07.030: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 26 19:34:07.030: INFO: wait on redis-master startup in kubectl-7126 
Mar 26 19:34:07.030: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance logs redis-master-ss5kk redis-master --namespace=kubectl-7126'
Mar 26 19:34:07.119: INFO: stderr: ""
Mar 26 19:34:07.119: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Mar 02:34:05.317 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Mar 02:34:05.317 # Server started, Redis version 3.2.12\n1:M 27 Mar 02:34:05.317 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Mar 02:34:05.317 * The server is now ready to accept connections on port 6379\n"
[1mSTEP[0m: exposing RC
Mar 26 19:34:07.120: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-7126'
Mar 26 19:34:07.209: INFO: stderr: ""
Mar 26 19:34:07.209: INFO: stdout: "service/rm2 exposed\n"
Mar 26 19:34:07.211: INFO: Service rm2 in namespace kubectl-7126 found.
[1mSTEP[0m: exposing service
Mar 26 19:34:09.216: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-7126'
Mar 26 19:34:09.305: INFO: stderr: ""
Mar 26 19:34:09.305: INFO: stdout: "service/rm3 exposed\n"
Mar 26 19:34:09.308: INFO: Service rm3 in namespace kubectl-7126 found.
[AfterEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:34:11.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-7126" for this suite.
Mar 26 19:34:33.328: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:34:33.393: INFO: namespace kubectl-7126 deletion completed in 22.074431308s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] ReplicationController[0m 
  [1mshould adopt matching pods on creation [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-apps] ReplicationController
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:34:33.393: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename replication-controller
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Given a Pod with a 'name' label pod-adoption is created
[1mSTEP[0m: When a replication controller with a matching selector is created
[1mSTEP[0m: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:34:38.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "replication-controller-5245" for this suite.
Mar 26 19:35:00.462: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:35:00.532: INFO: namespace replication-controller-5245 deletion completed in 22.081563861s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] EmptyDir volumes[0m 
  [1mshould support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] EmptyDir volumes
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:35:00.533: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename emptydir
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating a pod to test emptydir 0666 on node default medium
Mar 26 19:35:00.580: INFO: Waiting up to 5m0s for pod "pod-eb2ff2d3-5038-11e9-9719-a08cfdecc127" in namespace "emptydir-6794" to be "success or failure"
Mar 26 19:35:00.596: INFO: Pod "pod-eb2ff2d3-5038-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 16.433629ms
Mar 26 19:35:02.600: INFO: Pod "pod-eb2ff2d3-5038-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020550526s
Mar 26 19:35:04.604: INFO: Pod "pod-eb2ff2d3-5038-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024515386s
[1mSTEP[0m: Saw pod success
Mar 26 19:35:04.604: INFO: Pod "pod-eb2ff2d3-5038-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 19:35:04.608: INFO: Trying to get logs from node conformance-worker pod pod-eb2ff2d3-5038-11e9-9719-a08cfdecc127 container test-container: <nil>
[1mSTEP[0m: delete the pod
Mar 26 19:35:04.628: INFO: Waiting for pod pod-eb2ff2d3-5038-11e9-9719-a08cfdecc127 to disappear
Mar 26 19:35:04.630: INFO: Pod pod-eb2ff2d3-5038-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:35:04.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "emptydir-6794" for this suite.
Mar 26 19:35:10.641: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:35:10.714: INFO: namespace emptydir-6794 deletion completed in 6.081599067s
[32mâ€¢[0m
[90m------------------------------[0m
[0m[sig-network] Networking[0m [90mGranular Checks: Pods[0m 
  [1mshould function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-network] Networking
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:35:10.714: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename pod-network-test
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Performing setup for networking test in namespace pod-network-test-8481
[1mSTEP[0m: creating a selector
[1mSTEP[0m: Creating the service pods in kubernetes
Mar 26 19:35:10.746: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[1mSTEP[0m: Creating test pods
Mar 26 19:35:34.808: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.32.0.2:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8481 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 19:35:34.808: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
Mar 26 19:35:35.026: INFO: Found all expected endpoints: [netserver-0]
Mar 26 19:35:35.028: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.46.0.1:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8481 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 19:35:35.028: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
Mar 26 19:35:35.191: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:35:35.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "pod-network-test-8481" for this suite.
Mar 26 19:35:57.202: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:35:57.276: INFO: namespace pod-network-test-8481 deletion completed in 22.083380617s
[32mâ€¢[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected secret[0m 
  [1mshould be consumable from pods in volume [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Projected secret
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:35:57.276: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating projection with secret that has name projected-secret-test-0d02e4a2-5039-11e9-9719-a08cfdecc127
[1mSTEP[0m: Creating a pod to test consume secrets
Mar 26 19:35:57.327: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0d0432e3-5039-11e9-9719-a08cfdecc127" in namespace "projected-1325" to be "success or failure"
Mar 26 19:35:57.329: INFO: Pod "pod-projected-secrets-0d0432e3-5039-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 1.499947ms
Mar 26 19:35:59.333: INFO: Pod "pod-projected-secrets-0d0432e3-5039-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005170469s
[1mSTEP[0m: Saw pod success
Mar 26 19:35:59.333: INFO: Pod "pod-projected-secrets-0d0432e3-5039-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 19:35:59.336: INFO: Trying to get logs from node conformance-worker2 pod pod-projected-secrets-0d0432e3-5039-11e9-9719-a08cfdecc127 container projected-secret-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar 26 19:35:59.358: INFO: Waiting for pod pod-projected-secrets-0d0432e3-5039-11e9-9719-a08cfdecc127 to disappear
Mar 26 19:35:59.360: INFO: Pod pod-projected-secrets-0d0432e3-5039-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] Projected secret
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:35:59.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-1325" for this suite.
Mar 26 19:36:05.370: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:36:05.425: INFO: namespace projected-1325 deletion completed in 6.062231967s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-auth] ServiceAccounts[0m 
  [1mshould mount an API token into pods  [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-auth] ServiceAccounts
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:36:05.425: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename svcaccounts
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: getting the auto-created API token
[1mSTEP[0m: reading a file in the container
Mar 26 19:36:07.971: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl exec --namespace=svcaccounts-7080 pod-service-account-12299020-5039-11e9-9719-a08cfdecc127 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
[1mSTEP[0m: reading a file in the container
Mar 26 19:36:08.234: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl exec --namespace=svcaccounts-7080 pod-service-account-12299020-5039-11e9-9719-a08cfdecc127 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
[1mSTEP[0m: reading a file in the container
Mar 26 19:36:08.459: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl exec --namespace=svcaccounts-7080 pod-service-account-12299020-5039-11e9-9719-a08cfdecc127 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:36:08.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "svcaccounts-7080" for this suite.
Mar 26 19:36:14.702: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:36:14.754: INFO: namespace svcaccounts-7080 deletion completed in 6.059257141s
[32mâ€¢[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] EmptyDir volumes[0m 
  [1mvolume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] EmptyDir volumes
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:36:14.754: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename emptydir
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating a pod to test emptydir volume type on node default medium
Mar 26 19:36:14.782: INFO: Waiting up to 5m0s for pod "pod-176b906e-5039-11e9-9719-a08cfdecc127" in namespace "emptydir-3895" to be "success or failure"
Mar 26 19:36:14.784: INFO: Pod "pod-176b906e-5039-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 1.971454ms
Mar 26 19:36:16.788: INFO: Pod "pod-176b906e-5039-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005446425s
[1mSTEP[0m: Saw pod success
Mar 26 19:36:16.788: INFO: Pod "pod-176b906e-5039-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 19:36:16.791: INFO: Trying to get logs from node conformance-worker2 pod pod-176b906e-5039-11e9-9719-a08cfdecc127 container test-container: <nil>
[1mSTEP[0m: delete the pod
Mar 26 19:36:16.807: INFO: Waiting for pod pod-176b906e-5039-11e9-9719-a08cfdecc127 to disappear
Mar 26 19:36:16.809: INFO: Pod pod-176b906e-5039-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:36:16.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "emptydir-3895" for this suite.
Mar 26 19:36:22.819: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:36:22.882: INFO: namespace emptydir-3895 deletion completed in 6.070727987s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] InitContainer [NodeConformance][0m 
  [1mshould invoke init containers on a RestartNever pod [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:36:22.882: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename init-container
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: creating the pod
Mar 26 19:36:22.910: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:36:27.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "init-container-4935" for this suite.
Mar 26 19:36:33.347: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:36:33.416: INFO: namespace init-container-4935 deletion completed in 6.077189363s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected secret[0m 
  [1mshould be consumable in multiple volumes in a pod [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Projected secret
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:36:33.416: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating secret with name projected-secret-test-228b2d88-5039-11e9-9719-a08cfdecc127
[1mSTEP[0m: Creating a pod to test consume secrets
Mar 26 19:36:33.447: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-228b7686-5039-11e9-9719-a08cfdecc127" in namespace "projected-485" to be "success or failure"
Mar 26 19:36:33.451: INFO: Pod "pod-projected-secrets-228b7686-5039-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 3.610134ms
Mar 26 19:36:35.455: INFO: Pod "pod-projected-secrets-228b7686-5039-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007419921s
Mar 26 19:36:37.459: INFO: Pod "pod-projected-secrets-228b7686-5039-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011391169s
[1mSTEP[0m: Saw pod success
Mar 26 19:36:37.459: INFO: Pod "pod-projected-secrets-228b7686-5039-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 19:36:37.462: INFO: Trying to get logs from node conformance-worker2 pod pod-projected-secrets-228b7686-5039-11e9-9719-a08cfdecc127 container secret-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar 26 19:36:37.482: INFO: Waiting for pod pod-projected-secrets-228b7686-5039-11e9-9719-a08cfdecc127 to disappear
Mar 26 19:36:37.484: INFO: Pod pod-projected-secrets-228b7686-5039-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] Projected secret
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:36:37.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-485" for this suite.
Mar 26 19:36:43.495: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:36:43.554: INFO: namespace projected-485 deletion completed in 6.06706188s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Secrets[0m 
  [1mshould be consumable from pods in env vars [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-api-machinery] Secrets
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:36:43.554: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename secrets
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating secret with name secret-test-28966863-5039-11e9-9719-a08cfdecc127
[1mSTEP[0m: Creating a pod to test consume secrets
Mar 26 19:36:43.588: INFO: Waiting up to 5m0s for pod "pod-secrets-2896b39a-5039-11e9-9719-a08cfdecc127" in namespace "secrets-3670" to be "success or failure"
Mar 26 19:36:43.590: INFO: Pod "pod-secrets-2896b39a-5039-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 1.877408ms
Mar 26 19:36:45.593: INFO: Pod "pod-secrets-2896b39a-5039-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005675534s
[1mSTEP[0m: Saw pod success
Mar 26 19:36:45.593: INFO: Pod "pod-secrets-2896b39a-5039-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 19:36:45.596: INFO: Trying to get logs from node conformance-worker pod pod-secrets-2896b39a-5039-11e9-9719-a08cfdecc127 container secret-env-test: <nil>
[1mSTEP[0m: delete the pod
Mar 26 19:36:45.611: INFO: Waiting for pod pod-secrets-2896b39a-5039-11e9-9719-a08cfdecc127 to disappear
Mar 26 19:36:45.613: INFO: Pod pod-secrets-2896b39a-5039-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:36:45.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "secrets-3670" for this suite.
Mar 26 19:36:51.624: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:36:51.689: INFO: namespace secrets-3670 deletion completed in 6.073098968s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Probing container[0m 
  [1mshould be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [k8s.io] Probing container
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:36:51.689: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename container-probe
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating pod liveness-exec in namespace container-probe-9911
Mar 26 19:36:55.725: INFO: Started pod liveness-exec in namespace container-probe-9911
[1mSTEP[0m: checking the pod's current state and verifying that restartCount is present
Mar 26 19:36:55.728: INFO: Initial restart count of pod liveness-exec is 0
Mar 26 19:37:41.816: INFO: Restart count of pod container-probe-9911/liveness-exec is now 1 (46.087686741s elapsed)
[1mSTEP[0m: deleting the pod
[AfterEach] [k8s.io] Probing container
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:37:41.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "container-probe-9911" for this suite.
Mar 26 19:37:47.837: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:37:47.893: INFO: namespace container-probe-9911 deletion completed in 6.065642681s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Probing container[0m 
  [1mshould *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [k8s.io] Probing container
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:37:47.894: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename container-probe
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating pod liveness-exec in namespace container-probe-3483
Mar 26 19:37:49.933: INFO: Started pod liveness-exec in namespace container-probe-3483
[1mSTEP[0m: checking the pod's current state and verifying that restartCount is present
Mar 26 19:37:49.936: INFO: Initial restart count of pod liveness-exec is 0
[1mSTEP[0m: deleting the pod
[AfterEach] [k8s.io] Probing container
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:41:50.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "container-probe-3483" for this suite.
Mar 26 19:41:56.430: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:41:56.502: INFO: namespace container-probe-3483 deletion completed in 6.084316216s
[32mâ€¢[0m
[90m------------------------------[0m
[0m[sig-storage] Projected downwardAPI[0m 
  [1mshould update labels on modification [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Projected downwardAPI
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:41:56.502: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating the pod
Mar 26 19:41:59.066: INFO: Successfully updated pod "labelsupdatee31f6e6f-5039-11e9-9719-a08cfdecc127"
[AfterEach] [sig-storage] Projected downwardAPI
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:42:03.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-6585" for this suite.
Mar 26 19:42:25.108: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:42:25.176: INFO: namespace projected-6585 deletion completed in 22.077633385s
[32mâ€¢[0m
[90m------------------------------[0m
[0m[sig-network] Services[0m 
  [1mshould serve multiport endpoints from pods  [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-network] Services
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:42:25.176: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename services
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve multiport endpoints from pods  [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: creating service multi-endpoint-test in namespace services-5013
[1mSTEP[0m: waiting up to 3m0s for service multi-endpoint-test in namespace services-5013 to expose endpoints map[]
Mar 26 19:42:25.217: INFO: Get endpoints failed (2.971124ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Mar 26 19:42:26.221: INFO: successfully validated that service multi-endpoint-test in namespace services-5013 exposes endpoints map[] (1.00638163s elapsed)
[1mSTEP[0m: Creating pod pod1 in namespace services-5013
[1mSTEP[0m: waiting up to 3m0s for service multi-endpoint-test in namespace services-5013 to expose endpoints map[pod1:[100]]
Mar 26 19:42:28.253: INFO: successfully validated that service multi-endpoint-test in namespace services-5013 exposes endpoints map[pod1:[100]] (2.025497506s elapsed)
[1mSTEP[0m: Creating pod pod2 in namespace services-5013
[1mSTEP[0m: waiting up to 3m0s for service multi-endpoint-test in namespace services-5013 to expose endpoints map[pod1:[100] pod2:[101]]
Mar 26 19:42:30.280: INFO: successfully validated that service multi-endpoint-test in namespace services-5013 exposes endpoints map[pod1:[100] pod2:[101]] (2.022602595s elapsed)
[1mSTEP[0m: Deleting pod pod1 in namespace services-5013
[1mSTEP[0m: waiting up to 3m0s for service multi-endpoint-test in namespace services-5013 to expose endpoints map[pod2:[101]]
Mar 26 19:42:30.298: INFO: successfully validated that service multi-endpoint-test in namespace services-5013 exposes endpoints map[pod2:[101]] (12.516054ms elapsed)
[1mSTEP[0m: Deleting pod pod2 in namespace services-5013
[1mSTEP[0m: waiting up to 3m0s for service multi-endpoint-test in namespace services-5013 to expose endpoints map[]
Mar 26 19:42:30.310: INFO: successfully validated that service multi-endpoint-test in namespace services-5013 exposes endpoints map[] (6.473068ms elapsed)
[AfterEach] [sig-network] Services
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:42:30.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "services-5013" for this suite.
Mar 26 19:42:36.333: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:42:36.399: INFO: namespace services-5013 deletion completed in 6.075353343s
[AfterEach] [sig-network] Services
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90m[k8s.io] Kubectl run default[0m 
  [1mshould create an rc or deployment from an image  [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:42:36.399: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run default
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1318
[It] should create an rc or deployment from an image  [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: running the image docker.io/library/nginx:1.14-alpine
Mar 26 19:42:36.433: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-2403'
Mar 26 19:42:36.515: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar 26 19:42:36.515: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
[1mSTEP[0m: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1324
Mar 26 19:42:36.519: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance delete deployment e2e-test-nginx-deployment --namespace=kubectl-2403'
Mar 26 19:42:36.587: INFO: stderr: ""
Mar 26 19:42:36.587: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:42:36.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-2403" for this suite.
Mar 26 19:42:42.598: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:42:42.663: INFO: namespace kubectl-2403 deletion completed in 6.073720482s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Networking[0m [90mGranular Checks: Pods[0m 
  [1mshould function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-network] Networking
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:42:42.664: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename pod-network-test
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Performing setup for networking test in namespace pod-network-test-5210
[1mSTEP[0m: creating a selector
[1mSTEP[0m: Creating the service pods in kubernetes
Mar 26 19:42:42.694: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[1mSTEP[0m: Creating test pods
Mar 26 19:43:04.756: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.46.0.2:8080/dial?request=hostName&protocol=udp&host=10.32.0.3&port=8081&tries=1'] Namespace:pod-network-test-5210 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 19:43:04.756: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
Mar 26 19:43:04.963: INFO: Waiting for endpoints: map[]
Mar 26 19:43:04.966: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.46.0.2:8080/dial?request=hostName&protocol=udp&host=10.46.0.1&port=8081&tries=1'] Namespace:pod-network-test-5210 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 19:43:04.966: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
Mar 26 19:43:05.121: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:43:05.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "pod-network-test-5210" for this suite.
Mar 26 19:43:27.132: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:43:27.202: INFO: namespace pod-network-test-5210 deletion completed in 22.078503156s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected secret[0m 
  [1mshould be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Projected secret
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:43:27.203: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating projection with secret that has name projected-secret-test-1930601d-503a-11e9-9719-a08cfdecc127
[1mSTEP[0m: Creating a pod to test consume secrets
Mar 26 19:43:27.254: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-19311328-503a-11e9-9719-a08cfdecc127" in namespace "projected-8314" to be "success or failure"
Mar 26 19:43:27.265: INFO: Pod "pod-projected-secrets-19311328-503a-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 10.894837ms
Mar 26 19:43:29.269: INFO: Pod "pod-projected-secrets-19311328-503a-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014801942s
[1mSTEP[0m: Saw pod success
Mar 26 19:43:29.269: INFO: Pod "pod-projected-secrets-19311328-503a-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 19:43:29.272: INFO: Trying to get logs from node conformance-worker2 pod pod-projected-secrets-19311328-503a-11e9-9719-a08cfdecc127 container projected-secret-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar 26 19:43:29.291: INFO: Waiting for pod pod-projected-secrets-19311328-503a-11e9-9719-a08cfdecc127 to disappear
Mar 26 19:43:29.293: INFO: Pod pod-projected-secrets-19311328-503a-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] Projected secret
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:43:29.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-8314" for this suite.
Mar 26 19:43:35.303: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:43:35.369: INFO: namespace projected-8314 deletion completed in 6.074060014s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-auth] ServiceAccounts[0m 
  [1mshould allow opting out of API token automount  [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-auth] ServiceAccounts
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:43:35.370: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename svcaccounts
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: getting the auto-created API token
Mar 26 19:43:35.917: INFO: created pod pod-service-account-defaultsa
Mar 26 19:43:35.917: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Mar 26 19:43:35.932: INFO: created pod pod-service-account-mountsa
Mar 26 19:43:35.932: INFO: pod pod-service-account-mountsa service account token volume mount: true
Mar 26 19:43:35.935: INFO: created pod pod-service-account-nomountsa
Mar 26 19:43:35.935: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Mar 26 19:43:35.946: INFO: created pod pod-service-account-defaultsa-mountspec
Mar 26 19:43:35.947: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Mar 26 19:43:35.953: INFO: created pod pod-service-account-mountsa-mountspec
Mar 26 19:43:35.953: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Mar 26 19:43:35.959: INFO: created pod pod-service-account-nomountsa-mountspec
Mar 26 19:43:35.959: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Mar 26 19:43:35.964: INFO: created pod pod-service-account-defaultsa-nomountspec
Mar 26 19:43:35.964: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Mar 26 19:43:35.975: INFO: created pod pod-service-account-mountsa-nomountspec
Mar 26 19:43:35.975: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Mar 26 19:43:35.982: INFO: created pod pod-service-account-nomountsa-nomountspec
Mar 26 19:43:35.982: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:43:35.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "svcaccounts-5813" for this suite.
Mar 26 19:43:42.008: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:43:42.071: INFO: namespace svcaccounts-5813 deletion completed in 6.078491139s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90m[k8s.io] Kubectl label[0m 
  [1mshould update the label on a resource  [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:43:42.071: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl label
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1108
[1mSTEP[0m: creating the pod
Mar 26 19:43:42.100: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance create -f - --namespace=kubectl-3864'
Mar 26 19:43:42.251: INFO: stderr: ""
Mar 26 19:43:42.251: INFO: stdout: "pod/pause created\n"
Mar 26 19:43:42.251: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Mar 26 19:43:42.251: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-3864" to be "running and ready"
Mar 26 19:43:42.253: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042885ms
Mar 26 19:43:44.257: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006161156s
Mar 26 19:43:46.261: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.010023452s
Mar 26 19:43:46.261: INFO: Pod "pause" satisfied condition "running and ready"
Mar 26 19:43:46.261: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: adding the label testing-label with value testing-label-value to a pod
Mar 26 19:43:46.261: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance label pods pause testing-label=testing-label-value --namespace=kubectl-3864'
Mar 26 19:43:46.340: INFO: stderr: ""
Mar 26 19:43:46.340: INFO: stdout: "pod/pause labeled\n"
[1mSTEP[0m: verifying the pod has the label testing-label with the value testing-label-value
Mar 26 19:43:46.340: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get pod pause -L testing-label --namespace=kubectl-3864'
Mar 26 19:43:46.400: INFO: stderr: ""
Mar 26 19:43:46.400: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
[1mSTEP[0m: removing the label testing-label of a pod
Mar 26 19:43:46.400: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance label pods pause testing-label- --namespace=kubectl-3864'
Mar 26 19:43:46.471: INFO: stderr: ""
Mar 26 19:43:46.471: INFO: stdout: "pod/pause labeled\n"
[1mSTEP[0m: verifying the pod doesn't have the label testing-label
Mar 26 19:43:46.471: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get pod pause -L testing-label --namespace=kubectl-3864'
Mar 26 19:43:46.535: INFO: stderr: ""
Mar 26 19:43:46.535: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
[AfterEach] [k8s.io] Kubectl label
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1115
[1mSTEP[0m: using delete to clean up resources
Mar 26 19:43:46.535: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance delete --grace-period=0 --force -f - --namespace=kubectl-3864'
Mar 26 19:43:46.603: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 26 19:43:46.603: INFO: stdout: "pod \"pause\" force deleted\n"
Mar 26 19:43:46.603: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get rc,svc -l name=pause --no-headers --namespace=kubectl-3864'
Mar 26 19:43:46.671: INFO: stderr: "No resources found.\n"
Mar 26 19:43:46.671: INFO: stdout: ""
Mar 26 19:43:46.671: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get pods -l name=pause --namespace=kubectl-3864 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 26 19:43:46.748: INFO: stderr: ""
Mar 26 19:43:46.748: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:43:46.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-3864" for this suite.
Mar 26 19:43:52.758: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:43:52.817: INFO: namespace kubectl-3864 deletion completed in 6.066144585s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Services[0m 
  [1mshould serve a basic endpoint from pods  [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-network] Services
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:43:52.817: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename services
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve a basic endpoint from pods  [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: creating service endpoint-test2 in namespace services-9361
[1mSTEP[0m: waiting up to 3m0s for service endpoint-test2 in namespace services-9361 to expose endpoints map[]
Mar 26 19:43:52.874: INFO: Get endpoints failed (4.284228ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Mar 26 19:43:53.876: INFO: successfully validated that service endpoint-test2 in namespace services-9361 exposes endpoints map[] (1.006497244s elapsed)
[1mSTEP[0m: Creating pod pod1 in namespace services-9361
[1mSTEP[0m: waiting up to 3m0s for service endpoint-test2 in namespace services-9361 to expose endpoints map[pod1:[80]]
Mar 26 19:43:55.907: INFO: successfully validated that service endpoint-test2 in namespace services-9361 exposes endpoints map[pod1:[80]] (2.024714862s elapsed)
[1mSTEP[0m: Creating pod pod2 in namespace services-9361
[1mSTEP[0m: waiting up to 3m0s for service endpoint-test2 in namespace services-9361 to expose endpoints map[pod1:[80] pod2:[80]]
Mar 26 19:43:58.948: INFO: successfully validated that service endpoint-test2 in namespace services-9361 exposes endpoints map[pod1:[80] pod2:[80]] (3.037164856s elapsed)
[1mSTEP[0m: Deleting pod pod1 in namespace services-9361
[1mSTEP[0m: waiting up to 3m0s for service endpoint-test2 in namespace services-9361 to expose endpoints map[pod2:[80]]
Mar 26 19:43:59.967: INFO: successfully validated that service endpoint-test2 in namespace services-9361 exposes endpoints map[pod2:[80]] (1.01511805s elapsed)
[1mSTEP[0m: Deleting pod pod2 in namespace services-9361
[1mSTEP[0m: waiting up to 3m0s for service endpoint-test2 in namespace services-9361 to expose endpoints map[]
Mar 26 19:44:00.979: INFO: successfully validated that service endpoint-test2 in namespace services-9361 exposes endpoints map[] (1.005926658s elapsed)
[AfterEach] [sig-network] Services
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:44:00.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "services-9361" for this suite.
Mar 26 19:44:07.004: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:44:07.066: INFO: namespace services-9361 deletion completed in 6.070405422s
[AfterEach] [sig-network] Services
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Container Lifecycle Hook[0m [90mwhen create a pod with lifecycle hook[0m 
  [1mshould execute poststart http hook properly [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:44:07.067: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename container-lifecycle-hook
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
[1mSTEP[0m: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: create the pod with lifecycle hook
[1mSTEP[0m: check poststart hook
[1mSTEP[0m: delete the pod with lifecycle hook
Mar 26 19:44:13.187: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 26 19:44:13.195: INFO: Pod pod-with-poststart-http-hook still exists
Mar 26 19:44:15.195: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 26 19:44:15.199: INFO: Pod pod-with-poststart-http-hook still exists
Mar 26 19:44:17.195: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 26 19:44:17.199: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:44:17.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "container-lifecycle-hook-9967" for this suite.
Mar 26 19:44:39.212: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:44:39.279: INFO: namespace container-lifecycle-hook-9967 deletion completed in 22.076717639s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] EmptyDir volumes[0m 
  [1mshould support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] EmptyDir volumes
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:44:39.279: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename emptydir
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating a pod to test emptydir 0666 on tmpfs
Mar 26 19:44:39.311: INFO: Waiting up to 5m0s for pod "pod-4424787d-503a-11e9-9719-a08cfdecc127" in namespace "emptydir-8383" to be "success or failure"
Mar 26 19:44:39.313: INFO: Pod "pod-4424787d-503a-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045482ms
Mar 26 19:44:41.317: INFO: Pod "pod-4424787d-503a-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00529995s
[1mSTEP[0m: Saw pod success
Mar 26 19:44:41.317: INFO: Pod "pod-4424787d-503a-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 19:44:41.319: INFO: Trying to get logs from node conformance-worker pod pod-4424787d-503a-11e9-9719-a08cfdecc127 container test-container: <nil>
[1mSTEP[0m: delete the pod
Mar 26 19:44:41.332: INFO: Waiting for pod pod-4424787d-503a-11e9-9719-a08cfdecc127 to disappear
Mar 26 19:44:41.335: INFO: Pod pod-4424787d-503a-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:44:41.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "emptydir-8383" for this suite.
Mar 26 19:44:47.343: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:44:47.405: INFO: namespace emptydir-8383 deletion completed in 6.068304757s
[32mâ€¢[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Garbage collector[0m 
  [1mshould orphan pods created by rc if delete options say so [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-api-machinery] Garbage collector
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:44:47.405: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename gc
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: create the rc
[1mSTEP[0m: delete the rc
[1mSTEP[0m: wait for the rc to be deleted
[1mSTEP[0m: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
[1mSTEP[0m: Gathering metrics
W0326 19:45:27.475272  168179 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar 26 19:45:27.475: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:45:27.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "gc-3192" for this suite.
Mar 26 19:45:33.487: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:45:33.543: INFO: namespace gc-3192 deletion completed in 6.064848462s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Proxy[0m [90mversion v1[0m 
  [1mshould proxy logs on node using proxy subresource  [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] version v1
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:45:33.543: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename proxy
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Mar 26 19:45:33.572: INFO: (0) /api/v1/nodes/conformance-worker/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 3.010947ms)
Mar 26 19:45:33.573: INFO: (1) /api/v1/nodes/conformance-worker/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 1.604804ms)
Mar 26 19:45:33.577: INFO: (2) /api/v1/nodes/conformance-worker/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 3.287494ms)
Mar 26 19:45:33.578: INFO: (3) /api/v1/nodes/conformance-worker/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 1.720794ms)
Mar 26 19:45:33.580: INFO: (4) /api/v1/nodes/conformance-worker/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 1.571967ms)
Mar 26 19:45:33.582: INFO: (5) /api/v1/nodes/conformance-worker/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 1.791767ms)
Mar 26 19:45:33.584: INFO: (6) /api/v1/nodes/conformance-worker/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 1.766127ms)
Mar 26 19:45:33.586: INFO: (7) /api/v1/nodes/conformance-worker/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.081003ms)
Mar 26 19:45:33.588: INFO: (8) /api/v1/nodes/conformance-worker/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.032881ms)
Mar 26 19:45:33.590: INFO: (9) /api/v1/nodes/conformance-worker/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.106867ms)
Mar 26 19:45:33.592: INFO: (10) /api/v1/nodes/conformance-worker/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.072385ms)
Mar 26 19:45:33.594: INFO: (11) /api/v1/nodes/conformance-worker/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.147772ms)
Mar 26 19:45:33.596: INFO: (12) /api/v1/nodes/conformance-worker/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.242104ms)
Mar 26 19:45:33.599: INFO: (13) /api/v1/nodes/conformance-worker/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.442392ms)
Mar 26 19:45:33.601: INFO: (14) /api/v1/nodes/conformance-worker/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.104118ms)
Mar 26 19:45:33.603: INFO: (15) /api/v1/nodes/conformance-worker/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 1.774741ms)
Mar 26 19:45:33.605: INFO: (16) /api/v1/nodes/conformance-worker/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 1.840509ms)
Mar 26 19:45:33.606: INFO: (17) /api/v1/nodes/conformance-worker/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 1.786809ms)
Mar 26 19:45:33.609: INFO: (18) /api/v1/nodes/conformance-worker/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.098742ms)
Mar 26 19:45:33.611: INFO: (19) /api/v1/nodes/conformance-worker/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.214799ms)
[AfterEach] version v1
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:45:33.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "proxy-2830" for this suite.
Mar 26 19:45:39.620: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:45:39.683: INFO: namespace proxy-2830 deletion completed in 6.070091388s
[32mâ€¢[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Downward API volume[0m 
  [1mshould set mode on item file [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Downward API volume
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:45:39.683: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename downward-api
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar 26 19:45:39.709: INFO: Waiting up to 5m0s for pod "downwardapi-volume-68247468-503a-11e9-9719-a08cfdecc127" in namespace "downward-api-6098" to be "success or failure"
Mar 26 19:45:39.712: INFO: Pod "downwardapi-volume-68247468-503a-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 3.551138ms
Mar 26 19:45:41.716: INFO: Pod "downwardapi-volume-68247468-503a-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007116927s
[1mSTEP[0m: Saw pod success
Mar 26 19:45:41.716: INFO: Pod "downwardapi-volume-68247468-503a-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 19:45:41.719: INFO: Trying to get logs from node conformance-worker2 pod downwardapi-volume-68247468-503a-11e9-9719-a08cfdecc127 container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar 26 19:45:41.735: INFO: Waiting for pod downwardapi-volume-68247468-503a-11e9-9719-a08cfdecc127 to disappear
Mar 26 19:45:41.737: INFO: Pod downwardapi-volume-68247468-503a-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:45:41.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "downward-api-6098" for this suite.
Mar 26 19:45:47.748: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:45:47.808: INFO: namespace downward-api-6098 deletion completed in 6.068648042s
[32mâ€¢[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Kubelet[0m [90mwhen scheduling a busybox command in a pod[0m 
  [1mshould print the output to logs [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [k8s.io] Kubelet
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:45:47.808: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename kubelet-test
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:45:49.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubelet-test-1662" for this suite.
Mar 26 19:46:45.871: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:46:45.930: INFO: namespace kubelet-test-1662 deletion completed in 56.067389273s
[32mâ€¢[0m
[90m------------------------------[0m
[0m[sig-storage] Subpath[0m [90mAtomic writer volumes[0m 
  [1mshould support subpaths with projected pod [LinuxOnly] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Subpath
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:46:45.930: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename subpath
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
[1mSTEP[0m: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating pod pod-subpath-test-projected-6kz9
[1mSTEP[0m: Creating a pod to test atomic-volume-subpath
Mar 26 19:46:45.970: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-6kz9" in namespace "subpath-8322" to be "success or failure"
Mar 26 19:46:45.973: INFO: Pod "pod-subpath-test-projected-6kz9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.223906ms
Mar 26 19:46:47.977: INFO: Pod "pod-subpath-test-projected-6kz9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007152629s
Mar 26 19:46:49.982: INFO: Pod "pod-subpath-test-projected-6kz9": Phase="Running", Reason="", readiness=true. Elapsed: 4.011761187s
Mar 26 19:46:51.986: INFO: Pod "pod-subpath-test-projected-6kz9": Phase="Running", Reason="", readiness=true. Elapsed: 6.015841938s
Mar 26 19:46:53.990: INFO: Pod "pod-subpath-test-projected-6kz9": Phase="Running", Reason="", readiness=true. Elapsed: 8.019872009s
Mar 26 19:46:55.994: INFO: Pod "pod-subpath-test-projected-6kz9": Phase="Running", Reason="", readiness=true. Elapsed: 10.024085545s
Mar 26 19:46:57.998: INFO: Pod "pod-subpath-test-projected-6kz9": Phase="Running", Reason="", readiness=true. Elapsed: 12.028185569s
Mar 26 19:47:00.002: INFO: Pod "pod-subpath-test-projected-6kz9": Phase="Running", Reason="", readiness=true. Elapsed: 14.032070633s
Mar 26 19:47:02.006: INFO: Pod "pod-subpath-test-projected-6kz9": Phase="Running", Reason="", readiness=true. Elapsed: 16.036130511s
Mar 26 19:47:04.010: INFO: Pod "pod-subpath-test-projected-6kz9": Phase="Running", Reason="", readiness=true. Elapsed: 18.040189264s
Mar 26 19:47:06.014: INFO: Pod "pod-subpath-test-projected-6kz9": Phase="Running", Reason="", readiness=true. Elapsed: 20.044440888s
Mar 26 19:47:08.018: INFO: Pod "pod-subpath-test-projected-6kz9": Phase="Running", Reason="", readiness=true. Elapsed: 22.04833815s
Mar 26 19:47:10.022: INFO: Pod "pod-subpath-test-projected-6kz9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.052211125s
[1mSTEP[0m: Saw pod success
Mar 26 19:47:10.022: INFO: Pod "pod-subpath-test-projected-6kz9" satisfied condition "success or failure"
Mar 26 19:47:10.026: INFO: Trying to get logs from node conformance-worker2 pod pod-subpath-test-projected-6kz9 container test-container-subpath-projected-6kz9: <nil>
[1mSTEP[0m: delete the pod
Mar 26 19:47:10.045: INFO: Waiting for pod pod-subpath-test-projected-6kz9 to disappear
Mar 26 19:47:10.047: INFO: Pod pod-subpath-test-projected-6kz9 no longer exists
[1mSTEP[0m: Deleting pod pod-subpath-test-projected-6kz9
Mar 26 19:47:10.047: INFO: Deleting pod "pod-subpath-test-projected-6kz9" in namespace "subpath-8322"
[AfterEach] [sig-storage] Subpath
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:47:10.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "subpath-8322" for this suite.
Mar 26 19:47:16.061: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:47:16.128: INFO: namespace subpath-8322 deletion completed in 6.076799653s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] Deployment[0m 
  [1mdeployment should delete old replica sets [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-apps] Deployment
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:47:16.129: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename deployment
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Mar 26 19:47:16.161: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Mar 26 19:47:21.165: INFO: Pod name cleanup-pod: Found 1 pods out of 1
[1mSTEP[0m: ensuring each pod is running
Mar 26 19:47:21.165: INFO: Creating deployment test-cleanup-deployment
[1mSTEP[0m: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Mar 26 19:47:21.182: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-7937,SelfLink:/apis/apps/v1/namespaces/deployment-7937/deployments/test-cleanup-deployment,UID:a49ee38b-503a-11e9-b1ea-02429e4bb871,ResourceVersion:13510,Generation:1,CreationTimestamp:2019-03-26 19:47:21 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Mar 26 19:47:21.185: INFO: New ReplicaSet "test-cleanup-deployment-55cbfbc8f5" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55cbfbc8f5,GenerateName:,Namespace:deployment-7937,SelfLink:/apis/apps/v1/namespaces/deployment-7937/replicasets/test-cleanup-deployment-55cbfbc8f5,UID:a4a07240-503a-11e9-b1ea-02429e4bb871,ResourceVersion:13512,Generation:1,CreationTimestamp:2019-03-26 19:47:21 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment a49ee38b-503a-11e9-b1ea-02429e4bb871 0xc002b867a7 0xc002b867a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar 26 19:47:21.185: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Mar 26 19:47:21.185: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:deployment-7937,SelfLink:/apis/apps/v1/namespaces/deployment-7937/replicasets/test-cleanup-controller,UID:a1a1cc08-503a-11e9-b1ea-02429e4bb871,ResourceVersion:13511,Generation:1,CreationTimestamp:2019-03-26 19:47:16 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment a49ee38b-503a-11e9-b1ea-02429e4bb871 0xc002b866d7 0xc002b866d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Mar 26 19:47:21.193: INFO: Pod "test-cleanup-controller-x66xt" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-x66xt,GenerateName:test-cleanup-controller-,Namespace:deployment-7937,SelfLink:/api/v1/namespaces/deployment-7937/pods/test-cleanup-controller-x66xt,UID:a1a28232-503a-11e9-b1ea-02429e4bb871,ResourceVersion:13504,Generation:0,CreationTimestamp:2019-03-26 19:47:16 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller a1a1cc08-503a-11e9-b1ea-02429e4bb871 0xc002b87167 0xc002b87168}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ddvm7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ddvm7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-ddvm7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b871e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b87200}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:47:16 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:47:18 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:47:18 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 19:47:16 -0700 PDT  }],Message:,Reason:,HostIP:192.168.9.3,PodIP:10.32.0.2,StartTime:2019-03-26 19:47:16 -0700 PDT,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-03-26 19:47:17 -0700 PDT,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://09cbbdf0a4d949cf70461f7393d671793f07a482b64e6ffaf537b133dde245a6}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Mar 26 19:47:21.193: INFO: Pod "test-cleanup-deployment-55cbfbc8f5-9452f" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55cbfbc8f5-9452f,GenerateName:test-cleanup-deployment-55cbfbc8f5-,Namespace:deployment-7937,SelfLink:/api/v1/namespaces/deployment-7937/pods/test-cleanup-deployment-55cbfbc8f5-9452f,UID:a4a0d0a8-503a-11e9-b1ea-02429e4bb871,ResourceVersion:13515,Generation:0,CreationTimestamp:2019-03-26 19:47:21 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-55cbfbc8f5 a4a07240-503a-11e9-b1ea-02429e4bb871 0xc002b872d7 0xc002b872d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-ddvm7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-ddvm7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-ddvm7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b87340} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b87360}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:47:21.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "deployment-7937" for this suite.
Mar 26 19:47:27.210: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:47:27.278: INFO: namespace deployment-7937 deletion completed in 6.080557153s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] ConfigMap[0m 
  [1mshould be consumable from pods in volume with mappings [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] ConfigMap
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:47:27.278: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename configmap
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating configMap with name configmap-test-volume-map-a8478a0d-503a-11e9-9719-a08cfdecc127
[1mSTEP[0m: Creating a pod to test consume configMaps
Mar 26 19:47:27.318: INFO: Waiting up to 5m0s for pod "pod-configmaps-a847fb98-503a-11e9-9719-a08cfdecc127" in namespace "configmap-1683" to be "success or failure"
Mar 26 19:47:27.322: INFO: Pod "pod-configmaps-a847fb98-503a-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 3.736328ms
Mar 26 19:47:29.325: INFO: Pod "pod-configmaps-a847fb98-503a-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006864604s
Mar 26 19:47:31.329: INFO: Pod "pod-configmaps-a847fb98-503a-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010776792s
[1mSTEP[0m: Saw pod success
Mar 26 19:47:31.329: INFO: Pod "pod-configmaps-a847fb98-503a-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 19:47:31.332: INFO: Trying to get logs from node conformance-worker pod pod-configmaps-a847fb98-503a-11e9-9719-a08cfdecc127 container configmap-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar 26 19:47:31.354: INFO: Waiting for pod pod-configmaps-a847fb98-503a-11e9-9719-a08cfdecc127 to disappear
Mar 26 19:47:31.358: INFO: Pod pod-configmaps-a847fb98-503a-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:47:31.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "configmap-1683" for this suite.
Mar 26 19:47:37.369: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:47:37.427: INFO: namespace configmap-1683 deletion completed in 6.066684531s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] InitContainer [NodeConformance][0m 
  [1mshould not start app containers if init containers fail on a RestartAlways pod [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:47:37.428: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename init-container
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: creating the pod
Mar 26 19:47:37.452: INFO: PodSpec: initContainers in spec.initContainers
Mar 26 19:48:36.008: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-ae5341b3-503a-11e9-9719-a08cfdecc127", GenerateName:"", Namespace:"init-container-7822", SelfLink:"/api/v1/namespaces/init-container-7822/pods/pod-init-ae5341b3-503a-11e9-9719-a08cfdecc127", UID:"ae538ef0-503a-11e9-b1ea-02429e4bb871", ResourceVersion:"13726", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63689251657, loc:(*time.Location)(0x89eb0e0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"452796474"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-l576j", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc001fda980), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-l576j", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-l576j", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-l576j", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001c7e298), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"conformance-worker2", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0023cc060), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001c7e310)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001c7e330)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001c7e338), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc001c7e33c)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63689251657, loc:(*time.Location)(0x89eb0e0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63689251657, loc:(*time.Location)(0x89eb0e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63689251657, loc:(*time.Location)(0x89eb0e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63689251657, loc:(*time.Location)(0x89eb0e0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.9.2", PodIP:"10.46.0.1", StartTime:(*v1.Time)(0xc0036c8160), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000430850)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000430a80)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://f5e0552f48a7119c72baf53853c952824d7b64e6fd70f61ebfd189dc6afa3365"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0036c81c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0036c81a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:48:36.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "init-container-7822" for this suite.
Mar 26 19:48:58.021: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:48:58.090: INFO: namespace init-container-7822 deletion completed in 22.076647493s
[32mâ€¢[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90m[k8s.io] Kubectl run rc[0m 
  [1mshould create an rc from an image  [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:48:58.090: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run rc
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1354
[It] should create an rc from an image  [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: running the image docker.io/library/nginx:1.14-alpine
Mar 26 19:48:58.113: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-9724'
Mar 26 19:48:58.493: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar 26 19:48:58.493: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
[1mSTEP[0m: verifying the rc e2e-test-nginx-rc was created
[1mSTEP[0m: verifying the pod controlled by rc e2e-test-nginx-rc was created
[1mSTEP[0m: confirm that you can get logs from an rc
Mar 26 19:48:58.501: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-8qz82]
Mar 26 19:48:58.501: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-8qz82" in namespace "kubectl-9724" to be "running and ready"
Mar 26 19:48:58.510: INFO: Pod "e2e-test-nginx-rc-8qz82": Phase="Pending", Reason="", readiness=false. Elapsed: 8.775308ms
Mar 26 19:49:00.514: INFO: Pod "e2e-test-nginx-rc-8qz82": Phase="Running", Reason="", readiness=true. Elapsed: 2.01278438s
Mar 26 19:49:00.514: INFO: Pod "e2e-test-nginx-rc-8qz82" satisfied condition "running and ready"
Mar 26 19:49:00.514: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-8qz82]
Mar 26 19:49:00.514: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance logs rc/e2e-test-nginx-rc --namespace=kubectl-9724'
Mar 26 19:49:00.604: INFO: stderr: ""
Mar 26 19:49:00.604: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1359
Mar 26 19:49:00.604: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance delete rc e2e-test-nginx-rc --namespace=kubectl-9724'
Mar 26 19:49:00.679: INFO: stderr: ""
Mar 26 19:49:00.679: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:49:00.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-9724" for this suite.
Mar 26 19:49:06.700: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:49:06.764: INFO: namespace kubectl-9724 deletion completed in 6.081930163s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Watchers[0m 
  [1mshould be able to restart watching from the last resource version observed by the previous watch [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-api-machinery] Watchers
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:49:06.765: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename watch
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: creating a watch on configmaps
[1mSTEP[0m: creating a new configmap
[1mSTEP[0m: modifying the configmap once
[1mSTEP[0m: closing the watch once it receives two notifications
Mar 26 19:49:06.800: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4293,SelfLink:/api/v1/namespaces/watch-4293/configmaps/e2e-watch-test-watch-closed,UID:e393f860-503a-11e9-b1ea-02429e4bb871,ResourceVersion:13829,Generation:0,CreationTimestamp:2019-03-26 19:49:06 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar 26 19:49:06.800: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4293,SelfLink:/api/v1/namespaces/watch-4293/configmaps/e2e-watch-test-watch-closed,UID:e393f860-503a-11e9-b1ea-02429e4bb871,ResourceVersion:13830,Generation:0,CreationTimestamp:2019-03-26 19:49:06 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
[1mSTEP[0m: modifying the configmap a second time, while the watch is closed
[1mSTEP[0m: creating a new watch on configmaps from the last resource version observed by the first watch
[1mSTEP[0m: deleting the configmap
[1mSTEP[0m: Expecting to observe notifications for all changes to the configmap since the first watch closed
Mar 26 19:49:06.807: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4293,SelfLink:/api/v1/namespaces/watch-4293/configmaps/e2e-watch-test-watch-closed,UID:e393f860-503a-11e9-b1ea-02429e4bb871,ResourceVersion:13831,Generation:0,CreationTimestamp:2019-03-26 19:49:06 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar 26 19:49:06.807: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4293,SelfLink:/api/v1/namespaces/watch-4293/configmaps/e2e-watch-test-watch-closed,UID:e393f860-503a-11e9-b1ea-02429e4bb871,ResourceVersion:13832,Generation:0,CreationTimestamp:2019-03-26 19:49:06 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:49:06.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "watch-4293" for this suite.
Mar 26 19:49:12.817: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:49:12.881: INFO: namespace watch-4293 deletion completed in 6.072017072s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected downwardAPI[0m 
  [1mshould provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Projected downwardAPI
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:49:12.882: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar 26 19:49:12.912: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e738abbd-503a-11e9-9719-a08cfdecc127" in namespace "projected-9241" to be "success or failure"
Mar 26 19:49:12.915: INFO: Pod "downwardapi-volume-e738abbd-503a-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.928608ms
Mar 26 19:49:14.918: INFO: Pod "downwardapi-volume-e738abbd-503a-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006436581s
Mar 26 19:49:16.922: INFO: Pod "downwardapi-volume-e738abbd-503a-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010430815s
[1mSTEP[0m: Saw pod success
Mar 26 19:49:16.922: INFO: Pod "downwardapi-volume-e738abbd-503a-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 19:49:16.925: INFO: Trying to get logs from node conformance-worker2 pod downwardapi-volume-e738abbd-503a-11e9-9719-a08cfdecc127 container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar 26 19:49:16.949: INFO: Waiting for pod downwardapi-volume-e738abbd-503a-11e9-9719-a08cfdecc127 to disappear
Mar 26 19:49:16.952: INFO: Pod downwardapi-volume-e738abbd-503a-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:49:16.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-9241" for this suite.
Mar 26 19:49:22.964: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:49:23.029: INFO: namespace projected-9241 deletion completed in 6.072956866s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] ConfigMap[0m 
  [1mshould be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] ConfigMap
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:49:23.029: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename configmap
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating configMap with name configmap-test-volume-ed46d2d4-503a-11e9-9719-a08cfdecc127
[1mSTEP[0m: Creating a pod to test consume configMaps
Mar 26 19:49:23.073: INFO: Waiting up to 5m0s for pod "pod-configmaps-ed471097-503a-11e9-9719-a08cfdecc127" in namespace "configmap-9283" to be "success or failure"
Mar 26 19:49:23.075: INFO: Pod "pod-configmaps-ed471097-503a-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.1205ms
Mar 26 19:49:25.079: INFO: Pod "pod-configmaps-ed471097-503a-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006129146s
Mar 26 19:49:27.083: INFO: Pod "pod-configmaps-ed471097-503a-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009727385s
[1mSTEP[0m: Saw pod success
Mar 26 19:49:27.083: INFO: Pod "pod-configmaps-ed471097-503a-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 19:49:27.085: INFO: Trying to get logs from node conformance-worker2 pod pod-configmaps-ed471097-503a-11e9-9719-a08cfdecc127 container configmap-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar 26 19:49:27.101: INFO: Waiting for pod pod-configmaps-ed471097-503a-11e9-9719-a08cfdecc127 to disappear
Mar 26 19:49:27.105: INFO: Pod pod-configmaps-ed471097-503a-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:49:27.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "configmap-9283" for this suite.
Mar 26 19:49:33.115: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:49:33.178: INFO: namespace configmap-9283 deletion completed in 6.070660635s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Garbage collector[0m 
  [1mshould delete pods created by rc when not orphaning [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-api-machinery] Garbage collector
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:49:33.178: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename gc
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: create the rc
[1mSTEP[0m: delete the rc
[1mSTEP[0m: wait for all pods to be garbage collected
[1mSTEP[0m: Gathering metrics
W0326 19:49:43.236138  168179 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar 26 19:49:43.236: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:49:43.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "gc-8201" for this suite.
Mar 26 19:49:49.246: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:49:49.311: INFO: namespace gc-8201 deletion completed in 6.072510568s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90m[k8s.io] Guestbook application[0m 
  [1mshould create and stop a working application  [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:49:49.311: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create and stop a working application  [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: creating all guestbook components
Mar 26 19:49:49.337: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Mar 26 19:49:49.337: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance create -f - --namespace=kubectl-9630'
Mar 26 19:49:49.487: INFO: stderr: ""
Mar 26 19:49:49.487: INFO: stdout: "service/redis-slave created\n"
Mar 26 19:49:49.487: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Mar 26 19:49:49.487: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance create -f - --namespace=kubectl-9630'
Mar 26 19:49:49.662: INFO: stderr: ""
Mar 26 19:49:49.662: INFO: stdout: "service/redis-master created\n"
Mar 26 19:49:49.663: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Mar 26 19:49:49.663: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance create -f - --namespace=kubectl-9630'
Mar 26 19:49:49.833: INFO: stderr: ""
Mar 26 19:49:49.834: INFO: stdout: "service/frontend created\n"
Mar 26 19:49:49.834: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Mar 26 19:49:49.834: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance create -f - --namespace=kubectl-9630'
Mar 26 19:49:49.991: INFO: stderr: ""
Mar 26 19:49:49.991: INFO: stdout: "deployment.apps/frontend created\n"
Mar 26 19:49:49.991: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar 26 19:49:49.991: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance create -f - --namespace=kubectl-9630'
Mar 26 19:49:50.138: INFO: stderr: ""
Mar 26 19:49:50.138: INFO: stdout: "deployment.apps/redis-master created\n"
Mar 26 19:49:50.138: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Mar 26 19:49:50.138: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance create -f - --namespace=kubectl-9630'
Mar 26 19:49:50.298: INFO: stderr: ""
Mar 26 19:49:50.298: INFO: stdout: "deployment.apps/redis-slave created\n"
[1mSTEP[0m: validating guestbook app
Mar 26 19:49:50.298: INFO: Waiting for all frontend pods to be Running.
Mar 26 19:50:10.349: INFO: Waiting for frontend to serve content.
Mar 26 19:50:15.369: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection timed out [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection time...', 110)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stre in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Mar 26 19:50:20.390: INFO: Trying to add a new entry to the guestbook.
Mar 26 19:50:20.407: INFO: Verifying that added entry can be retrieved.
[1mSTEP[0m: using delete to clean up resources
Mar 26 19:50:20.420: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance delete --grace-period=0 --force -f - --namespace=kubectl-9630'
Mar 26 19:50:20.508: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 26 19:50:20.508: INFO: stdout: "service \"redis-slave\" force deleted\n"
[1mSTEP[0m: using delete to clean up resources
Mar 26 19:50:20.508: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance delete --grace-period=0 --force -f - --namespace=kubectl-9630'
Mar 26 19:50:20.608: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 26 19:50:20.608: INFO: stdout: "service \"redis-master\" force deleted\n"
[1mSTEP[0m: using delete to clean up resources
Mar 26 19:50:20.608: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance delete --grace-period=0 --force -f - --namespace=kubectl-9630'
Mar 26 19:50:20.699: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 26 19:50:20.699: INFO: stdout: "service \"frontend\" force deleted\n"
[1mSTEP[0m: using delete to clean up resources
Mar 26 19:50:20.699: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance delete --grace-period=0 --force -f - --namespace=kubectl-9630'
Mar 26 19:50:20.792: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 26 19:50:20.792: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
[1mSTEP[0m: using delete to clean up resources
Mar 26 19:50:20.792: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance delete --grace-period=0 --force -f - --namespace=kubectl-9630'
Mar 26 19:50:20.879: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 26 19:50:20.879: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
[1mSTEP[0m: using delete to clean up resources
Mar 26 19:50:20.880: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance delete --grace-period=0 --force -f - --namespace=kubectl-9630'
Mar 26 19:50:20.955: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 26 19:50:20.955: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:50:20.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-9630" for this suite.
Mar 26 19:50:58.985: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:50:59.047: INFO: namespace kubectl-9630 deletion completed in 38.086700775s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected downwardAPI[0m 
  [1mshould set mode on item file [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Projected downwardAPI
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:50:59.047: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar 26 19:50:59.080: INFO: Waiting up to 5m0s for pod "downwardapi-volume-26802632-503b-11e9-9719-a08cfdecc127" in namespace "projected-651" to be "success or failure"
Mar 26 19:50:59.083: INFO: Pod "downwardapi-volume-26802632-503b-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.982647ms
Mar 26 19:51:01.087: INFO: Pod "downwardapi-volume-26802632-503b-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007139125s
[1mSTEP[0m: Saw pod success
Mar 26 19:51:01.087: INFO: Pod "downwardapi-volume-26802632-503b-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 19:51:01.090: INFO: Trying to get logs from node conformance-worker2 pod downwardapi-volume-26802632-503b-11e9-9719-a08cfdecc127 container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar 26 19:51:01.106: INFO: Waiting for pod downwardapi-volume-26802632-503b-11e9-9719-a08cfdecc127 to disappear
Mar 26 19:51:01.108: INFO: Pod downwardapi-volume-26802632-503b-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:51:01.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-651" for this suite.
Mar 26 19:51:07.120: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:51:07.186: INFO: namespace projected-651 deletion completed in 6.075404968s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90m[k8s.io] Kubectl patch[0m 
  [1mshould add annotations for pods in rc  [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:51:07.186: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should add annotations for pods in rc  [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: creating Redis RC
Mar 26 19:51:07.212: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance create -f - --namespace=kubectl-9568'
Mar 26 19:51:07.369: INFO: stderr: ""
Mar 26 19:51:07.369: INFO: stdout: "replicationcontroller/redis-master created\n"
[1mSTEP[0m: Waiting for Redis master to start.
Mar 26 19:51:08.373: INFO: Selector matched 1 pods for map[app:redis]
Mar 26 19:51:08.373: INFO: Found 0 / 1
Mar 26 19:51:09.373: INFO: Selector matched 1 pods for map[app:redis]
Mar 26 19:51:09.373: INFO: Found 1 / 1
Mar 26 19:51:09.373: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
[1mSTEP[0m: patching all pods
Mar 26 19:51:09.376: INFO: Selector matched 1 pods for map[app:redis]
Mar 26 19:51:09.376: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 26 19:51:09.376: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance patch pod redis-master-nxfkb --namespace=kubectl-9568 -p {"metadata":{"annotations":{"x":"y"}}}'
Mar 26 19:51:09.455: INFO: stderr: ""
Mar 26 19:51:09.455: INFO: stdout: "pod/redis-master-nxfkb patched\n"
[1mSTEP[0m: checking annotations
Mar 26 19:51:09.458: INFO: Selector matched 1 pods for map[app:redis]
Mar 26 19:51:09.458: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:51:09.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-9568" for this suite.
Mar 26 19:51:31.467: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:51:31.532: INFO: namespace kubectl-9568 deletion completed in 22.072637405s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected secret[0m 
  [1mshould be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Projected secret
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:51:31.533: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating projection with secret that has name projected-secret-test-map-39de04fb-503b-11e9-9719-a08cfdecc127
[1mSTEP[0m: Creating a pod to test consume secrets
Mar 26 19:51:31.571: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-39de578f-503b-11e9-9719-a08cfdecc127" in namespace "projected-4395" to be "success or failure"
Mar 26 19:51:31.572: INFO: Pod "pod-projected-secrets-39de578f-503b-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 1.655985ms
Mar 26 19:51:33.576: INFO: Pod "pod-projected-secrets-39de578f-503b-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005627551s
[1mSTEP[0m: Saw pod success
Mar 26 19:51:33.576: INFO: Pod "pod-projected-secrets-39de578f-503b-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 19:51:33.579: INFO: Trying to get logs from node conformance-worker2 pod pod-projected-secrets-39de578f-503b-11e9-9719-a08cfdecc127 container projected-secret-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar 26 19:51:33.594: INFO: Waiting for pod pod-projected-secrets-39de578f-503b-11e9-9719-a08cfdecc127 to disappear
Mar 26 19:51:33.596: INFO: Pod pod-projected-secrets-39de578f-503b-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] Projected secret
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:51:33.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-4395" for this suite.
Mar 26 19:51:39.607: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:51:39.686: INFO: namespace projected-4395 deletion completed in 6.087436446s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Services[0m 
  [1mshould provide secure master service  [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-network] Services
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:51:39.686: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename services
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should provide secure master service  [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [sig-network] Services
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:51:39.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "services-3153" for this suite.
Mar 26 19:51:45.725: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:51:45.787: INFO: namespace services-3153 deletion completed in 6.074129252s
[AfterEach] [sig-network] Services
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90m[k8s.io] Proxy server[0m 
  [1mshould support proxy with --port 0  [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:51:45.787: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support proxy with --port 0  [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: starting the proxy server
Mar 26 19:51:45.819: INFO: Asynchronously running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance proxy -p 0 --disable-filter'
[1mSTEP[0m: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:51:45.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-975" for this suite.
Mar 26 19:51:51.914: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:51:51.976: INFO: namespace kubectl-975 deletion completed in 6.071580041s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Secrets[0m 
  [1mshould be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Secrets
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:51:51.976: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename secrets
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating secret with name secret-test-460c5642-503b-11e9-9719-a08cfdecc127
[1mSTEP[0m: Creating a pod to test consume secrets
Mar 26 19:51:52.009: INFO: Waiting up to 5m0s for pod "pod-secrets-460cacd1-503b-11e9-9719-a08cfdecc127" in namespace "secrets-4375" to be "success or failure"
Mar 26 19:51:52.013: INFO: Pod "pod-secrets-460cacd1-503b-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 3.624835ms
Mar 26 19:51:54.017: INFO: Pod "pod-secrets-460cacd1-503b-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007397117s
Mar 26 19:51:56.020: INFO: Pod "pod-secrets-460cacd1-503b-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010378434s
[1mSTEP[0m: Saw pod success
Mar 26 19:51:56.020: INFO: Pod "pod-secrets-460cacd1-503b-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 19:51:56.022: INFO: Trying to get logs from node conformance-worker pod pod-secrets-460cacd1-503b-11e9-9719-a08cfdecc127 container secret-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar 26 19:51:56.039: INFO: Waiting for pod pod-secrets-460cacd1-503b-11e9-9719-a08cfdecc127 to disappear
Mar 26 19:51:56.040: INFO: Pod pod-secrets-460cacd1-503b-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] Secrets
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:51:56.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "secrets-4375" for this suite.
Mar 26 19:52:02.049: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:52:02.111: INFO: namespace secrets-4375 deletion completed in 6.069352307s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90m[k8s.io] Kubectl run pod[0m 
  [1mshould create a pod from an image when restart is Never  [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:52:02.111: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run pod
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1583
[It] should create a pod from an image when restart is Never  [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: running the image docker.io/library/nginx:1.14-alpine
Mar 26 19:52:02.142: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-3493'
Mar 26 19:52:02.228: INFO: stderr: ""
Mar 26 19:52:02.228: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
[1mSTEP[0m: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1588
Mar 26 19:52:02.230: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance delete pods e2e-test-nginx-pod --namespace=kubectl-3493'
Mar 26 19:52:05.541: INFO: stderr: ""
Mar 26 19:52:05.541: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:52:05.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-3493" for this suite.
Mar 26 19:52:11.551: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:52:11.616: INFO: namespace kubectl-3493 deletion completed in 6.07212802s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Namespaces [Serial][0m 
  [1mshould ensure that all services are removed when a namespace is deleted [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:52:11.616: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename namespaces
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating a test namespace
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[1mSTEP[0m: Creating a service in the namespace
[1mSTEP[0m: Deleting the namespace
[1mSTEP[0m: Waiting for the namespace to be removed.
[1mSTEP[0m: Recreating the namespace
[1mSTEP[0m: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:52:17.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "namespaces-1745" for this suite.
Mar 26 19:52:23.744: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:52:23.813: INFO: namespace namespaces-1745 deletion completed in 6.075613003s
[1mSTEP[0m: Destroying namespace "nsdeletetest-6779" for this suite.
Mar 26 19:52:23.815: INFO: Namespace nsdeletetest-6779 was already deleted
[1mSTEP[0m: Destroying namespace "nsdeletetest-6827" for this suite.
Mar 26 19:52:29.823: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:52:29.888: INFO: namespace nsdeletetest-6827 deletion completed in 6.072039551s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected configMap[0m 
  [1moptional updates should be reflected in volume [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Projected configMap
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:52:29.888: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating configMap with name cm-test-opt-del-5ca64008-503b-11e9-9719-a08cfdecc127
[1mSTEP[0m: Creating configMap with name cm-test-opt-upd-5ca6403c-503b-11e9-9719-a08cfdecc127
[1mSTEP[0m: Creating the pod
[1mSTEP[0m: Deleting configmap cm-test-opt-del-5ca64008-503b-11e9-9719-a08cfdecc127
[1mSTEP[0m: Updating configmap cm-test-opt-upd-5ca6403c-503b-11e9-9719-a08cfdecc127
[1mSTEP[0m: Creating configMap with name cm-test-opt-create-5ca6404f-503b-11e9-9719-a08cfdecc127
[1mSTEP[0m: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:53:56.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-4698" for this suite.
Mar 26 19:54:18.462: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:54:18.525: INFO: namespace projected-4698 deletion completed in 22.072154452s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] CustomResourceDefinition resources[0m [90mSimple CustomResourceDefinition[0m 
  [1mcreating/deleting custom resource definition objects works  [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:54:18.525: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename custom-resource-definition
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Mar 26 19:54:18.555: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:54:19.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "custom-resource-definition-847" for this suite.
Mar 26 19:54:25.613: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:54:25.679: INFO: namespace custom-resource-definition-847 deletion completed in 6.076853652s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Variable Expansion[0m 
  [1mshould allow composing env vars into new env vars [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [k8s.io] Variable Expansion
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:54:25.679: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename var-expansion
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating a pod to test env composition
Mar 26 19:54:25.723: INFO: Waiting up to 5m0s for pod "var-expansion-a1aa6a74-503b-11e9-9719-a08cfdecc127" in namespace "var-expansion-8746" to be "success or failure"
Mar 26 19:54:25.726: INFO: Pod "var-expansion-a1aa6a74-503b-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 3.092857ms
Mar 26 19:54:27.730: INFO: Pod "var-expansion-a1aa6a74-503b-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006582162s
[1mSTEP[0m: Saw pod success
Mar 26 19:54:27.730: INFO: Pod "var-expansion-a1aa6a74-503b-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 19:54:27.733: INFO: Trying to get logs from node conformance-worker2 pod var-expansion-a1aa6a74-503b-11e9-9719-a08cfdecc127 container dapi-container: <nil>
[1mSTEP[0m: delete the pod
Mar 26 19:54:27.751: INFO: Waiting for pod var-expansion-a1aa6a74-503b-11e9-9719-a08cfdecc127 to disappear
Mar 26 19:54:27.753: INFO: Pod var-expansion-a1aa6a74-503b-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:54:27.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "var-expansion-8746" for this suite.
Mar 26 19:54:33.764: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:54:33.828: INFO: namespace var-expansion-8746 deletion completed in 6.07260392s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-node] Downward API[0m 
  [1mshould provide host IP as an env var [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-node] Downward API
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:54:33.828: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename downward-api
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating a pod to test downward api env vars
Mar 26 19:54:33.864: INFO: Waiting up to 5m0s for pod "downward-api-a686269f-503b-11e9-9719-a08cfdecc127" in namespace "downward-api-8693" to be "success or failure"
Mar 26 19:54:33.866: INFO: Pod "downward-api-a686269f-503b-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 1.916171ms
Mar 26 19:54:35.869: INFO: Pod "downward-api-a686269f-503b-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00491168s
[1mSTEP[0m: Saw pod success
Mar 26 19:54:35.869: INFO: Pod "downward-api-a686269f-503b-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 19:54:35.872: INFO: Trying to get logs from node conformance-worker pod downward-api-a686269f-503b-11e9-9719-a08cfdecc127 container dapi-container: <nil>
[1mSTEP[0m: delete the pod
Mar 26 19:54:35.889: INFO: Waiting for pod downward-api-a686269f-503b-11e9-9719-a08cfdecc127 to disappear
Mar 26 19:54:35.891: INFO: Pod downward-api-a686269f-503b-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-node] Downward API
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:54:35.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "downward-api-8693" for this suite.
Mar 26 19:54:41.900: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:54:41.963: INFO: namespace downward-api-8693 deletion completed in 6.069407075s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected downwardAPI[0m 
  [1mshould provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Projected downwardAPI
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:54:41.963: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar 26 19:54:42.003: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ab5f85f8-503b-11e9-9719-a08cfdecc127" in namespace "projected-5678" to be "success or failure"
Mar 26 19:54:42.007: INFO: Pod "downwardapi-volume-ab5f85f8-503b-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 4.150671ms
Mar 26 19:54:44.011: INFO: Pod "downwardapi-volume-ab5f85f8-503b-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008394233s
[1mSTEP[0m: Saw pod success
Mar 26 19:54:44.011: INFO: Pod "downwardapi-volume-ab5f85f8-503b-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 19:54:44.014: INFO: Trying to get logs from node conformance-worker2 pod downwardapi-volume-ab5f85f8-503b-11e9-9719-a08cfdecc127 container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar 26 19:54:44.029: INFO: Waiting for pod downwardapi-volume-ab5f85f8-503b-11e9-9719-a08cfdecc127 to disappear
Mar 26 19:54:44.032: INFO: Pod downwardapi-volume-ab5f85f8-503b-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:54:44.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-5678" for this suite.
Mar 26 19:54:50.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:54:50.109: INFO: namespace projected-5678 deletion completed in 6.074286621s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] [sig-node] Pods Extended[0m [90m[k8s.io] Pods Set QOS Class[0m 
  [1mshould be submitted and removed  [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:54:50.109: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename pods
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:177
[It] should be submitted and removed  [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: creating the pod
[1mSTEP[0m: submitting the pod to kubernetes
[1mSTEP[0m: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:54:50.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "pods-297" for this suite.
Mar 26 19:55:12.155: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:55:12.220: INFO: namespace pods-297 deletion completed in 22.075325466s
[32mâ€¢[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Probing container[0m 
  [1mshould have monotonically increasing restart count [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [k8s.io] Probing container
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:55:12.220: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename container-probe
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating pod liveness-http in namespace container-probe-883
Mar 26 19:55:16.259: INFO: Started pod liveness-http in namespace container-probe-883
[1mSTEP[0m: checking the pod's current state and verifying that restartCount is present
Mar 26 19:55:16.261: INFO: Initial restart count of pod liveness-http is 0
Mar 26 19:55:28.287: INFO: Restart count of pod container-probe-883/liveness-http is now 1 (12.025380461s elapsed)
Mar 26 19:55:50.330: INFO: Restart count of pod container-probe-883/liveness-http is now 2 (34.068865644s elapsed)
Mar 26 19:56:08.366: INFO: Restart count of pod container-probe-883/liveness-http is now 3 (52.104276275s elapsed)
Mar 26 19:56:28.404: INFO: Restart count of pod container-probe-883/liveness-http is now 4 (1m12.142552266s elapsed)
Mar 26 19:57:38.539: INFO: Restart count of pod container-probe-883/liveness-http is now 5 (2m22.277482528s elapsed)
[1mSTEP[0m: deleting the pod
[AfterEach] [k8s.io] Probing container
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:57:38.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "container-probe-883" for this suite.
Mar 26 19:57:44.561: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:57:44.623: INFO: namespace container-probe-883 deletion completed in 6.073394536s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] EmptyDir volumes[0m 
  [1mshould support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] EmptyDir volumes
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:57:44.624: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename emptydir
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating a pod to test emptydir 0666 on node default medium
Mar 26 19:57:44.659: INFO: Waiting up to 5m0s for pod "pod-183ed1ed-503c-11e9-9719-a08cfdecc127" in namespace "emptydir-8780" to be "success or failure"
Mar 26 19:57:44.661: INFO: Pod "pod-183ed1ed-503c-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 1.666016ms
Mar 26 19:57:46.665: INFO: Pod "pod-183ed1ed-503c-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005692254s
Mar 26 19:57:48.668: INFO: Pod "pod-183ed1ed-503c-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009380092s
[1mSTEP[0m: Saw pod success
Mar 26 19:57:48.668: INFO: Pod "pod-183ed1ed-503c-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 19:57:48.671: INFO: Trying to get logs from node conformance-worker pod pod-183ed1ed-503c-11e9-9719-a08cfdecc127 container test-container: <nil>
[1mSTEP[0m: delete the pod
Mar 26 19:57:48.689: INFO: Waiting for pod pod-183ed1ed-503c-11e9-9719-a08cfdecc127 to disappear
Mar 26 19:57:48.692: INFO: Pod pod-183ed1ed-503c-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:57:48.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "emptydir-8780" for this suite.
Mar 26 19:57:54.704: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:57:54.773: INFO: namespace emptydir-8780 deletion completed in 6.076570842s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Kubelet[0m [90mwhen scheduling a read only busybox container[0m 
  [1mshould not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [k8s.io] Kubelet
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:57:54.773: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename kubelet-test
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:57:56.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubelet-test-721" for this suite.
Mar 26 19:58:34.832: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:58:34.900: INFO: namespace kubelet-test-721 deletion completed in 38.077636768s
[32mâ€¢[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90m[k8s.io] Proxy server[0m 
  [1mshould support --unix-socket=/path  [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:58:34.900: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support --unix-socket=/path  [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Starting the proxy
Mar 26 19:58:34.920: INFO: Asynchronously running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance proxy --unix-socket=/tmp/kubectl-proxy-unix435394199/test'
[1mSTEP[0m: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 19:58:34.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-2280" for this suite.
Mar 26 19:58:40.974: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 19:58:41.039: INFO: namespace kubectl-2280 deletion completed in 6.073199767s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Container Lifecycle Hook[0m [90mwhen create a pod with lifecycle hook[0m 
  [1mshould execute poststart exec hook properly [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 19:58:41.040: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename container-lifecycle-hook
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
[1mSTEP[0m: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: create the pod with lifecycle hook
[1mSTEP[0m: check poststart hook
[1mSTEP[0m: delete the pod with lifecycle hook
Mar 26 20:01:27.104: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 26 20:01:27.109: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 26 20:01:29.109: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 26 20:01:29.111: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 26 20:01:31.109: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 26 20:01:31.113: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 26 20:01:33.109: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 26 20:01:33.111: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 26 20:01:35.109: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 26 20:01:35.112: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 26 20:01:37.109: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 26 20:01:37.113: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 26 20:01:39.109: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 26 20:01:39.113: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 26 20:01:41.109: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 26 20:01:41.112: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 26 20:01:43.109: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 26 20:01:43.112: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 26 20:01:45.109: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 26 20:01:45.113: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 26 20:01:47.109: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 26 20:01:47.113: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:01:47.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "container-lifecycle-hook-3647" for this suite.
Mar 26 20:02:09.128: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:02:09.194: INFO: namespace container-lifecycle-hook-3647 deletion completed in 22.076981165s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] EmptyDir volumes[0m 
  [1mshould support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] EmptyDir volumes
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:02:09.194: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename emptydir
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating a pod to test emptydir 0644 on node default medium
Mar 26 20:02:09.223: INFO: Waiting up to 5m0s for pod "pod-b5f0671d-503c-11e9-9719-a08cfdecc127" in namespace "emptydir-9054" to be "success or failure"
Mar 26 20:02:09.225: INFO: Pod "pod-b5f0671d-503c-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 1.823801ms
Mar 26 20:02:11.230: INFO: Pod "pod-b5f0671d-503c-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007164342s
Mar 26 20:02:13.235: INFO: Pod "pod-b5f0671d-503c-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011306955s
[1mSTEP[0m: Saw pod success
Mar 26 20:02:13.235: INFO: Pod "pod-b5f0671d-503c-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 20:02:13.238: INFO: Trying to get logs from node conformance-worker pod pod-b5f0671d-503c-11e9-9719-a08cfdecc127 container test-container: <nil>
[1mSTEP[0m: delete the pod
Mar 26 20:02:13.256: INFO: Waiting for pod pod-b5f0671d-503c-11e9-9719-a08cfdecc127 to disappear
Mar 26 20:02:13.258: INFO: Pod pod-b5f0671d-503c-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:02:13.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "emptydir-9054" for this suite.
Mar 26 20:02:19.268: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:02:19.331: INFO: namespace emptydir-9054 deletion completed in 6.071292352s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] HostPath[0m 
  [1mshould give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] HostPath
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:02:19.331: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename hostpath
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating a pod to test hostPath mode
Mar 26 20:02:19.376: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-6687" to be "success or failure"
Mar 26 20:02:19.387: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 11.089768ms
Mar 26 20:02:21.391: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015056681s
[1mSTEP[0m: Saw pod success
Mar 26 20:02:21.391: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Mar 26 20:02:21.395: INFO: Trying to get logs from node conformance-worker2 pod pod-host-path-test container test-container-1: <nil>
[1mSTEP[0m: delete the pod
Mar 26 20:02:21.415: INFO: Waiting for pod pod-host-path-test to disappear
Mar 26 20:02:21.417: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:02:21.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "hostpath-6687" for this suite.
Mar 26 20:02:27.427: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:02:27.492: INFO: namespace hostpath-6687 deletion completed in 6.072493206s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Subpath[0m [90mAtomic writer volumes[0m 
  [1mshould support subpaths with secret pod [LinuxOnly] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Subpath
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:02:27.493: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename subpath
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
[1mSTEP[0m: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating pod pod-subpath-test-secret-tkl2
[1mSTEP[0m: Creating a pod to test atomic-volume-subpath
Mar 26 20:02:27.529: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-tkl2" in namespace "subpath-9806" to be "success or failure"
Mar 26 20:02:27.533: INFO: Pod "pod-subpath-test-secret-tkl2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.966101ms
Mar 26 20:02:29.536: INFO: Pod "pod-subpath-test-secret-tkl2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006942584s
Mar 26 20:02:31.540: INFO: Pod "pod-subpath-test-secret-tkl2": Phase="Running", Reason="", readiness=true. Elapsed: 4.010966551s
Mar 26 20:02:33.544: INFO: Pod "pod-subpath-test-secret-tkl2": Phase="Running", Reason="", readiness=true. Elapsed: 6.01442449s
Mar 26 20:02:35.548: INFO: Pod "pod-subpath-test-secret-tkl2": Phase="Running", Reason="", readiness=true. Elapsed: 8.018421874s
Mar 26 20:02:37.552: INFO: Pod "pod-subpath-test-secret-tkl2": Phase="Running", Reason="", readiness=true. Elapsed: 10.022498639s
Mar 26 20:02:39.556: INFO: Pod "pod-subpath-test-secret-tkl2": Phase="Running", Reason="", readiness=true. Elapsed: 12.026746293s
Mar 26 20:02:41.560: INFO: Pod "pod-subpath-test-secret-tkl2": Phase="Running", Reason="", readiness=true. Elapsed: 14.030995717s
Mar 26 20:02:43.564: INFO: Pod "pod-subpath-test-secret-tkl2": Phase="Running", Reason="", readiness=true. Elapsed: 16.034463762s
Mar 26 20:02:45.567: INFO: Pod "pod-subpath-test-secret-tkl2": Phase="Running", Reason="", readiness=true. Elapsed: 18.038359022s
Mar 26 20:02:47.571: INFO: Pod "pod-subpath-test-secret-tkl2": Phase="Running", Reason="", readiness=true. Elapsed: 20.041910549s
Mar 26 20:02:49.575: INFO: Pod "pod-subpath-test-secret-tkl2": Phase="Running", Reason="", readiness=true. Elapsed: 22.045638526s
Mar 26 20:02:51.578: INFO: Pod "pod-subpath-test-secret-tkl2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.049182664s
[1mSTEP[0m: Saw pod success
Mar 26 20:02:51.578: INFO: Pod "pod-subpath-test-secret-tkl2" satisfied condition "success or failure"
Mar 26 20:02:51.581: INFO: Trying to get logs from node conformance-worker pod pod-subpath-test-secret-tkl2 container test-container-subpath-secret-tkl2: <nil>
[1mSTEP[0m: delete the pod
Mar 26 20:02:51.608: INFO: Waiting for pod pod-subpath-test-secret-tkl2 to disappear
Mar 26 20:02:51.615: INFO: Pod pod-subpath-test-secret-tkl2 no longer exists
[1mSTEP[0m: Deleting pod pod-subpath-test-secret-tkl2
Mar 26 20:02:51.615: INFO: Deleting pod "pod-subpath-test-secret-tkl2" in namespace "subpath-9806"
[AfterEach] [sig-storage] Subpath
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:02:51.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "subpath-9806" for this suite.
Mar 26 20:02:57.627: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:02:57.693: INFO: namespace subpath-9806 deletion completed in 6.072793089s
[32mâ€¢[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected downwardAPI[0m 
  [1mshould provide container's cpu request [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Projected downwardAPI
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:02:57.693: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar 26 20:02:57.727: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d2d966bd-503c-11e9-9719-a08cfdecc127" in namespace "projected-7128" to be "success or failure"
Mar 26 20:02:57.728: INFO: Pod "downwardapi-volume-d2d966bd-503c-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 1.607342ms
Mar 26 20:02:59.732: INFO: Pod "downwardapi-volume-d2d966bd-503c-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005826001s
Mar 26 20:03:01.736: INFO: Pod "downwardapi-volume-d2d966bd-503c-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009394613s
[1mSTEP[0m: Saw pod success
Mar 26 20:03:01.736: INFO: Pod "downwardapi-volume-d2d966bd-503c-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 20:03:01.738: INFO: Trying to get logs from node conformance-worker2 pod downwardapi-volume-d2d966bd-503c-11e9-9719-a08cfdecc127 container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar 26 20:03:01.758: INFO: Waiting for pod downwardapi-volume-d2d966bd-503c-11e9-9719-a08cfdecc127 to disappear
Mar 26 20:03:01.760: INFO: Pod downwardapi-volume-d2d966bd-503c-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:03:01.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-7128" for this suite.
Mar 26 20:03:07.771: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:03:07.837: INFO: namespace projected-7128 deletion completed in 6.072873171s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected configMap[0m 
  [1mshould be consumable from pods in volume [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Projected configMap
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:03:07.837: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating configMap with name projected-configmap-test-volume-d8e4e8fa-503c-11e9-9719-a08cfdecc127
[1mSTEP[0m: Creating a pod to test consume configMaps
Mar 26 20:03:07.873: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d8e546cb-503c-11e9-9719-a08cfdecc127" in namespace "projected-73" to be "success or failure"
Mar 26 20:03:07.875: INFO: Pod "pod-projected-configmaps-d8e546cb-503c-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 1.740764ms
Mar 26 20:03:09.878: INFO: Pod "pod-projected-configmaps-d8e546cb-503c-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004856627s
Mar 26 20:03:11.881: INFO: Pod "pod-projected-configmaps-d8e546cb-503c-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007764304s
[1mSTEP[0m: Saw pod success
Mar 26 20:03:11.881: INFO: Pod "pod-projected-configmaps-d8e546cb-503c-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 20:03:11.883: INFO: Trying to get logs from node conformance-worker pod pod-projected-configmaps-d8e546cb-503c-11e9-9719-a08cfdecc127 container projected-configmap-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar 26 20:03:11.904: INFO: Waiting for pod pod-projected-configmaps-d8e546cb-503c-11e9-9719-a08cfdecc127 to disappear
Mar 26 20:03:11.906: INFO: Pod pod-projected-configmaps-d8e546cb-503c-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:03:11.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-73" for this suite.
Mar 26 20:03:17.921: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:03:17.979: INFO: namespace projected-73 deletion completed in 6.067607714s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Garbage collector[0m 
  [1mshould not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-api-machinery] Garbage collector
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:03:17.979: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename gc
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: create the rc1
[1mSTEP[0m: create the rc2
[1mSTEP[0m: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
[1mSTEP[0m: delete the rc simpletest-rc-to-be-deleted
[1mSTEP[0m: wait for the rc to be deleted
[1mSTEP[0m: Gathering metrics
W0326 20:03:28.054543  168179 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar 26 20:03:28.054: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:03:28.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "gc-479" for this suite.
Mar 26 20:03:34.066: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:03:34.128: INFO: namespace gc-479 deletion completed in 6.071300335s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected combined[0m 
  [1mshould project all components that make up the projection API [Projection][NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Projected combined
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:03:34.128: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating configMap with name configmap-projected-all-test-volume-e891ca0f-503c-11e9-9719-a08cfdecc127
[1mSTEP[0m: Creating secret with name secret-projected-all-test-volume-e891c9ff-503c-11e9-9719-a08cfdecc127
[1mSTEP[0m: Creating a pod to test Check all projections for projected volume plugin
Mar 26 20:03:34.178: INFO: Waiting up to 5m0s for pod "projected-volume-e891c9ce-503c-11e9-9719-a08cfdecc127" in namespace "projected-3250" to be "success or failure"
Mar 26 20:03:34.184: INFO: Pod "projected-volume-e891c9ce-503c-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 5.948174ms
Mar 26 20:03:36.187: INFO: Pod "projected-volume-e891c9ce-503c-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008568903s
[1mSTEP[0m: Saw pod success
Mar 26 20:03:36.187: INFO: Pod "projected-volume-e891c9ce-503c-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 20:03:36.189: INFO: Trying to get logs from node conformance-worker2 pod projected-volume-e891c9ce-503c-11e9-9719-a08cfdecc127 container projected-all-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar 26 20:03:36.202: INFO: Waiting for pod projected-volume-e891c9ce-503c-11e9-9719-a08cfdecc127 to disappear
Mar 26 20:03:36.204: INFO: Pod projected-volume-e891c9ce-503c-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] Projected combined
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:03:36.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-3250" for this suite.
Mar 26 20:03:42.212: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:03:42.278: INFO: namespace projected-3250 deletion completed in 6.072007791s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] ConfigMap[0m 
  [1mbinary data should be reflected in volume [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] ConfigMap
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:03:42.278: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename configmap
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating configMap with name configmap-test-upd-ed6cbe2c-503c-11e9-9719-a08cfdecc127
[1mSTEP[0m: Creating the pod
[1mSTEP[0m: Waiting for pod with text data
[1mSTEP[0m: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:03:46.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "configmap-22" for this suite.
Mar 26 20:04:08.359: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:04:08.420: INFO: namespace configmap-22 deletion completed in 22.070704448s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90m[k8s.io] Update Demo[0m 
  [1mshould scale a replication controller  [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:04:08.420: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should scale a replication controller  [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: creating a replication controller
Mar 26 20:04:08.444: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance create -f - --namespace=kubectl-8403'
Mar 26 20:04:08.915: INFO: stderr: ""
Mar 26 20:04:08.915: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
[1mSTEP[0m: waiting for all containers in name=update-demo pods to come up.
Mar 26 20:04:08.915: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8403'
Mar 26 20:04:08.989: INFO: stderr: ""
Mar 26 20:04:08.989: INFO: stdout: "update-demo-nautilus-rd84c update-demo-nautilus-xtddv "
Mar 26 20:04:08.989: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get pods update-demo-nautilus-rd84c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8403'
Mar 26 20:04:09.059: INFO: stderr: ""
Mar 26 20:04:09.059: INFO: stdout: ""
Mar 26 20:04:09.059: INFO: update-demo-nautilus-rd84c is created but not running
Mar 26 20:04:14.060: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8403'
Mar 26 20:04:14.132: INFO: stderr: ""
Mar 26 20:04:14.132: INFO: stdout: "update-demo-nautilus-rd84c update-demo-nautilus-xtddv "
Mar 26 20:04:14.133: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get pods update-demo-nautilus-rd84c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8403'
Mar 26 20:04:14.196: INFO: stderr: ""
Mar 26 20:04:14.196: INFO: stdout: "true"
Mar 26 20:04:14.196: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get pods update-demo-nautilus-rd84c -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8403'
Mar 26 20:04:14.262: INFO: stderr: ""
Mar 26 20:04:14.262: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 26 20:04:14.262: INFO: validating pod update-demo-nautilus-rd84c
Mar 26 20:04:14.268: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 26 20:04:14.268: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 26 20:04:14.268: INFO: update-demo-nautilus-rd84c is verified up and running
Mar 26 20:04:14.268: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get pods update-demo-nautilus-xtddv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8403'
Mar 26 20:04:14.346: INFO: stderr: ""
Mar 26 20:04:14.346: INFO: stdout: "true"
Mar 26 20:04:14.346: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get pods update-demo-nautilus-xtddv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8403'
Mar 26 20:04:14.415: INFO: stderr: ""
Mar 26 20:04:14.415: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 26 20:04:14.415: INFO: validating pod update-demo-nautilus-xtddv
Mar 26 20:04:14.419: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 26 20:04:14.420: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 26 20:04:14.420: INFO: update-demo-nautilus-xtddv is verified up and running
[1mSTEP[0m: scaling down the replication controller
Mar 26 20:04:14.485: INFO: scanned /usr/local/google/home/bentheelder for discovery docs: <nil>
Mar 26 20:04:14.485: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-8403'
Mar 26 20:04:15.579: INFO: stderr: ""
Mar 26 20:04:15.579: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
[1mSTEP[0m: waiting for all containers in name=update-demo pods to come up.
Mar 26 20:04:15.579: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8403'
Mar 26 20:04:15.648: INFO: stderr: ""
Mar 26 20:04:15.648: INFO: stdout: "update-demo-nautilus-rd84c update-demo-nautilus-xtddv "
[1mSTEP[0m: Replicas for name=update-demo: expected=1 actual=2
Mar 26 20:04:20.648: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8403'
Mar 26 20:04:20.729: INFO: stderr: ""
Mar 26 20:04:20.729: INFO: stdout: "update-demo-nautilus-rd84c update-demo-nautilus-xtddv "
[1mSTEP[0m: Replicas for name=update-demo: expected=1 actual=2
Mar 26 20:04:25.729: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8403'
Mar 26 20:04:25.809: INFO: stderr: ""
Mar 26 20:04:25.809: INFO: stdout: "update-demo-nautilus-rd84c "
Mar 26 20:04:25.809: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get pods update-demo-nautilus-rd84c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8403'
Mar 26 20:04:25.877: INFO: stderr: ""
Mar 26 20:04:25.877: INFO: stdout: "true"
Mar 26 20:04:25.877: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get pods update-demo-nautilus-rd84c -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8403'
Mar 26 20:04:25.956: INFO: stderr: ""
Mar 26 20:04:25.956: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 26 20:04:25.956: INFO: validating pod update-demo-nautilus-rd84c
Mar 26 20:04:25.958: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 26 20:04:25.959: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 26 20:04:25.959: INFO: update-demo-nautilus-rd84c is verified up and running
[1mSTEP[0m: scaling up the replication controller
Mar 26 20:04:26.022: INFO: scanned /usr/local/google/home/bentheelder for discovery docs: <nil>
Mar 26 20:04:26.022: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-8403'
Mar 26 20:04:27.109: INFO: stderr: ""
Mar 26 20:04:27.109: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
[1mSTEP[0m: waiting for all containers in name=update-demo pods to come up.
Mar 26 20:04:27.110: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8403'
Mar 26 20:04:27.179: INFO: stderr: ""
Mar 26 20:04:27.179: INFO: stdout: "update-demo-nautilus-9s5xt update-demo-nautilus-rd84c "
Mar 26 20:04:27.179: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get pods update-demo-nautilus-9s5xt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8403'
Mar 26 20:04:27.248: INFO: stderr: ""
Mar 26 20:04:27.248: INFO: stdout: ""
Mar 26 20:04:27.248: INFO: update-demo-nautilus-9s5xt is created but not running
Mar 26 20:04:32.249: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8403'
Mar 26 20:04:32.325: INFO: stderr: ""
Mar 26 20:04:32.325: INFO: stdout: "update-demo-nautilus-9s5xt update-demo-nautilus-rd84c "
Mar 26 20:04:32.325: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get pods update-demo-nautilus-9s5xt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8403'
Mar 26 20:04:32.390: INFO: stderr: ""
Mar 26 20:04:32.390: INFO: stdout: "true"
Mar 26 20:04:32.390: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get pods update-demo-nautilus-9s5xt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8403'
Mar 26 20:04:32.461: INFO: stderr: ""
Mar 26 20:04:32.461: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 26 20:04:32.461: INFO: validating pod update-demo-nautilus-9s5xt
Mar 26 20:04:32.465: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 26 20:04:32.465: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 26 20:04:32.465: INFO: update-demo-nautilus-9s5xt is verified up and running
Mar 26 20:04:32.465: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get pods update-demo-nautilus-rd84c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8403'
Mar 26 20:04:32.524: INFO: stderr: ""
Mar 26 20:04:32.524: INFO: stdout: "true"
Mar 26 20:04:32.524: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get pods update-demo-nautilus-rd84c -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8403'
Mar 26 20:04:32.606: INFO: stderr: ""
Mar 26 20:04:32.606: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Mar 26 20:04:32.606: INFO: validating pod update-demo-nautilus-rd84c
Mar 26 20:04:32.609: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 26 20:04:32.609: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 26 20:04:32.609: INFO: update-demo-nautilus-rd84c is verified up and running
[1mSTEP[0m: using delete to clean up resources
Mar 26 20:04:32.609: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance delete --grace-period=0 --force -f - --namespace=kubectl-8403'
Mar 26 20:04:32.675: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 26 20:04:32.675: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 26 20:04:32.675: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8403'
Mar 26 20:04:32.757: INFO: stderr: "No resources found.\n"
Mar 26 20:04:32.757: INFO: stdout: ""
Mar 26 20:04:32.757: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get pods -l name=update-demo --namespace=kubectl-8403 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 26 20:04:32.830: INFO: stderr: ""
Mar 26 20:04:32.830: INFO: stdout: "update-demo-nautilus-9s5xt\nupdate-demo-nautilus-rd84c\n"
Mar 26 20:04:33.330: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8403'
Mar 26 20:04:33.401: INFO: stderr: "No resources found.\n"
Mar 26 20:04:33.401: INFO: stdout: ""
Mar 26 20:04:33.401: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get pods -l name=update-demo --namespace=kubectl-8403 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 26 20:04:33.472: INFO: stderr: ""
Mar 26 20:04:33.472: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:04:33.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-8403" for this suite.
Mar 26 20:04:55.483: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:04:55.544: INFO: namespace kubectl-8403 deletion completed in 22.069239285s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] Deployment[0m 
  [1mdeployment should support rollover [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-apps] Deployment
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:04:55.544: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename deployment
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Mar 26 20:04:55.576: INFO: Pod name rollover-pod: Found 0 pods out of 1
Mar 26 20:05:00.580: INFO: Pod name rollover-pod: Found 1 pods out of 1
[1mSTEP[0m: ensuring each pod is running
Mar 26 20:05:00.580: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Mar 26 20:05:02.582: INFO: Creating deployment "test-rollover-deployment"
Mar 26 20:05:02.587: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Mar 26 20:05:04.592: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Mar 26 20:05:04.597: INFO: Ensure that both replica sets have 1 created replica
Mar 26 20:05:04.603: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Mar 26 20:05:04.608: INFO: Updating deployment test-rollover-deployment
Mar 26 20:05:04.608: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Mar 26 20:05:06.617: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Mar 26 20:05:06.629: INFO: Make sure deployment "test-rollover-deployment" is complete
Mar 26 20:05:06.633: INFO: all replica sets need to contain the pod-template-hash label
Mar 26 20:05:06.633: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63689252702, loc:(*time.Location)(0x89eb0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63689252702, loc:(*time.Location)(0x89eb0e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63689252706, loc:(*time.Location)(0x89eb0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63689252702, loc:(*time.Location)(0x89eb0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 20:05:08.638: INFO: all replica sets need to contain the pod-template-hash label
Mar 26 20:05:08.638: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63689252702, loc:(*time.Location)(0x89eb0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63689252702, loc:(*time.Location)(0x89eb0e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63689252706, loc:(*time.Location)(0x89eb0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63689252702, loc:(*time.Location)(0x89eb0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 20:05:10.642: INFO: all replica sets need to contain the pod-template-hash label
Mar 26 20:05:10.642: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63689252702, loc:(*time.Location)(0x89eb0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63689252702, loc:(*time.Location)(0x89eb0e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63689252706, loc:(*time.Location)(0x89eb0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63689252702, loc:(*time.Location)(0x89eb0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 20:05:12.641: INFO: all replica sets need to contain the pod-template-hash label
Mar 26 20:05:12.641: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63689252702, loc:(*time.Location)(0x89eb0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63689252702, loc:(*time.Location)(0x89eb0e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63689252706, loc:(*time.Location)(0x89eb0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63689252702, loc:(*time.Location)(0x89eb0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 20:05:14.640: INFO: all replica sets need to contain the pod-template-hash label
Mar 26 20:05:14.640: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63689252702, loc:(*time.Location)(0x89eb0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63689252702, loc:(*time.Location)(0x89eb0e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63689252706, loc:(*time.Location)(0x89eb0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63689252702, loc:(*time.Location)(0x89eb0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 20:05:16.642: INFO: 
Mar 26 20:05:16.642: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:2, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63689252702, loc:(*time.Location)(0x89eb0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63689252702, loc:(*time.Location)(0x89eb0e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63689252716, loc:(*time.Location)(0x89eb0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63689252702, loc:(*time.Location)(0x89eb0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 26 20:05:18.641: INFO: 
Mar 26 20:05:18.641: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Mar 26 20:05:18.650: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-9626,SelfLink:/apis/apps/v1/namespaces/deployment-9626/deployments/test-rollover-deployment,UID:1d458dab-503d-11e9-b1ea-02429e4bb871,ResourceVersion:16847,Generation:2,CreationTimestamp:2019-03-26 20:05:02 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-03-26 20:05:02 -0700 PDT 2019-03-26 20:05:02 -0700 PDT MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-03-26 20:05:16 -0700 PDT 2019-03-26 20:05:02 -0700 PDT NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-766b4d6c9d" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Mar 26 20:05:18.652: INFO: New ReplicaSet "test-rollover-deployment-766b4d6c9d" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d,GenerateName:,Namespace:deployment-9626,SelfLink:/apis/apps/v1/namespaces/deployment-9626/replicasets/test-rollover-deployment-766b4d6c9d,UID:1e7a8a48-503d-11e9-b1ea-02429e4bb871,ResourceVersion:16836,Generation:2,CreationTimestamp:2019-03-26 20:05:04 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 1d458dab-503d-11e9-b1ea-02429e4bb871 0xc002b86a57 0xc002b86a58}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Mar 26 20:05:18.652: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Mar 26 20:05:18.653: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-9626,SelfLink:/apis/apps/v1/namespaces/deployment-9626/replicasets/test-rollover-controller,UID:1917bd86-503d-11e9-b1ea-02429e4bb871,ResourceVersion:16846,Generation:2,CreationTimestamp:2019-03-26 20:04:55 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 1d458dab-503d-11e9-b1ea-02429e4bb871 0xc002b868a7 0xc002b868a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar 26 20:05:18.653: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6455657675,GenerateName:,Namespace:deployment-9626,SelfLink:/apis/apps/v1/namespaces/deployment-9626/replicasets/test-rollover-deployment-6455657675,UID:1d46af4a-503d-11e9-b1ea-02429e4bb871,ResourceVersion:16806,Generation:2,CreationTimestamp:2019-03-26 20:05:02 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 1d458dab-503d-11e9-b1ea-02429e4bb871 0xc002b86977 0xc002b86978}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar 26 20:05:18.656: INFO: Pod "test-rollover-deployment-766b4d6c9d-5nwq9" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d-5nwq9,GenerateName:test-rollover-deployment-766b4d6c9d-,Namespace:deployment-9626,SelfLink:/api/v1/namespaces/deployment-9626/pods/test-rollover-deployment-766b4d6c9d-5nwq9,UID:1e7cc2cc-503d-11e9-b1ea-02429e4bb871,ResourceVersion:16817,Generation:0,CreationTimestamp:2019-03-26 20:05:04 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-766b4d6c9d 1e7a8a48-503d-11e9-b1ea-02429e4bb871 0xc002b876f7 0xc002b876f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-6vmwk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6vmwk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-6vmwk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b87760} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b87780}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 20:05:04 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 20:05:06 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 20:05:06 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 20:05:04 -0700 PDT  }],Message:,Reason:,HostIP:192.168.9.3,PodIP:10.32.0.3,StartTime:2019-03-26 20:05:04 -0700 PDT,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-03-26 20:05:05 -0700 PDT,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://1747cc3d3c0072a99461f88fc33057515449a501490e0da8cb42bcef96e499d2}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:05:18.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "deployment-9626" for this suite.
Mar 26 20:05:24.668: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:05:24.734: INFO: namespace deployment-9626 deletion completed in 6.074777655s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected configMap[0m 
  [1mshould be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Projected configMap
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:05:24.734: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating configMap with name projected-configmap-test-volume-map-2a7d7dbb-503d-11e9-9719-a08cfdecc127
[1mSTEP[0m: Creating a pod to test consume configMaps
Mar 26 20:05:24.770: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2a7e5d0c-503d-11e9-9719-a08cfdecc127" in namespace "projected-3095" to be "success or failure"
Mar 26 20:05:24.774: INFO: Pod "pod-projected-configmaps-2a7e5d0c-503d-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 4.389504ms
Mar 26 20:05:26.778: INFO: Pod "pod-projected-configmaps-2a7e5d0c-503d-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008173014s
Mar 26 20:05:28.782: INFO: Pod "pod-projected-configmaps-2a7e5d0c-503d-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011529742s
[1mSTEP[0m: Saw pod success
Mar 26 20:05:28.782: INFO: Pod "pod-projected-configmaps-2a7e5d0c-503d-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 20:05:28.784: INFO: Trying to get logs from node conformance-worker2 pod pod-projected-configmaps-2a7e5d0c-503d-11e9-9719-a08cfdecc127 container projected-configmap-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar 26 20:05:28.801: INFO: Waiting for pod pod-projected-configmaps-2a7e5d0c-503d-11e9-9719-a08cfdecc127 to disappear
Mar 26 20:05:28.804: INFO: Pod pod-projected-configmaps-2a7e5d0c-503d-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:05:28.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-3095" for this suite.
Mar 26 20:05:34.812: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:05:34.884: INFO: namespace projected-3095 deletion completed in 6.07878165s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Garbage collector[0m 
  [1mshould orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-api-machinery] Garbage collector
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:05:34.884: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename gc
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: create the deployment
[1mSTEP[0m: Wait for the Deployment to create new ReplicaSet
[1mSTEP[0m: delete the deployment
[1mSTEP[0m: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
[1mSTEP[0m: Gathering metrics
W0326 20:06:05.436148  168179 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Mar 26 20:06:05.436: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:06:05.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "gc-7411" for this suite.
Mar 26 20:06:11.448: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:06:11.511: INFO: namespace gc-7411 deletion completed in 6.072752462s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Container Runtime[0m [90mblackbox test[0m [0mwhen starting a container that exits[0m 
  [1mshould run with the expected status [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [k8s.io] Container Runtime
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:06:11.511: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename container-runtime
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
[1mSTEP[0m: Container 'terminate-cmd-rpa': should get the expected 'Phase'
[1mSTEP[0m: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
[1mSTEP[0m: Container 'terminate-cmd-rpa': should get the expected 'State'
[1mSTEP[0m: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
[1mSTEP[0m: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
[1mSTEP[0m: Container 'terminate-cmd-rpof': should get the expected 'Phase'
[1mSTEP[0m: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
[1mSTEP[0m: Container 'terminate-cmd-rpof': should get the expected 'State'
[1mSTEP[0m: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
[1mSTEP[0m: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
[1mSTEP[0m: Container 'terminate-cmd-rpn': should get the expected 'Phase'
[1mSTEP[0m: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
[1mSTEP[0m: Container 'terminate-cmd-rpn': should get the expected 'State'
[1mSTEP[0m: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:06:33.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "container-runtime-3365" for this suite.
Mar 26 20:06:39.735: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:06:39.802: INFO: namespace container-runtime-3365 deletion completed in 6.075127353s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Secrets[0m 
  [1mshould be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Secrets
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:06:39.802: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename secrets
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating secret with name secret-test-573d3c9f-503d-11e9-9719-a08cfdecc127
[1mSTEP[0m: Creating a pod to test consume secrets
Mar 26 20:06:39.848: INFO: Waiting up to 5m0s for pod "pod-secrets-573dfcef-503d-11e9-9719-a08cfdecc127" in namespace "secrets-5087" to be "success or failure"
Mar 26 20:06:39.850: INFO: Pod "pod-secrets-573dfcef-503d-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.359026ms
Mar 26 20:06:41.853: INFO: Pod "pod-secrets-573dfcef-503d-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005320617s
[1mSTEP[0m: Saw pod success
Mar 26 20:06:41.854: INFO: Pod "pod-secrets-573dfcef-503d-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 20:06:41.856: INFO: Trying to get logs from node conformance-worker2 pod pod-secrets-573dfcef-503d-11e9-9719-a08cfdecc127 container secret-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar 26 20:06:41.867: INFO: Waiting for pod pod-secrets-573dfcef-503d-11e9-9719-a08cfdecc127 to disappear
Mar 26 20:06:41.869: INFO: Pod pod-secrets-573dfcef-503d-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] Secrets
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:06:41.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "secrets-5087" for this suite.
Mar 26 20:06:47.881: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:06:47.947: INFO: namespace secrets-5087 deletion completed in 6.074785294s
[32mâ€¢[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] EmptyDir volumes[0m 
  [1mshould support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] EmptyDir volumes
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:06:47.947: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename emptydir
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating a pod to test emptydir 0644 on tmpfs
Mar 26 20:06:47.987: INFO: Waiting up to 5m0s for pod "pod-5c16d367-503d-11e9-9719-a08cfdecc127" in namespace "emptydir-6828" to be "success or failure"
Mar 26 20:06:47.993: INFO: Pod "pod-5c16d367-503d-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 5.951013ms
Mar 26 20:06:49.997: INFO: Pod "pod-5c16d367-503d-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009943971s
[1mSTEP[0m: Saw pod success
Mar 26 20:06:49.997: INFO: Pod "pod-5c16d367-503d-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 20:06:50.000: INFO: Trying to get logs from node conformance-worker pod pod-5c16d367-503d-11e9-9719-a08cfdecc127 container test-container: <nil>
[1mSTEP[0m: delete the pod
Mar 26 20:06:50.017: INFO: Waiting for pod pod-5c16d367-503d-11e9-9719-a08cfdecc127 to disappear
Mar 26 20:06:50.019: INFO: Pod pod-5c16d367-503d-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:06:50.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "emptydir-6828" for this suite.
Mar 26 20:06:56.028: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:06:56.092: INFO: namespace emptydir-6828 deletion completed in 6.070724263s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected secret[0m 
  [1moptional updates should be reflected in volume [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Projected secret
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:06:56.093: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating secret with name s-test-opt-del-60f38e43-503d-11e9-9719-a08cfdecc127
[1mSTEP[0m: Creating secret with name s-test-opt-upd-60f38e8a-503d-11e9-9719-a08cfdecc127
[1mSTEP[0m: Creating the pod
[1mSTEP[0m: Deleting secret s-test-opt-del-60f38e43-503d-11e9-9719-a08cfdecc127
[1mSTEP[0m: Updating secret s-test-opt-upd-60f38e8a-503d-11e9-9719-a08cfdecc127
[1mSTEP[0m: Creating secret with name s-test-opt-create-60f38eba-503d-11e9-9719-a08cfdecc127
[1mSTEP[0m: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:07:04.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-8194" for this suite.
Mar 26 20:07:26.249: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:07:26.317: INFO: namespace projected-8194 deletion completed in 22.07942623s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] EmptyDir volumes[0m 
  [1mshould support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] EmptyDir volumes
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:07:26.317: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename emptydir
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating a pod to test emptydir 0666 on tmpfs
Mar 26 20:07:26.342: INFO: Waiting up to 5m0s for pod "pod-72f4f99d-503d-11e9-9719-a08cfdecc127" in namespace "emptydir-7996" to be "success or failure"
Mar 26 20:07:26.346: INFO: Pod "pod-72f4f99d-503d-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053257ms
Mar 26 20:07:28.350: INFO: Pod "pod-72f4f99d-503d-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007735115s
[1mSTEP[0m: Saw pod success
Mar 26 20:07:28.350: INFO: Pod "pod-72f4f99d-503d-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 20:07:28.354: INFO: Trying to get logs from node conformance-worker pod pod-72f4f99d-503d-11e9-9719-a08cfdecc127 container test-container: <nil>
[1mSTEP[0m: delete the pod
Mar 26 20:07:28.374: INFO: Waiting for pod pod-72f4f99d-503d-11e9-9719-a08cfdecc127 to disappear
Mar 26 20:07:28.376: INFO: Pod pod-72f4f99d-503d-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:07:28.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "emptydir-7996" for this suite.
Mar 26 20:07:34.386: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:07:34.459: INFO: namespace emptydir-7996 deletion completed in 6.080485522s
[32mâ€¢[0m
[90m------------------------------[0m
[0m[sig-network] DNS[0m 
  [1mshould provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-network] DNS
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:07:34.459: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename dns
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7876.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-7876.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7876.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

[1mSTEP[0m: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7876.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-7876.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7876.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

[1mSTEP[0m: creating a pod to probe /etc/hosts
[1mSTEP[0m: submitting the pod to kubernetes
[1mSTEP[0m: retrieving the pod
[1mSTEP[0m: looking for the results for each expected name from probers
Mar 26 20:07:50.515: INFO: Unable to read wheezy_udp@PodARecord from pod dns-7876/dns-test-77d03ab5-503d-11e9-9719-a08cfdecc127: the server could not find the requested resource (get pods dns-test-77d03ab5-503d-11e9-9719-a08cfdecc127)
Mar 26 20:07:50.518: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-7876/dns-test-77d03ab5-503d-11e9-9719-a08cfdecc127: the server could not find the requested resource (get pods dns-test-77d03ab5-503d-11e9-9719-a08cfdecc127)
Mar 26 20:07:50.521: INFO: Unable to read jessie_hosts@dns-querier-1.dns-test-service.dns-7876.svc.cluster.local from pod dns-7876/dns-test-77d03ab5-503d-11e9-9719-a08cfdecc127: the server could not find the requested resource (get pods dns-test-77d03ab5-503d-11e9-9719-a08cfdecc127)
Mar 26 20:07:50.523: INFO: Unable to read jessie_hosts@dns-querier-1 from pod dns-7876/dns-test-77d03ab5-503d-11e9-9719-a08cfdecc127: the server could not find the requested resource (get pods dns-test-77d03ab5-503d-11e9-9719-a08cfdecc127)
Mar 26 20:07:50.526: INFO: Unable to read jessie_udp@PodARecord from pod dns-7876/dns-test-77d03ab5-503d-11e9-9719-a08cfdecc127: the server could not find the requested resource (get pods dns-test-77d03ab5-503d-11e9-9719-a08cfdecc127)
Mar 26 20:07:50.528: INFO: Unable to read jessie_tcp@PodARecord from pod dns-7876/dns-test-77d03ab5-503d-11e9-9719-a08cfdecc127: the server could not find the requested resource (get pods dns-test-77d03ab5-503d-11e9-9719-a08cfdecc127)
Mar 26 20:07:50.528: INFO: Lookups using dns-7876/dns-test-77d03ab5-503d-11e9-9719-a08cfdecc127 failed for: [wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_hosts@dns-querier-1.dns-test-service.dns-7876.svc.cluster.local jessie_hosts@dns-querier-1 jessie_udp@PodARecord jessie_tcp@PodARecord]

Mar 26 20:07:55.551: INFO: DNS probes using dns-7876/dns-test-77d03ab5-503d-11e9-9719-a08cfdecc127 succeeded

[1mSTEP[0m: deleting the pod
[AfterEach] [sig-network] DNS
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:07:55.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "dns-7876" for this suite.
Mar 26 20:08:01.574: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:08:01.640: INFO: namespace dns-7876 deletion completed in 6.075853993s
[32mâ€¢[0m
[90m------------------------------[0m
[0m[k8s.io] Container Lifecycle Hook[0m [90mwhen create a pod with lifecycle hook[0m 
  [1mshould execute prestop exec hook properly [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:08:01.640: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename container-lifecycle-hook
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
[1mSTEP[0m: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: create the pod with lifecycle hook
[1mSTEP[0m: delete the pod with lifecycle hook
Mar 26 20:08:05.705: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 26 20:08:05.713: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 26 20:08:07.713: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 26 20:08:07.716: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 26 20:08:09.713: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 26 20:08:09.717: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 26 20:08:11.713: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 26 20:08:11.717: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 26 20:08:13.713: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 26 20:08:13.717: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 26 20:08:15.713: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 26 20:08:15.717: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 26 20:08:17.713: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 26 20:08:17.717: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 26 20:08:19.713: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 26 20:08:19.717: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 26 20:08:21.713: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 26 20:08:21.716: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 26 20:08:23.713: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 26 20:08:23.717: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 26 20:08:25.713: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 26 20:08:25.717: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 26 20:08:27.713: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 26 20:08:27.723: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 26 20:08:29.713: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 26 20:08:29.717: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 26 20:08:31.713: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 26 20:08:31.717: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 26 20:08:33.713: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 26 20:08:33.716: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 26 20:08:35.713: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 26 20:08:35.717: INFO: Pod pod-with-prestop-exec-hook no longer exists
[1mSTEP[0m: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:08:35.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "container-lifecycle-hook-6802" for this suite.
Mar 26 20:08:57.744: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:08:57.807: INFO: namespace container-lifecycle-hook-6802 deletion completed in 22.076431768s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90m[k8s.io] Kubectl logs[0m 
  [1mshould be able to retrieve and filter logs  [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:08:57.808: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl logs
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1190
[1mSTEP[0m: creating an rc
Mar 26 20:08:57.832: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance create -f - --namespace=kubectl-5404'
Mar 26 20:08:58.014: INFO: stderr: ""
Mar 26 20:08:58.014: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Waiting for Redis master to start.
Mar 26 20:08:59.018: INFO: Selector matched 1 pods for map[app:redis]
Mar 26 20:08:59.018: INFO: Found 0 / 1
Mar 26 20:09:00.018: INFO: Selector matched 1 pods for map[app:redis]
Mar 26 20:09:00.019: INFO: Found 1 / 1
Mar 26 20:09:00.019: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 26 20:09:00.021: INFO: Selector matched 1 pods for map[app:redis]
Mar 26 20:09:00.021: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[1mSTEP[0m: checking for a matching strings
Mar 26 20:09:00.021: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance logs redis-master-fxdng redis-master --namespace=kubectl-5404'
Mar 26 20:09:00.126: INFO: stderr: ""
Mar 26 20:09:00.126: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Mar 03:08:59.362 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Mar 03:08:59.362 # Server started, Redis version 3.2.12\n1:M 27 Mar 03:08:59.362 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Mar 03:08:59.362 * The server is now ready to accept connections on port 6379\n"
[1mSTEP[0m: limiting log lines
Mar 26 20:09:00.126: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance log redis-master-fxdng redis-master --namespace=kubectl-5404 --tail=1'
Mar 26 20:09:00.206: INFO: stderr: ""
Mar 26 20:09:00.206: INFO: stdout: "1:M 27 Mar 03:08:59.362 * The server is now ready to accept connections on port 6379\n"
[1mSTEP[0m: limiting log bytes
Mar 26 20:09:00.206: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance log redis-master-fxdng redis-master --namespace=kubectl-5404 --limit-bytes=1'
Mar 26 20:09:00.295: INFO: stderr: ""
Mar 26 20:09:00.295: INFO: stdout: " "
[1mSTEP[0m: exposing timestamps
Mar 26 20:09:00.295: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance log redis-master-fxdng redis-master --namespace=kubectl-5404 --tail=1 --timestamps'
Mar 26 20:09:00.380: INFO: stderr: ""
Mar 26 20:09:00.380: INFO: stdout: "2019-03-27T03:08:59.362458259Z 1:M 27 Mar 03:08:59.362 * The server is now ready to accept connections on port 6379\n"
[1mSTEP[0m: restricting to a time range
Mar 26 20:09:02.881: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance log redis-master-fxdng redis-master --namespace=kubectl-5404 --since=1s'
Mar 26 20:09:02.977: INFO: stderr: ""
Mar 26 20:09:02.977: INFO: stdout: ""
Mar 26 20:09:02.977: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance log redis-master-fxdng redis-master --namespace=kubectl-5404 --since=24h'
Mar 26 20:09:03.056: INFO: stderr: ""
Mar 26 20:09:03.056: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Mar 03:08:59.362 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Mar 03:08:59.362 # Server started, Redis version 3.2.12\n1:M 27 Mar 03:08:59.362 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 27 Mar 03:08:59.362 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1196
[1mSTEP[0m: using delete to clean up resources
Mar 26 20:09:03.056: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance delete --grace-period=0 --force -f - --namespace=kubectl-5404'
Mar 26 20:09:03.125: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 26 20:09:03.125: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Mar 26 20:09:03.125: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get rc,svc -l name=nginx --no-headers --namespace=kubectl-5404'
Mar 26 20:09:03.189: INFO: stderr: "No resources found.\n"
Mar 26 20:09:03.189: INFO: stdout: ""
Mar 26 20:09:03.189: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get pods -l name=nginx --namespace=kubectl-5404 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 26 20:09:03.277: INFO: stderr: ""
Mar 26 20:09:03.277: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:09:03.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-5404" for this suite.
Mar 26 20:09:25.289: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:09:25.360: INFO: namespace kubectl-5404 deletion completed in 22.080649354s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Docker Containers[0m 
  [1mshould be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [k8s.io] Docker Containers
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:09:25.360: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename containers
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating a pod to test override command
Mar 26 20:09:25.395: INFO: Waiting up to 5m0s for pod "client-containers-b9eaf7e0-503d-11e9-9719-a08cfdecc127" in namespace "containers-8359" to be "success or failure"
Mar 26 20:09:25.398: INFO: Pod "client-containers-b9eaf7e0-503d-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.427951ms
Mar 26 20:09:27.401: INFO: Pod "client-containers-b9eaf7e0-503d-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005822452s
Mar 26 20:09:29.404: INFO: Pod "client-containers-b9eaf7e0-503d-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009057747s
[1mSTEP[0m: Saw pod success
Mar 26 20:09:29.405: INFO: Pod "client-containers-b9eaf7e0-503d-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 20:09:29.407: INFO: Trying to get logs from node conformance-worker2 pod client-containers-b9eaf7e0-503d-11e9-9719-a08cfdecc127 container test-container: <nil>
[1mSTEP[0m: delete the pod
Mar 26 20:09:29.427: INFO: Waiting for pod client-containers-b9eaf7e0-503d-11e9-9719-a08cfdecc127 to disappear
Mar 26 20:09:29.430: INFO: Pod client-containers-b9eaf7e0-503d-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:09:29.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "containers-8359" for this suite.
Mar 26 20:09:35.443: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:09:35.506: INFO: namespace containers-8359 deletion completed in 6.073652512s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Variable Expansion[0m 
  [1mshould allow substituting values in a container's args [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [k8s.io] Variable Expansion
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:09:35.506: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename var-expansion
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating a pod to test substitution in container's args
Mar 26 20:09:35.549: INFO: Waiting up to 5m0s for pod "var-expansion-bff810e6-503d-11e9-9719-a08cfdecc127" in namespace "var-expansion-3491" to be "success or failure"
Mar 26 20:09:35.558: INFO: Pod "var-expansion-bff810e6-503d-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 8.724072ms
Mar 26 20:09:37.562: INFO: Pod "var-expansion-bff810e6-503d-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012703499s
[1mSTEP[0m: Saw pod success
Mar 26 20:09:37.562: INFO: Pod "var-expansion-bff810e6-503d-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 20:09:37.565: INFO: Trying to get logs from node conformance-worker pod var-expansion-bff810e6-503d-11e9-9719-a08cfdecc127 container dapi-container: <nil>
[1mSTEP[0m: delete the pod
Mar 26 20:09:37.582: INFO: Waiting for pod var-expansion-bff810e6-503d-11e9-9719-a08cfdecc127 to disappear
Mar 26 20:09:37.584: INFO: Pod var-expansion-bff810e6-503d-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:09:37.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "var-expansion-3491" for this suite.
Mar 26 20:09:43.598: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:09:43.669: INFO: namespace var-expansion-3491 deletion completed in 6.082370777s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Pods[0m 
  [1mshould allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [k8s.io] Pods
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:09:43.669: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename pods
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: creating the pod
[1mSTEP[0m: submitting the pod to kubernetes
[1mSTEP[0m: verifying the pod is in kubernetes
[1mSTEP[0m: updating the pod
Mar 26 20:09:46.226: INFO: Successfully updated pod "pod-update-activedeadlineseconds-c4d471ba-503d-11e9-9719-a08cfdecc127"
Mar 26 20:09:46.226: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-c4d471ba-503d-11e9-9719-a08cfdecc127" in namespace "pods-4415" to be "terminated due to deadline exceeded"
Mar 26 20:09:46.231: INFO: Pod "pod-update-activedeadlineseconds-c4d471ba-503d-11e9-9719-a08cfdecc127": Phase="Running", Reason="", readiness=true. Elapsed: 5.228346ms
Mar 26 20:09:48.235: INFO: Pod "pod-update-activedeadlineseconds-c4d471ba-503d-11e9-9719-a08cfdecc127": Phase="Running", Reason="", readiness=true. Elapsed: 2.009167681s
Mar 26 20:09:50.239: INFO: Pod "pod-update-activedeadlineseconds-c4d471ba-503d-11e9-9719-a08cfdecc127": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.012666355s
Mar 26 20:09:50.239: INFO: Pod "pod-update-activedeadlineseconds-c4d471ba-503d-11e9-9719-a08cfdecc127" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:09:50.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "pods-4415" for this suite.
Mar 26 20:09:56.259: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:09:56.321: INFO: namespace pods-4415 deletion completed in 6.078813075s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Secrets[0m 
  [1mshould be consumable in multiple volumes in a pod [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Secrets
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:09:56.321: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename secrets
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating secret with name secret-test-cc5e63b5-503d-11e9-9719-a08cfdecc127
[1mSTEP[0m: Creating a pod to test consume secrets
Mar 26 20:09:56.355: INFO: Waiting up to 5m0s for pod "pod-secrets-cc5ed088-503d-11e9-9719-a08cfdecc127" in namespace "secrets-8766" to be "success or failure"
Mar 26 20:09:56.357: INFO: Pod "pod-secrets-cc5ed088-503d-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 1.950475ms
Mar 26 20:09:58.361: INFO: Pod "pod-secrets-cc5ed088-503d-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005806331s
Mar 26 20:10:00.365: INFO: Pod "pod-secrets-cc5ed088-503d-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009745292s
[1mSTEP[0m: Saw pod success
Mar 26 20:10:00.365: INFO: Pod "pod-secrets-cc5ed088-503d-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 20:10:00.368: INFO: Trying to get logs from node conformance-worker pod pod-secrets-cc5ed088-503d-11e9-9719-a08cfdecc127 container secret-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar 26 20:10:00.391: INFO: Waiting for pod pod-secrets-cc5ed088-503d-11e9-9719-a08cfdecc127 to disappear
Mar 26 20:10:00.393: INFO: Pod pod-secrets-cc5ed088-503d-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] Secrets
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:10:00.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "secrets-8766" for this suite.
Mar 26 20:10:06.406: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:10:06.478: INFO: namespace secrets-8766 deletion completed in 6.082123645s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected downwardAPI[0m 
  [1mshould set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Projected downwardAPI
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:10:06.479: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar 26 20:10:06.527: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d26eecd4-503d-11e9-9719-a08cfdecc127" in namespace "projected-9742" to be "success or failure"
Mar 26 20:10:06.530: INFO: Pod "downwardapi-volume-d26eecd4-503d-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 3.055605ms
Mar 26 20:10:08.534: INFO: Pod "downwardapi-volume-d26eecd4-503d-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006988239s
Mar 26 20:10:10.538: INFO: Pod "downwardapi-volume-d26eecd4-503d-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010619302s
[1mSTEP[0m: Saw pod success
Mar 26 20:10:10.538: INFO: Pod "downwardapi-volume-d26eecd4-503d-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 20:10:10.540: INFO: Trying to get logs from node conformance-worker2 pod downwardapi-volume-d26eecd4-503d-11e9-9719-a08cfdecc127 container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar 26 20:10:10.556: INFO: Waiting for pod downwardapi-volume-d26eecd4-503d-11e9-9719-a08cfdecc127 to disappear
Mar 26 20:10:10.558: INFO: Pod downwardapi-volume-d26eecd4-503d-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:10:10.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-9742" for this suite.
Mar 26 20:10:16.571: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:10:16.636: INFO: namespace projected-9742 deletion completed in 6.075637167s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] ConfigMap[0m 
  [1mshould be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] ConfigMap
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:10:16.637: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename configmap
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating configMap with name configmap-test-volume-map-d87a8a00-503d-11e9-9719-a08cfdecc127
[1mSTEP[0m: Creating a pod to test consume configMaps
Mar 26 20:10:16.672: INFO: Waiting up to 5m0s for pod "pod-configmaps-d87b1708-503d-11e9-9719-a08cfdecc127" in namespace "configmap-6821" to be "success or failure"
Mar 26 20:10:16.673: INFO: Pod "pod-configmaps-d87b1708-503d-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 1.604229ms
Mar 26 20:10:18.677: INFO: Pod "pod-configmaps-d87b1708-503d-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005410759s
Mar 26 20:10:20.681: INFO: Pod "pod-configmaps-d87b1708-503d-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00952529s
[1mSTEP[0m: Saw pod success
Mar 26 20:10:20.681: INFO: Pod "pod-configmaps-d87b1708-503d-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 20:10:20.685: INFO: Trying to get logs from node conformance-worker pod pod-configmaps-d87b1708-503d-11e9-9719-a08cfdecc127 container configmap-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar 26 20:10:20.703: INFO: Waiting for pod pod-configmaps-d87b1708-503d-11e9-9719-a08cfdecc127 to disappear
Mar 26 20:10:20.710: INFO: Pod pod-configmaps-d87b1708-503d-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:10:20.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "configmap-6821" for this suite.
Mar 26 20:10:26.721: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:10:26.786: INFO: namespace configmap-6821 deletion completed in 6.073494813s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Downward API volume[0m 
  [1mshould provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Downward API volume
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:10:26.786: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename downward-api
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar 26 20:10:26.814: INFO: Waiting up to 5m0s for pod "downwardapi-volume-de86bfde-503d-11e9-9719-a08cfdecc127" in namespace "downward-api-4072" to be "success or failure"
Mar 26 20:10:26.817: INFO: Pod "downwardapi-volume-de86bfde-503d-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.351042ms
Mar 26 20:10:28.820: INFO: Pod "downwardapi-volume-de86bfde-503d-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005822114s
Mar 26 20:10:30.826: INFO: Pod "downwardapi-volume-de86bfde-503d-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011729857s
[1mSTEP[0m: Saw pod success
Mar 26 20:10:30.826: INFO: Pod "downwardapi-volume-de86bfde-503d-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 20:10:30.829: INFO: Trying to get logs from node conformance-worker2 pod downwardapi-volume-de86bfde-503d-11e9-9719-a08cfdecc127 container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar 26 20:10:30.845: INFO: Waiting for pod downwardapi-volume-de86bfde-503d-11e9-9719-a08cfdecc127 to disappear
Mar 26 20:10:30.847: INFO: Pod downwardapi-volume-de86bfde-503d-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:10:30.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "downward-api-4072" for this suite.
Mar 26 20:10:36.858: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:10:36.919: INFO: namespace downward-api-4072 deletion completed in 6.069172792s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Proxy[0m [90mversion v1[0m 
  [1mshould proxy through a service and a pod  [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] version v1
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:10:36.920: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename proxy
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: starting an echo server on multiple ports
[1mSTEP[0m: creating replication controller proxy-service-m568g in namespace proxy-4794
I0326 20:10:36.980031  168179 runners.go:184] Created replication controller with name: proxy-service-m568g, namespace: proxy-4794, replica count: 1
I0326 20:10:38.030417  168179 runners.go:184] proxy-service-m568g Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0326 20:10:39.030579  168179 runners.go:184] proxy-service-m568g Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0326 20:10:40.030757  168179 runners.go:184] proxy-service-m568g Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0326 20:10:41.030951  168179 runners.go:184] proxy-service-m568g Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0326 20:10:42.031180  168179 runners.go:184] proxy-service-m568g Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0326 20:10:43.031345  168179 runners.go:184] proxy-service-m568g Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0326 20:10:44.031623  168179 runners.go:184] proxy-service-m568g Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 26 20:10:44.035: INFO: setup took 7.076300635s, starting test cases
[1mSTEP[0m: running 16 cases, 20 attempts per case, 320 total attempts
Mar 26 20:10:44.042: INFO: (0) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6/proxy/rewriteme">test</a> (200; 6.683366ms)
Mar 26 20:10:44.042: INFO: (0) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:162/proxy/: bar (200; 6.73193ms)
Mar 26 20:10:44.043: INFO: (0) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:162/proxy/: bar (200; 7.573098ms)
Mar 26 20:10:44.047: INFO: (0) /api/v1/namespaces/proxy-4794/services/proxy-service-m568g:portname1/proxy/: foo (200; 11.352703ms)
Mar 26 20:10:44.047: INFO: (0) /api/v1/namespaces/proxy-4794/services/http:proxy-service-m568g:portname1/proxy/: foo (200; 11.477497ms)
Mar 26 20:10:44.047: INFO: (0) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:160/proxy/: foo (200; 11.479215ms)
Mar 26 20:10:44.047: INFO: (0) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:160/proxy/: foo (200; 11.605601ms)
Mar 26 20:10:44.051: INFO: (0) /api/v1/namespaces/proxy-4794/services/proxy-service-m568g:portname2/proxy/: bar (200; 15.590691ms)
Mar 26 20:10:44.051: INFO: (0) /api/v1/namespaces/proxy-4794/services/http:proxy-service-m568g:portname2/proxy/: bar (200; 15.778696ms)
Mar 26 20:10:44.051: INFO: (0) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:1080/proxy/rewriteme">... (200; 15.81017ms)
Mar 26 20:10:44.051: INFO: (0) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:1080/proxy/rewriteme">test<... (200; 15.809675ms)
Mar 26 20:10:44.052: INFO: (0) /api/v1/namespaces/proxy-4794/services/https:proxy-service-m568g:tlsportname2/proxy/: tls qux (200; 16.764194ms)
Mar 26 20:10:44.052: INFO: (0) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:460/proxy/: tls baz (200; 16.705306ms)
Mar 26 20:10:44.052: INFO: (0) /api/v1/namespaces/proxy-4794/services/https:proxy-service-m568g:tlsportname1/proxy/: tls baz (200; 16.838941ms)
Mar 26 20:10:44.052: INFO: (0) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:443/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:443/proxy/tlsrewritem... (200; 17.013693ms)
Mar 26 20:10:44.052: INFO: (0) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:462/proxy/: tls qux (200; 16.985604ms)
Mar 26 20:10:44.057: INFO: (1) /api/v1/namespaces/proxy-4794/services/https:proxy-service-m568g:tlsportname1/proxy/: tls baz (200; 4.837071ms)
Mar 26 20:10:44.058: INFO: (1) /api/v1/namespaces/proxy-4794/services/proxy-service-m568g:portname2/proxy/: bar (200; 5.228053ms)
Mar 26 20:10:44.058: INFO: (1) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:462/proxy/: tls qux (200; 5.678272ms)
Mar 26 20:10:44.058: INFO: (1) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:443/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:443/proxy/tlsrewritem... (200; 5.639998ms)
Mar 26 20:10:44.058: INFO: (1) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:162/proxy/: bar (200; 5.64339ms)
Mar 26 20:10:44.058: INFO: (1) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6/proxy/rewriteme">test</a> (200; 5.672617ms)
Mar 26 20:10:44.058: INFO: (1) /api/v1/namespaces/proxy-4794/services/proxy-service-m568g:portname1/proxy/: foo (200; 5.676062ms)
Mar 26 20:10:44.058: INFO: (1) /api/v1/namespaces/proxy-4794/services/http:proxy-service-m568g:portname1/proxy/: foo (200; 5.690351ms)
Mar 26 20:10:44.058: INFO: (1) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:460/proxy/: tls baz (200; 5.724825ms)
Mar 26 20:10:44.058: INFO: (1) /api/v1/namespaces/proxy-4794/services/http:proxy-service-m568g:portname2/proxy/: bar (200; 5.779041ms)
Mar 26 20:10:44.058: INFO: (1) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:162/proxy/: bar (200; 5.696525ms)
Mar 26 20:10:44.058: INFO: (1) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:1080/proxy/rewriteme">... (200; 5.789048ms)
Mar 26 20:10:44.058: INFO: (1) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:1080/proxy/rewriteme">test<... (200; 5.80915ms)
Mar 26 20:10:44.058: INFO: (1) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:160/proxy/: foo (200; 5.760933ms)
Mar 26 20:10:44.058: INFO: (1) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:160/proxy/: foo (200; 5.747485ms)
Mar 26 20:10:44.058: INFO: (1) /api/v1/namespaces/proxy-4794/services/https:proxy-service-m568g:tlsportname2/proxy/: tls qux (200; 5.790965ms)
Mar 26 20:10:44.061: INFO: (2) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:443/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:443/proxy/tlsrewritem... (200; 2.938886ms)
Mar 26 20:10:44.061: INFO: (2) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:162/proxy/: bar (200; 3.081298ms)
Mar 26 20:10:44.061: INFO: (2) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:460/proxy/: tls baz (200; 3.112869ms)
Mar 26 20:10:44.061: INFO: (2) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:1080/proxy/rewriteme">... (200; 3.146278ms)
Mar 26 20:10:44.063: INFO: (2) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:160/proxy/: foo (200; 4.512905ms)
Mar 26 20:10:44.063: INFO: (2) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:162/proxy/: bar (200; 4.648991ms)
Mar 26 20:10:44.063: INFO: (2) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:1080/proxy/rewriteme">test<... (200; 4.776301ms)
Mar 26 20:10:44.063: INFO: (2) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:160/proxy/: foo (200; 4.971785ms)
Mar 26 20:10:44.063: INFO: (2) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6/proxy/rewriteme">test</a> (200; 5.153469ms)
Mar 26 20:10:44.063: INFO: (2) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:462/proxy/: tls qux (200; 5.25453ms)
Mar 26 20:10:44.064: INFO: (2) /api/v1/namespaces/proxy-4794/services/proxy-service-m568g:portname2/proxy/: bar (200; 5.910987ms)
Mar 26 20:10:44.064: INFO: (2) /api/v1/namespaces/proxy-4794/services/http:proxy-service-m568g:portname2/proxy/: bar (200; 6.163082ms)
Mar 26 20:10:44.064: INFO: (2) /api/v1/namespaces/proxy-4794/services/http:proxy-service-m568g:portname1/proxy/: foo (200; 6.112266ms)
Mar 26 20:10:44.064: INFO: (2) /api/v1/namespaces/proxy-4794/services/https:proxy-service-m568g:tlsportname1/proxy/: tls baz (200; 6.131718ms)
Mar 26 20:10:44.064: INFO: (2) /api/v1/namespaces/proxy-4794/services/https:proxy-service-m568g:tlsportname2/proxy/: tls qux (200; 6.135329ms)
Mar 26 20:10:44.064: INFO: (2) /api/v1/namespaces/proxy-4794/services/proxy-service-m568g:portname1/proxy/: foo (200; 6.172176ms)
Mar 26 20:10:44.067: INFO: (3) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:443/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:443/proxy/tlsrewritem... (200; 2.912157ms)
Mar 26 20:10:44.068: INFO: (3) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:162/proxy/: bar (200; 3.626299ms)
Mar 26 20:10:44.068: INFO: (3) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6/proxy/rewriteme">test</a> (200; 3.701525ms)
Mar 26 20:10:44.068: INFO: (3) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:460/proxy/: tls baz (200; 3.695717ms)
Mar 26 20:10:44.068: INFO: (3) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:160/proxy/: foo (200; 3.692798ms)
Mar 26 20:10:44.068: INFO: (3) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:1080/proxy/rewriteme">... (200; 3.748458ms)
Mar 26 20:10:44.068: INFO: (3) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:160/proxy/: foo (200; 3.650993ms)
Mar 26 20:10:44.068: INFO: (3) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:162/proxy/: bar (200; 3.669002ms)
Mar 26 20:10:44.068: INFO: (3) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:462/proxy/: tls qux (200; 4.052168ms)
Mar 26 20:10:44.068: INFO: (3) /api/v1/namespaces/proxy-4794/services/proxy-service-m568g:portname1/proxy/: foo (200; 4.089608ms)
Mar 26 20:10:44.069: INFO: (3) /api/v1/namespaces/proxy-4794/services/http:proxy-service-m568g:portname2/proxy/: bar (200; 4.928399ms)
Mar 26 20:10:44.069: INFO: (3) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:1080/proxy/rewriteme">test<... (200; 4.970214ms)
Mar 26 20:10:44.069: INFO: (3) /api/v1/namespaces/proxy-4794/services/proxy-service-m568g:portname2/proxy/: bar (200; 4.992408ms)
Mar 26 20:10:44.069: INFO: (3) /api/v1/namespaces/proxy-4794/services/http:proxy-service-m568g:portname1/proxy/: foo (200; 4.993488ms)
Mar 26 20:10:44.070: INFO: (3) /api/v1/namespaces/proxy-4794/services/https:proxy-service-m568g:tlsportname1/proxy/: tls baz (200; 5.413144ms)
Mar 26 20:10:44.070: INFO: (3) /api/v1/namespaces/proxy-4794/services/https:proxy-service-m568g:tlsportname2/proxy/: tls qux (200; 5.451664ms)
Mar 26 20:10:44.072: INFO: (4) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6/proxy/rewriteme">test</a> (200; 2.509457ms)
Mar 26 20:10:44.073: INFO: (4) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:160/proxy/: foo (200; 2.51713ms)
Mar 26 20:10:44.073: INFO: (4) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:460/proxy/: tls baz (200; 2.552544ms)
Mar 26 20:10:44.075: INFO: (4) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:160/proxy/: foo (200; 5.101615ms)
Mar 26 20:10:44.075: INFO: (4) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:1080/proxy/rewriteme">... (200; 5.16261ms)
Mar 26 20:10:44.075: INFO: (4) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:162/proxy/: bar (200; 5.191192ms)
Mar 26 20:10:44.076: INFO: (4) /api/v1/namespaces/proxy-4794/services/proxy-service-m568g:portname2/proxy/: bar (200; 6.034852ms)
Mar 26 20:10:44.076: INFO: (4) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:462/proxy/: tls qux (200; 6.087322ms)
Mar 26 20:10:44.076: INFO: (4) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:443/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:443/proxy/tlsrewritem... (200; 6.059222ms)
Mar 26 20:10:44.076: INFO: (4) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:1080/proxy/rewriteme">test<... (200; 6.056535ms)
Mar 26 20:10:44.076: INFO: (4) /api/v1/namespaces/proxy-4794/services/http:proxy-service-m568g:portname1/proxy/: foo (200; 6.147201ms)
Mar 26 20:10:44.076: INFO: (4) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:162/proxy/: bar (200; 6.093273ms)
Mar 26 20:10:44.076: INFO: (4) /api/v1/namespaces/proxy-4794/services/proxy-service-m568g:portname1/proxy/: foo (200; 6.347454ms)
Mar 26 20:10:44.076: INFO: (4) /api/v1/namespaces/proxy-4794/services/http:proxy-service-m568g:portname2/proxy/: bar (200; 6.388033ms)
Mar 26 20:10:44.076: INFO: (4) /api/v1/namespaces/proxy-4794/services/https:proxy-service-m568g:tlsportname1/proxy/: tls baz (200; 6.347137ms)
Mar 26 20:10:44.076: INFO: (4) /api/v1/namespaces/proxy-4794/services/https:proxy-service-m568g:tlsportname2/proxy/: tls qux (200; 6.428732ms)
Mar 26 20:10:44.081: INFO: (5) /api/v1/namespaces/proxy-4794/services/http:proxy-service-m568g:portname2/proxy/: bar (200; 4.314377ms)
Mar 26 20:10:44.081: INFO: (5) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:160/proxy/: foo (200; 4.465986ms)
Mar 26 20:10:44.081: INFO: (5) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:162/proxy/: bar (200; 4.590367ms)
Mar 26 20:10:44.081: INFO: (5) /api/v1/namespaces/proxy-4794/services/proxy-service-m568g:portname1/proxy/: foo (200; 4.880306ms)
Mar 26 20:10:44.081: INFO: (5) /api/v1/namespaces/proxy-4794/services/http:proxy-service-m568g:portname1/proxy/: foo (200; 4.842709ms)
Mar 26 20:10:44.081: INFO: (5) /api/v1/namespaces/proxy-4794/services/https:proxy-service-m568g:tlsportname2/proxy/: tls qux (200; 4.893144ms)
Mar 26 20:10:44.081: INFO: (5) /api/v1/namespaces/proxy-4794/services/https:proxy-service-m568g:tlsportname1/proxy/: tls baz (200; 4.87334ms)
Mar 26 20:10:44.082: INFO: (5) /api/v1/namespaces/proxy-4794/services/proxy-service-m568g:portname2/proxy/: bar (200; 5.130897ms)
Mar 26 20:10:44.082: INFO: (5) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6/proxy/rewriteme">test</a> (200; 5.20061ms)
Mar 26 20:10:44.082: INFO: (5) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:460/proxy/: tls baz (200; 5.155668ms)
Mar 26 20:10:44.082: INFO: (5) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:162/proxy/: bar (200; 5.240339ms)
Mar 26 20:10:44.082: INFO: (5) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:443/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:443/proxy/tlsrewritem... (200; 5.179124ms)
Mar 26 20:10:44.082: INFO: (5) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:160/proxy/: foo (200; 5.187965ms)
Mar 26 20:10:44.082: INFO: (5) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:1080/proxy/rewriteme">... (200; 5.240233ms)
Mar 26 20:10:44.082: INFO: (5) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:462/proxy/: tls qux (200; 5.369135ms)
Mar 26 20:10:44.082: INFO: (5) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:1080/proxy/rewriteme">test<... (200; 5.355793ms)
Mar 26 20:10:44.085: INFO: (6) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:162/proxy/: bar (200; 3.055798ms)
Mar 26 20:10:44.085: INFO: (6) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:160/proxy/: foo (200; 3.357823ms)
Mar 26 20:10:44.085: INFO: (6) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:160/proxy/: foo (200; 3.356848ms)
Mar 26 20:10:44.085: INFO: (6) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:460/proxy/: tls baz (200; 3.414337ms)
Mar 26 20:10:44.086: INFO: (6) /api/v1/namespaces/proxy-4794/services/https:proxy-service-m568g:tlsportname1/proxy/: tls baz (200; 3.763001ms)
Mar 26 20:10:44.086: INFO: (6) /api/v1/namespaces/proxy-4794/services/http:proxy-service-m568g:portname2/proxy/: bar (200; 3.833304ms)
Mar 26 20:10:44.086: INFO: (6) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:162/proxy/: bar (200; 3.905962ms)
Mar 26 20:10:44.086: INFO: (6) /api/v1/namespaces/proxy-4794/services/http:proxy-service-m568g:portname1/proxy/: foo (200; 4.281769ms)
Mar 26 20:10:44.086: INFO: (6) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:1080/proxy/rewriteme">test<... (200; 4.377075ms)
Mar 26 20:10:44.086: INFO: (6) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:1080/proxy/rewriteme">... (200; 4.383345ms)
Mar 26 20:10:44.086: INFO: (6) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:443/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:443/proxy/tlsrewritem... (200; 4.387591ms)
Mar 26 20:10:44.086: INFO: (6) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6/proxy/rewriteme">test</a> (200; 4.362602ms)
Mar 26 20:10:44.086: INFO: (6) /api/v1/namespaces/proxy-4794/services/proxy-service-m568g:portname1/proxy/: foo (200; 4.511369ms)
Mar 26 20:10:44.087: INFO: (6) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:462/proxy/: tls qux (200; 4.952662ms)
Mar 26 20:10:44.087: INFO: (6) /api/v1/namespaces/proxy-4794/services/proxy-service-m568g:portname2/proxy/: bar (200; 4.955498ms)
Mar 26 20:10:44.087: INFO: (6) /api/v1/namespaces/proxy-4794/services/https:proxy-service-m568g:tlsportname2/proxy/: tls qux (200; 5.060725ms)
Mar 26 20:10:44.091: INFO: (7) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:162/proxy/: bar (200; 4.021893ms)
Mar 26 20:10:44.091: INFO: (7) /api/v1/namespaces/proxy-4794/services/proxy-service-m568g:portname2/proxy/: bar (200; 4.076323ms)
Mar 26 20:10:44.091: INFO: (7) /api/v1/namespaces/proxy-4794/services/https:proxy-service-m568g:tlsportname1/proxy/: tls baz (200; 4.044626ms)
Mar 26 20:10:44.091: INFO: (7) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:160/proxy/: foo (200; 4.152075ms)
Mar 26 20:10:44.091: INFO: (7) /api/v1/namespaces/proxy-4794/services/https:proxy-service-m568g:tlsportname2/proxy/: tls qux (200; 4.178885ms)
Mar 26 20:10:44.091: INFO: (7) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:443/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:443/proxy/tlsrewritem... (200; 4.160646ms)
Mar 26 20:10:44.092: INFO: (7) /api/v1/namespaces/proxy-4794/services/proxy-service-m568g:portname1/proxy/: foo (200; 4.440396ms)
Mar 26 20:10:44.092: INFO: (7) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:160/proxy/: foo (200; 4.722021ms)
Mar 26 20:10:44.092: INFO: (7) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:1080/proxy/rewriteme">... (200; 4.803391ms)
Mar 26 20:10:44.092: INFO: (7) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:460/proxy/: tls baz (200; 4.825407ms)
Mar 26 20:10:44.092: INFO: (7) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:1080/proxy/rewriteme">test<... (200; 4.759192ms)
Mar 26 20:10:44.092: INFO: (7) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6/proxy/rewriteme">test</a> (200; 4.781432ms)
Mar 26 20:10:44.092: INFO: (7) /api/v1/namespaces/proxy-4794/services/http:proxy-service-m568g:portname2/proxy/: bar (200; 4.869631ms)
Mar 26 20:10:44.092: INFO: (7) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:462/proxy/: tls qux (200; 4.833629ms)
Mar 26 20:10:44.092: INFO: (7) /api/v1/namespaces/proxy-4794/services/http:proxy-service-m568g:portname1/proxy/: foo (200; 4.833437ms)
Mar 26 20:10:44.092: INFO: (7) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:162/proxy/: bar (200; 5.297333ms)
Mar 26 20:10:44.095: INFO: (8) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6/proxy/rewriteme">test</a> (200; 2.287099ms)
Mar 26 20:10:44.095: INFO: (8) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:160/proxy/: foo (200; 2.339725ms)
Mar 26 20:10:44.096: INFO: (8) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:1080/proxy/rewriteme">... (200; 3.427804ms)
Mar 26 20:10:44.096: INFO: (8) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:462/proxy/: tls qux (200; 3.476758ms)
Mar 26 20:10:44.096: INFO: (8) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:162/proxy/: bar (200; 3.610706ms)
Mar 26 20:10:44.096: INFO: (8) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:460/proxy/: tls baz (200; 3.511084ms)
Mar 26 20:10:44.096: INFO: (8) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:443/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:443/proxy/tlsrewritem... (200; 3.478952ms)
Mar 26 20:10:44.096: INFO: (8) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:160/proxy/: foo (200; 3.700714ms)
Mar 26 20:10:44.096: INFO: (8) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:162/proxy/: bar (200; 3.676409ms)
Mar 26 20:10:44.096: INFO: (8) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:1080/proxy/rewriteme">test<... (200; 3.532716ms)
Mar 26 20:10:44.097: INFO: (8) /api/v1/namespaces/proxy-4794/services/http:proxy-service-m568g:portname2/proxy/: bar (200; 4.783855ms)
Mar 26 20:10:44.098: INFO: (8) /api/v1/namespaces/proxy-4794/services/http:proxy-service-m568g:portname1/proxy/: foo (200; 4.68658ms)
Mar 26 20:10:44.098: INFO: (8) /api/v1/namespaces/proxy-4794/services/proxy-service-m568g:portname2/proxy/: bar (200; 4.941575ms)
Mar 26 20:10:44.098: INFO: (8) /api/v1/namespaces/proxy-4794/services/https:proxy-service-m568g:tlsportname2/proxy/: tls qux (200; 4.902512ms)
Mar 26 20:10:44.098: INFO: (8) /api/v1/namespaces/proxy-4794/services/proxy-service-m568g:portname1/proxy/: foo (200; 5.019173ms)
Mar 26 20:10:44.098: INFO: (8) /api/v1/namespaces/proxy-4794/services/https:proxy-service-m568g:tlsportname1/proxy/: tls baz (200; 4.979474ms)
Mar 26 20:10:44.101: INFO: (9) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:160/proxy/: foo (200; 3.489779ms)
Mar 26 20:10:44.102: INFO: (9) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:1080/proxy/rewriteme">test<... (200; 3.997208ms)
Mar 26 20:10:44.102: INFO: (9) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6/proxy/rewriteme">test</a> (200; 4.047084ms)
Mar 26 20:10:44.102: INFO: (9) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:443/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:443/proxy/tlsrewritem... (200; 4.004158ms)
Mar 26 20:10:44.102: INFO: (9) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:460/proxy/: tls baz (200; 4.039213ms)
Mar 26 20:10:44.102: INFO: (9) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:162/proxy/: bar (200; 4.101818ms)
Mar 26 20:10:44.102: INFO: (9) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:160/proxy/: foo (200; 4.073367ms)
Mar 26 20:10:44.102: INFO: (9) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:1080/proxy/rewriteme">... (200; 4.078553ms)
Mar 26 20:10:44.102: INFO: (9) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:162/proxy/: bar (200; 4.157371ms)
Mar 26 20:10:44.102: INFO: (9) /api/v1/namespaces/proxy-4794/services/https:proxy-service-m568g:tlsportname1/proxy/: tls baz (200; 4.149414ms)
Mar 26 20:10:44.102: INFO: (9) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:462/proxy/: tls qux (200; 4.265729ms)
Mar 26 20:10:44.102: INFO: (9) /api/v1/namespaces/proxy-4794/services/proxy-service-m568g:portname1/proxy/: foo (200; 4.242788ms)
Mar 26 20:10:44.102: INFO: (9) /api/v1/namespaces/proxy-4794/services/https:proxy-service-m568g:tlsportname2/proxy/: tls qux (200; 4.508582ms)
Mar 26 20:10:44.102: INFO: (9) /api/v1/namespaces/proxy-4794/services/http:proxy-service-m568g:portname1/proxy/: foo (200; 4.539163ms)
Mar 26 20:10:44.102: INFO: (9) /api/v1/namespaces/proxy-4794/services/http:proxy-service-m568g:portname2/proxy/: bar (200; 4.562386ms)
Mar 26 20:10:44.102: INFO: (9) /api/v1/namespaces/proxy-4794/services/proxy-service-m568g:portname2/proxy/: bar (200; 4.70531ms)
Mar 26 20:10:44.105: INFO: (10) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:1080/proxy/rewriteme">test<... (200; 2.366814ms)
Mar 26 20:10:44.105: INFO: (10) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:460/proxy/: tls baz (200; 2.423828ms)
Mar 26 20:10:44.107: INFO: (10) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:443/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:443/proxy/tlsrewritem... (200; 4.019311ms)
Mar 26 20:10:44.107: INFO: (10) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:162/proxy/: bar (200; 4.984051ms)
Mar 26 20:10:44.107: INFO: (10) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:160/proxy/: foo (200; 4.981463ms)
Mar 26 20:10:44.107: INFO: (10) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6/proxy/rewriteme">test</a> (200; 4.982761ms)
Mar 26 20:10:44.107: INFO: (10) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:462/proxy/: tls qux (200; 5.042426ms)
Mar 26 20:10:44.107: INFO: (10) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:162/proxy/: bar (200; 5.00704ms)
Mar 26 20:10:44.108: INFO: (10) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:160/proxy/: foo (200; 5.244743ms)
Mar 26 20:10:44.108: INFO: (10) /api/v1/namespaces/proxy-4794/services/http:proxy-service-m568g:portname2/proxy/: bar (200; 5.465481ms)
Mar 26 20:10:44.108: INFO: (10) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:1080/proxy/rewriteme">... (200; 5.526243ms)
Mar 26 20:10:44.109: INFO: (10) /api/v1/namespaces/proxy-4794/services/https:proxy-service-m568g:tlsportname2/proxy/: tls qux (200; 6.564347ms)
Mar 26 20:10:44.110: INFO: (10) /api/v1/namespaces/proxy-4794/services/proxy-service-m568g:portname2/proxy/: bar (200; 7.61553ms)
Mar 26 20:10:44.110: INFO: (10) /api/v1/namespaces/proxy-4794/services/http:proxy-service-m568g:portname1/proxy/: foo (200; 7.632247ms)
Mar 26 20:10:44.110: INFO: (10) /api/v1/namespaces/proxy-4794/services/https:proxy-service-m568g:tlsportname1/proxy/: tls baz (200; 8.012374ms)
Mar 26 20:10:44.111: INFO: (10) /api/v1/namespaces/proxy-4794/services/proxy-service-m568g:portname1/proxy/: foo (200; 8.160789ms)
Mar 26 20:10:44.113: INFO: (11) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:162/proxy/: bar (200; 2.202473ms)
Mar 26 20:10:44.113: INFO: (11) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:160/proxy/: foo (200; 2.452805ms)
Mar 26 20:10:44.113: INFO: (11) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:443/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:443/proxy/tlsrewritem... (200; 2.74007ms)
Mar 26 20:10:44.114: INFO: (11) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:162/proxy/: bar (200; 3.717622ms)
Mar 26 20:10:44.115: INFO: (11) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6/proxy/rewriteme">test</a> (200; 4.006939ms)
Mar 26 20:10:44.115: INFO: (11) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:160/proxy/: foo (200; 4.053043ms)
Mar 26 20:10:44.115: INFO: (11) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:460/proxy/: tls baz (200; 4.026225ms)
Mar 26 20:10:44.115: INFO: (11) /api/v1/namespaces/proxy-4794/services/http:proxy-service-m568g:portname2/proxy/: bar (200; 4.255433ms)
Mar 26 20:10:44.115: INFO: (11) /api/v1/namespaces/proxy-4794/services/http:proxy-service-m568g:portname1/proxy/: foo (200; 4.336581ms)
Mar 26 20:10:44.115: INFO: (11) /api/v1/namespaces/proxy-4794/services/proxy-service-m568g:portname2/proxy/: bar (200; 4.29179ms)
Mar 26 20:10:44.115: INFO: (11) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:1080/proxy/rewriteme">... (200; 4.283324ms)
Mar 26 20:10:44.115: INFO: (11) /api/v1/namespaces/proxy-4794/services/proxy-service-m568g:portname1/proxy/: foo (200; 4.337888ms)
Mar 26 20:10:44.115: INFO: (11) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:1080/proxy/rewriteme">test<... (200; 4.331203ms)
Mar 26 20:10:44.115: INFO: (11) /api/v1/namespaces/proxy-4794/services/https:proxy-service-m568g:tlsportname1/proxy/: tls baz (200; 4.45436ms)
Mar 26 20:10:44.115: INFO: (11) /api/v1/namespaces/proxy-4794/services/https:proxy-service-m568g:tlsportname2/proxy/: tls qux (200; 4.468686ms)
Mar 26 20:10:44.115: INFO: (11) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:462/proxy/: tls qux (200; 4.464511ms)
Mar 26 20:10:44.118: INFO: (12) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:160/proxy/: foo (200; 2.896351ms)
Mar 26 20:10:44.118: INFO: (12) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:1080/proxy/rewriteme">test<... (200; 2.924552ms)
Mar 26 20:10:44.118: INFO: (12) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:1080/proxy/rewriteme">... (200; 2.993465ms)
Mar 26 20:10:44.118: INFO: (12) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:460/proxy/: tls baz (200; 2.957327ms)
Mar 26 20:10:44.118: INFO: (12) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:160/proxy/: foo (200; 2.960935ms)
Mar 26 20:10:44.118: INFO: (12) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:162/proxy/: bar (200; 2.984866ms)
Mar 26 20:10:44.118: INFO: (12) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:462/proxy/: tls qux (200; 3.017872ms)
Mar 26 20:10:44.119: INFO: (12) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:162/proxy/: bar (200; 3.316901ms)
Mar 26 20:10:44.119: INFO: (12) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6/proxy/rewriteme">test</a> (200; 3.306552ms)
Mar 26 20:10:44.119: INFO: (12) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:443/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:443/proxy/tlsrewritem... (200; 3.366578ms)
Mar 26 20:10:44.119: INFO: (12) /api/v1/namespaces/proxy-4794/services/http:proxy-service-m568g:portname2/proxy/: bar (200; 3.629021ms)
Mar 26 20:10:44.119: INFO: (12) /api/v1/namespaces/proxy-4794/services/http:proxy-service-m568g:portname1/proxy/: foo (200; 3.758703ms)
Mar 26 20:10:44.119: INFO: (12) /api/v1/namespaces/proxy-4794/services/https:proxy-service-m568g:tlsportname1/proxy/: tls baz (200; 3.777622ms)
Mar 26 20:10:44.119: INFO: (12) /api/v1/namespaces/proxy-4794/services/https:proxy-service-m568g:tlsportname2/proxy/: tls qux (200; 3.777485ms)
Mar 26 20:10:44.120: INFO: (12) /api/v1/namespaces/proxy-4794/services/proxy-service-m568g:portname2/proxy/: bar (200; 4.23437ms)
Mar 26 20:10:44.120: INFO: (12) /api/v1/namespaces/proxy-4794/services/proxy-service-m568g:portname1/proxy/: foo (200; 4.412818ms)
Mar 26 20:10:44.122: INFO: (13) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:460/proxy/: tls baz (200; 2.39913ms)
Mar 26 20:10:44.123: INFO: (13) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:1080/proxy/rewriteme">test<... (200; 2.889311ms)
Mar 26 20:10:44.123: INFO: (13) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6/proxy/rewriteme">test</a> (200; 3.666102ms)
Mar 26 20:10:44.124: INFO: (13) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:443/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:443/proxy/tlsrewritem... (200; 3.715906ms)
Mar 26 20:10:44.124: INFO: (13) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:160/proxy/: foo (200; 3.710432ms)
Mar 26 20:10:44.124: INFO: (13) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:1080/proxy/rewriteme">... (200; 3.771334ms)
Mar 26 20:10:44.124: INFO: (13) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:160/proxy/: foo (200; 3.778204ms)
Mar 26 20:10:44.124: INFO: (13) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:162/proxy/: bar (200; 3.823849ms)
Mar 26 20:10:44.124: INFO: (13) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:162/proxy/: bar (200; 4.386815ms)
Mar 26 20:10:44.124: INFO: (13) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:462/proxy/: tls qux (200; 4.456275ms)
Mar 26 20:10:44.125: INFO: (13) /api/v1/namespaces/proxy-4794/services/http:proxy-service-m568g:portname2/proxy/: bar (200; 5.452247ms)
Mar 26 20:10:44.125: INFO: (13) /api/v1/namespaces/proxy-4794/services/proxy-service-m568g:portname2/proxy/: bar (200; 5.511595ms)
Mar 26 20:10:44.125: INFO: (13) /api/v1/namespaces/proxy-4794/services/proxy-service-m568g:portname1/proxy/: foo (200; 5.552273ms)
Mar 26 20:10:44.125: INFO: (13) /api/v1/namespaces/proxy-4794/services/https:proxy-service-m568g:tlsportname1/proxy/: tls baz (200; 5.685087ms)
Mar 26 20:10:44.126: INFO: (13) /api/v1/namespaces/proxy-4794/services/http:proxy-service-m568g:portname1/proxy/: foo (200; 5.822397ms)
Mar 26 20:10:44.126: INFO: (13) /api/v1/namespaces/proxy-4794/services/https:proxy-service-m568g:tlsportname2/proxy/: tls qux (200; 5.809319ms)
Mar 26 20:10:44.128: INFO: (14) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:162/proxy/: bar (200; 2.22632ms)
Mar 26 20:10:44.128: INFO: (14) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:460/proxy/: tls baz (200; 2.451519ms)
Mar 26 20:10:44.128: INFO: (14) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:162/proxy/: bar (200; 2.384383ms)
Mar 26 20:10:44.130: INFO: (14) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:160/proxy/: foo (200; 4.075386ms)
Mar 26 20:10:44.130: INFO: (14) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:1080/proxy/rewriteme">... (200; 4.118708ms)
Mar 26 20:10:44.130: INFO: (14) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:1080/proxy/rewriteme">test<... (200; 4.165167ms)
Mar 26 20:10:44.130: INFO: (14) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:462/proxy/: tls qux (200; 4.263861ms)
Mar 26 20:10:44.130: INFO: (14) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:443/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:443/proxy/tlsrewritem... (200; 4.293633ms)
Mar 26 20:10:44.130: INFO: (14) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6/proxy/rewriteme">test</a> (200; 4.383188ms)
Mar 26 20:10:44.130: INFO: (14) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:160/proxy/: foo (200; 4.418616ms)
Mar 26 20:10:44.131: INFO: (14) /api/v1/namespaces/proxy-4794/services/http:proxy-service-m568g:portname1/proxy/: foo (200; 5.414158ms)
Mar 26 20:10:44.131: INFO: (14) /api/v1/namespaces/proxy-4794/services/https:proxy-service-m568g:tlsportname2/proxy/: tls qux (200; 5.548452ms)
Mar 26 20:10:44.131: INFO: (14) /api/v1/namespaces/proxy-4794/services/proxy-service-m568g:portname2/proxy/: bar (200; 5.78822ms)
Mar 26 20:10:44.132: INFO: (14) /api/v1/namespaces/proxy-4794/services/proxy-service-m568g:portname1/proxy/: foo (200; 5.814114ms)
Mar 26 20:10:44.132: INFO: (14) /api/v1/namespaces/proxy-4794/services/https:proxy-service-m568g:tlsportname1/proxy/: tls baz (200; 5.837176ms)
Mar 26 20:10:44.132: INFO: (14) /api/v1/namespaces/proxy-4794/services/http:proxy-service-m568g:portname2/proxy/: bar (200; 5.857197ms)
Mar 26 20:10:44.135: INFO: (15) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:162/proxy/: bar (200; 3.103504ms)
Mar 26 20:10:44.135: INFO: (15) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:460/proxy/: tls baz (200; 3.228928ms)
Mar 26 20:10:44.135: INFO: (15) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6/proxy/rewriteme">test</a> (200; 3.20409ms)
Mar 26 20:10:44.135: INFO: (15) /api/v1/namespaces/proxy-4794/services/http:proxy-service-m568g:portname1/proxy/: foo (200; 3.221196ms)
Mar 26 20:10:44.137: INFO: (15) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:443/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:443/proxy/tlsrewritem... (200; 5.678309ms)
Mar 26 20:10:44.137: INFO: (15) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:160/proxy/: foo (200; 5.727772ms)
Mar 26 20:10:44.137: INFO: (15) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:462/proxy/: tls qux (200; 5.69427ms)
Mar 26 20:10:44.137: INFO: (15) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:160/proxy/: foo (200; 5.732223ms)
Mar 26 20:10:44.138: INFO: (15) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:162/proxy/: bar (200; 6.773969ms)
Mar 26 20:10:44.138: INFO: (15) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:1080/proxy/rewriteme">test<... (200; 6.875543ms)
Mar 26 20:10:44.139: INFO: (15) /api/v1/namespaces/proxy-4794/services/http:proxy-service-m568g:portname2/proxy/: bar (200; 7.130837ms)
Mar 26 20:10:44.139: INFO: (15) /api/v1/namespaces/proxy-4794/services/https:proxy-service-m568g:tlsportname1/proxy/: tls baz (200; 7.520915ms)
Mar 26 20:10:44.140: INFO: (15) /api/v1/namespaces/proxy-4794/services/https:proxy-service-m568g:tlsportname2/proxy/: tls qux (200; 8.150844ms)
Mar 26 20:10:44.140: INFO: (15) /api/v1/namespaces/proxy-4794/services/proxy-service-m568g:portname2/proxy/: bar (200; 8.164655ms)
Mar 26 20:10:44.140: INFO: (15) /api/v1/namespaces/proxy-4794/services/proxy-service-m568g:portname1/proxy/: foo (200; 8.846021ms)
Mar 26 20:10:44.150: INFO: (15) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:1080/proxy/rewriteme">... (200; 18.834991ms)
Mar 26 20:10:44.161: INFO: (16) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:460/proxy/: tls baz (200; 10.303773ms)
Mar 26 20:10:44.161: INFO: (16) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:162/proxy/: bar (200; 10.177596ms)
Mar 26 20:10:44.161: INFO: (16) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:1080/proxy/rewriteme">... (200; 10.235496ms)
Mar 26 20:10:44.161: INFO: (16) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:160/proxy/: foo (200; 10.239777ms)
Mar 26 20:10:44.161: INFO: (16) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:162/proxy/: bar (200; 10.356367ms)
Mar 26 20:10:44.161: INFO: (16) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:443/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:443/proxy/tlsrewritem... (200; 10.529475ms)
Mar 26 20:10:44.161: INFO: (16) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:160/proxy/: foo (200; 10.453744ms)
Mar 26 20:10:44.161: INFO: (16) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:462/proxy/: tls qux (200; 10.384471ms)
Mar 26 20:10:44.161: INFO: (16) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:1080/proxy/rewriteme">test<... (200; 10.507405ms)
Mar 26 20:10:44.161: INFO: (16) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6/proxy/rewriteme">test</a> (200; 10.451417ms)
Mar 26 20:10:44.161: INFO: (16) /api/v1/namespaces/proxy-4794/services/https:proxy-service-m568g:tlsportname2/proxy/: tls qux (200; 10.395943ms)
Mar 26 20:10:44.161: INFO: (16) /api/v1/namespaces/proxy-4794/services/http:proxy-service-m568g:portname1/proxy/: foo (200; 10.478681ms)
Mar 26 20:10:44.161: INFO: (16) /api/v1/namespaces/proxy-4794/services/http:proxy-service-m568g:portname2/proxy/: bar (200; 10.389217ms)
Mar 26 20:10:44.161: INFO: (16) /api/v1/namespaces/proxy-4794/services/proxy-service-m568g:portname1/proxy/: foo (200; 10.519059ms)
Mar 26 20:10:44.161: INFO: (16) /api/v1/namespaces/proxy-4794/services/https:proxy-service-m568g:tlsportname1/proxy/: tls baz (200; 10.38008ms)
Mar 26 20:10:44.161: INFO: (16) /api/v1/namespaces/proxy-4794/services/proxy-service-m568g:portname2/proxy/: bar (200; 10.710038ms)
Mar 26 20:10:44.164: INFO: (17) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:162/proxy/: bar (200; 2.370417ms)
Mar 26 20:10:44.165: INFO: (17) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:460/proxy/: tls baz (200; 3.16057ms)
Mar 26 20:10:44.165: INFO: (17) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:162/proxy/: bar (200; 3.69927ms)
Mar 26 20:10:44.165: INFO: (17) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:1080/proxy/rewriteme">test<... (200; 3.815542ms)
Mar 26 20:10:44.165: INFO: (17) /api/v1/namespaces/proxy-4794/services/https:proxy-service-m568g:tlsportname2/proxy/: tls qux (200; 3.797674ms)
Mar 26 20:10:44.165: INFO: (17) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:1080/proxy/rewriteme">... (200; 3.810011ms)
Mar 26 20:10:44.165: INFO: (17) /api/v1/namespaces/proxy-4794/services/http:proxy-service-m568g:portname2/proxy/: bar (200; 3.983645ms)
Mar 26 20:10:44.165: INFO: (17) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6/proxy/rewriteme">test</a> (200; 3.897113ms)
Mar 26 20:10:44.165: INFO: (17) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:443/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:443/proxy/tlsrewritem... (200; 3.95701ms)
Mar 26 20:10:44.166: INFO: (17) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:462/proxy/: tls qux (200; 4.110513ms)
Mar 26 20:10:44.166: INFO: (17) /api/v1/namespaces/proxy-4794/services/http:proxy-service-m568g:portname1/proxy/: foo (200; 4.32133ms)
Mar 26 20:10:44.166: INFO: (17) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:160/proxy/: foo (200; 4.210996ms)
Mar 26 20:10:44.166: INFO: (17) /api/v1/namespaces/proxy-4794/services/proxy-service-m568g:portname1/proxy/: foo (200; 4.336183ms)
Mar 26 20:10:44.166: INFO: (17) /api/v1/namespaces/proxy-4794/services/proxy-service-m568g:portname2/proxy/: bar (200; 4.330708ms)
Mar 26 20:10:44.166: INFO: (17) /api/v1/namespaces/proxy-4794/services/https:proxy-service-m568g:tlsportname1/proxy/: tls baz (200; 4.318855ms)
Mar 26 20:10:44.166: INFO: (17) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:160/proxy/: foo (200; 4.558971ms)
Mar 26 20:10:44.170: INFO: (18) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:1080/proxy/rewriteme">test<... (200; 3.507456ms)
Mar 26 20:10:44.170: INFO: (18) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:1080/proxy/rewriteme">... (200; 3.583284ms)
Mar 26 20:10:44.170: INFO: (18) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:443/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:443/proxy/tlsrewritem... (200; 3.579181ms)
Mar 26 20:10:44.170: INFO: (18) /api/v1/namespaces/proxy-4794/services/http:proxy-service-m568g:portname1/proxy/: foo (200; 3.631836ms)
Mar 26 20:10:44.170: INFO: (18) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6/proxy/rewriteme">test</a> (200; 3.569767ms)
Mar 26 20:10:44.170: INFO: (18) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:462/proxy/: tls qux (200; 3.651915ms)
Mar 26 20:10:44.170: INFO: (18) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:162/proxy/: bar (200; 3.573655ms)
Mar 26 20:10:44.170: INFO: (18) /api/v1/namespaces/proxy-4794/services/https:proxy-service-m568g:tlsportname2/proxy/: tls qux (200; 3.651917ms)
Mar 26 20:10:44.170: INFO: (18) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:160/proxy/: foo (200; 3.611265ms)
Mar 26 20:10:44.170: INFO: (18) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:460/proxy/: tls baz (200; 3.680421ms)
Mar 26 20:10:44.170: INFO: (18) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:162/proxy/: bar (200; 3.62176ms)
Mar 26 20:10:44.170: INFO: (18) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:160/proxy/: foo (200; 3.658725ms)
Mar 26 20:10:44.170: INFO: (18) /api/v1/namespaces/proxy-4794/services/https:proxy-service-m568g:tlsportname1/proxy/: tls baz (200; 3.785481ms)
Mar 26 20:10:44.170: INFO: (18) /api/v1/namespaces/proxy-4794/services/http:proxy-service-m568g:portname2/proxy/: bar (200; 3.949925ms)
Mar 26 20:10:44.170: INFO: (18) /api/v1/namespaces/proxy-4794/services/proxy-service-m568g:portname2/proxy/: bar (200; 3.969229ms)
Mar 26 20:10:44.170: INFO: (18) /api/v1/namespaces/proxy-4794/services/proxy-service-m568g:portname1/proxy/: foo (200; 4.003187ms)
Mar 26 20:10:44.173: INFO: (19) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6/proxy/rewriteme">test</a> (200; 3.098703ms)
Mar 26 20:10:44.175: INFO: (19) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:1080/proxy/rewriteme">... (200; 4.222167ms)
Mar 26 20:10:44.175: INFO: (19) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:162/proxy/: bar (200; 4.809442ms)
Mar 26 20:10:44.175: INFO: (19) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:460/proxy/: tls baz (200; 4.912991ms)
Mar 26 20:10:44.176: INFO: (19) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:160/proxy/: foo (200; 5.164166ms)
Mar 26 20:10:44.176: INFO: (19) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:1080/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:1080/proxy/rewriteme">test<... (200; 5.186001ms)
Mar 26 20:10:44.176: INFO: (19) /api/v1/namespaces/proxy-4794/pods/proxy-service-m568g-p9js6:160/proxy/: foo (200; 5.359271ms)
Mar 26 20:10:44.176: INFO: (19) /api/v1/namespaces/proxy-4794/pods/http:proxy-service-m568g-p9js6:162/proxy/: bar (200; 5.328711ms)
Mar 26 20:10:44.176: INFO: (19) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:443/proxy/: <a href="/api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:443/proxy/tlsrewritem... (200; 5.4134ms)
Mar 26 20:10:44.176: INFO: (19) /api/v1/namespaces/proxy-4794/pods/https:proxy-service-m568g-p9js6:462/proxy/: tls qux (200; 5.411034ms)
Mar 26 20:10:44.176: INFO: (19) /api/v1/namespaces/proxy-4794/services/https:proxy-service-m568g:tlsportname2/proxy/: tls qux (200; 6.113553ms)
Mar 26 20:10:44.176: INFO: (19) /api/v1/namespaces/proxy-4794/services/https:proxy-service-m568g:tlsportname1/proxy/: tls baz (200; 6.155447ms)
Mar 26 20:10:44.176: INFO: (19) /api/v1/namespaces/proxy-4794/services/proxy-service-m568g:portname1/proxy/: foo (200; 6.163538ms)
Mar 26 20:10:44.177: INFO: (19) /api/v1/namespaces/proxy-4794/services/http:proxy-service-m568g:portname1/proxy/: foo (200; 6.27201ms)
Mar 26 20:10:44.177: INFO: (19) /api/v1/namespaces/proxy-4794/services/http:proxy-service-m568g:portname2/proxy/: bar (200; 6.221378ms)
Mar 26 20:10:44.177: INFO: (19) /api/v1/namespaces/proxy-4794/services/proxy-service-m568g:portname2/proxy/: bar (200; 6.273013ms)
[1mSTEP[0m: deleting ReplicationController proxy-service-m568g in namespace proxy-4794, will wait for the garbage collector to delete the pods
Mar 26 20:10:44.232: INFO: Deleting ReplicationController proxy-service-m568g took: 4.12461ms
Mar 26 20:10:44.533: INFO: Terminating ReplicationController proxy-service-m568g pods took: 300.264348ms
[AfterEach] version v1
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:10:46.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "proxy-4794" for this suite.
Mar 26 20:10:52.347: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:10:52.411: INFO: namespace proxy-4794 deletion completed in 6.073725709s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] DNS[0m 
  [1mshould provide DNS for the cluster  [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-network] DNS
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:10:52.411: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename dns
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9572.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

[1mSTEP[0m: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9572.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

[1mSTEP[0m: creating a pod to probe DNS
[1mSTEP[0m: submitting the pod to kubernetes
[1mSTEP[0m: retrieving the pod
[1mSTEP[0m: looking for the results for each expected name from probers
Mar 26 20:10:56.459: INFO: Unable to read wheezy_udp@kubernetes.default.svc.cluster.local from pod dns-9572/dns-test-edccdcc7-503d-11e9-9719-a08cfdecc127: the server could not find the requested resource (get pods dns-test-edccdcc7-503d-11e9-9719-a08cfdecc127)
Mar 26 20:10:56.462: INFO: Unable to read wheezy_tcp@kubernetes.default.svc.cluster.local from pod dns-9572/dns-test-edccdcc7-503d-11e9-9719-a08cfdecc127: the server could not find the requested resource (get pods dns-test-edccdcc7-503d-11e9-9719-a08cfdecc127)
Mar 26 20:10:56.465: INFO: Unable to read wheezy_udp@PodARecord from pod dns-9572/dns-test-edccdcc7-503d-11e9-9719-a08cfdecc127: the server could not find the requested resource (get pods dns-test-edccdcc7-503d-11e9-9719-a08cfdecc127)
Mar 26 20:10:56.468: INFO: Unable to read wheezy_tcp@PodARecord from pod dns-9572/dns-test-edccdcc7-503d-11e9-9719-a08cfdecc127: the server could not find the requested resource (get pods dns-test-edccdcc7-503d-11e9-9719-a08cfdecc127)
Mar 26 20:10:56.470: INFO: Unable to read jessie_udp@kubernetes.default.svc.cluster.local from pod dns-9572/dns-test-edccdcc7-503d-11e9-9719-a08cfdecc127: the server could not find the requested resource (get pods dns-test-edccdcc7-503d-11e9-9719-a08cfdecc127)
Mar 26 20:10:56.473: INFO: Unable to read jessie_tcp@kubernetes.default.svc.cluster.local from pod dns-9572/dns-test-edccdcc7-503d-11e9-9719-a08cfdecc127: the server could not find the requested resource (get pods dns-test-edccdcc7-503d-11e9-9719-a08cfdecc127)
Mar 26 20:10:56.475: INFO: Unable to read jessie_udp@PodARecord from pod dns-9572/dns-test-edccdcc7-503d-11e9-9719-a08cfdecc127: the server could not find the requested resource (get pods dns-test-edccdcc7-503d-11e9-9719-a08cfdecc127)
Mar 26 20:10:56.477: INFO: Unable to read jessie_tcp@PodARecord from pod dns-9572/dns-test-edccdcc7-503d-11e9-9719-a08cfdecc127: the server could not find the requested resource (get pods dns-test-edccdcc7-503d-11e9-9719-a08cfdecc127)
Mar 26 20:10:56.477: INFO: Lookups using dns-9572/dns-test-edccdcc7-503d-11e9-9719-a08cfdecc127 failed for: [wheezy_udp@kubernetes.default.svc.cluster.local wheezy_tcp@kubernetes.default.svc.cluster.local wheezy_udp@PodARecord wheezy_tcp@PodARecord jessie_udp@kubernetes.default.svc.cluster.local jessie_tcp@kubernetes.default.svc.cluster.local jessie_udp@PodARecord jessie_tcp@PodARecord]

Mar 26 20:11:01.500: INFO: DNS probes using dns-9572/dns-test-edccdcc7-503d-11e9-9719-a08cfdecc127 succeeded

[1mSTEP[0m: deleting the pod
[AfterEach] [sig-network] DNS
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:11:01.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "dns-9572" for this suite.
Mar 26 20:11:07.535: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:11:07.602: INFO: namespace dns-9572 deletion completed in 6.07484168s
[32mâ€¢[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90m[k8s.io] Kubectl rolling-update[0m 
  [1mshould support rolling-update to same image  [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:11:07.602: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl rolling-update
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1414
[It] should support rolling-update to same image  [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: running the image docker.io/library/nginx:1.14-alpine
Mar 26 20:11:07.625: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-1843'
Mar 26 20:11:07.694: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar 26 20:11:07.694: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
[1mSTEP[0m: verifying the rc e2e-test-nginx-rc was created
[1mSTEP[0m: rolling-update to same image controller
Mar 26 20:11:07.752: INFO: scanned /usr/local/google/home/bentheelder for discovery docs: <nil>
Mar 26 20:11:07.752: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-1843'
Mar 26 20:11:23.507: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Mar 26 20:11:23.507: INFO: stdout: "Created e2e-test-nginx-rc-0e8e7f1b69f30eda1b6b91c468d2a297\nScaling up e2e-test-nginx-rc-0e8e7f1b69f30eda1b6b91c468d2a297 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-0e8e7f1b69f30eda1b6b91c468d2a297 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-0e8e7f1b69f30eda1b6b91c468d2a297 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Mar 26 20:11:23.507: INFO: stdout: "Created e2e-test-nginx-rc-0e8e7f1b69f30eda1b6b91c468d2a297\nScaling up e2e-test-nginx-rc-0e8e7f1b69f30eda1b6b91c468d2a297 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-0e8e7f1b69f30eda1b6b91c468d2a297 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-0e8e7f1b69f30eda1b6b91c468d2a297 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
[1mSTEP[0m: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Mar 26 20:11:23.507: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-1843'
Mar 26 20:11:23.582: INFO: stderr: ""
Mar 26 20:11:23.582: INFO: stdout: "e2e-test-nginx-rc-0e8e7f1b69f30eda1b6b91c468d2a297-j89x2 e2e-test-nginx-rc-znmgk "
[1mSTEP[0m: Replicas for run=e2e-test-nginx-rc: expected=1 actual=2
Mar 26 20:11:28.582: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-1843'
Mar 26 20:11:28.653: INFO: stderr: ""
Mar 26 20:11:28.653: INFO: stdout: "e2e-test-nginx-rc-0e8e7f1b69f30eda1b6b91c468d2a297-j89x2 "
Mar 26 20:11:28.653: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get pods e2e-test-nginx-rc-0e8e7f1b69f30eda1b6b91c468d2a297-j89x2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1843'
Mar 26 20:11:28.718: INFO: stderr: ""
Mar 26 20:11:28.718: INFO: stdout: "true"
Mar 26 20:11:28.718: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance get pods e2e-test-nginx-rc-0e8e7f1b69f30eda1b6b91c468d2a297-j89x2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1843'
Mar 26 20:11:28.787: INFO: stderr: ""
Mar 26 20:11:28.787: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Mar 26 20:11:28.787: INFO: e2e-test-nginx-rc-0e8e7f1b69f30eda1b6b91c468d2a297-j89x2 is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1420
Mar 26 20:11:28.787: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance delete rc e2e-test-nginx-rc --namespace=kubectl-1843'
Mar 26 20:11:28.865: INFO: stderr: ""
Mar 26 20:11:28.865: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:11:28.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-1843" for this suite.
Mar 26 20:11:50.880: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:11:50.944: INFO: namespace kubectl-1843 deletion completed in 22.076686738s
[32mâ€¢[0m
[90m------------------------------[0m
[0m[sig-apps] StatefulSet[0m [90m[k8s.io] Basic StatefulSet functionality [StatefulSetBasic][0m 
  [1mshould perform canary updates and phased rolling updates of template modifications [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-apps] StatefulSet
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:11:50.944: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename statefulset
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
[1mSTEP[0m: Creating service test in namespace statefulset-3655
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating a new StatefulSet
Mar 26 20:11:50.979: INFO: Found 0 stateful pods, waiting for 3
Mar 26 20:12:00.983: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 26 20:12:00.983: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 26 20:12:00.983: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
[1mSTEP[0m: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Mar 26 20:12:01.010: INFO: Updating stateful set ss2
[1mSTEP[0m: Creating a new revision
[1mSTEP[0m: Not applying an update when the partition is greater than the number of replicas
[1mSTEP[0m: Performing a canary update
Mar 26 20:12:11.040: INFO: Updating stateful set ss2
Mar 26 20:12:11.050: INFO: Waiting for Pod statefulset-3655/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
[1mSTEP[0m: Restoring Pods to the correct revision when they are deleted
Mar 26 20:12:21.097: INFO: Found 2 stateful pods, waiting for 3
Mar 26 20:12:31.101: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 26 20:12:31.101: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 26 20:12:31.101: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
[1mSTEP[0m: Performing a phased rolling update
Mar 26 20:12:31.123: INFO: Updating stateful set ss2
Mar 26 20:12:31.142: INFO: Waiting for Pod statefulset-3655/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Mar 26 20:12:41.167: INFO: Updating stateful set ss2
Mar 26 20:12:41.175: INFO: Waiting for StatefulSet statefulset-3655/ss2 to complete update
Mar 26 20:12:41.175: INFO: Waiting for Pod statefulset-3655/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Mar 26 20:12:51.182: INFO: Deleting all statefulset in ns statefulset-3655
Mar 26 20:12:51.185: INFO: Scaling statefulset ss2 to 0
Mar 26 20:13:01.198: INFO: Waiting for statefulset status.replicas updated to 0
Mar 26 20:13:01.202: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:13:01.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "statefulset-3655" for this suite.
Mar 26 20:13:07.223: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:13:07.288: INFO: namespace statefulset-3655 deletion completed in 6.072867857s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90m[k8s.io] Kubectl describe[0m 
  [1mshould check if kubectl describe prints relevant information for rc and pods  [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:13:07.288: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Mar 26 20:13:07.325: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance version --client'
Mar 26 20:13:07.381: INFO: stderr: ""
Mar 26 20:13:07.381: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.0\", GitCommit:\"641856db18352033a0d96dbc99153fa3b27298e5\", GitTreeState:\"clean\", BuildDate:\"2019-03-26T00:21:23Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Mar 26 20:13:07.382: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance create -f - --namespace=kubectl-8020'
Mar 26 20:13:07.520: INFO: stderr: ""
Mar 26 20:13:07.520: INFO: stdout: "replicationcontroller/redis-master created\n"
Mar 26 20:13:07.520: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance create -f - --namespace=kubectl-8020'
Mar 26 20:13:07.683: INFO: stderr: ""
Mar 26 20:13:07.683: INFO: stdout: "service/redis-master created\n"
[1mSTEP[0m: Waiting for Redis master to start.
Mar 26 20:13:08.692: INFO: Selector matched 1 pods for map[app:redis]
Mar 26 20:13:08.692: INFO: Found 0 / 1
Mar 26 20:13:09.687: INFO: Selector matched 1 pods for map[app:redis]
Mar 26 20:13:09.687: INFO: Found 0 / 1
Mar 26 20:13:10.687: INFO: Selector matched 1 pods for map[app:redis]
Mar 26 20:13:10.687: INFO: Found 1 / 1
Mar 26 20:13:10.687: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 26 20:13:10.691: INFO: Selector matched 1 pods for map[app:redis]
Mar 26 20:13:10.691: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 26 20:13:10.691: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance describe pod redis-master-q5wrk --namespace=kubectl-8020'
Mar 26 20:13:10.771: INFO: stderr: ""
Mar 26 20:13:10.771: INFO: stdout: "Name:               redis-master-q5wrk\nNamespace:          kubectl-8020\nPriority:           0\nPriorityClassName:  <none>\nNode:               conformance-worker/192.168.9.3\nStart Time:         Tue, 26 Mar 2019 20:13:07 -0700\nLabels:             app=redis\n                    role=master\nAnnotations:        <none>\nStatus:             Running\nIP:                 10.32.0.2\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://e641aa7e48e0ab29e592e2f126f316da3bcded03d452bba75d468ce77978cd47\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 26 Mar 2019 20:13:08 -0700\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-jc4c7 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-jc4c7:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-jc4c7\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type     Reason            Age              From                         Message\n  ----     ------            ----             ----                         -------\n  Normal   Scheduled         3s               default-scheduler            Successfully assigned kubectl-8020/redis-master-q5wrk to conformance-worker\n  Normal   Pulled            2s               kubelet, conformance-worker  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal   Created           2s               kubelet, conformance-worker  Created container redis-master\n  Normal   Started           2s               kubelet, conformance-worker  Started container redis-master\n  Warning  DNSConfigForming  1s (x3 over 3s)  kubelet, conformance-worker  Search Line limits were exceeded, some search paths have been omitted, the applied search line is: kubectl-8020.svc.cluster.local svc.cluster.local cluster.local corp.google.com prod.google.com prodz.google.com\n"
Mar 26 20:13:10.771: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance describe rc redis-master --namespace=kubectl-8020'
Mar 26 20:13:10.863: INFO: stderr: ""
Mar 26 20:13:10.863: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-8020\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: redis-master-q5wrk\n"
Mar 26 20:13:10.863: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance describe service redis-master --namespace=kubectl-8020'
Mar 26 20:13:10.933: INFO: stderr: ""
Mar 26 20:13:10.933: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-8020\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.103.179.185\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.32.0.2:6379\nSession Affinity:  None\nEvents:            <none>\n"
Mar 26 20:13:10.936: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance describe node conformance-control-plane'
Mar 26 20:13:11.021: INFO: stderr: ""
Mar 26 20:13:11.021: INFO: stdout: "Name:               conformance-control-plane\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=conformance-control-plane\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 26 Mar 2019 18:50:52 -0700\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Tue, 26 Mar 2019 18:51:39 -0700   Tue, 26 Mar 2019 18:51:39 -0700   WeaveIsUp                    Weave pod has set this\n  MemoryPressure       False   Tue, 26 Mar 2019 20:12:45 -0700   Tue, 26 Mar 2019 18:50:49 -0700   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Tue, 26 Mar 2019 20:12:45 -0700   Tue, 26 Mar 2019 18:50:49 -0700   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Tue, 26 Mar 2019 20:12:45 -0700   Tue, 26 Mar 2019 18:50:49 -0700   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Tue, 26 Mar 2019 20:12:45 -0700   Tue, 26 Mar 2019 18:51:42 -0700   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  192.168.9.4\n  Hostname:    conformance-control-plane\nCapacity:\n cpu:                12\n ephemeral-storage:  367702144Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             65951824Ki\n pods:               110\nAllocatable:\n cpu:                12\n ephemeral-storage:  367702144Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             65951824Ki\n pods:               110\nSystem Info:\n Machine ID:                 01869843d28e4065bff36449b25d562a\n System UUID:                9f029e76-4671-11e7-9c43-bc0000fe0000\n Boot ID:                    bfd7ec8d-380a-4c0e-b698-1b2e431d9aa6\n Kernel Version:             4.19.20-1rodete1-amd64\n OS Image:                   Ubuntu 18.04.1 LTS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.6.3\n Kubelet Version:            v1.14.0\n Kube-Proxy Version:         v1.14.0\nNon-terminated Pods:         (8 in total)\n  Namespace                  Name                                                 CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                 ------------  ----------  ---------------  -------------  ---\n  kube-system                coredns-fb8b8dccf-5bwzj                              100m (0%)     0 (0%)      70Mi (0%)        170Mi (0%)     82m\n  kube-system                coredns-fb8b8dccf-wvb9q                              100m (0%)     0 (0%)      70Mi (0%)        170Mi (0%)     82m\n  kube-system                etcd-conformance-control-plane                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         81m\n  kube-system                kube-apiserver-conformance-control-plane             250m (2%)     0 (0%)      0 (0%)           0 (0%)         80m\n  kube-system                kube-controller-manager-conformance-control-plane    200m (1%)     0 (0%)      0 (0%)           0 (0%)         80m\n  kube-system                kube-proxy-jm9hw                                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         82m\n  kube-system                kube-scheduler-conformance-control-plane             100m (0%)     0 (0%)      0 (0%)           0 (0%)         80m\n  kube-system                weave-net-m2p9k                                      20m (0%)      0 (0%)      0 (0%)           0 (0%)         82m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                770m (6%)   0 (0%)\n  memory             140Mi (0%)  340Mi (0%)\n  ephemeral-storage  0 (0%)      0 (0%)\nEvents:              <none>\n"
Mar 26 20:13:11.022: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance describe namespace kubectl-8020'
Mar 26 20:13:11.102: INFO: stderr: ""
Mar 26 20:13:11.102: INFO: stdout: "Name:         kubectl-8020\nLabels:       e2e-framework=kubectl\n              e2e-run=e0596475-5032-11e9-9719-a08cfdecc127\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:13:11.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-8020" for this suite.
Mar 26 20:13:33.113: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:13:33.179: INFO: namespace kubectl-8020 deletion completed in 22.074618128s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Networking[0m [90mGranular Checks: Pods[0m 
  [1mshould function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-network] Networking
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:13:33.179: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename pod-network-test
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Performing setup for networking test in namespace pod-network-test-3759
[1mSTEP[0m: creating a selector
[1mSTEP[0m: Creating the service pods in kubernetes
Mar 26 20:13:33.206: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[1mSTEP[0m: Creating test pods
Mar 26 20:13:57.261: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.32.0.2 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3759 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 20:13:57.261: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
Mar 26 20:13:58.474: INFO: Found all expected endpoints: [netserver-0]
Mar 26 20:13:58.477: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.46.0.1 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3759 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Mar 26 20:13:58.477: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
Mar 26 20:13:59.659: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:13:59.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "pod-network-test-3759" for this suite.
Mar 26 20:14:21.670: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:14:21.720: INFO: namespace pod-network-test-3759 deletion completed in 22.058459127s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Watchers[0m 
  [1mshould observe add, update, and delete watch notifications on configmaps [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-api-machinery] Watchers
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:14:21.720: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename watch
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: creating a watch on configmaps with label A
[1mSTEP[0m: creating a watch on configmaps with label B
[1mSTEP[0m: creating a watch on configmaps with label A or B
[1mSTEP[0m: creating a configmap with label A and ensuring the correct watchers observe the notification
Mar 26 20:14:21.749: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1654,SelfLink:/api/v1/namespaces/watch-1654/configmaps/e2e-watch-test-configmap-a,UID:6a8f5116-503e-11e9-b1ea-02429e4bb871,ResourceVersion:18975,Generation:0,CreationTimestamp:2019-03-26 20:14:21 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar 26 20:14:21.749: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1654,SelfLink:/api/v1/namespaces/watch-1654/configmaps/e2e-watch-test-configmap-a,UID:6a8f5116-503e-11e9-b1ea-02429e4bb871,ResourceVersion:18975,Generation:0,CreationTimestamp:2019-03-26 20:14:21 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[1mSTEP[0m: modifying configmap A and ensuring the correct watchers observe the notification
Mar 26 20:14:31.756: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1654,SelfLink:/api/v1/namespaces/watch-1654/configmaps/e2e-watch-test-configmap-a,UID:6a8f5116-503e-11e9-b1ea-02429e4bb871,ResourceVersion:18990,Generation:0,CreationTimestamp:2019-03-26 20:14:21 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Mar 26 20:14:31.756: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1654,SelfLink:/api/v1/namespaces/watch-1654/configmaps/e2e-watch-test-configmap-a,UID:6a8f5116-503e-11e9-b1ea-02429e4bb871,ResourceVersion:18990,Generation:0,CreationTimestamp:2019-03-26 20:14:21 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
[1mSTEP[0m: modifying configmap A again and ensuring the correct watchers observe the notification
Mar 26 20:14:41.762: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1654,SelfLink:/api/v1/namespaces/watch-1654/configmaps/e2e-watch-test-configmap-a,UID:6a8f5116-503e-11e9-b1ea-02429e4bb871,ResourceVersion:19005,Generation:0,CreationTimestamp:2019-03-26 20:14:21 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar 26 20:14:41.763: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1654,SelfLink:/api/v1/namespaces/watch-1654/configmaps/e2e-watch-test-configmap-a,UID:6a8f5116-503e-11e9-b1ea-02429e4bb871,ResourceVersion:19005,Generation:0,CreationTimestamp:2019-03-26 20:14:21 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[1mSTEP[0m: deleting configmap A and ensuring the correct watchers observe the notification
Mar 26 20:14:51.769: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1654,SelfLink:/api/v1/namespaces/watch-1654/configmaps/e2e-watch-test-configmap-a,UID:6a8f5116-503e-11e9-b1ea-02429e4bb871,ResourceVersion:19021,Generation:0,CreationTimestamp:2019-03-26 20:14:21 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar 26 20:14:51.769: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1654,SelfLink:/api/v1/namespaces/watch-1654/configmaps/e2e-watch-test-configmap-a,UID:6a8f5116-503e-11e9-b1ea-02429e4bb871,ResourceVersion:19021,Generation:0,CreationTimestamp:2019-03-26 20:14:21 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[1mSTEP[0m: creating a configmap with label B and ensuring the correct watchers observe the notification
Mar 26 20:15:01.775: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-1654,SelfLink:/api/v1/namespaces/watch-1654/configmaps/e2e-watch-test-configmap-b,UID:826a467b-503e-11e9-b1ea-02429e4bb871,ResourceVersion:19036,Generation:0,CreationTimestamp:2019-03-26 20:15:01 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar 26 20:15:01.775: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-1654,SelfLink:/api/v1/namespaces/watch-1654/configmaps/e2e-watch-test-configmap-b,UID:826a467b-503e-11e9-b1ea-02429e4bb871,ResourceVersion:19036,Generation:0,CreationTimestamp:2019-03-26 20:15:01 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[1mSTEP[0m: deleting configmap B and ensuring the correct watchers observe the notification
Mar 26 20:15:11.780: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-1654,SelfLink:/api/v1/namespaces/watch-1654/configmaps/e2e-watch-test-configmap-b,UID:826a467b-503e-11e9-b1ea-02429e4bb871,ResourceVersion:19053,Generation:0,CreationTimestamp:2019-03-26 20:15:01 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Mar 26 20:15:11.780: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-1654,SelfLink:/api/v1/namespaces/watch-1654/configmaps/e2e-watch-test-configmap-b,UID:826a467b-503e-11e9-b1ea-02429e4bb871,ResourceVersion:19053,Generation:0,CreationTimestamp:2019-03-26 20:15:01 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:15:21.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "watch-1654" for this suite.
Mar 26 20:15:27.798: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:15:27.865: INFO: namespace watch-1654 deletion completed in 6.081489794s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Downward API volume[0m 
  [1mshould update annotations on modification [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Downward API volume
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:15:27.866: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename downward-api
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating the pod
Mar 26 20:15:30.443: INFO: Successfully updated pod "annotationupdate91fd6f02-503e-11e9-9719-a08cfdecc127"
[AfterEach] [sig-storage] Downward API volume
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:15:32.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "downward-api-350" for this suite.
Mar 26 20:15:54.479: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:15:54.550: INFO: namespace downward-api-350 deletion completed in 22.085577186s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] Daemon set [Serial][0m 
  [1mshould retry creating failed daemon pods [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-apps] Daemon set [Serial]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:15:54.551: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename daemonsets
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating a simple DaemonSet "daemon-set"
[1mSTEP[0m: Check that daemon pods launch on every node of the cluster.
Mar 26 20:15:54.592: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 20:15:54.595: INFO: Number of nodes with available pods: 0
Mar 26 20:15:54.596: INFO: Node conformance-worker is running more than one daemon pod
Mar 26 20:15:55.599: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 20:15:55.602: INFO: Number of nodes with available pods: 0
Mar 26 20:15:55.602: INFO: Node conformance-worker is running more than one daemon pod
Mar 26 20:15:56.600: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 20:15:56.603: INFO: Number of nodes with available pods: 1
Mar 26 20:15:56.603: INFO: Node conformance-worker2 is running more than one daemon pod
Mar 26 20:15:57.600: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 20:15:57.603: INFO: Number of nodes with available pods: 2
Mar 26 20:15:57.603: INFO: Number of running nodes: 2, number of available pods: 2
[1mSTEP[0m: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Mar 26 20:15:57.619: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 20:15:57.622: INFO: Number of nodes with available pods: 1
Mar 26 20:15:57.622: INFO: Node conformance-worker is running more than one daemon pod
Mar 26 20:15:58.626: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 20:15:58.628: INFO: Number of nodes with available pods: 1
Mar 26 20:15:58.628: INFO: Node conformance-worker is running more than one daemon pod
Mar 26 20:15:59.627: INFO: DaemonSet pods can't tolerate node conformance-control-plane with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 26 20:15:59.630: INFO: Number of nodes with available pods: 2
Mar 26 20:15:59.630: INFO: Number of running nodes: 2, number of available pods: 2
[1mSTEP[0m: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
[1mSTEP[0m: Deleting DaemonSet "daemon-set"
[1mSTEP[0m: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8581, will wait for the garbage collector to delete the pods
Mar 26 20:15:59.694: INFO: Deleting DaemonSet.extensions daemon-set took: 5.220791ms
Mar 26 20:15:59.994: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.217895ms
Mar 26 20:16:05.596: INFO: Number of nodes with available pods: 0
Mar 26 20:16:05.596: INFO: Number of running nodes: 0, number of available pods: 0
Mar 26 20:16:05.598: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8581/daemonsets","resourceVersion":"19236"},"items":null}

Mar 26 20:16:05.600: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8581/pods","resourceVersion":"19236"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:16:05.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "daemonsets-8581" for this suite.
Mar 26 20:16:11.619: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:16:11.682: INFO: namespace daemonsets-8581 deletion completed in 6.070665142s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] InitContainer [NodeConformance][0m 
  [1mshould invoke init containers on a RestartAlways pod [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:16:11.682: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename init-container
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: creating the pod
Mar 26 20:16:11.708: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:16:24.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "init-container-3319" for this suite.
Mar 26 20:16:46.332: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:16:46.400: INFO: namespace init-container-3319 deletion completed in 22.076642296s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Container Lifecycle Hook[0m [90mwhen create a pod with lifecycle hook[0m 
  [1mshould execute prestop http hook properly [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:16:46.401: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename container-lifecycle-hook
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
[1mSTEP[0m: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: create the pod with lifecycle hook
[1mSTEP[0m: delete the pod with lifecycle hook
Mar 26 20:16:52.461: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 26 20:16:52.467: INFO: Pod pod-with-prestop-http-hook still exists
Mar 26 20:16:54.467: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 26 20:16:54.471: INFO: Pod pod-with-prestop-http-hook still exists
Mar 26 20:16:56.467: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 26 20:16:56.470: INFO: Pod pod-with-prestop-http-hook still exists
Mar 26 20:16:58.467: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 26 20:16:58.471: INFO: Pod pod-with-prestop-http-hook still exists
Mar 26 20:17:00.467: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 26 20:17:00.471: INFO: Pod pod-with-prestop-http-hook still exists
Mar 26 20:17:02.467: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 26 20:17:02.471: INFO: Pod pod-with-prestop-http-hook still exists
Mar 26 20:17:04.467: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 26 20:17:04.471: INFO: Pod pod-with-prestop-http-hook still exists
Mar 26 20:17:06.467: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 26 20:17:06.470: INFO: Pod pod-with-prestop-http-hook no longer exists
[1mSTEP[0m: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:17:06.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "container-lifecycle-hook-1346" for this suite.
Mar 26 20:17:28.491: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:17:28.561: INFO: namespace container-lifecycle-hook-1346 deletion completed in 22.078208537s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected configMap[0m 
  [1mshould be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Projected configMap
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:17:28.562: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating configMap with name projected-configmap-test-volume-map-d9ede008-503e-11e9-9719-a08cfdecc127
[1mSTEP[0m: Creating a pod to test consume configMaps
Mar 26 20:17:28.602: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d9ee2cbc-503e-11e9-9719-a08cfdecc127" in namespace "projected-8811" to be "success or failure"
Mar 26 20:17:28.608: INFO: Pod "pod-projected-configmaps-d9ee2cbc-503e-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 5.483749ms
Mar 26 20:17:30.611: INFO: Pod "pod-projected-configmaps-d9ee2cbc-503e-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008453122s
[1mSTEP[0m: Saw pod success
Mar 26 20:17:30.611: INFO: Pod "pod-projected-configmaps-d9ee2cbc-503e-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 20:17:30.613: INFO: Trying to get logs from node conformance-worker2 pod pod-projected-configmaps-d9ee2cbc-503e-11e9-9719-a08cfdecc127 container projected-configmap-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar 26 20:17:30.627: INFO: Waiting for pod pod-projected-configmaps-d9ee2cbc-503e-11e9-9719-a08cfdecc127 to disappear
Mar 26 20:17:30.632: INFO: Pod pod-projected-configmaps-d9ee2cbc-503e-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:17:30.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-8811" for this suite.
Mar 26 20:17:36.642: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:17:36.703: INFO: namespace projected-8811 deletion completed in 6.06919597s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Docker Containers[0m 
  [1mshould be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [k8s.io] Docker Containers
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:17:36.703: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename containers
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating a pod to test override arguments
Mar 26 20:17:36.746: INFO: Waiting up to 5m0s for pod "client-containers-dec89cbe-503e-11e9-9719-a08cfdecc127" in namespace "containers-6364" to be "success or failure"
Mar 26 20:17:36.751: INFO: Pod "client-containers-dec89cbe-503e-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 4.968605ms
Mar 26 20:17:38.755: INFO: Pod "client-containers-dec89cbe-503e-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008820535s
Mar 26 20:17:40.759: INFO: Pod "client-containers-dec89cbe-503e-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012839147s
[1mSTEP[0m: Saw pod success
Mar 26 20:17:40.759: INFO: Pod "client-containers-dec89cbe-503e-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 20:17:40.763: INFO: Trying to get logs from node conformance-worker pod client-containers-dec89cbe-503e-11e9-9719-a08cfdecc127 container test-container: <nil>
[1mSTEP[0m: delete the pod
Mar 26 20:17:40.781: INFO: Waiting for pod client-containers-dec89cbe-503e-11e9-9719-a08cfdecc127 to disappear
Mar 26 20:17:40.783: INFO: Pod client-containers-dec89cbe-503e-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:17:40.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "containers-6364" for this suite.
Mar 26 20:17:46.796: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:17:46.860: INFO: namespace containers-6364 deletion completed in 6.070974069s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Downward API volume[0m 
  [1mshould provide container's cpu limit [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Downward API volume
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:17:46.860: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename downward-api
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar 26 20:17:46.900: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e4d5cad3-503e-11e9-9719-a08cfdecc127" in namespace "downward-api-5160" to be "success or failure"
Mar 26 20:17:46.901: INFO: Pod "downwardapi-volume-e4d5cad3-503e-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 1.837414ms
Mar 26 20:17:48.905: INFO: Pod "downwardapi-volume-e4d5cad3-503e-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005193077s
[1mSTEP[0m: Saw pod success
Mar 26 20:17:48.905: INFO: Pod "downwardapi-volume-e4d5cad3-503e-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 20:17:48.908: INFO: Trying to get logs from node conformance-worker2 pod downwardapi-volume-e4d5cad3-503e-11e9-9719-a08cfdecc127 container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar 26 20:17:48.928: INFO: Waiting for pod downwardapi-volume-e4d5cad3-503e-11e9-9719-a08cfdecc127 to disappear
Mar 26 20:17:48.930: INFO: Pod downwardapi-volume-e4d5cad3-503e-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:17:48.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "downward-api-5160" for this suite.
Mar 26 20:17:54.940: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:17:55.004: INFO: namespace downward-api-5160 deletion completed in 6.072511284s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] StatefulSet[0m [90m[k8s.io] Basic StatefulSet functionality [StatefulSetBasic][0m 
  [1mShould recreate evicted statefulset [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-apps] StatefulSet
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:17:55.005: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename statefulset
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
[1mSTEP[0m: Creating service test in namespace statefulset-7383
[It] Should recreate evicted statefulset [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Looking for a node to schedule stateful set and pod
[1mSTEP[0m: Creating pod with conflicting port in namespace statefulset-7383
[1mSTEP[0m: Creating statefulset with conflicting port in namespace statefulset-7383
[1mSTEP[0m: Waiting until pod test-pod will start running in namespace statefulset-7383
[1mSTEP[0m: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-7383
Mar 26 20:17:59.073: INFO: Observed stateful pod in namespace: statefulset-7383, name: ss-0, uid: ebfb0e37-503e-11e9-b1ea-02429e4bb871, status phase: Pending. Waiting for statefulset controller to delete.
Mar 26 20:17:59.465: INFO: Observed stateful pod in namespace: statefulset-7383, name: ss-0, uid: ebfb0e37-503e-11e9-b1ea-02429e4bb871, status phase: Failed. Waiting for statefulset controller to delete.
Mar 26 20:17:59.468: INFO: Observed stateful pod in namespace: statefulset-7383, name: ss-0, uid: ebfb0e37-503e-11e9-b1ea-02429e4bb871, status phase: Failed. Waiting for statefulset controller to delete.
Mar 26 20:17:59.475: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-7383
[1mSTEP[0m: Removing pod with conflicting port in namespace statefulset-7383
[1mSTEP[0m: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-7383 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Mar 26 20:18:03.497: INFO: Deleting all statefulset in ns statefulset-7383
Mar 26 20:18:03.500: INFO: Scaling statefulset ss to 0
Mar 26 20:18:13.513: INFO: Waiting for statefulset status.replicas updated to 0
Mar 26 20:18:13.516: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:18:13.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "statefulset-7383" for this suite.
Mar 26 20:18:19.539: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:18:19.605: INFO: namespace statefulset-7383 deletion completed in 6.073538085s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Pods[0m 
  [1mshould get a host IP [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [k8s.io] Pods
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:18:19.605: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename pods
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should get a host IP [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: creating pod
Mar 26 20:18:21.655: INFO: Pod pod-hostip-f85af248-503e-11e9-9719-a08cfdecc127 has hostIP: 192.168.9.3
[AfterEach] [k8s.io] Pods
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:18:21.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "pods-8790" for this suite.
Mar 26 20:18:43.666: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:18:43.730: INFO: namespace pods-8790 deletion completed in 22.072529717s
[32mâ€¢[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-scheduling] SchedulerPredicates [Serial][0m 
  [1mvalidates resource limits of pods that are allowed to run  [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:18:43.730: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename sched-pred
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Mar 26 20:18:43.758: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 26 20:18:43.762: INFO: Waiting for terminating namespaces to be deleted...
Mar 26 20:18:43.764: INFO: 
Logging pods the kubelet thinks is on node conformance-worker before test
Mar 26 20:18:43.766: INFO: weave-net-n8g4b from kube-system started at 2019-03-26 18:51:05 -0700 PDT (2 container statuses recorded)
Mar 26 20:18:43.766: INFO: 	Container weave ready: true, restart count 1
Mar 26 20:18:43.766: INFO: 	Container weave-npc ready: true, restart count 0
Mar 26 20:18:43.766: INFO: kube-proxy-tvs75 from kube-system started at 2019-03-26 18:51:05 -0700 PDT (1 container statuses recorded)
Mar 26 20:18:43.766: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 26 20:18:43.766: INFO: 
Logging pods the kubelet thinks is on node conformance-worker2 before test
Mar 26 20:18:43.769: INFO: kube-proxy-4cdcp from kube-system started at 2019-03-26 18:51:05 -0700 PDT (1 container statuses recorded)
Mar 26 20:18:43.769: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 26 20:18:43.769: INFO: weave-net-pdrl8 from kube-system started at 2019-03-26 18:51:05 -0700 PDT (2 container statuses recorded)
Mar 26 20:18:43.769: INFO: 	Container weave ready: true, restart count 1
Mar 26 20:18:43.769: INFO: 	Container weave-npc ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: verifying the node has the label node conformance-worker
[1mSTEP[0m: verifying the node has the label node conformance-worker2
Mar 26 20:18:43.791: INFO: Pod kube-proxy-4cdcp requesting resource cpu=0m on Node conformance-worker2
Mar 26 20:18:43.792: INFO: Pod kube-proxy-tvs75 requesting resource cpu=0m on Node conformance-worker
Mar 26 20:18:43.792: INFO: Pod weave-net-n8g4b requesting resource cpu=20m on Node conformance-worker
Mar 26 20:18:43.792: INFO: Pod weave-net-pdrl8 requesting resource cpu=20m on Node conformance-worker2
[1mSTEP[0m: Starting Pods to consume most of the cluster CPU.
[1mSTEP[0m: Creating another pod that requires unavailable amount of CPU.
[1mSTEP[0m: Considering event: 
Type = [Normal], Name = [filler-pod-06c0065f-503f-11e9-9719-a08cfdecc127.158fb28f04e4492f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1563/filler-pod-06c0065f-503f-11e9-9719-a08cfdecc127 to conformance-worker]
[1mSTEP[0m: Considering event: 
Type = [Warning], Name = [filler-pod-06c0065f-503f-11e9-9719-a08cfdecc127.158fb28f17585ead], Reason = [DNSConfigForming], Message = [Search Line limits were exceeded, some search paths have been omitted, the applied search line is: sched-pred-1563.svc.cluster.local svc.cluster.local cluster.local corp.google.com prod.google.com prodz.google.com]
[1mSTEP[0m: Considering event: 
Type = [Normal], Name = [filler-pod-06c0065f-503f-11e9-9719-a08cfdecc127.158fb28f421a5528], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
[1mSTEP[0m: Considering event: 
Type = [Normal], Name = [filler-pod-06c0065f-503f-11e9-9719-a08cfdecc127.158fb28f4820dbfb], Reason = [Created], Message = [Created container filler-pod-06c0065f-503f-11e9-9719-a08cfdecc127]
[1mSTEP[0m: Considering event: 
Type = [Normal], Name = [filler-pod-06c0065f-503f-11e9-9719-a08cfdecc127.158fb28f57d6c6ec], Reason = [Started], Message = [Started container filler-pod-06c0065f-503f-11e9-9719-a08cfdecc127]
[1mSTEP[0m: Considering event: 
Type = [Normal], Name = [filler-pod-06c09feb-503f-11e9-9719-a08cfdecc127.158fb28f050fc68f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1563/filler-pod-06c09feb-503f-11e9-9719-a08cfdecc127 to conformance-worker2]
[1mSTEP[0m: Considering event: 
Type = [Warning], Name = [filler-pod-06c09feb-503f-11e9-9719-a08cfdecc127.158fb28f17a305b5], Reason = [DNSConfigForming], Message = [Search Line limits were exceeded, some search paths have been omitted, the applied search line is: sched-pred-1563.svc.cluster.local svc.cluster.local cluster.local corp.google.com prod.google.com prodz.google.com]
[1mSTEP[0m: Considering event: 
Type = [Normal], Name = [filler-pod-06c09feb-503f-11e9-9719-a08cfdecc127.158fb28f4221f88d], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
[1mSTEP[0m: Considering event: 
Type = [Normal], Name = [filler-pod-06c09feb-503f-11e9-9719-a08cfdecc127.158fb28f481f0780], Reason = [Created], Message = [Created container filler-pod-06c09feb-503f-11e9-9719-a08cfdecc127]
[1mSTEP[0m: Considering event: 
Type = [Normal], Name = [filler-pod-06c09feb-503f-11e9-9719-a08cfdecc127.158fb28f581c8293], Reason = [Started], Message = [Started container filler-pod-06c09feb-503f-11e9-9719-a08cfdecc127]
[1mSTEP[0m: Considering event: 
Type = [Warning], Name = [additional-pod.158fb28ff4c48535], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had taints that the pod didn't tolerate, 2 Insufficient cpu.]
[1mSTEP[0m: removing the label node off the node conformance-worker
[1mSTEP[0m: verifying the node doesn't have the label node
[1mSTEP[0m: removing the label node off the node conformance-worker2
[1mSTEP[0m: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:18:48.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "sched-pred-1563" for this suite.
Mar 26 20:18:54.868: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:18:54.929: INFO: namespace sched-pred-1563 deletion completed in 6.075213362s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-node] Downward API[0m 
  [1mshould provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-node] Downward API
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:18:54.929: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename downward-api
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating a pod to test downward api env vars
Mar 26 20:18:54.970: INFO: Waiting up to 5m0s for pod "downward-api-0d6875fe-503f-11e9-9719-a08cfdecc127" in namespace "downward-api-4664" to be "success or failure"
Mar 26 20:18:54.980: INFO: Pod "downward-api-0d6875fe-503f-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 10.805022ms
Mar 26 20:18:56.984: INFO: Pod "downward-api-0d6875fe-503f-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014723796s
[1mSTEP[0m: Saw pod success
Mar 26 20:18:56.984: INFO: Pod "downward-api-0d6875fe-503f-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 20:18:56.987: INFO: Trying to get logs from node conformance-worker2 pod downward-api-0d6875fe-503f-11e9-9719-a08cfdecc127 container dapi-container: <nil>
[1mSTEP[0m: delete the pod
Mar 26 20:18:57.006: INFO: Waiting for pod downward-api-0d6875fe-503f-11e9-9719-a08cfdecc127 to disappear
Mar 26 20:18:57.008: INFO: Pod downward-api-0d6875fe-503f-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-node] Downward API
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:18:57.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "downward-api-4664" for this suite.
Mar 26 20:19:03.018: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:19:03.090: INFO: namespace downward-api-4664 deletion completed in 6.07895735s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected secret[0m 
  [1mshould be consumable from pods in volume with mappings [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Projected secret
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:19:03.090: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating projection with secret that has name projected-secret-test-map-124501e2-503f-11e9-9719-a08cfdecc127
[1mSTEP[0m: Creating a pod to test consume secrets
Mar 26 20:19:03.124: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-12454f16-503f-11e9-9719-a08cfdecc127" in namespace "projected-5307" to be "success or failure"
Mar 26 20:19:03.125: INFO: Pod "pod-projected-secrets-12454f16-503f-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 1.571598ms
Mar 26 20:19:05.129: INFO: Pod "pod-projected-secrets-12454f16-503f-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005142461s
[1mSTEP[0m: Saw pod success
Mar 26 20:19:05.129: INFO: Pod "pod-projected-secrets-12454f16-503f-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 20:19:05.131: INFO: Trying to get logs from node conformance-worker pod pod-projected-secrets-12454f16-503f-11e9-9719-a08cfdecc127 container projected-secret-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar 26 20:19:05.149: INFO: Waiting for pod pod-projected-secrets-12454f16-503f-11e9-9719-a08cfdecc127 to disappear
Mar 26 20:19:05.152: INFO: Pod pod-projected-secrets-12454f16-503f-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] Projected secret
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:19:05.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-5307" for this suite.
Mar 26 20:19:11.164: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:19:11.221: INFO: namespace projected-5307 deletion completed in 6.066302158s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] ConfigMap[0m 
  [1mshould be consumable in multiple volumes in the same pod [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] ConfigMap
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:19:11.222: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename configmap
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating configMap with name configmap-test-volume-171d72cb-503f-11e9-9719-a08cfdecc127
[1mSTEP[0m: Creating a pod to test consume configMaps
Mar 26 20:19:11.257: INFO: Waiting up to 5m0s for pod "pod-configmaps-171dd2f7-503f-11e9-9719-a08cfdecc127" in namespace "configmap-2359" to be "success or failure"
Mar 26 20:19:11.262: INFO: Pod "pod-configmaps-171dd2f7-503f-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 4.987728ms
Mar 26 20:19:13.265: INFO: Pod "pod-configmaps-171dd2f7-503f-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008684877s
[1mSTEP[0m: Saw pod success
Mar 26 20:19:13.266: INFO: Pod "pod-configmaps-171dd2f7-503f-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 20:19:13.269: INFO: Trying to get logs from node conformance-worker2 pod pod-configmaps-171dd2f7-503f-11e9-9719-a08cfdecc127 container configmap-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar 26 20:19:13.286: INFO: Waiting for pod pod-configmaps-171dd2f7-503f-11e9-9719-a08cfdecc127 to disappear
Mar 26 20:19:13.289: INFO: Pod pod-configmaps-171dd2f7-503f-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:19:13.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "configmap-2359" for this suite.
Mar 26 20:19:19.301: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:19:19.368: INFO: namespace configmap-2359 deletion completed in 6.076632939s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90m[k8s.io] Kubectl run job[0m 
  [1mshould create a job from an image when restart is OnFailure  [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:19:19.369: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run job
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1510
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: running the image docker.io/library/nginx:1.14-alpine
Mar 26 20:19:19.392: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-484'
Mar 26 20:19:19.801: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Mar 26 20:19:19.801: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
[1mSTEP[0m: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1515
Mar 26 20:19:19.808: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance delete jobs e2e-test-nginx-job --namespace=kubectl-484'
Mar 26 20:19:19.882: INFO: stderr: ""
Mar 26 20:19:19.882: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:19:19.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-484" for this suite.
Mar 26 20:19:25.897: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:19:25.962: INFO: namespace kubectl-484 deletion completed in 6.077024034s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Docker Containers[0m 
  [1mshould be able to override the image's default command and arguments [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [k8s.io] Docker Containers
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:19:25.962: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename containers
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating a pod to test override all
Mar 26 20:19:25.995: INFO: Waiting up to 5m0s for pod "client-containers-1fe72931-503f-11e9-9719-a08cfdecc127" in namespace "containers-4237" to be "success or failure"
Mar 26 20:19:25.997: INFO: Pod "client-containers-1fe72931-503f-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.588637ms
Mar 26 20:19:28.001: INFO: Pod "client-containers-1fe72931-503f-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006234337s
[1mSTEP[0m: Saw pod success
Mar 26 20:19:28.001: INFO: Pod "client-containers-1fe72931-503f-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 20:19:28.004: INFO: Trying to get logs from node conformance-worker2 pod client-containers-1fe72931-503f-11e9-9719-a08cfdecc127 container test-container: <nil>
[1mSTEP[0m: delete the pod
Mar 26 20:19:28.021: INFO: Waiting for pod client-containers-1fe72931-503f-11e9-9719-a08cfdecc127 to disappear
Mar 26 20:19:28.022: INFO: Pod client-containers-1fe72931-503f-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:19:28.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "containers-4237" for this suite.
Mar 26 20:19:34.032: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:19:34.096: INFO: namespace containers-4237 deletion completed in 6.07180822s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] EmptyDir wrapper volumes[0m 
  [1mshould not conflict [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:19:34.097: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename emptydir-wrapper
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Cleaning up the secret
[1mSTEP[0m: Cleaning up the configmap
[1mSTEP[0m: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:19:38.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "emptydir-wrapper-5949" for this suite.
Mar 26 20:19:44.173: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:19:44.237: INFO: namespace emptydir-wrapper-5949 deletion completed in 6.071539937s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Probing container[0m 
  [1mwith readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [k8s.io] Probing container
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:19:44.237: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename container-probe
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Mar 26 20:20:10.294: INFO: Container started at 2019-03-26 20:19:45 -0700 PDT, pod became ready at 2019-03-26 20:20:09 -0700 PDT
[AfterEach] [k8s.io] Probing container
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:20:10.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "container-probe-7554" for this suite.
Mar 26 20:20:32.307: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:20:32.377: INFO: namespace container-probe-7554 deletion completed in 22.078841872s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] EmptyDir volumes[0m 
  [1mshould support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] EmptyDir volumes
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:20:32.377: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename emptydir
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating a pod to test emptydir 0777 on tmpfs
Mar 26 20:20:32.403: INFO: Waiting up to 5m0s for pod "pod-477c4bb7-503f-11e9-9719-a08cfdecc127" in namespace "emptydir-6322" to be "success or failure"
Mar 26 20:20:32.407: INFO: Pod "pod-477c4bb7-503f-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 3.453629ms
Mar 26 20:20:34.411: INFO: Pod "pod-477c4bb7-503f-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007404315s
[1mSTEP[0m: Saw pod success
Mar 26 20:20:34.411: INFO: Pod "pod-477c4bb7-503f-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 20:20:34.413: INFO: Trying to get logs from node conformance-worker pod pod-477c4bb7-503f-11e9-9719-a08cfdecc127 container test-container: <nil>
[1mSTEP[0m: delete the pod
Mar 26 20:20:34.432: INFO: Waiting for pod pod-477c4bb7-503f-11e9-9719-a08cfdecc127 to disappear
Mar 26 20:20:34.434: INFO: Pod pod-477c4bb7-503f-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:20:34.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "emptydir-6322" for this suite.
Mar 26 20:20:40.445: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:20:40.505: INFO: namespace emptydir-6322 deletion completed in 6.06817231s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Pods[0m 
  [1mshould support retrieving logs from the container over websockets [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [k8s.io] Pods
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:20:40.505: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename pods
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Mar 26 20:20:40.537: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: creating the pod
[1mSTEP[0m: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:20:42.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "pods-3318" for this suite.
Mar 26 20:21:26.586: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:21:26.651: INFO: namespace pods-3318 deletion completed in 44.073569508s
[32mâ€¢[0m
[90m------------------------------[0m
[0m[sig-storage] Secrets[0m 
  [1mshould be consumable from pods in volume with mappings [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Secrets
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:21:26.651: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename secrets
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating secret with name secret-test-map-67d5b2dc-503f-11e9-9719-a08cfdecc127
[1mSTEP[0m: Creating a pod to test consume secrets
Mar 26 20:21:26.680: INFO: Waiting up to 5m0s for pod "pod-secrets-67d61caf-503f-11e9-9719-a08cfdecc127" in namespace "secrets-606" to be "success or failure"
Mar 26 20:21:26.683: INFO: Pod "pod-secrets-67d61caf-503f-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.712659ms
Mar 26 20:21:28.686: INFO: Pod "pod-secrets-67d61caf-503f-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005732317s
[1mSTEP[0m: Saw pod success
Mar 26 20:21:28.686: INFO: Pod "pod-secrets-67d61caf-503f-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 20:21:28.689: INFO: Trying to get logs from node conformance-worker pod pod-secrets-67d61caf-503f-11e9-9719-a08cfdecc127 container secret-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar 26 20:21:28.705: INFO: Waiting for pod pod-secrets-67d61caf-503f-11e9-9719-a08cfdecc127 to disappear
Mar 26 20:21:28.707: INFO: Pod pod-secrets-67d61caf-503f-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] Secrets
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:21:28.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "secrets-606" for this suite.
Mar 26 20:21:34.716: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:21:34.783: INFO: namespace secrets-606 deletion completed in 6.074096903s
[32mâ€¢[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-network] Proxy[0m [90mversion v1[0m 
  [1mshould proxy logs on node with explicit kubelet port using proxy subresource  [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] version v1
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:21:34.783: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename proxy
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Mar 26 20:21:34.815: INFO: (0) /api/v1/nodes/conformance-worker:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 3.328102ms)
Mar 26 20:21:34.817: INFO: (1) /api/v1/nodes/conformance-worker:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 1.937672ms)
Mar 26 20:21:34.819: INFO: (2) /api/v1/nodes/conformance-worker:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 1.700316ms)
Mar 26 20:21:34.820: INFO: (3) /api/v1/nodes/conformance-worker:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 1.830477ms)
Mar 26 20:21:34.823: INFO: (4) /api/v1/nodes/conformance-worker:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.238769ms)
Mar 26 20:21:34.825: INFO: (5) /api/v1/nodes/conformance-worker:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 1.950345ms)
Mar 26 20:21:34.826: INFO: (6) /api/v1/nodes/conformance-worker:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 1.729986ms)
Mar 26 20:21:34.828: INFO: (7) /api/v1/nodes/conformance-worker:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 1.701814ms)
Mar 26 20:21:34.830: INFO: (8) /api/v1/nodes/conformance-worker:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 1.875729ms)
Mar 26 20:21:34.832: INFO: (9) /api/v1/nodes/conformance-worker:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 1.993443ms)
Mar 26 20:21:34.835: INFO: (10) /api/v1/nodes/conformance-worker:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.505032ms)
Mar 26 20:21:34.837: INFO: (11) /api/v1/nodes/conformance-worker:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.347666ms)
Mar 26 20:21:34.839: INFO: (12) /api/v1/nodes/conformance-worker:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.254624ms)
Mar 26 20:21:34.842: INFO: (13) /api/v1/nodes/conformance-worker:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.553496ms)
Mar 26 20:21:34.847: INFO: (14) /api/v1/nodes/conformance-worker:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 4.931112ms)
Mar 26 20:21:34.850: INFO: (15) /api/v1/nodes/conformance-worker:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 3.475305ms)
Mar 26 20:21:34.854: INFO: (16) /api/v1/nodes/conformance-worker:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 3.183191ms)
Mar 26 20:21:34.856: INFO: (17) /api/v1/nodes/conformance-worker:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.851761ms)
Mar 26 20:21:34.859: INFO: (18) /api/v1/nodes/conformance-worker:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.688592ms)
Mar 26 20:21:34.862: INFO: (19) /api/v1/nodes/conformance-worker:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 2.455095ms)
[AfterEach] version v1
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:21:34.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "proxy-8044" for this suite.
Mar 26 20:21:40.871: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:21:40.936: INFO: namespace proxy-8044 deletion completed in 6.072446089s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-apps] ReplicationController[0m 
  [1mshould serve a basic image on each replica with a public image  [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-apps] ReplicationController
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:21:40.937: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename replication-controller
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating replication controller my-hostname-basic-7059b78b-503f-11e9-9719-a08cfdecc127
Mar 26 20:21:40.966: INFO: Pod name my-hostname-basic-7059b78b-503f-11e9-9719-a08cfdecc127: Found 0 pods out of 1
Mar 26 20:21:45.971: INFO: Pod name my-hostname-basic-7059b78b-503f-11e9-9719-a08cfdecc127: Found 1 pods out of 1
Mar 26 20:21:45.971: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-7059b78b-503f-11e9-9719-a08cfdecc127" are running
Mar 26 20:21:45.974: INFO: Pod "my-hostname-basic-7059b78b-503f-11e9-9719-a08cfdecc127-89dxf" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-26 20:21:40 -0700 PDT Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-26 20:21:42 -0700 PDT Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-26 20:21:42 -0700 PDT Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-03-26 20:21:40 -0700 PDT Reason: Message:}])
Mar 26 20:21:45.974: INFO: Trying to dial the pod
Mar 26 20:21:50.986: INFO: Controller my-hostname-basic-7059b78b-503f-11e9-9719-a08cfdecc127: Got expected result from replica 1 [my-hostname-basic-7059b78b-503f-11e9-9719-a08cfdecc127-89dxf]: "my-hostname-basic-7059b78b-503f-11e9-9719-a08cfdecc127-89dxf", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:21:50.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "replication-controller-9953" for this suite.
Mar 26 20:21:56.999: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:21:57.068: INFO: namespace replication-controller-9953 deletion completed in 6.077784315s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected configMap[0m 
  [1mshould be consumable from pods in volume with mappings [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Projected configMap
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:21:57.068: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating configMap with name projected-configmap-test-volume-map-79f84b43-503f-11e9-9719-a08cfdecc127
[1mSTEP[0m: Creating a pod to test consume configMaps
Mar 26 20:21:57.136: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-79fcc11b-503f-11e9-9719-a08cfdecc127" in namespace "projected-1750" to be "success or failure"
Mar 26 20:21:57.138: INFO: Pod "pod-projected-configmaps-79fcc11b-503f-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 1.625643ms
Mar 26 20:21:59.141: INFO: Pod "pod-projected-configmaps-79fcc11b-503f-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004840703s
[1mSTEP[0m: Saw pod success
Mar 26 20:21:59.141: INFO: Pod "pod-projected-configmaps-79fcc11b-503f-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 20:21:59.144: INFO: Trying to get logs from node conformance-worker pod pod-projected-configmaps-79fcc11b-503f-11e9-9719-a08cfdecc127 container projected-configmap-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar 26 20:21:59.162: INFO: Waiting for pod pod-projected-configmaps-79fcc11b-503f-11e9-9719-a08cfdecc127 to disappear
Mar 26 20:21:59.164: INFO: Pod pod-projected-configmaps-79fcc11b-503f-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:21:59.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-1750" for this suite.
Mar 26 20:22:05.173: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:22:05.238: INFO: namespace projected-1750 deletion completed in 6.07192996s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Projected secret[0m 
  [1mshould be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Projected secret
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:22:05.238: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating projection with secret that has name projected-secret-test-7ed93553-503f-11e9-9719-a08cfdecc127
[1mSTEP[0m: Creating a pod to test consume secrets
Mar 26 20:22:05.289: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7ed98414-503f-11e9-9719-a08cfdecc127" in namespace "projected-7921" to be "success or failure"
Mar 26 20:22:05.290: INFO: Pod "pod-projected-secrets-7ed98414-503f-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 1.790793ms
Mar 26 20:22:07.294: INFO: Pod "pod-projected-secrets-7ed98414-503f-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005704515s
[1mSTEP[0m: Saw pod success
Mar 26 20:22:07.294: INFO: Pod "pod-projected-secrets-7ed98414-503f-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 20:22:07.297: INFO: Trying to get logs from node conformance-worker2 pod pod-projected-secrets-7ed98414-503f-11e9-9719-a08cfdecc127 container projected-secret-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar 26 20:22:07.315: INFO: Waiting for pod pod-projected-secrets-7ed98414-503f-11e9-9719-a08cfdecc127 to disappear
Mar 26 20:22:07.317: INFO: Pod pod-projected-secrets-7ed98414-503f-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] Projected secret
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:22:07.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-7921" for this suite.
Mar 26 20:22:13.328: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:22:13.397: INFO: namespace projected-7921 deletion completed in 6.076879894s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-cli] Kubectl client[0m [90m[k8s.io] Kubectl version[0m 
  [1mshould check is all data is printed  [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:22:13.397: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename kubectl
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check is all data is printed  [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Mar 26 20:22:13.423: INFO: Running '/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://localhost:38707 --kubeconfig=/usr/local/google/home/bentheelder/.kube/kind-config-conformance version'
Mar 26 20:22:13.502: INFO: stderr: ""
Mar 26 20:22:13.502: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.0\", GitCommit:\"641856db18352033a0d96dbc99153fa3b27298e5\", GitTreeState:\"clean\", BuildDate:\"2019-03-26T00:21:23Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.0\", GitCommit:\"641856db18352033a0d96dbc99153fa3b27298e5\", GitTreeState:\"clean\", BuildDate:\"2019-03-25T23:47:43Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:22:13.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "kubectl-7552" for this suite.
Mar 26 20:22:19.515: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:22:19.576: INFO: namespace kubectl-7552 deletion completed in 6.06982714s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-node] ConfigMap[0m 
  [1mshould be consumable via the environment [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-node] ConfigMap
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:22:19.576: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename configmap
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating configMap configmap-759/configmap-test-87631197-503f-11e9-9719-a08cfdecc127
[1mSTEP[0m: Creating a pod to test consume configMaps
Mar 26 20:22:19.613: INFO: Waiting up to 5m0s for pod "pod-configmaps-876358cd-503f-11e9-9719-a08cfdecc127" in namespace "configmap-759" to be "success or failure"
Mar 26 20:22:19.615: INFO: Pod "pod-configmaps-876358cd-503f-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 1.951168ms
Mar 26 20:22:21.618: INFO: Pod "pod-configmaps-876358cd-503f-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005074404s
[1mSTEP[0m: Saw pod success
Mar 26 20:22:21.618: INFO: Pod "pod-configmaps-876358cd-503f-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 20:22:21.621: INFO: Trying to get logs from node conformance-worker pod pod-configmaps-876358cd-503f-11e9-9719-a08cfdecc127 container env-test: <nil>
[1mSTEP[0m: delete the pod
Mar 26 20:22:21.634: INFO: Waiting for pod pod-configmaps-876358cd-503f-11e9-9719-a08cfdecc127 to disappear
Mar 26 20:22:21.638: INFO: Pod pod-configmaps-876358cd-503f-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-node] ConfigMap
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:22:21.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "configmap-759" for this suite.
Mar 26 20:22:27.650: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:22:27.716: INFO: namespace configmap-759 deletion completed in 6.076771242s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] Probing container[0m 
  [1mwith readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [k8s.io] Probing container
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:22:27.717: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename container-probe
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Probing container
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:23:27.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "container-probe-9346" for this suite.
Mar 26 20:23:49.777: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:23:49.843: INFO: namespace container-probe-9346 deletion completed in 22.088381608s
[32mâ€¢[0m
[90m------------------------------[0m
[0m[sig-apps] Deployment[0m 
  [1mRollingUpdateDeployment should delete old pods and create new ones [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-apps] Deployment
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:23:49.843: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename deployment
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Mar 26 20:23:49.873: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Mar 26 20:23:49.882: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar 26 20:23:54.886: INFO: Pod name sample-pod: Found 1 pods out of 1
[1mSTEP[0m: ensuring each pod is running
Mar 26 20:23:54.886: INFO: Creating deployment "test-rolling-update-deployment"
Mar 26 20:23:54.890: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Mar 26 20:23:54.895: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Mar 26 20:23:56.902: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Mar 26 20:23:56.904: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Mar 26 20:23:56.911: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-2529,SelfLink:/apis/apps/v1/namespaces/deployment-2529/deployments/test-rolling-update-deployment,UID:c02d65d0-503f-11e9-b1ea-02429e4bb871,ResourceVersion:20906,Generation:1,CreationTimestamp:2019-03-26 20:23:54 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-03-26 20:23:54 -0700 PDT 2019-03-26 20:23:54 -0700 PDT MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-03-26 20:23:56 -0700 PDT 2019-03-26 20:23:54 -0700 PDT NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-67599b4d9" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Mar 26 20:23:56.913: INFO: New ReplicaSet "test-rolling-update-deployment-67599b4d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9,GenerateName:,Namespace:deployment-2529,SelfLink:/apis/apps/v1/namespaces/deployment-2529/replicasets/test-rolling-update-deployment-67599b4d9,UID:c02eebf0-503f-11e9-b1ea-02429e4bb871,ResourceVersion:20895,Generation:1,CreationTimestamp:2019-03-26 20:23:54 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment c02d65d0-503f-11e9-b1ea-02429e4bb871 0xc0025ad430 0xc0025ad431}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Mar 26 20:23:56.913: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Mar 26 20:23:56.913: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-2529,SelfLink:/apis/apps/v1/namespaces/deployment-2529/replicasets/test-rolling-update-controller,UID:bd307c1c-503f-11e9-b1ea-02429e4bb871,ResourceVersion:20904,Generation:2,CreationTimestamp:2019-03-26 20:23:49 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment c02d65d0-503f-11e9-b1ea-02429e4bb871 0xc0025ad367 0xc0025ad368}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Mar 26 20:23:56.915: INFO: Pod "test-rolling-update-deployment-67599b4d9-h2qzk" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9-h2qzk,GenerateName:test-rolling-update-deployment-67599b4d9-,Namespace:deployment-2529,SelfLink:/api/v1/namespaces/deployment-2529/pods/test-rolling-update-deployment-67599b4d9-h2qzk,UID:c02f7502-503f-11e9-b1ea-02429e4bb871,ResourceVersion:20894,Generation:0,CreationTimestamp:2019-03-26 20:23:54 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-67599b4d9 c02eebf0-503f-11e9-b1ea-02429e4bb871 0xc0025adca0 0xc0025adca1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-phm4z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-phm4z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-phm4z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025add00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025add20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 20:23:54 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 20:23:56 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 20:23:56 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 20:23:54 -0700 PDT  }],Message:,Reason:,HostIP:192.168.9.2,PodIP:10.46.0.1,StartTime:2019-03-26 20:23:54 -0700 PDT,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-03-26 20:23:56 -0700 PDT,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://7e06fd2e64dbdfcdeb5e8555f68ddaf52ad6e0ac92b75b645fae130d84da9dab}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:23:56.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "deployment-2529" for this suite.
Mar 26 20:24:02.925: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:24:02.990: INFO: namespace deployment-2529 deletion completed in 6.072939894s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Secrets[0m 
  [1mshould be consumable from pods in volume [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Secrets
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:24:02.990: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename secrets
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating secret with name secret-test-c5053007-503f-11e9-9719-a08cfdecc127
[1mSTEP[0m: Creating a pod to test consume secrets
Mar 26 20:24:03.022: INFO: Waiting up to 5m0s for pod "pod-secrets-c5057096-503f-11e9-9719-a08cfdecc127" in namespace "secrets-6722" to be "success or failure"
Mar 26 20:24:03.023: INFO: Pod "pod-secrets-c5057096-503f-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 1.626518ms
Mar 26 20:24:05.027: INFO: Pod "pod-secrets-c5057096-503f-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005330148s
Mar 26 20:24:07.031: INFO: Pod "pod-secrets-c5057096-503f-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009681308s
[1mSTEP[0m: Saw pod success
Mar 26 20:24:07.032: INFO: Pod "pod-secrets-c5057096-503f-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 20:24:07.035: INFO: Trying to get logs from node conformance-worker pod pod-secrets-c5057096-503f-11e9-9719-a08cfdecc127 container secret-volume-test: <nil>
[1mSTEP[0m: delete the pod
Mar 26 20:24:07.055: INFO: Waiting for pod pod-secrets-c5057096-503f-11e9-9719-a08cfdecc127 to disappear
Mar 26 20:24:07.057: INFO: Pod pod-secrets-c5057096-503f-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] Secrets
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:24:07.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "secrets-6722" for this suite.
Mar 26 20:24:13.067: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:24:13.123: INFO: namespace secrets-6722 deletion completed in 6.063707995s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Downward API volume[0m 
  [1mshould set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Downward API volume
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:24:13.123: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename downward-api
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar 26 20:24:13.150: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cb0f8ebc-503f-11e9-9719-a08cfdecc127" in namespace "downward-api-6613" to be "success or failure"
Mar 26 20:24:13.152: INFO: Pod "downwardapi-volume-cb0f8ebc-503f-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.135588ms
Mar 26 20:24:15.155: INFO: Pod "downwardapi-volume-cb0f8ebc-503f-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004987794s
[1mSTEP[0m: Saw pod success
Mar 26 20:24:15.155: INFO: Pod "downwardapi-volume-cb0f8ebc-503f-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 20:24:15.157: INFO: Trying to get logs from node conformance-worker2 pod downwardapi-volume-cb0f8ebc-503f-11e9-9719-a08cfdecc127 container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar 26 20:24:15.173: INFO: Waiting for pod downwardapi-volume-cb0f8ebc-503f-11e9-9719-a08cfdecc127 to disappear
Mar 26 20:24:15.175: INFO: Pod downwardapi-volume-cb0f8ebc-503f-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:24:15.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "downward-api-6613" for this suite.
Mar 26 20:24:21.185: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:24:21.247: INFO: namespace downward-api-6613 deletion completed in 6.069995514s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Secrets[0m 
  [1moptional updates should be reflected in volume [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Secrets
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:24:21.247: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename secrets
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating secret with name s-test-opt-del-cfe8ee73-503f-11e9-9719-a08cfdecc127
[1mSTEP[0m: Creating secret with name s-test-opt-upd-cfe8eea8-503f-11e9-9719-a08cfdecc127
[1mSTEP[0m: Creating the pod
[1mSTEP[0m: Deleting secret s-test-opt-del-cfe8ee73-503f-11e9-9719-a08cfdecc127
[1mSTEP[0m: Updating secret s-test-opt-upd-cfe8eea8-503f-11e9-9719-a08cfdecc127
[1mSTEP[0m: Creating secret with name s-test-opt-create-cfe8eeb5-503f-11e9-9719-a08cfdecc127
[1mSTEP[0m: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:25:37.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "secrets-5911" for this suite.
Mar 26 20:25:59.838: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:25:59.894: INFO: namespace secrets-5911 deletion completed in 22.072687857s
[32mâ€¢[0m
[90m------------------------------[0m
[0m[sig-storage] Projected downwardAPI[0m 
  [1mshould provide podname only [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Projected downwardAPI
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:25:59.894: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename projected
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar 26 20:25:59.922: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0ab3b0d0-5040-11e9-9719-a08cfdecc127" in namespace "projected-1536" to be "success or failure"
Mar 26 20:25:59.924: INFO: Pod "downwardapi-volume-0ab3b0d0-5040-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.001668ms
Mar 26 20:26:01.927: INFO: Pod "downwardapi-volume-0ab3b0d0-5040-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005131731s
[1mSTEP[0m: Saw pod success
Mar 26 20:26:01.927: INFO: Pod "downwardapi-volume-0ab3b0d0-5040-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 20:26:01.930: INFO: Trying to get logs from node conformance-worker2 pod downwardapi-volume-0ab3b0d0-5040-11e9-9719-a08cfdecc127 container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar 26 20:26:01.948: INFO: Waiting for pod downwardapi-volume-0ab3b0d0-5040-11e9-9719-a08cfdecc127 to disappear
Mar 26 20:26:01.951: INFO: Pod downwardapi-volume-0ab3b0d0-5040-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:26:01.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "projected-1536" for this suite.
Mar 26 20:26:07.961: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:26:08.024: INFO: namespace projected-1536 deletion completed in 6.07116011s
[32mâ€¢[0m
[90m------------------------------[0m
[0m[sig-storage] Subpath[0m [90mAtomic writer volumes[0m 
  [1mshould support subpaths with configmap pod [LinuxOnly] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Subpath
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:26:08.024: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename subpath
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
[1mSTEP[0m: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating pod pod-subpath-test-configmap-cjpl
[1mSTEP[0m: Creating a pod to test atomic-volume-subpath
Mar 26 20:26:08.059: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-cjpl" in namespace "subpath-7400" to be "success or failure"
Mar 26 20:26:08.062: INFO: Pod "pod-subpath-test-configmap-cjpl": Phase="Pending", Reason="", readiness=false. Elapsed: 1.989286ms
Mar 26 20:26:10.065: INFO: Pod "pod-subpath-test-configmap-cjpl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005375997s
Mar 26 20:26:12.069: INFO: Pod "pod-subpath-test-configmap-cjpl": Phase="Running", Reason="", readiness=true. Elapsed: 4.009316526s
Mar 26 20:26:14.073: INFO: Pod "pod-subpath-test-configmap-cjpl": Phase="Running", Reason="", readiness=true. Elapsed: 6.013430927s
Mar 26 20:26:16.077: INFO: Pod "pod-subpath-test-configmap-cjpl": Phase="Running", Reason="", readiness=true. Elapsed: 8.017390125s
Mar 26 20:26:18.081: INFO: Pod "pod-subpath-test-configmap-cjpl": Phase="Running", Reason="", readiness=true. Elapsed: 10.02147137s
Mar 26 20:26:20.085: INFO: Pod "pod-subpath-test-configmap-cjpl": Phase="Running", Reason="", readiness=true. Elapsed: 12.025544326s
Mar 26 20:26:22.089: INFO: Pod "pod-subpath-test-configmap-cjpl": Phase="Running", Reason="", readiness=true. Elapsed: 14.029547825s
Mar 26 20:26:24.093: INFO: Pod "pod-subpath-test-configmap-cjpl": Phase="Running", Reason="", readiness=true. Elapsed: 16.033726401s
Mar 26 20:26:26.097: INFO: Pod "pod-subpath-test-configmap-cjpl": Phase="Running", Reason="", readiness=true. Elapsed: 18.037689193s
Mar 26 20:26:28.101: INFO: Pod "pod-subpath-test-configmap-cjpl": Phase="Running", Reason="", readiness=true. Elapsed: 20.041639589s
Mar 26 20:26:30.105: INFO: Pod "pod-subpath-test-configmap-cjpl": Phase="Running", Reason="", readiness=true. Elapsed: 22.045571641s
Mar 26 20:26:32.109: INFO: Pod "pod-subpath-test-configmap-cjpl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.049859428s
[1mSTEP[0m: Saw pod success
Mar 26 20:26:32.109: INFO: Pod "pod-subpath-test-configmap-cjpl" satisfied condition "success or failure"
Mar 26 20:26:32.113: INFO: Trying to get logs from node conformance-worker pod pod-subpath-test-configmap-cjpl container test-container-subpath-configmap-cjpl: <nil>
[1mSTEP[0m: delete the pod
Mar 26 20:26:32.129: INFO: Waiting for pod pod-subpath-test-configmap-cjpl to disappear
Mar 26 20:26:32.131: INFO: Pod pod-subpath-test-configmap-cjpl no longer exists
[1mSTEP[0m: Deleting pod pod-subpath-test-configmap-cjpl
Mar 26 20:26:32.131: INFO: Deleting pod "pod-subpath-test-configmap-cjpl" in namespace "subpath-7400"
[AfterEach] [sig-storage] Subpath
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:26:32.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "subpath-7400" for this suite.
Mar 26 20:26:38.143: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:26:38.206: INFO: namespace subpath-7400 deletion completed in 6.071015223s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[k8s.io] [sig-node] Events[0m 
  [1mshould be sent by kubelets and the scheduler about pods scheduling and running  [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [k8s.io] [sig-node] Events
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:26:38.207: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename events
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: creating the pod
[1mSTEP[0m: submitting the pod to kubernetes
[1mSTEP[0m: verifying the pod is in kubernetes
[1mSTEP[0m: retrieving the pod
Mar 26 20:26:40.263: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-218b301f-5040-11e9-9719-a08cfdecc127,GenerateName:,Namespace:events-8137,SelfLink:/api/v1/namespaces/events-8137/pods/send-events-218b301f-5040-11e9-9719-a08cfdecc127,UID:218c2906-5040-11e9-b1ea-02429e4bb871,ResourceVersion:21358,Generation:0,CreationTimestamp:2019-03-26 20:26:38 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 240981084,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7krc4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7krc4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-7krc4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance-worker2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002ea2e20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002ea2e40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 20:26:38 -0700 PDT  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 20:26:40 -0700 PDT  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 20:26:40 -0700 PDT  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-03-26 20:26:38 -0700 PDT  }],Message:,Reason:,HostIP:192.168.9.2,PodIP:10.46.0.1,StartTime:2019-03-26 20:26:38 -0700 PDT,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-03-26 20:26:39 -0700 PDT,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://ff095a96e01aab119b9b530930b2c01b3eac1ea1a2b6ed2bff416651c7570d28}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

[1mSTEP[0m: checking for scheduler event about the pod
Mar 26 20:26:42.268: INFO: Saw scheduler event for our pod.
[1mSTEP[0m: checking for kubelet event about the pod
Mar 26 20:26:44.272: INFO: Saw kubelet event for our pod.
[1mSTEP[0m: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:26:44.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "events-8137" for this suite.
Mar 26 20:27:22.294: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:27:22.361: INFO: namespace events-8137 deletion completed in 38.079781016s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] Downward API volume[0m 
  [1mshould provide container's memory limit [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] Downward API volume
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:27:22.362: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename downward-api
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating a pod to test downward API volume plugin
Mar 26 20:27:22.398: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3bdc63b1-5040-11e9-9719-a08cfdecc127" in namespace "downward-api-8188" to be "success or failure"
Mar 26 20:27:22.399: INFO: Pod "downwardapi-volume-3bdc63b1-5040-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 1.613273ms
Mar 26 20:27:24.403: INFO: Pod "downwardapi-volume-3bdc63b1-5040-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005477454s
Mar 26 20:27:26.407: INFO: Pod "downwardapi-volume-3bdc63b1-5040-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009515966s
[1mSTEP[0m: Saw pod success
Mar 26 20:27:26.407: INFO: Pod "downwardapi-volume-3bdc63b1-5040-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 20:27:26.411: INFO: Trying to get logs from node conformance-worker pod downwardapi-volume-3bdc63b1-5040-11e9-9719-a08cfdecc127 container client-container: <nil>
[1mSTEP[0m: delete the pod
Mar 26 20:27:26.429: INFO: Waiting for pod downwardapi-volume-3bdc63b1-5040-11e9-9719-a08cfdecc127 to disappear
Mar 26 20:27:26.431: INFO: Pod downwardapi-volume-3bdc63b1-5040-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:27:26.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "downward-api-8188" for this suite.
Mar 26 20:27:32.442: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:27:32.508: INFO: namespace downward-api-8188 deletion completed in 6.07401061s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-api-machinery] Watchers[0m 
  [1mshould be able to start watching from a specific resource version [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-api-machinery] Watchers
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:27:32.508: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename watch
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: creating a new configmap
[1mSTEP[0m: modifying the configmap once
[1mSTEP[0m: modifying the configmap a second time
[1mSTEP[0m: deleting the configmap
[1mSTEP[0m: creating a watch on configmaps from the resource version returned by the first update
[1mSTEP[0m: Expecting to observe notifications for all changes to the configmap after the first update
Mar 26 20:27:32.558: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-3990,SelfLink:/api/v1/namespaces/watch-3990/configmaps/e2e-watch-test-resource-version,UID:41e79c8e-5040-11e9-b1ea-02429e4bb871,ResourceVersion:21486,Generation:0,CreationTimestamp:2019-03-26 20:27:32 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Mar 26 20:27:32.558: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-3990,SelfLink:/api/v1/namespaces/watch-3990/configmaps/e2e-watch-test-resource-version,UID:41e79c8e-5040-11e9-b1ea-02429e4bb871,ResourceVersion:21487,Generation:0,CreationTimestamp:2019-03-26 20:27:32 -0700 PDT,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:27:32.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "watch-3990" for this suite.
Mar 26 20:27:38.571: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:27:38.636: INFO: namespace watch-3990 deletion completed in 6.073979352s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m
[90m------------------------------[0m
[0m[sig-storage] EmptyDir volumes[0m 
  [1mshould support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance][0m
  [37m/usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692[0m
[BeforeEach] [sig-storage] EmptyDir volumes
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
[1mSTEP[0m: Creating a kubernetes client
Mar 26 20:27:38.636: INFO: >>> kubeConfig: /usr/local/google/home/bentheelder/.kube/kind-config-conformance
[1mSTEP[0m: Building a namespace api object, basename emptydir
[1mSTEP[0m: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[1mSTEP[0m: Creating a pod to test emptydir 0777 on node default medium
Mar 26 20:27:38.669: INFO: Waiting up to 5m0s for pod "pod-458f242e-5040-11e9-9719-a08cfdecc127" in namespace "emptydir-7013" to be "success or failure"
Mar 26 20:27:38.670: INFO: Pod "pod-458f242e-5040-11e9-9719-a08cfdecc127": Phase="Pending", Reason="", readiness=false. Elapsed: 1.721453ms
Mar 26 20:27:40.673: INFO: Pod "pod-458f242e-5040-11e9-9719-a08cfdecc127": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004522755s
[1mSTEP[0m: Saw pod success
Mar 26 20:27:40.673: INFO: Pod "pod-458f242e-5040-11e9-9719-a08cfdecc127" satisfied condition "success or failure"
Mar 26 20:27:40.675: INFO: Trying to get logs from node conformance-worker2 pod pod-458f242e-5040-11e9-9719-a08cfdecc127 container test-container: <nil>
[1mSTEP[0m: delete the pod
Mar 26 20:27:40.692: INFO: Waiting for pod pod-458f242e-5040-11e9-9719-a08cfdecc127 to disappear
Mar 26 20:27:40.694: INFO: Pod pod-458f242e-5040-11e9-9719-a08cfdecc127 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /usr/local/google/home/bentheelder/go/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Mar 26 20:27:40.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[1mSTEP[0m: Destroying namespace "emptydir-7013" for this suite.
Mar 26 20:27:46.706: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Mar 26 20:27:46.774: INFO: namespace emptydir-7013 deletion completed in 6.078080387s
[32mâ€¢[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0m[36mS[0mMar 26 20:27:46.774: INFO: Running AfterSuite actions on all nodes
Mar 26 20:27:46.774: INFO: Running AfterSuite actions on node 1
Mar 26 20:27:46.774: INFO: Skipping dumping logs from cluster

[1m[32mRan 204 of 3584 Specs in 5760.548 seconds[0m
[1m[32mSUCCESS![0m -- [32m[1m204 Passed[0m | [91m[1m0 Failed[0m | [33m[1m0 Pending[0m | [36m[1m3380 Skipped[0m PASS

Ginkgo ran 1 suite in 1h36m1.458409494s
Test Suite Passed
